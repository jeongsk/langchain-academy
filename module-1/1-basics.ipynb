{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "660ce795-9307-4c2c-98a1-beabcb36c740",
            "metadata": {},
            "source": [
                "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-0/basics.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/56295530-getting-set-up-video-guide)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
            "metadata": {},
            "source": [
                "# LangChain Academy\n",
                "\n",
                "LangChain Academy에 오신 것을 환영합니다! \n",
                "\n",
                "## 배경\n",
                "\n",
                "LangChain에서 저희는 LLM 애플리케이션을 쉽게 구축할 수 있도록 하는 것을 목표로 합니다. 구축할 수 있는 LLM 애플리케이션 중 하나는 에이전트입니다. 에이전트는 이전에는 불가능했던 광범위한 작업을 자동화할 수 있기 때문에 에이전트 구축에 대한 많은 관심이 있습니다.\n",
                "\n",
                "하지만 실제로는 이러한 작업을 안정적으로 실행하는 시스템을 구축하는 것은 매우 어렵습니다. 사용자들과 함께 에이전트를 프로덕션에 투입하면서, 더 많은 제어가 종종 필요하다는 것을 배웠습니다. 에이전트가 항상 특정 도구를 먼저 호출하거나 상태에 따라 다른 프롬프트를 사용해야 할 수도 있습니다.\n",
                "\n",
                "이 문제를 해결하기 위해 저희는 [LangGraph](https://langchain-ai.github.io/langgraph/)를 구축했습니다 — 에이전트 및 멀티 에이전트 애플리케이션을 구축하기 위한 프레임워크입니다. LangChain 패키지와는 별개로, LangGraph의 핵심 설계 철학은 개발자가 실제 시스템의 복잡성에 적합한 에이전트 워크플로우에 더 나은 정밀도와 제어를 추가하도록 돕는 것입니다.\n",
                "\n",
                "## 강좌 구조\n",
                "\n",
                "강좌는 일련의 모듈로 구성되어 있으며, 각 모듈은 LangGraph와 관련된 특정 주제에 중점을 둡니다. 각 모듈에 대한 폴더를 볼 수 있으며, 여기에는 일련의 노트북이 포함되어 있습니다. 개념을 안내하는 데 도움이 되는 비디오가 각 노트북과 함께 제공되지만, 노트북은 독립적이기도 합니다. 즉, 설명이 포함되어 있어 비디오와 독립적으로 볼 수 있습니다. 각 모듈 폴더에는 `studio` 폴더도 포함되어 있으며, 여기에는 LangGraph 애플리케이션 구축을 위한 IDE인 [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio)에 로드할 수 있는 그래프 세트가 포함되어 있습니다.\n",
                "\n",
                "## 설정\n",
                "\n",
                "시작하기 전에 `README`의 지침에 따라 환경을 생성하고 종속성을 설치해주세요.\n",
                "\n",
                "## Chat 모델\n",
                "\n",
                "이 강좌에서는 [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models)을 사용할 것입니다. 이는 메시지 시퀀스를 입력으로 받아 채팅 메시지를 출력으로 반환하는 몇 가지 작업을 수행합니다. LangChain은 Chat Models을 호스팅하지 않으며, 대신 타사 통합에 의존합니다. [여기](https://python.langchain.com/v0.2/docs/integrations/chat/)에 LangChain 내의 타사 채팅 모델 통합 목록이 있습니다! 기본적으로 강좌는 인기가 많고 성능이 좋기 때문에 [ChatOpenAI](https://python.langchain.com/v0.2/docs/integrations/chat/openai/)를 사용합니다. 언급한 바와 같이, `OPENAI_API_KEY`가 있는지 확인해주세요.\n",
                "\n",
                "`OPENAI_API_KEY`가 설정되어 있는지 확인해보고, 설정되어 있지 않다면 입력하라는 메시지가 표시됩니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0f9a52c8",
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture --no-stderr\n",
                "%pip install --quiet -U langchain_openai langchain_core langchain_community tavily-python"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c2a15227",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, getpass\n",
                "\n",
                "def _set_env(var: str):\n",
                "    if not os.environ.get(var):\n",
                "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
                "\n",
                "_set_env(\"OPENAI_API_KEY\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "a326f35b",
            "metadata": {},
            "source": [
                "[여기](https://python.langchain.com/v0.2/docs/how_to/#chat-models)에 채팅 모델로 할 수 있는 모든 것에 대한 유용한 가이드가 있지만, 아래에서 몇 가지 하이라이트를 보여드리겠습니다. README에서 언급한 대로 `pip install -r requirements.txt`를 실행했다면, `langchain-openai` 패키지를 설치한 것입니다. 이를 통해 `ChatOpenAI` 모델 객체를 인스턴스화할 수 있습니다. API에 처음 가입하는 경우, 모든 모델에 적용할 수 있는 [무료 크레딧](https://community.openai.com/t/understanding-api-limits-and-free-tier/498517)을 받게 됩니다. [여기](https://openai.com/api/pricing/)에서 다양한 모델의 가격을 확인할 수 있습니다. 노트북은 품질, 가격, 속도의 좋은 균형이기 때문에 기본적으로 `gpt-4o`를 사용합니다([자세히 보기](https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4-gpt-4-turbo-gpt-4o-and-gpt-4o-mini)), 하지만 더 저렴한 `gpt-3.5` 시리즈 모델을 선택할 수도 있습니다.\n",
                "\n",
                "채팅 모델에서 설정할 수 있는 [몇 가지 표준 매개변수](https://python.langchain.com/v0.2/docs/concepts/#chat-models)가 있습니다. 가장 일반적인 두 가지는 다음과 같습니다:\n",
                "\n",
                "* `model`: 모델의 이름\n",
                "* `temperature`: 샘플링 온도\n",
                "\n",
                "`Temperature`는 모델 출력의 임의성 또는 창의성을 제어합니다. 낮은 온도(0에 가까운)는 더 결정적이고 집중된 출력을 생성합니다. 이는 정확성이나 사실적 응답이 필요한 작업에 좋습니다. 높은 온도(1에 가까운)는 창의적인 작업이나 다양한 응답 생성에 좋습니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "e19a54d3",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_openai import ChatOpenAI\n",
                "gpt4o_chat = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
                "gpt35_chat = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temperature=0)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "28450d1b",
            "metadata": {},
            "source": [
                "LangChain의 채팅 모델에는 여러 [기본 메서드](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface)가 있습니다. 대부분의 경우 다음을 사용할 것입니다:\n",
                "\n",
                "* `stream`: 응답의 청크를 스트림으로 반환\n",
                "* `invoke`: 입력에 대해 체인을 호출\n",
                "\n",
                "그리고 앞서 언급했듯이, 채팅 모델은 [메시지](https://python.langchain.com/v0.2/docs/concepts/#messages)를 입력으로 받습니다. 메시지는 역할(누가 메시지를 말하는지 설명)과 콘텐츠 속성을 가집니다. 이에 대해서는 나중에 더 자세히 다루겠지만, 여기서는 기본 사항만 보여드리겠습니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b1280e1b",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_core.messages import HumanMessage\n",
                "\n",
                "# Create a message\n",
                "msg = HumanMessage(content=\"Hello world\", name=\"Lance\")\n",
                "\n",
                "# Message list\n",
                "messages = [msg]\n",
                "\n",
                "# Invoke the model with a list of messages \n",
                "gpt4o_chat.invoke(messages)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "cac73e4c",
            "metadata": {},
            "source": [
                "`AIMessage` 응답을 받습니다. 또한 문자열로 채팅 모델을 호출할 수도 있습니다. 문자열이 입력으로 전달되면, `HumanMessage`로 변환된 후 기본 모델에 전달됩니다.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f27c6c9a",
            "metadata": {},
            "outputs": [],
            "source": [
                "gpt4o_chat.invoke(\"hello world\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "fdc2f0ca",
            "metadata": {},
            "outputs": [],
            "source": [
                "gpt35_chat.invoke(\"hello world\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "582c0e5a",
            "metadata": {},
            "source": [
                "인터페이스는 모든 채팅 모델에서 일관되며, 모델은 일반적으로 각 노트북 시작 시 한 번 초기화됩니다.\n",
                "\n",
                "따라서 다른 공급자에 대한 강한 선호가 있다면 다운스트림 코드를 변경하지 않고도 모델 간에 쉽게 전환할 수 있습니다.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "3ad0069a",
            "metadata": {},
            "source": [
                "## 검색 도구\n",
                "\n",
                "README에서 [Tavily](https://tavily.com/)도 볼 수 있습니다. 이는 LLM과 RAG에 최적화된 검색 엔진으로, 효율적이고 빠르며 지속적인 검색 결과를 목표로 합니다. 언급했듯이 가입이 쉽고 관대한 무료 계층을 제공합니다. 일부 강의(모듈 4)에서는 기본적으로 Tavily를 사용하지만, 물론 코드를 직접 수정하려면 다른 검색 도구를 사용할 수도 있습니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "091dff13",
            "metadata": {},
            "outputs": [],
            "source": [
                "_set_env(\"TAVILY_API_KEY\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "52d69da9",
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain_community.tools.tavily_search import TavilySearchResults\n",
                "tavily_search = TavilySearchResults(max_results=3)\n",
                "search_docs = tavily_search.invoke(\"What is LangGraph?\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "d06f87e6",
            "metadata": {},
            "outputs": [],
            "source": [
                "search_docs"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bafd7d5d",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}