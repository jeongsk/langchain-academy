{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d140c86",
   "metadata": {},
   "source": [
    "## STORM: 연구를 위한 멀티 에이전트\n",
    "\n",
    "### 개요\n",
    "\n",
    "STORM(Synthesis of Topic Outline through Retrieval and Multi-perspective Question Asking)은 Stanford 대학에서 개발한 LLM 기반의 지식 큐레이션 시스템입니다. 이 시스템은 인터넷 리서치를 통해 Wikipedia 수준의 포괄적이고 체계적인 장문의 기사를 자동으로 생성하는 것을 목표로 합니다.\n",
    "\n",
    "![](https://github.com/stanford-oval/storm/raw/main/assets/two_stages.jpg)\n",
    "\n",
    "### 핵심 아키텍처\n",
    "\n",
    "STORM은 두 단계의 파이프라인으로 구성됩니다:\n",
    "\n",
    "1. **사전 작성 단계(Pre-writing Stage)**\n",
    "   - 인터넷 기반 리서치를 수행하여 참고 자료 수집\n",
    "   - 다양한 관점(perspective) 발견\n",
    "   - 주제에 대한 개요(outline) 생성\n",
    "\n",
    "2. **작성 단계(Writing Stage)**\n",
    "   - 생성된 개요와 수집된 참고 자료를 활용\n",
    "   - 인용(citation)이 포함된 전체 기사 작성\n",
    "\n",
    "### 멀티 에이전트 접근법\n",
    "\n",
    "STORM의 핵심은 **관점 기반 질문 생성(Perspective-Guided Question Asking)** 과 **시뮬레이션된 대화(Simulated Conversation)** 전략입니다:\n",
    "\n",
    "- **다양한 관점 발견**: 유사한 주제의 기존 기사들을 조사하여 다양한 시각을 발견하고, 이를 질문 생성 과정에 활용\n",
    "- **역할 기반 대화 시뮬레이션**: Wikipedia 작성자와 주제 전문가 간의 대화를 시뮬레이션\n",
    "  - 작성자 에이전트: 다양한 관점에서 질문 제기\n",
    "  - 전문가 에이전트: 인터넷 소스에 기반한 답변 제공\n",
    "  - 이를 통해 이해도를 업데이트하고 후속 질문 생성\n",
    "\n",
    "### Co-STORM: 협업 확장\n",
    "\n",
    "Co-STORM은 STORM을 협업 기능으로 확장한 버전으로, 다음과 같은 멀티 에이전트 구성을 포함합니다:\n",
    "\n",
    "\n",
    "- **LLM 전문가 에이전트**: 외부 소스에 기반한 답변 생성 및 후속 질문 제기\n",
    "- **중재자 에이전트(Moderator)**: 발견된 정보에서 영감을 받은 사고를 자극하는 질문 생성\n",
    "- **동적 마인드맵**: 정보를 계층적으로 정리하여 인간과 시스템 간의 공유 개념 공간 생성\n",
    "\n",
    "![](https://github.com/stanford-oval/storm/raw/main/assets/co-storm-workflow.jpg)\n",
    "\n",
    "### 주요 특징\n",
    "\n",
    "- **포괄적 커버리지**: 다양한 관점에서 주제를 탐색하여 Wikipedia 수준의 광범위한 내용 생성\n",
    "- **구조화된 정보**: 자동으로 생성된 개요를 통해 체계적으로 정보 조직\n",
    "- **신뢰할 수 있는 출처**: 인터넷 소스에 기반하여 모든 정보에 인용 포함\n",
    "- **평가 검증**: FreshWiki 데이터셋을 통한 평가에서 기존 방법 대비 조직성 25%, 커버리지 10% 향상\n",
    "\n",
    "STORM은 복잡한 연구 작업을 자동화하고, 다양한 관점에서 정보를 종합하며, 신뢰할 수 있는 장문의 리포트를 생성하는 멀티 에이전트 시스템의 우수한 사례입니다.\n",
    "\n",
    "---\n",
    "\n",
    "- 참고 자료: https://wikidocs.net/270693\n",
    "- 관련 논문: https://arxiv.org/abs/2402.14207\n",
    "- GitHub 저장소: https://github.com/stanford-oval/storm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb325a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "112e4070",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "84dd060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\", override=True)\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    env_value = os.environ.get(var)\n",
    "    if not env_value:\n",
    "        env_value = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "    os.environ[var] = env_value\n",
    "\n",
    "\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\"\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c360a",
   "metadata": {},
   "source": [
    "## 분석가 생성 에이전트 with Human-In-The-Loop\n",
    "\n",
    "분석가 생성이 필요한 클래스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "a7e07bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Analyst(BaseModel):\n",
    "    \"\"\"분석가 속성과 메타데이터를 정의\"\"\"\n",
    "\n",
    "    affiliation: Annotated[str, Field(description=\"분석가의 주요 소속 기관\")]\n",
    "    name: Annotated[str, Field(description=\"분석가 이름\")]\n",
    "    role: Annotated[str, Field(description=\"주제 맥락에서의 분석가의 역할\")]\n",
    "    description: Annotated[\n",
    "        str, Field(description=\"분석가의 관심사, 우려 사항 및 동기 설명\")\n",
    "    ]\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return (\n",
    "            f\"이름: {self.name}\\n\"\n",
    "            f\"역할: {self.role}\\n\"\n",
    "            f\"소속 기관: {self.affiliation}\\n\"\n",
    "            f\"설명: {self.description}\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    \"\"\"분석가들의 집합\"\"\"\n",
    "\n",
    "    analysts: Annotated[\n",
    "        list[Analyst],\n",
    "        Field(description=\"분석가들의 역할 및 소속 기관을 포함한 종합 목록\"),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6778d7",
   "metadata": {},
   "source": [
    "### 분석가 생성 상태 및 노드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b61f39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 정의\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    topic: Annotated[str, \"연구 주제\"]\n",
    "    max_analysts: Annotated[int, \"생성할 분석가의 최대 수\"]\n",
    "    human_analyst_feedback: Annotated[str, \"휴먼 피드백\"]\n",
    "    analysts: Annotated[list[Analyst], \"분석가 목록\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "46502385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석가 생성 프롬프트\n",
    "analyst_instructions = \"\"\"AI 분석가 페르소나 세트를 생성하는 임무를 맡았습니다.\n",
    "\n",
    "다음 지침을 주의 깊게 따르십시오:\n",
    "\n",
    "1. 먼저 연구 주제를 검토하십시오:\n",
    "{topic}\n",
    "\n",
    "2. 분석가 생성 가이드로 제공된 선택적 편집 피드백을 검토하십시오:\n",
    "{human_analyst_feedback}\n",
    "\n",
    "3. 위 문서 및/또는 피드백을 바탕으로 가장 흥미로운 테마를 결정하십시오.\n",
    "\n",
    "4. 상위 {max_analysts}개 테마를 선정하십시오.\n",
    "\n",
    "5. 각 테마에 한 명의 분석가를 배정하십시오.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "d7bcfdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-4.1-mini\")\n",
    "\n",
    "\n",
    "# 분석가 생성 노드\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    \"\"\"분석가 페르소나를 생성합니다.\"\"\"\n",
    "\n",
    "    topic = state[\"topic\"]\n",
    "    max_analysts = state[\"max_analysts\"]\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", \"\")\n",
    "\n",
    "    llm_with_structured = llm.with_structured_output(Perspectives)\n",
    "\n",
    "    system_message = analyst_instructions.format(\n",
    "        topic=topic,\n",
    "        human_analyst_feedback=human_analyst_feedback,\n",
    "        max_analysts=max_analysts,\n",
    "    )\n",
    "\n",
    "    response = llm_with_structured.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=\"Generate the set of analysts.\")]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"analysts\": response.analysts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "47c51fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysts': [Analyst(affiliation='MIT Computer Science and Artificial Intelligence Laboratory (CSAIL)', name='Dr. Elena Vasquez', role='Professor of Multi-Agent Systems', description='Dr. Vasquez specializes in decentralized coordination algorithms and the dynamics of multi-agent interactions in complex environments. Her work advances adaptive strategies for collaboration and competition among autonomous agents.'),\n",
       "  Analyst(affiliation='DeepMind', name='Michael Chen', role='Research Scientist in Artificial Intelligence', description='Michael focuses on the application of multi-agent reinforcement learning to develop scalable AI systems. He investigates emergent behaviors and cooperation protocols to enhance collective problem-solving efficiency.'),\n",
       "  Analyst(affiliation='Stanford University', name='Prof. Aisha Ahmed', role='Assistant Professor of Robotics and AI', description='Prof. Ahmed researches communication and learning mechanisms in heterogeneous multi-agent networks, aiming to optimize task allocation and decision making under uncertainty.')]}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_analysts({\"topic\": \"멀티 에이전트\", \"max_analysts\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e2b276ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 피드백 노드\n",
    "def human_feedback(state: GenerateAnalystsState):\n",
    "    \"\"\"사용자 피드백을 받기 위한 중단점 노드\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a9c93",
   "metadata": {},
   "source": [
    "### 분석가 생성 그래프 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "509107a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "def should_continue(state: GenerateAnalystsState) -> Literal[\"create_analysts\", END]:\n",
    "    \"\"\"워크플로우의 다음 노드를 결정합니다.\"\"\"\n",
    "\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", \"\")\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "    return END\n",
    "\n",
    "\n",
    "builder = StateGraph(GenerateAnalystsState)\n",
    "builder.add_node(create_analysts)\n",
    "builder.add_node(human_feedback)\n",
    "\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\", should_continue, {\"create_analysts\": \"create_analysts\", END: END}\n",
    ")\n",
    "builder.set_entry_point(\"create_analysts\")\n",
    "graph = builder.compile(\n",
    "    interrupt_before=[\"human_feedback\"], checkpointer=InMemorySaver()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8274f81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e503e",
   "metadata": {},
   "source": [
    "### 분석가 생성 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "07008ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### create_analysts #####\n",
      "{'analysts': [Analyst(affiliation='AI Research Lab, Tech University', name='Dr. Eva Kim', role='Comparative Analyst', description='Dr. Eva Kim specializes in evaluating and contrasting advanced retrieval-augmented generation (RAG) models. Her work focuses on understanding the structural and functional differences between agentic and adaptive RAG architectures, highlighting how model agency versus adaptability impacts performance.'),\n",
      "              Analyst(affiliation='NextGen AI Innovations, Inc.', name='Mr. Liam Park', role='Systems Architect', description='Liam Park is a systems architect concentrating on the design and implementation of RAG systems. He provides insights into the engineering challenges and solutions when building agentic versus adaptive RAG, focusing on modularity, control flow, and adaptability to dynamic contexts.'),\n",
      "              Analyst(affiliation='Cognitive Computing Group, Global AI Labs', name='Dr. Sophia Lee', role='Human-AI Interaction Specialist', description='Dr. Sophia Lee researches how adaptive RAG models enhance user interactions by adjusting responses based on contextual feedback, in contrast to agentic RAG which operates with a predefined agency. Her focus is on usability, flexibility, and improving human-AI collaboration.')]}\n",
      "\n",
      "##### __interrupt__ #####\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from random import random\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(configurable={\"thread_id\": random()})\n",
    "\n",
    "inputs = {\n",
    "    \"max_analysts\": 3,\n",
    "    \"topic\": \"Agentic RAG와 Adaptive RAG의 차이점은 무엇인가요?\",\n",
    "}\n",
    "for event in graph.stream(inputs, config=config):\n",
    "    for key, value in event.items():\n",
    "        print(f\"\\n##### {key} #####\")\n",
    "        pprint(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "bab4a06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysts': [Analyst(affiliation='AI Research Lab, Tech University', name='Dr. Eva Kim', role='Comparative Analyst', description='Dr. Eva Kim specializes in evaluating and contrasting advanced retrieval-augmented generation (RAG) models. Her work focuses on understanding the structural and functional differences between agentic and adaptive RAG architectures, highlighting how model agency versus adaptability impacts performance.'),\n",
      "              Analyst(affiliation='NextGen AI Innovations, Inc.', name='Mr. Liam Park', role='Systems Architect', description='Liam Park is a systems architect concentrating on the design and implementation of RAG systems. He provides insights into the engineering challenges and solutions when building agentic versus adaptive RAG, focusing on modularity, control flow, and adaptability to dynamic contexts.'),\n",
      "              Analyst(affiliation='Cognitive Computing Group, Global AI Labs', name='Dr. Sophia Lee', role='Human-AI Interaction Specialist', description='Dr. Sophia Lee researches how adaptive RAG models enhance user interactions by adjusting responses based on contextual feedback, in contrast to agentic RAG which operates with a predefined agency. Her focus is on usability, flexibility, and improving human-AI collaboration.')],\n",
      " 'max_analysts': 3,\n",
      " 'topic': 'Agentic RAG와 Adaptive RAG의 차이점은 무엇인가요?'}\n",
      "('human_feedback',)\n"
     ]
    }
   ],
   "source": [
    "# 현재 상태 스냅샷\n",
    "snapshot = graph.get_state(config)\n",
    "pprint(snapshot.values)\n",
    "pprint(snapshot.next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "fb64c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### human_feedback #####\n",
      "None\n",
      "\n",
      "##### create_analysts #####\n",
      "{'analysts': [Analyst(affiliation='AI Research Lab at a major university', name='Dr. Han Ji-won', role='Technical Analyst', description='Dr. Han Ji-won specializes in natural language processing and retrieval-augmented generation techniques. She focuses on the architectural distinctions and technical capabilities that differentiate Agentic RAG from Adaptive RAG, especially in terms of autonomy and adaptability in AI systems.'),\n",
      "              Analyst(affiliation='Startup company specializing in AI solutions', name='Seok Ho-pil', role='Entrepreneurial Analyst', description='Seok Ho-pil is a founder experienced in AI startups, bringing an entrepreneurial perspective on how Agentic RAG and Adaptive RAG can be leveraged for business innovation and competitive advantage. He emphasizes practical applications and market potential of both RAG approaches.'),\n",
      "              Analyst(affiliation='Industry-leading AI consultancy', name='Prof. Kim Min-seo', role='Strategic Analyst', description='Prof. Kim Min-seo provides insights on the strategic implications of Agentic RAG versus Adaptive RAG for enterprises, including scalability, deployment strategies, and integration challenges within diverse organizational environments.')]}\n",
      "\n",
      "##### __interrupt__ #####\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# 휴먼 피드백 전달\n",
    "from langgraph.types import Command\n",
    "\n",
    "for event in graph.stream(\n",
    "    Command(\n",
    "        update={\n",
    "            \"human_analyst_feedback\": \"스타트업 출신의 석호필이라는 인물을 추가해 기업가적 관점을 더해주세요.\"\n",
    "        },\n",
    "    ),\n",
    "    config=config,\n",
    "):\n",
    "    for key, value in event.items():\n",
    "        print(f\"\\n##### {key} #####\")\n",
    "        pprint(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "913e6244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### human_feedback #####\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream(\n",
    "    Command(update={\"human_analyst_feedback\": None}),\n",
    "    config=config,\n",
    "):\n",
    "    for key, value in event.items():\n",
    "        print(f\"\\n##### {key} #####\")\n",
    "        pprint(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5bcba8",
   "metadata": {},
   "source": [
    "최종 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0da29cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSnapshot(values={'topic': 'Agentic RAG와 Adaptive RAG의 차이점은 무엇인가요?', 'max_analysts': 3, 'human_analyst_feedback': None, 'analysts': [Analyst(affiliation='AI Research Lab at a major university', name='Dr. Han Ji-won', role='Technical Analyst', description='Dr. Han Ji-won specializes in natural language processing and retrieval-augmented generation techniques. She focuses on the architectural distinctions and technical capabilities that differentiate Agentic RAG from Adaptive RAG, especially in terms of autonomy and adaptability in AI systems.'), Analyst(affiliation='Startup company specializing in AI solutions', name='Seok Ho-pil', role='Entrepreneurial Analyst', description='Seok Ho-pil is a founder experienced in AI startups, bringing an entrepreneurial perspective on how Agentic RAG and Adaptive RAG can be leveraged for business innovation and competitive advantage. He emphasizes practical applications and market potential of both RAG approaches.'), Analyst(affiliation='Industry-leading AI consultancy', name='Prof. Kim Min-seo', role='Strategic Analyst', description='Prof. Kim Min-seo provides insights on the strategic implications of Agentic RAG versus Adaptive RAG for enterprises, including scalability, deployment strategies, and integration challenges within diverse organizational environments.')]}, next=(), config={'configurable': {'thread_id': '0.21434318031944666', 'checkpoint_ns': '', 'checkpoint_id': '1f0abc9e-31e6-6298-8004-01c95498797d'}}, metadata={'source': 'loop', 'step': 4, 'parents': {}}, created_at='2025-10-18T02:26:43.568084+00:00', parent_config={'configurable': {'thread_id': '0.21434318031944666', 'checkpoint_ns': '', 'checkpoint_id': '1f0abc9e-31d6-61a4-8003-ab10c89f9a3b'}}, tasks=(), interrupts=())\n"
     ]
    }
   ],
   "source": [
    "# 스냅샷을 가져옵니다.\n",
    "final_state = graph.get_state(config)\n",
    "pprint(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a99468a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 분석가 수: 3\n",
      "================================\n",
      "이름: Dr. Han Ji-won\n",
      "역할: Technical Analyst\n",
      "소속 기관: AI Research Lab at a major university\n",
      "설명: Dr. Han Ji-won specializes in natural language processing and retrieval-augmented generation techniques. She focuses on the architectural distinctions and technical capabilities that differentiate Agentic RAG from Adaptive RAG, especially in terms of autonomy and adaptability in AI systems.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "이름: Seok Ho-pil\n",
      "역할: Entrepreneurial Analyst\n",
      "소속 기관: Startup company specializing in AI solutions\n",
      "설명: Seok Ho-pil is a founder experienced in AI startups, bringing an entrepreneurial perspective on how Agentic RAG and Adaptive RAG can be leveraged for business innovation and competitive advantage. He emphasizes practical applications and market potential of both RAG approaches.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "이름: Prof. Kim Min-seo\n",
      "역할: Strategic Analyst\n",
      "소속 기관: Industry-leading AI consultancy\n",
      "설명: Prof. Kim Min-seo provides insights on the strategic implications of Agentic RAG versus Adaptive RAG for enterprises, including scalability, deployment strategies, and integration challenges within diverse organizational environments.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "analysts = final_state.values.get(\"analysts\")\n",
    "print(f\"생성된 분석가 수: {len(analysts)}\", end=\"\\n================================\\n\")\n",
    "\n",
    "for analyst in analysts:\n",
    "    print(analyst.persona)\n",
    "    print(\"- \" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90393571",
   "metadata": {},
   "source": [
    "## 인터뷰 에이전트\n",
    "\n",
    "### 질문 생성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "364a5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class InterviewState(MessagesState):\n",
    "    \"\"\"인터뷰 정보를 저장합니다.\"\"\"\n",
    "\n",
    "    topic: Annotated[str, \"연구 주제\"]\n",
    "    max_num: Annotated[int, \"대화 턴수\"]\n",
    "    context: Annotated[list, operator.add]\n",
    "    analyst: Annotated[Analyst, \"분석가\"]\n",
    "    interview: Annotated[str, \"인터뷰 내용\"]\n",
    "    sections: Annotated[list, \"보고서 섹션 목록\"]\n",
    "\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: Annotated[str, Field(None, description=\"retrieval를 위한 검색 쿼리\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7d484c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인터뷰 시스템 프롬프트\n",
    "question_instructions = \"\"\"당신은 특정 주제에 대해 알아보기 위해 전문가를 인터뷰하는 임무를 맡은 분석가입니다.\n",
    "\n",
    "당신의 목표는 주제에 관련된 흥미롭고 구체적인 통찰력을 추출하는 것입니다.\n",
    "\n",
    "1. 흥미로움: 사람들이 놀라워하거나 당연하지 않다고 느낄 만한 통찰력.\n",
    "2. 구체성: 일반론을 피하고 전문가의 구체적인 사례를 포함하는 통찰력.\n",
    "\n",
    "다음은 집중할 주제입니다:\n",
    "{topic}\n",
    "\n",
    "첫 대화를 시작할때에 당신의 인물을 반영하는 이름으로 자신을 소개한 후 질문을 시작하세요.\n",
    "\n",
    "주제에 대한 이해를 심화하고 정교화하기 위해 계속해서 질문을 이어가세요.\n",
    "\n",
    "이해가 충분하다고 판단되면 \"도움 주셔서 정말 감사합니다!\"라고 말하며 인터뷰를 마무리하세요.\n",
    "\n",
    "응답 전반에 걸쳐 제공된 인물과 목표를 반영하여 캐릭터를 유지하는 것을 잊지 마세요.\n",
    "\n",
    "<Persona>\n",
    "{persona}\n",
    "<Persona>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "32f5de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 생성 노드\n",
    "def generate_question(state: InterviewState):\n",
    "    \"\"\"통찰력있는 질문을 생성합니다.\"\"\"\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    system_message = question_instructions.format(\n",
    "        topic=\"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점\",\n",
    "        persona=analyst.persona,\n",
    "    )\n",
    "    response = llm.invoke([SystemMessage(content=system_message)] + state[\"messages\"])\n",
    "    response.name = \"분석가\"\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "7b9591fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 분석가\n",
      "\n",
      "안녕하세요, 김필입니다. Tech Innovators Inc.에서 스타트업과 신기술 접목에 관한 분석을 맡고 있습니다. 오늘은 Modular RAG가 기존 Naive RAG와 어떤 차이가 있고, production 환경에서 어떤 이점이 있는지에 대해 심층적으로 이야기 나누고 싶습니다.\n",
      "\n",
      "먼저, Modular RAG가 Naive RAG와 구조적으로 가장 크게 다른 점이 무엇인지 구체적인 사례나 실제 적용 방식을 통해 설명해 주실 수 있을까요?\n"
     ]
    }
   ],
   "source": [
    "analyst = Analyst(\n",
    "    name=\"김필\",\n",
    "    affiliation=\"Tech Innovators Inc.\",\n",
    "    role=\"기업가적 분석가\",\n",
    "    description=\"창업자 출신 분석가로, 기업가 정신과 비즈니스 모델 혁신에 주력합니다. 스타트업 관점에서 적응형 시스템에 대한 통찰력을 제공하며, 유연성과 시장 적응력을 강조합니다.\",\n",
    ")\n",
    "response = generate_question(\n",
    "    {\n",
    "        \"analyst\": analyst,\n",
    "        \"messages\": [],\n",
    "    }\n",
    ")\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53075650",
   "metadata": {},
   "source": [
    "### 도구 정의\n",
    "\n",
    "#### 웹검색 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "f2a89434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "web_search = TavilySearch(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fc7fd6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '지난 윔블던에서 무슨 일이 있었나요?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'url': 'https://www.chosun.com/sports/sports_general/2025/07/13/WL5WE45CLBEGTISJHNMPNE3CXE/',\n",
       "   'title': '“윔블던에 베이글 가게 오픈”… 시비옹테크 여자단식 결승서 진기록',\n",
       "   'content': '테니스 메이저 대회 윔블던 챔피언십 결승에서 1968년 오픈 시대 개막 이후 처음으로 ‘더블 베이글’이 나왔다. 이가 시비옹테크(4위·폴란드)와 어맨다 아니시모바(12위·미국)의 12일(현지 시각) 여자 단식 최종전. 지난 준결승에서 세계 1위 아리나 사발렌카(벨라루스)를 2대1로 꺾고 생애 첫 메이저 대회 결승에 안착한 아니시모바였지만 결과는 허무했다. 한편 올해 윔블던 남자 단식에서는 각각 세계 1·2위인 얀니크 신네르(이탈리아), 카를로스 알카라스(스페인)의 빅 매치가 성사됐다. 쟁쟁한 신예 사이 준결승에까지 오른 ‘노장’ 노바크 조코비치(6위·세르비아)는 11일 신네르와 준결승에서 0대3 완패했다. 얀니크 신네르(세계 1위·이탈리아)와 노바크 조코비치(6위·세르비아), 카를로스 알카라스(2위·스페인)와 테일러 프리츠(5위·미국)가 윔블던 결... 조코비치는 지난 5일 영국 런던 올잉글랜드 클럽에서 열린 윔블던 챔피언십... ‘베컴 조항’을 품은 손흥민(33, LAFC)을 향해 유럽 클럽들의 임대 영입 경쟁이 본격화될 것으로 보인다. 지난 20년간 윔블던 남자단식 우승한 사람이 꼴랑 5명인데 여자는 지난 10년간 10명..',\n",
       "   'score': 0.39733082,\n",
       "   'raw_content': None},\n",
       "  {'url': 'http://m.tennispeople.kr/news/articleView.html?idxno=17168',\n",
       "   'title': '윔블던 컴퓨터 라인 판정 믿어도 되나 - 테니스피플',\n",
       "   'content': '사건은 6일 윔블던 센터 코트에서 열린 여자 단식 16강전 1세트 4-4 상황에서 발생했다. 영국 소니 카르탈과 러시아의 아나스타샤 파블류첸코바가 맞붙',\n",
       "   'score': 0.38498205,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://sports.news.nate.com/view/20250704n04849',\n",
       "   'title': '\"아웃, 인은 기계가 판정\" 148년 전통 깬 윔블던, 인간 선심 사라졌다',\n",
       "   'content': '올해 윔블던 테니스 대회는 148년 역사상 처음으로 인간 선심이 사라진 대회로 기록됐다. CNN은 4일 “윔블던 라인 콜은 전적으로 전자 판독',\n",
       "   'score': 0.34846824,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 1.28,\n",
       " 'request_id': '7e2eaca6-7019-4186-8fc2-a8dc2e0131e1'}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_search.invoke({\"query\": \"지난 윔블던에서 무슨 일이 있었나요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac7eed",
   "metadata": {},
   "source": [
    "#### 논문 검색 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3f49ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "arxiv_retriever = ArxivRetriever(\n",
    "    load_max_docs=3,\n",
    "    load_all_available_meta=True,\n",
    "    get_full_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f016ac79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2024-07-26', 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks', 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang', 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.', 'entry_id': 'http://arxiv.org/abs/2407.21059v1', 'published_first_time': '2024-07-26', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI', 'cs.IR'], 'links': ['http://arxiv.org/abs/2407.21059v1', 'http://arxiv.org/pdf/2407.21059v1']}, page_content='1\\nModular RAG: Transforming RAG Systems into\\nLEGO-like Reconfigurable Frameworks\\nYunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\\nAbstract—Retrieval-augmented\\nGeneration\\n(RAG)\\nhas\\nmarkedly enhanced the capabilities of Large Language Models\\n(LLMs) in tackling knowledge-intensive tasks. The increasing\\ndemands of application scenarios have driven the evolution\\nof RAG, leading to the integration of advanced retrievers,\\nLLMs and other complementary technologies, which in turn\\nhas amplified the intricacy of RAG systems. However, the rapid\\nadvancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process\\nof “retrieve-then-generate”. In this context, this paper examines\\nthe limitations of the existing RAG paradigm and introduces\\nthe modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a\\nmore advanced design that integrates routing, scheduling, and\\nfusion mechanisms. Drawing on extensive research, this paper\\nfurther identifies prevalent RAG patterns—linear, conditional,\\nbranching, and looping—and offers a comprehensive analysis\\nof their respective implementation nuances. Modular RAG\\npresents\\ninnovative\\nopportunities\\nfor\\nthe\\nconceptualization\\nand deployment of RAG systems. Finally, the paper explores\\nthe potential emergence of new operators and paradigms,\\nestablishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment\\nof RAG technologies.\\nIndex Terms—Retrieval-augmented generation, large language\\nmodel, modular system, information retrieval\\nI. INTRODUCTION\\nL\\nARGE Language Models (LLMs) have demonstrated\\nremarkable capabilities, yet they still face numerous\\nchallenges, such as hallucination and the lag in information up-\\ndates [1]. Retrieval-augmented Generation (RAG), by access-\\ning external knowledge bases, provides LLMs with important\\ncontextual information, significantly enhancing their perfor-\\nmance on knowledge-intensive tasks [2]. Currently, RAG, as\\nan enhancement method, has been widely applied in various\\npractical application scenarios, including knowledge question\\nanswering, recommendation systems, customer service, and\\npersonal assistants. [3]–[6]\\nDuring the nascent stages of RAG , its core framework is\\nconstituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the\\nYunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\\nSystems, Tongji University, Shanghai, 201210, China.\\nYun Xiong is with Shanghai Key Laboratory of Data Science, School of\\nComputer Science, Fudan University, Shanghai, 200438, China.\\nMeng Wang and Haofen Wang are with College of Design and Innovation,\\nTongji University, Shanghai, 20092, China. (Corresponding author: Haofen\\nWang. E-mail: carter.whfcarter@gmail.com)\\nlimitations of Naive RAG have become increasingly apparent.\\nAs depicted in Figure 1, it predominantly hinges on the\\nstraightforward similarity of chunks, result in poor perfor-\\nmance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic\\nsimilarity between a query and document chunk is not always\\nhighly consistent. Relying solely on similarity calculations\\nfor retrieval lacks an in-depth exploration of the relationship\\nbetween the query and the document [8]. 2) Retrieval Re-\\ndundancy and Noise. Feeding all retrieved chunks directly\\ninto LLMs is not always beneficial. Research indicates that\\nan excess of redundant and noisy information may interfere\\nwith the LLM’s identification of key information, thereby\\nincreasing the risk of generating erroneous and hallucinated\\nresponses. [9]\\nTo overcome the aforementioned limitations, '),\n",
       " Document(metadata={'Published': '2024-03-27', 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'published_first_time': '2023-12-18', 'comment': 'Ongoing Work', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI'], 'links': ['http://arxiv.org/abs/2312.10997v5', 'http://arxiv.org/pdf/2312.10997v5']}, page_content='1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Terms—Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. INTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. Thi'),\n",
       " Document(metadata={'Published': '2025-10-11', 'Title': 'MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning', 'Authors': 'Thang Nguyen, Peter Chin, Yu-Wing Tai', 'Summary': 'We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\\nend-to-end fine-tuning or isolated component enhancements, MA-RAG orchestrates\\na collaborative set of specialized AI agents: Planner, Step Definer, Extractor,\\nand QA Agents, each responsible for a distinct stage of the RAG pipeline. By\\ndecomposing tasks into subtasks such as query disambiguation, evidence\\nextraction, and answer synthesis, and enabling agents to communicate\\nintermediate reasoning via chain-of-thought prompting, MA-RAG progressively\\nrefines retrieval and synthesis while maintaining modular interpretability.\\nExtensive experiments on multi-hop and ambiguous QA benchmarks, including NQ,\\nHotpotQA, 2WikimQA, and TriviaQA, demonstrate that MA-RAG significantly\\noutperforms standalone LLMs and existing RAG methods across all model scales.\\nNotably, even a small LLaMA3-8B model equipped with MA-RAG surpasses larger\\nstandalone LLMs, while larger variants (LLaMA3-70B and GPT-4o-mini) set new\\nstate-of-the-art results on challenging multi-hop datasets. Ablation studies\\nreveal that both the planner and extractor agents are critical for multi-hop\\nreasoning, and that high-capacity models are especially important for the QA\\nagent to synthesize answers effectively. Beyond general-domain QA, MA-RAG\\ngeneralizes to specialized domains such as medical QA, achieving competitive\\nperformance against domain-specific models without any domain-specific\\nfine-tuning. Our results highlight the effectiveness of collaborative, modular\\nreasoning in retrieval-augmented systems: MA-RAG not only improves answer\\naccuracy and robustness but also provides interpretable intermediate reasoning\\nsteps, establishing a new paradigm for efficient and reliable multi-agent RAG.', 'entry_id': 'http://arxiv.org/abs/2505.20096v2', 'published_first_time': '2025-05-26', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI'], 'links': ['http://arxiv.org/abs/2505.20096v2', 'http://arxiv.org/pdf/2505.20096v2']}, page_content='MA-RAG: MULTI-AGENT RETRIEVAL-AUGMENTED\\nGENERATION\\nVIA\\nCOLLABORATIVE\\nCHAIN-OF-\\nTHOUGHT REASONING\\nThang Nguyen & Peter Chin & Yu-Wing Tai\\nDartmouth College\\n{thangnv.th, peter.chin, yu-wing.tai}@dartmouth.edu\\nABSTRACT\\nWe present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Gener-\\nation (RAG) that addresses the inherent ambiguities and reasoning challenges in\\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely\\non end-to-end fine-tuning or isolated component enhancements, MA-RAG orches-\\ntrates a collaborative set of specialized AI agents: Planner, Step Definer, Extrac-\\ntor, and QA Agents, each responsible for a distinct stage of the RAG pipeline.\\nBy decomposing tasks into subtasks such as query disambiguation, evidence ex-\\ntraction, and answer synthesis, and enabling agents to communicate intermedi-\\nate reasoning via chain-of-thought prompting, MA-RAG progressively refines re-\\ntrieval and synthesis while maintaining modular interpretability. Extensive exper-\\niments on multi-hop and ambiguous QA benchmarks, including NQ, HotpotQA,\\n2WikimQA, and TriviaQA, demonstrate that MA-RAG significantly outperforms\\nstandalone LLMs and existing RAG methods across all model scales. Notably,\\neven a small LLaMA3-8B model equipped with MA-RAG surpasses larger stan-\\ndalone LLMs, while larger variants (LLaMA3-70B and GPT-4o-mini) set new\\nstate-of-the-art results on challenging multi-hop datasets. Ablation studies reveal\\nthat both the planner and extractor agents are critical for multi-hop reasoning,\\nand that high-capacity models are especially important for the QA agent to syn-\\nthesize answers effectively. Beyond general-domain QA, MA-RAG generalizes\\nto specialized domains such as medical QA, achieving competitive performance\\nagainst domain-specific models without any domain-specific fine-tuning. Our re-\\nsults highlight the effectiveness of collaborative, modular reasoning in retrieval-\\naugmented systems: MA-RAG not only improves answer accuracy and robustness\\nbut also provides interpretable intermediate reasoning steps, establishing a new\\nparadigm for efficient and reliable multi-agent RAG1.\\n1\\nINTRODUCTION\\nRecent advances in natural language processing have driven the development of Retrieval-\\nAugmented Generation (RAG) models, which aim to enhance the factual accuracy and contextual\\nrelevance of generated text by integrating external knowledge sources (Lewis et al., 2020; Guu et al.,\\n2020; Izacard & Grave, 2021; Lin et al., 2024). These systems address core limitations of Large Lan-\\nguage Models (LLMs), such as outdated knowledge (Zhang et al., 2023b; Kasai et al., 2023) and\\npoor generalization to domain-specific queries (Siriwardhana et al., 2023; Xiong et al., 2024), by\\nretrieving top-k documents from an external corpus (Formal et al., 2022; Izacard et al., 2022; Wang\\net al., 2022a) to ground the model’s output in relevant evidence.\\nPrior research in RAG has largely concentrated on optimizing three key components—retrieval,\\naugmentation, and generation (Gao et al., 2024; Fan et al., 2024) (Figure 1(a)). Retrieval strate-\\ngies span sparse methods (Jones, 1972; Robertson & Zaragoza, 2009) and dense retrieval (Reimers\\n& Gurevych, 2019; Karpukhin et al., 2020), each with respective weaknesses such as lexical\\ngaps (Berger et al., 2000) or retrieval failure on out-of-distribution and multi-hop queries (Dai et al.,\\n1Our code is available at https://github.com/thangylvp/MA-RAG\\n1\\narXiv:2505.20096v2  [cs.CL]  11 Oct 2025\\nQuery\\nDocs\\nAnswer\\nDocs\\nAnswer\\nCoT\\nNotes\\nPost-process\\na) Vanilla RAG\\nStep 1\\nStep ...\\nCoT\\nSub-Query\\nQuery\\nDocs\\nNotes\\nCoT\\nQuery\\nSub-Answer\\nCoT\\nb) RAG with post-\\nprocessing retrieved docs\\nd) MA-RAG\\nQuery\\nDocs\\nAnswer\\nc) RAG with interleaving\\nretrieval and thoughts\\nAnswer\\nCoT\\nFigure 1: Architectural Comparison of MA-RAG and Prior RAG Methods. a) A naive RAG sys-\\ntem performs one-shot retrieval followed by direct answer generation. b) Enhanced systems incor-\\nporate post-retrieval processing such as document re-')]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_retriever.invoke(\"Modular RAG vs Naive RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da7d4c3",
   "metadata": {},
   "source": [
    "### 노드 작성\n",
    "\n",
    "#### 웹 검색 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f6ef8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 쿼리 변환 프롬프트\n",
    "search_instructions = \"\"\"분석가와 전문가 간의 대화가 제시됩니다.\n",
    "\n",
    "목표는 해당 대화와 관련된 검색 및/또는 웹 검색에 사용할 잘 구조화된 쿼리를 생성하는 것입니다.\n",
    "\n",
    "먼저 전체 대화를 분석하십시오.\n",
    "\n",
    "특히 분석가가 마지막에 제기한 질문에 주목하십시오.\n",
    "\n",
    "이 마지막 질문을 잘 구조화된 웹 검색 쿼리로 변환하십시오.\"\"\"\n",
    "\n",
    "\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\"웹 검색을 통한 문서 검색\"\"\"\n",
    "\n",
    "    llm_with_structured = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "    response = llm_with_structured.invoke(\n",
    "        [(\"system\", search_instructions)] + state[\"messages\"]\n",
    "    )\n",
    "\n",
    "    results = web_search.invoke(response.search_query)\n",
    "    context = [\n",
    "        f'<Document source=\"web\" url=\"{doc[\"url\"]}\" title=\"{doc[\"title\"]}\">{doc[\"content\"]}</Document>'\n",
    "        for doc in results[\"results\"]\n",
    "    ]\n",
    "\n",
    "    return {\"context\": [*context]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f9e12267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': ['<Document source=\"web\" url=\"https://dictionary.cambridge.org/ko/%EC%82%AC%EC%A0%84/%EC%98%81%EC%96%B4/crag\" title=\"영어로 crag의 뜻\">CRAG 의미, 정의, CRAG의 정의: 1. a high, rough mass of rock that sticks out from the land around it 2. a high, rough mass of rock…. 자세히 알아보기.</Document>',\n",
       "  '<Document source=\"web\" url=\"https://www.themoonlight.io/ko/review/corrective-retrieval-augmented-generation\" title=\"[논문 리뷰] Corrective Retrieval Augmented Generation - Moonlight\"># [논문 리뷰] Corrective Retrieval Augmented Generation 네, 요청하신 대로 Corrective Retrieval Augmented Generation (CRAG) 논문에 대한 자세한 설명과 핵심 방법론에 대한 기술적인 설명을 한국어로 제공해 드리겠습니다. 본 논문은 Large Language Model (LLM)의 문제점 중 하나인 hallucination, 즉 환각 현상을 완화하기 위한 방법론인 Corrective Retrieval Augmented Generation (CRAG)을 제안합니다. Retrieval-Augmented Generation (RAG)은 외부 지식을 활용하여 LLM을 보완하는 효과적인 방법이지만, 검색된 문서의 관련성에 크게 의존합니다. CRAG는 이러한 RAG의 취약점을 개선하기 위해 제안된 방법으로, 검색된 문서의 품질을 평가하고, 필요에 따라 웹 검색을 통해 추가적인 지식을 보충하며, 검색된 문서에서 핵심 정보만 추출하여 활용하는 방식으로 LLM의 생성 능력을 향상시킵니다. CRAG는 plug-and-play 방식으로 기존 RAG 기반 접근 방식에 쉽게 통합될 수 있으며, 다양한 데이터 세트에서 실험을 통해 성능 향상을 입증했습니다. * 변환된 키워드를 사용하여 웹 검색을 수행하고, 검색 결과를 활용하여 외부 지식 (external knowledge)을 얻습니다.</Document>',\n",
       "  '<Document source=\"web\" url=\"https://aiforeveryone.tistory.com/44\" title=\"[논문리뷰] Corrective Retrieval Augmented Generation(CRAG, 2024)\">- 검색 증강 생성(RAG)은 LLM의 할루시네이션을 보완할 수 있지만 검색된 문서에 크게 의존 - RAG의 답변 품질을 개선하기 위해 수정 검색 증강 생성(CRAG)을 제안 - Retrieval evaluator를 통해 쿼리에 대해 검색된 문서의 전반적인 품질을 평가 - 검색된 문서의 정보가 불충분한 경우, 대규모 웹 검색을 통해 검색 결과를 보강 - 검색 증강 생성(RAG)은 LLM의 할루시네이션을 보완할 수 있지만 검색된 문서의 관련성과 정확성에 크게 의존 - CRAG(Corrective Retrieval-Augmented Generation) : 검색 결과를 스스로 수정하고 증강 생성된 문서의 활용도를 높임 - Retrieval evaluator로 쿼리에 대해 검색된 문서의 관련성과 신뢰도를 평가 ②Incorrect(부정확) => 검색된 문서 제거 후 웹 검색을 통해 지식 증강(Web Search) -\\xa0검색된 결과가 모두 관련성이 없는 경우 웹 검색 실행</Document>']}"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_web({\"messages\": [(\"user\", \"CRAG에 대해서 설명해주세요.\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9959f7f",
   "metadata": {},
   "source": [
    "#### 논문 검색 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "f25ee249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(state: InterviewState):\n",
    "    \"\"\"Arxiv 검색 노드\"\"\"\n",
    "\n",
    "    llm_with_structured = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "    response = llm_with_structured.invoke(\n",
    "        [(\"system\", search_instructions)] + state[\"messages\"]\n",
    "    )\n",
    "\n",
    "    results = arxiv_retriever.invoke(\n",
    "        response.search_query,\n",
    "        load_max_docs=2,\n",
    "        load_all_available_meta=True,\n",
    "        get_full_documents=True,\n",
    "    )\n",
    "\n",
    "    context = [\n",
    "        f'<Document source=\"arxiv\" url=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
    "        for doc in results\n",
    "    ]\n",
    "\n",
    "    return {\"context\": [*context]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "606944ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': ['<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2502.19629v1\" date=\"2025-02-26\" authors=\"Tiffany J. Callahan, Nathaniel H. Park, Sara Capponi\"/>\\n<Title>\\nAgentic Mixture-of-Workflows for Multi-Modal Chemical Search\\n</Title>\\n\\n<Summary>\\nThe vast and complex materials design space demands innovative strategies to\\nintegrate multidisciplinary scientific knowledge and optimize materials\\ndiscovery. While large language models (LLMs) have demonstrated promising\\nreasoning and automation capabilities across various domains, their application\\nin materials science remains limited due to a lack of benchmarking standards\\nand practical implementation frameworks. To address these challenges, we\\nintroduce Mixture-of-Workflows for Self-Corrective Retrieval-Augmented\\nGeneration (CRAG-MoW) - a novel paradigm that orchestrates multiple agentic\\nworkflows employing distinct CRAG strategies using open-source LLMs. Unlike\\nprior approaches, CRAG-MoW synthesizes diverse outputs through an orchestration\\nagent, enabling direct evaluation of multiple LLMs across the same problem\\ndomain. We benchmark CRAG-MoWs across small molecules, polymers, and chemical\\nreactions, as well as multi-modal nuclear magnetic resonance (NMR) spectral\\nretrieval. Our results demonstrate that CRAG-MoWs achieve performance\\ncomparable to GPT-4o while being preferred more frequently in comparative\\nevaluations, highlighting the advantage of structured retrieval and multi-agent\\nsynthesis. By revealing performance variations across data types, CRAG-MoW\\nprovides a scalable, interpretable, and benchmark-driven approach to optimizing\\nAI architectures for materials discovery. These insights are pivotal in\\naddressing fundamental gaps in benchmarking LLMs and autonomous AI agents for\\nscientific applications.\\n</Summary>\\n\\n<Content>\\nAgentic Mixture-of-Workflows for Multi-Modal Chemical Search \\n \\n \\nTiffany J. Callahan1, Nathaniel H. Park1*, and Sara Capponi1 \\n1IBM Research–Almaden, 650 Harry Rd. San Jose, CA 95120  \\n \\n*Corresponding author. Email: npark@us.ibm.com \\n \\n \\nABSTRACT \\nThe vast and complex materials design space demands innovative strategies to integrate \\nmultidisciplinary scientific knowledge and optimize materials discovery. While large \\nlanguage models (LLMs) have demonstrated promising reasoning and automation \\ncapabilities across various domains, their application in materials science remains limited \\ndue to a lack of benchmarking standards and practical implementation frameworks. To \\naddress these challenges, we introduce Mixture-of-Workflows for Self-Corrective \\nRetrieval-Augmented Generation (CRAG-MoW)—a novel paradigm that orchestrates \\nmultiple agentic workflows employing distinct CRAG strategies using open-source LLMs. \\nUnlike prior approaches, CRAG-MoW synthesizes diverse outputs through an \\norchestration agent, enabling direct evaluation of multiple LLMs across the same problem \\ndomain. We benchmark CRAG-MoWs across small molecules, polymers, and chemical \\nreactions, as well as multi-modal nuclear magnetic resonance (NMR) spectral retrieval. \\nOur results demonstrate that CRAG-MoWs achieve performance comparable to GPT-4o \\nwhile being preferred more frequently in comparative evaluations, highlighting the \\nadvantage of structured retrieval and multi-agent synthesis. By revealing performance \\nvariations across data types, CRAG-MoW provides a scalable, interpretable, and \\nbenchmark-driven approach to optimizing AI architectures for materials discovery. These \\ninsights are pivotal in addressing fundamental gaps in benchmarking LLMs and \\nautonomous AI agents for scientific applications. \\n \\n \\nINTRODUCTION \\nThe vast size, high dimensionality, and complexity of the materials design space require \\nnew strategies for synthesizing and integrating multidisciplinary scientific knowledge. \\nSuch approaches are essential to drive advances in performance, cost-efficiency, and \\nsustainability [1]. Large language models (LLM), which are trained on large amounts of \\ndata and designed for human interaction, have demonstrated impressive reasoning \\nabilities in natural language processing [2–5]. Within the materials domain, LLMs have \\nbeen used to predict chemical properties [6–8], design new molecules [4, 9–12], automate \\nscientific coding [13, 14], develop AI agents [6, 15–17], extract and synthesize knowledge \\n[18, 19], and summarize and generate text [20]. Despite these successes, LLM adoption \\nin materials science lags behind other fields. This gap stems from a lack of standardized \\n \\n2\\nbenchmarks for validating LLM-based analyses, limited application to practical tasks, and \\nthe specialized expertise required for materials development [21–23].  \\n \\nThese challenges have led to increased interest in agentic workflows—LLM-driven \\nautonomous systems designed to perform complex reasoning, tool use, and multi-step \\ndecision-making [24]. Within the materials domain, agentic systems have been developed \\nto review the literature [25–27], implement routine chemical tasks [16, 21, 28–31] plan \\nexperiments [32–35], automate chemoinformatics analysis [36–39], and generate novel \\nhypotheses [40–42]. One of the most widely adopted implementations of agentic systems \\nis retrieval-augmented generation (RAG), which enhances LLM outputs by integrating \\ninformation retrieval techniques [25]. Rather than relying solely on pre-trained knowledge, \\nRAG dynamically retrieves and incorporates relevant external information, improving \\nresponse accuracy and contextual relevance. This capability is particularly valuable in \\nmaterials science, where access to domain-specific literature, experimental data, and \\nproperty databases is critical for precise predictions and reasoning. By leveraging RAG, \\nagentic workflows can provide more reliable and up-to-date insights, addressing key\\n</Content>\\n</Document>',\n",
       "  '<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2406.04744v2\" date=\"2024-11-01\" authors=\"Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, Xin Luna Dong\"/>\\n<Title>\\nCRAG -- Comprehensive RAG Benchmark\\n</Title>\\n\\n<Summary>\\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising\\nsolution to alleviate Large Language Model (LLM)\\'s deficiency in lack of\\nknowledge. Existing RAG datasets, however, do not adequately represent the\\ndiverse and dynamic nature of real-world Question Answering (QA) tasks. To\\nbridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual\\nquestion answering benchmark of 4,409 question-answer pairs and mock APIs to\\nsimulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a\\ndiverse array of questions across five domains and eight question categories,\\nreflecting varied entity popularity from popular to long-tail, and temporal\\ndynamisms ranging from years to seconds. Our evaluation of this benchmark\\nhighlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve\\n<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the\\naccuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%\\nof questions without any hallucination. CRAG also reveals much lower accuracy\\nin answering questions regarding facts with higher dynamism, lower popularity,\\nor higher complexity, suggesting future research directions. The CRAG benchmark\\nlaid the groundwork for a KDD Cup 2024 challenge and attracted thousands of\\nparticipants and submissions. We commit to maintaining CRAG to serve research\\ncommunities in advancing RAG solutions and general QA solutions. CRAG is\\navailable at https://github.com/facebookresearch/CRAG/.\\n</Summary>\\n\\n<Content>\\nCRAG – Comprehensive RAG Benchmark\\nXiao Yang˚1, Kai Sun˚1, Hao Xin˚3, Yushi Sun˚3, Nikita Bhalla1, Xiangsen Chen4, Sajal\\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\\nand Xin Luna Dong1\\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\\nAbstract\\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\\ntion to alleviate Large Language Model (LLM)’s deficiency in lack of knowledge.\\nExisting RAG datasets, however, do not adequately represent the diverse and dy-\\nnamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we\\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\\nand Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse\\narray of questions across five domains and eight question categories, reflecting\\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\\nfrom years to seconds. Our evaluation of this benchmark highlights the gap to\\nfully trustworthy QA. Whereas most advanced LLMs achieve ď 34% accuracy\\non CRAG, adding RAG in a straightforward manner improves the accuracy only\\nto 44%. State-of-the-art industry RAG solutions only answer 63% of questions\\nwithout any hallucination. CRAG also reveals much lower accuracy in answer-\\ning questions regarding facts with higher dynamism, lower popularity, or higher\\ncomplexity, suggesting future research directions. The CRAG benchmark laid the\\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\\nand submissions. We commit to maintaining CRAG to serve research communities\\nin advancing RAG solutions and general QA solutions. CRAG is available at\\nhttps://github.com/facebookresearch/CRAG/.\\n1\\nIntroduction\\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. Despite the advancements,\\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\\nfactual accuracy or grounding [14,27,30,32]. Studies have shown that GPT-4’s accuracy in answering\\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\\nchanging) facts, GPT-4’s accuracy in answering questions referring to torso-to-tail (less popular)\\nentities is below 35% [29]. Overcoming hallucinations thus becomes a priority in building reliable\\nQA systems [13,14].\\nRetrieval-Augmented Generation (RAG) [6,8,12,19] has recently emerged as a promising solution to\\nalleviate LLM’s deficiency in lack of knowledge and attracted a lot of attention from both academia\\nresearch and industry. Given a question, a RAG system searches external sources to retrieve relevant\\ninformation and then provides grounded answers [7,12,19] (see Figure 1 for an illustration). Despite\\n˚Equal contribution. Correspondence to: Xiao Yang (xiaoyangfb@meta.com).\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.\\narXiv:2406.04744v2  [cs.CL]  1 Nov 2024\\nLLM\\nWhat is the gold \\nprice today?\\nGold price is at $1626.81 per \\nounce today Oct 21 2022.\\nGold price is at $2020.8 per \\nounce today Jan 28 2024.\\nDocuments\\nWeb \\nSearch\\nReal-time \\nAPIs\\nKnowledge \\nGraph\\nRetrieved \\nrelevant\\nknowledge\\nQuestion \\n(a) LLM Direct Generation\\n(b) RAG: Retrieved-Augmented \\nGeneration with LLM\\nFigure 1: QA using LLMs (a) without RAG vs. (b) with RAG.\\nits potential, RAG still faces many challenges, such as selecting the most relevant information,\\nreducing question answering latency, and synthesizing information to answer complex questions.\\nA comprehensive benchmark is currently missing to advance continued research efforts \\n</Content>\\n</Document>',\n",
       "  '<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2409.15337v1\" date=\"2024-09-09\" authors=\"Jie Ouyang, Yucong Luo, Mingyue Cheng, Daoyu Wang, Shuo Yu, Qi Liu, Enhong Chen\"/>\\n<Title>\\nRevisiting the Solution of Meta KDD Cup 2024: CRAG\\n</Title>\\n\\n<Summary>\\nThis paper presents the solution of our team APEX in the Meta KDD CUP 2024:\\nCRAG Comprehensive RAG Benchmark Challenge. The CRAG benchmark addresses the\\nlimitations of existing QA benchmarks in evaluating the diverse and dynamic\\nchallenges faced by Retrieval-Augmented Generation (RAG) systems. It provides a\\nmore comprehensive assessment of RAG performance and contributes to advancing\\nresearch in this field. We propose a routing-based domain and dynamic adaptive\\nRAG pipeline, which performs specific processing for the diverse and dynamic\\nnature of the question in all three stages: retrieval, augmentation, and\\ngeneration. Our method achieved superior performance on CRAG and ranked 2nd for\\nTask 2&3 on the final competition leaderboard. Our implementation is available\\nat this link: https://github.com/USTCAGI/CRAG-in-KDD-Cup2024.\\n</Summary>\\n\\n<Content>\\nRevisiting the Solution of Meta KDD Cup 2024: CRAG\\nJie Ouyang\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nouyang_jie@mail.ustc.edu.cn\\nYucong Luo\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nprime666@mail.ustc.edu.cn\\nMingyue Cheng∗\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nmycheng@ustc.edu.cn\\nDaoyu Wang\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nwdy030428@mail.ustc.edu.cn\\nShuo Yu\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nyu12345@mail.ustc.edu.cn\\nQi Liu\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nqiliuql@ustc.edu.cn\\nEnhong Chen\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\ncheneh@ustc.edu.cn\\nAbstract\\nThis paper presents the solution of our team APEX in the Meta KDD\\nCUP 2024: CRAG Comprehensive RAG Benchmark Challenge. The\\nCRAG benchmark addresses the limitations of existing QA bench-\\nmarks in evaluating the diverse and dynamic challenges faced by\\nRetrieval-Augmented Generation (RAG) systems. It provides a more\\ncomprehensive assessment of RAG performance and contributes\\nto advancing research in this field. We propose a routing-based do-\\nmain and dynamic adaptive RAG pipeline, which performs specific\\nprocessing for the diverse and dynamic nature of the question in all\\nthree stages: retrieval, augmentation, and generation. Our method\\nachieved superior performance on CRAG and ranked 2nd for Task\\n2&3 on the final competition leaderboard. Our implementation is\\navailable at this link: https://github.com/USTCAGI/CRAG-in-KDD-\\nCup2024.\\nCCS Concepts\\n• Information systems →Information retrieval.\\nKeywords\\nRetrieval-Augmented Generation, Large Language Model\\n1\\nIntroduction\\nLarge Language Models (LLMs) have revolutionized the landscape\\nof Natural Language Processing (NLP) tasks [5, 8, 10], particularly in\\nquestion answering (QA). Despite advances in LLMs, hallucination\\nremains a significant challenge, particularly for dynamic facts and\\ninformation about less prominent entities.\\nRetrieval-Augmented Generation (RAG) [9] has recently emerged\\nas a promising solution to mitigate LLMs’ knowledge deficiencies.\\n∗Mingyue Cheng is the corresponding author.\\nGiven a question, a RAG system queries external sources to re-\\ntrieve relevant information and subsequently provides grounded\\nanswers. Despite its potential, RAG continues to face numerous\\nchallenges, including the selection of the most relevant information,\\nthe reduction of question answering latency, and the synthesis of\\ninformation to address complex questions.\\nTo bridge this gap, Meta introduced the Comprehensive RAG\\nBenchmark (CRAG) [13], a factual question answering benchmark\\nof 4,409 question-answer pairs and Mock APIs to simulate web\\nand Knowledge Graph (KG) search, and hosted the KDD CUP 2024\\nChallenge.\\n1.1\\nDataset Description\\nThe CRAG contains two parts of data: the QA pairs and the content\\nfor retrieval.\\nQA pairs. The CRAG dataset contains a rich set of 4,409 QA\\npairs covering five domains: finance, sports, music, movie, and\\nopen domain, and eight types of questions. For the KDD CUP 2024\\nChallenge, the benchmark data were splited into three sets with\\nsimilar distributions: validation, public test, and private test at 30%,\\n30%, and 40%, respectively. In total, 2,706 examples from validation\\nand public test sets were shared.\\nThe dataset also reflects varied entity popularity from popular\\nto long-tail entities, and temporal spans ranging from seconds to\\nyears. Given the temporal nature of many questions, each question-\\nanswer pair is accompanied by an additional field denoted as \"query\\ntime.\" This temporal mark\\n</Content>\\n</Document>']}"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_arxiv({\"messages\": [(\"user\", \"CRAG에 대해서 설명해주세요.\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac6790",
   "metadata": {},
   "source": [
    "#### 답변 생성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "a01ce399",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_instructions = \"\"\"당신은 분석가에게 인터뷰를 받는 전문가입니다.\n",
    "\n",
    "분석가의 주요 관심 분야는 다음과 같습니다: {goals}. \n",
    "\n",
    "당신의 목표는 분석가가 제기한 질문에 답변하는 것입니다.\n",
    "\n",
    "질문에 답변할 때는 다음 맥락을 활용하십시오:\n",
    "<Context>\n",
    "{context}\n",
    "<Context>\n",
    "\n",
    "질문에 답변할 때는 다음 지침을 따르십시오:\n",
    "1. 맥락에 제공된 정보만 사용하십시오. \n",
    "2. 외부 정보를 도입하거나 맥락에 명시적으로 언급된 내용을 넘어선 추측을 하지 마십시오.\n",
    "3. 맥락에는 각 개별 문서의 주제별 출처가 포함되어 있습니다.\n",
    "4. 답변에서 관련 진술 옆에 해당 출처를 포함하십시오. 예를 들어 출처 #1의 경우 [1]을 사용하십시오.\n",
    "5. 답변 하단에 출처를 순서대로 나열하십시오. [1] 출처 1, [2] 출처 2, 등\n",
    "6. 출처가 다음과 같은 경우: <Document url=\"assistant/docs/llama3_1.pdf\" page=\"7\" />' \n",
    "    다음처럼 기재하십시오: [1] assistant/docs/llama3_1.pdf, 7 페이지\n",
    "7. 제공된 맥락이 없다면 출처를 기재하기 마십시오.\n",
    "    \n",
    "인용 시 괄호 추가 및 Document 서두 문구는 생략하십시오.\"\"\"\n",
    "\n",
    "\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\"질문에 대한 답변 노드\"\"\"\n",
    "\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    system_message = answer_instructions.format(\n",
    "        goals=analyst.description,\n",
    "        context=\"\\n\".join(state[\"context\"]),\n",
    "    )\n",
    "    response = llm.invoke([(\"system\", system_message)] + state[\"messages\"])\n",
    "    response.name = \"전문가\"\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "37fe5c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 전문가\n",
      "\n",
      "Modular RAG와 기존의 Naive RAG의 차이는 구조적 유연성과 성능 최적화 측면에서 나타납니다. Naive RAG는 단일 통합형 시스템으로서 정보를 검색하고 생성하는 과정을 하나의 블랙박스처럼 처리하는 반면, Modular RAG는 검색 모듈, 생성 모듈 등 각 기능을 독립적으로 모듈화하여 구성합니다.\n",
      "\n",
      "이러한 모듈화는 다음과 같은 생산 환경(production level)에서의 이점을 제공합니다:\n",
      "\n",
      "1. **유지보수와 확장성 향상**: 각 모듈을 독립적으로 업데이트하거나 교체할 수 있어, 변화하는 비즈니스 요구나 데이터 환경에 빠르게 적응할 수 있습니다. 이는 스타트업 환경에서 요구되는 유연성과 신속한 시장 대응과 잘 맞아떨어집니다.\n",
      "\n",
      "2. **성능 최적화**: 각 모듈별로 최적의 알고리즘이나 모델을 적용할 수 있어, 전체 시스템의 효율성과 정확도를 높일 수 있습니다.\n",
      "\n",
      "3. **디버깅 및 문제 해결 용이**: 모듈별로 문제가 발생했을 때 해당 부분만 집중적으로 점검할 수 있어, 시스템 안정성을 보장하는 데 유리합니다.\n",
      "\n",
      "따라서 Modular RAG는 Naive RAG에 비해 적응성과 유연성이 뛰어나 생산 환경에서 더 효과적으로 활용될 수 있습니다. 이는 비즈니스 모델 혁신과 빠른 시장 적응력을 중시하는 창업자 출신 분석가의 시각과도 부합합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = generate_answer(\n",
    "    {\n",
    "        \"analyst\": analyst,\n",
    "        \"context\": [],\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점\",\n",
    "                name=\"분석가\",\n",
    "            )\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624648b",
   "metadata": {},
   "source": [
    "### 문석 작성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "82c9e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기술 문서 작성 프롬프트\n",
    "section_writer_instructions = \"\"\"당신은 전문 기술 문서 작성자입니다.\n",
    "\n",
    "당신의 임무는 일련의 원본 문서를 철저히 분석하여 보고서의 상세하고 포괄적인 섹션을 작성하는 것입니다.\n",
    "이는 핵심 통찰력을 추출하고, 관련 사항을 상세히 설명하며, 명확성과 이해를 보장하기 위한 심층적인 해설을 제공하는 것을 포함합니다. 필요한 배경 정보, 뒷받침하는 증거, 예시를 포함하여 독자의 이해를 높여야 합니다. 논리적이고 체계적인 구조를 유지하며, 모든 핵심 사항이 상세히 다루어지고 전문적인 어조로 제시되도록 하십시오.\n",
    "\n",
    "다음 지침을 따르십시오:\n",
    "1. 원본 문서의 내용 분석:\n",
    "- 각 원본 문서의 이름은 문서 시작 부분에 <Document> 태그와 함께 기재되어 있습니다.\n",
    "\n",
    "2. 마크다운 서식을 사용하여 보고서 구조 생성:\n",
    "- 섹션 제목에는 ## 사용\n",
    "- 하위 섹션 헤더에는 ### 사용\n",
    "\n",
    "3. 다음 구조에 따라 보고서 작성:\n",
    "a. 제목 (## 헤더)\n",
    "b. 요약 (### 헤더)\n",
    "c. 종합 분석 (### 헤더)\n",
    "d. 출처 (### 헤더)\n",
    "\n",
    "4. 분석가의 중점 분야를 반영하여 제목을 흥미롭게 작성하십시오: \n",
    "{focus}\n",
    "\n",
    "5. 요약 섹션 작성 시:\n",
    "- 분석가의 중점 분야와 관련된 일반적 배경/맥락을 요약으로 제시하십시오\n",
    "- 인터뷰에서 수집한 통찰 중 새롭거나 흥미롭거나 놀라운 점을 강조하십시오\n",
    "- 사용한 출처 문서를 번호 매긴 목록으로 작성하십시오\n",
    "- 인터뷰어 또는 전문가의 이름은 언급하지 마십시오\n",
    "- 최대 약 400단어를 목표로 하십시오\n",
    "- 출처 문서의 정보에 기반하여 보고서에서 번호 매긴 출처([1], [2] 등)를 사용하십시오\n",
    "\n",
    "6. 종합 분석 섹션:\n",
    "- 출처 문서의 정보를 상세히 검토하십시오.\n",
    "- 복잡한 아이디어를 이해하기 쉬운 단위로 분해하고 논리적인 흐름을 유지하십시오.\n",
    "- 분석의 다양한 관점이나 차원을 다루기 위해 필요한 경우 하위 섹션을 사용하십시오.\n",
    "- 원본 문서의 데이터, 직접 인용문, 예시를 통해 분석을 뒷받침하십시오.\n",
    "- 각 논점이 보고서의 전반적인 초점과 어떻게 관련되는지 명확히 설명하십시오.\n",
    "- 여러 관련 아이디어를 제시할 때는 명확성을 위해 글머리 기호나 번호 매기기 목록을 사용하십시오.\n",
    "- 전문적이고 객관적인 어조를 유지하며 편향되거나 근거 없는 의견을 피하십시오.\n",
    "- 분석이 철저하도록 최소 800단어를 목표로 하십시오.\n",
    "\n",
    "7. 출처 섹션에서:\n",
    "- 보고서에 사용된 모든 출처를 포함하십시오\n",
    "- 관련 웹사이트 또는 특정 문서 경로의 전체 링크를 제공하십시오\n",
    "- 각 출처는 새 줄로 구분하십시오. 마크다운에서 새 줄을 만들기 위해 각 줄 끝에 두 개의 공백을 사용하십시오.\n",
    "- 예시:\n",
    "    ### 출처\n",
    "    [1] 링크 또는 문서명\n",
    "    [2] 링크 또는 문서명\n",
    "\n",
    "8. 출처를 반드시 통합하십시오. 예를 들어 다음은 올바르지 않습니다:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "중복 출처는 없어야 합니다. 다음과 같이 간결하게 작성하십시오:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "9. 최종 검토:\n",
    "- 보고서가 요구되는 구조를 따르는지 확인하십시오.\n",
    "- 보고서 제목 앞에 서문을 포함하지 마십시오.\n",
    "- 모든 지침이 준수되었는지 확인하십시오.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.messages import get_buffer_string\n",
    "\n",
    "\n",
    "def write_section(state: InterviewState):\n",
    "    \"\"\"인터뷰에 대한 리포트 작성\"\"\"\n",
    "\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    # 인터뷰를 문자열로 변환\n",
    "    interview = get_buffer_string(state[\"messages\"])\n",
    "\n",
    "    # 섹션 작성을 위한 시스템 프롬프트 정의\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "    response = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [\n",
    "            HumanMessage(\n",
    "                content=f\"분석가와 전문가의 인터뷰 내용입니다:\\n<Interview>\\n{interview}\\n</Interview>\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"이 자료를 사용하여 해당 섹션을 작성하십시오:\\n<Resources>\\n{state['context']}\\n</Resources>\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return {\"sections\": [response.content]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6b207",
   "metadata": {},
   "source": [
    "### 인터뷰 그래프 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "fd3d133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "def should_continue_interview(state: InterviewState) -> Literal[\"finish\", \"continue\"]:\n",
    "    \"\"\"인터뷰를 계속 진행할 지 여부를 판단합니다.\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get(\"max_num_turns\", 2)\n",
    "\n",
    "    expert_message_count = sum(\n",
    "        1 for m in messages if isinstance(m, AIMessage) and m.name == \"전문가\"\n",
    "    )\n",
    "\n",
    "    # 전문가가 최대 턴 이상 답변했다면 인터뷰를 종료합니다.\n",
    "    if expert_message_count >= max_num_turns:\n",
    "        return \"finish\"\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    if \"도움 주셔서 정말 감사합니다\" in last_message.content:\n",
    "        return \"finish\"\n",
    "\n",
    "    return \"continue\"\n",
    "\n",
    "\n",
    "interview_builder = StateGraph(InterviewState)\n",
    "interview_builder.add_node(generate_question)  # 질문 생성\n",
    "interview_builder.add_node(search_web)  # 웹 검색\n",
    "interview_builder.add_node(search_arxiv)  # arxiv 검색\n",
    "interview_builder.add_node(generate_answer)  # 답변 생성\n",
    "interview_builder.add_node(write_section)  # 문서 작성\n",
    "\n",
    "interview_builder.set_entry_point(\"generate_question\")\n",
    "interview_builder.add_edge(\"generate_question\", \"search_web\")\n",
    "interview_builder.add_edge(\"generate_question\", \"search_arxiv\")\n",
    "interview_builder.add_edge(\"search_web\", \"generate_answer\")\n",
    "interview_builder.add_edge(\"search_arxiv\", \"generate_answer\")\n",
    "interview_builder.add_conditional_edges(\n",
    "    \"generate_answer\",\n",
    "    should_continue_interview,\n",
    "    {\n",
    "        \"continue\": \"generate_question\",\n",
    "        \"finish\": \"write_section\",\n",
    "    },\n",
    ")\n",
    "interview_builder.set_finish_point(\"write_section\")\n",
    "\n",
    "interview_graph = interview_builder.compile(\n",
    "    checkpointer=InMemorySaver(),\n",
    ").with_config(run_name=\"Conduct Interviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "9cf57641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(interview_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252d966",
   "metadata": {},
   "source": [
    "### 인터뷰 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3f343ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 분석가\n",
      "\n",
      "안녕하세요, 저는 Dr. Han Ji-won입니다. AI Research Lab에서 자연어 처리와 Retrieval-Augmented Generation, 특히 RAG 모델의 구조적 차이와 실생활 응용에 대해 연구하고 있습니다.\n",
      "\n",
      "네, 맞습니다. 현재 Modular RAG와 Naive RAG 사이의 핵심 차이점과 Modular RAG가 실제 생산 환경에서 갖는 장점에 대해 글을 준비 중입니다. 우선 Modular RAG는 Naive RAG 대비 모듈화된 구성 요소로 설계되어, 각각의 Retrieval 모듈이나 Generator 모듈을 독립적으로 조정 및 교체할 수 있다는 특징이 있습니다. 이런 분리 구조 덕분에 실제 운영 환경에서 요구사항 변화나 데이터 특성 변화에 유연하게 대응이 가능하죠.\n",
      "\n",
      "먼저, Naive RAG가 가진 한계 중 하나는 Retrieval과 Generation이 밀접하게 결합되어 있어, 특정 부분만 성능 개선이 어렵고 확장성에도 제약이 있다는 점입니다. 반면, Modular RAG는 각 모듈이 명확하게 분리되어 있어, 예를 들어 도메인 특화된 Retriever를 도입하거나, 더 발전된 Generator를 손쉽게 통합할 수 있게 합니다.\n",
      "\n",
      "이 점이 production level에서 어떤 구체적인 이점을 주는지에 대해 더 설명드리면, 예를 들어 금융 서비스용 챗봇을 운영한다고 할 때, 법규나 금융상품 관련 최신 문서만 빠르게 업데이트하여 Retriever 모듈만 교체해도 전체 시스템 성능 저하 없이 최신 정보 반영이 가능한 점이 대표적입니다.\n",
      "\n",
      "더 궁금한 점 있으시면 어떤 부분을 좀 더 상세히 파고들까요?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_answer\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 전문가\n",
      "\n",
      "네, Modular RAG와 Naive RAG의 차이점과 Modular RAG가 production 환경에서 가지는 이점을 정리해 드리겠습니다.\n",
      "\n",
      "1. 구조적 차이점  \n",
      "- Naive RAG는 Retrieval과 Generation이 일직선 형태로 연결된 단순한 파이프라인 구조를 가지고 있습니다. 즉, 쿼리 요청 → 문서 검색 → 문서 기반 생성이 순차적으로 진행되며 각 단계가 밀접하게 결합되어 있습니다. 이로 인해 복잡한 쿼리 처리나 다양하고 동적인 요구에 대응하는 데 한계가 있습니다.  \n",
      "- 반면, Modular RAG는 Retrieval, Generation, 경로 선택 및 결과 융합 등 기능별로 독립적인 모듈과 특수 연산자를 구성요소로 분해하여 LEGO 블록처럼 조립할 수 있는 유연한 프레임워크를 지향합니다. 모듈별로 독립적인 제어와 교체가 가능하며, 복잡한 분기, 반복, 조건 처리 등 다양한 패턴을 수용할 수 있습니다 [1][4].\n",
      "\n",
      "2. 생산 환경(production level)에서의 장점  \n",
      "- 유연성: 필요에 따라 특정 모듈만 업데이트하거나 교체 가능하여 고객 요구사항 변화에 빠르게 대응할 수 있습니다. 예를 들어, 특정 도메인의 검색엔진을 별도로 개선해도 전체 시스템 영향을 최소화할 수 있습니다.  \n",
      "- 확장성: 모듈 간 독립성이 높아 새로운 기능이나 기술 도입이 비교적 용이합니다. 예를 들어, 멀티모달 입력이나 고도화된 후처리 모듈을 추가할 수 있습니다 [1][2].  \n",
      "- 유지보수 용이성: 문제 발생 시 특정 모듈만 점검하고 조치하면 되므로 시스템 안정성과 관리 편의성이 향상됩니다 [4].  \n",
      "- 성능 최적화: 각 모듈별로 개별 최적화가 가능해, 중복된 정보 제거, 검색 결과 정제, 쿼리 이해 증진 등의 세밀한 기법 적용이 수월합니다 [1][4].\n",
      "\n",
      "요약하면, Naive RAG는 간단하고 처음 도입하기는 쉬우나 복잡한 실제 업무 요구와 대규모 서비스 운영에는 한계가 있습니다. Modular RAG는 이러한 한계를 극복하는 한편, 체계적인 모듈 설계로 실시간 업그레이드, 복잡한 논리 구현, 그리고 유지보수를 용이하게 해 실제 운영 환경에서 매우 유리한 구조입니다.\n",
      "\n",
      "참고 문헌:  \n",
      "[1] Yunfan Gao et al., \"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\", arXiv 2024  \n",
      "[2] Jiajie Jin et al., \"FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research\", arXiv 2025  \n",
      "[4] 슈퍼브 블로그, \"RAG 기술의 진화: Naive에서 Modular까지 총정리\"\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 분석가\n",
      "\n",
      "안녕하세요, Dr. Han Ji-won입니다. 저는 자연어 처리 및 RAG 모델 연구를 주로 하고 있는데요, Modular RAG와 기존의 Naive RAG의 차이점 및 production 환경에서의 장점에 대해 말씀드리겠습니다.\n",
      "\n",
      "우선, Naive RAG는 Retrieval과 Generation이 거의 일직선으로 결합된 단순 파이프라인 구조입니다. 이 구조에서는 문서 검색기와 생성기가 밀접하게 연결되어 있어 하나의 구성요소를 바꾸거나 확장하기가 어렵고, 시스템 전반의 재학습이나 재튜닝이 필요해집니다. 반면 Modular RAG는 Retrieval, Generation, 그리고 결과 조합과 같은 각 기능을 독립적인 모듈로 분리하는 구조를 갖추고 있습니다. 이 모듈화 덕분에, 예를 들어 특정 도메인에 맞춰 Retriever만 교체하거나 Generator를 최신 모델로 바꾸는 작업이 전체 시스템을 크게 바꾸지 않고 가능합니다.\n",
      "\n",
      "이 점이 실제 production 환경에서 매우 큰 강점입니다. 예를 들어 금융 서비스 챗봇에 적용한다면, 금융 규정 변경 시 관련 문서를 재수집하고 Retriever 모듈만 업데이트함으로써 빠르게 최신 정보를 반영할 수 있습니다. 더불어, 특정 모듈에 문제가 생겨도 해당 부분만 점검과 개선을 할 수 있기 때문에 운영 안정성과 유지보수 효율도 올라갑니다.\n",
      "\n",
      "혹시 Naive RAG와 Modular RAG 중간 단계의 사례나, 특정 도메인 적용 시 겪은 실제 운영상의 도전과 해결책에 대해 더 구체적인 예시를 듣고 싶으신가요?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_answer\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 전문가\n",
      "\n",
      "Modular RAG는 기존의 Naive RAG와 비교했을 때, 구조적·기능적으로 여러 면에서 차별화됩니다. Naive RAG는 인덱싱, 검색, 생성이 단순히 순차적으로 연결된 선형 파이프라인 구조를 갖고 있는데, 이 구조에서는 검색 시 단순한 벡터 유사도만을 활용하는 경우가 많아 복잡한 쿼리나 다양한 정보 변별에 한계가 있습니다. 또한, 중복되거나 잡음이 많은 검색 결과가 생성 과정에 바로 투입되어 성능 저하나 비일관성 답변의 위험이 존재합니다.\n",
      "\n",
      "반면, Modular RAG는 이러한 Naive RAG의 한계를 극복하기 위해 Retrieval, Generation, 라우팅, 스케줄링, 융합 등 기능별로 독립된 모듈과 특수 연산자를 기반으로 설계되어 있습니다. 각 모듈이 LEGO 블록처럼 분리되어 있어 필요에 따라 모듈을 교체하거나 추가할 수 있는 높은 유연성을 가집니다. 이를 통해 복잡하고 동적인 작업 환경에서도 유연하게 대응 가능하며, 멀티 모달 데이터 처리나 반복적·조건적 흐름 제어 등 다양한 고도화된 패턴도 지원합니다.\n",
      "\n",
      "생산 환경(Production level)에서 Modular RAG의 주요 이점은 다음과 같습니다.  \n",
      "1) 유지보수 및 확장성에서 우수: 모듈 단위로 관리가 가능해 새로운 기술이나 도메인 특화 모듈을 손쉽게 도입할 수 있습니다.  \n",
      "2) 정보의 잡음과 중복 감소: 사전/사후 처리 모듈을 통해 불필요한 정보를 걸러내고 핵심 정보만 LLM에 전달하여 응답 품질을 높입니다.  \n",
      "3) 복잡한 작업 흐름도 지원: 조건부 분기, 루프 같은 복잡한 로직을 모듈간 조합으로 구현하여 다양한 산업 현장 요구에 맞춤 대응이 가능합니다.\n",
      "\n",
      "따라서 Modular RAG는 Naive RAG 대비 실시간 최신 정보 반영, 다양한 정보 소스의 융합, 맞춤형 파이프라인 설계가 가능하여 실제 서비스 환경에서 높은 활용성과 운영 효율성을 보장합니다[1][4].\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mwrite_section\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "## Architectural Distinctions and Production Advantages of Modular RAG over Naive RAG: Insights from Dr. Han Ji-won\n",
      "\n",
      "### Summary\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) frameworks have substantially improved large language models (LLMs) by enabling access to external knowledge, particularly for knowledge-intensive tasks. Dr. Han Ji-won, specializing in natural language processing and retrieval-augmented generation, emphasizes the critical architectural distinctions between the traditional Naive RAG and the emergent Modular RAG approaches, highlighting how these differences impact production-level deployment. Naive RAG, characterized by a linear pipeline that sequentially processes retrieval followed by generation, struggles with flexibility and adaptability in complex, dynamic environments. It tightly couples retrieval and generation modules, limiting selective improvements on either side and reducing scalability.\n",
      "\n",
      "In contrast, Modular RAG deconstructs the overall RAG process into independently manageable modules—such as retrieval, generation, routing, scheduling, and fusion—arranged akin to LEGO blocks. This decomposition enables seamless integration, selective module replacement, and complex reasoning through conditional branching and looping workflows. This architectural modularity enhances system flexibility, allowing, for instance, rapid domain-specific retriever updates without overhauling the entire system. Notably, such features provide considerable advantages in real-world applications, including maintaining currency of information and simplifying maintenance.\n",
      "\n",
      "Among the surprising insights Dr. Han shares is the emphasis on production-level benefits derived from modularity, such as improved maintainability—since individual modules can be independently optimized or debugged—and enhanced performance via fine-grained optimization (e.g., noise filtering or query refinement at module boundaries). Additionally, Modular RAG's capacity to incorporate multi-modal inputs and support advanced control flows aligns it with growing industrial demands for robust, interpretable AI services. The synthesis of academic research (notably Gao et al., 2024; Jin et al., 2025) and practical exemplification in finance chatbot updates illustrates Modular RAG’s operational viability and extensibility beyond the limitations of Naive RAG.\n",
      "\n",
      "#### Key Sources:\n",
      "1. Yunfan Gao et al., \"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks,\" arXiv, 2024  \n",
      "2. Jiajie Jin et al., \"FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research,\" arXiv, 2025  \n",
      "3. Reasoning Agentic RAG survey, arXiv, 2025  \n",
      "4. Superb Blog, \"RAG 기술의 진화: Naive에서 Modular까지 총정리\"\n",
      "\n",
      "---\n",
      "\n",
      "### Comprehensive Analysis\n",
      "\n",
      "#### 1. Fundamental Architectural Distinctions\n",
      "\n",
      "Naive RAG operates on a simple linear pipeline: a user query prompts a retrieval of relevant documents, which are directly passed to a generation module to synthesize a response. This tight coupling of Retrieval and Generation modules means minimal flexibility in updating or optimizing independent components. Gao et al. (2024) highlight that this yields inherent limitations—chiefly, shallow semantic understanding during retrieval, excessive redundancy and noise propagation into generation, and difficulties handling complex queries or diverse data types [1].\n",
      "\n",
      "Modular RAG restructures this process by fragmenting the workflow into discrete, specialized modules interconnected through orchestrated routing, scheduling, and fusion operators. Each functional component—such as retrievers, generators, context filters, or fusion modules—is encapsulated as an independent module that can be individually replaced, updated, or extended. Conceptually likened to LEGO blocks, this modularity supports complex workflows including conditional logic, iterative loops, and branching paths beyond the linear \"retrieve-then-generate\" paradigm [1][4].\n",
      "\n",
      "This paradigm shift results in several architectural advantages:\n",
      "\n",
      "- **Modularity and Replaceability**: Each module can be developed and optimized independently; for example, a domain-specific retriever optimized for financial documents can replace a generic one without impacting other modules.\n",
      "- **Complex Control Flow Support**: Routing and scheduling operators allow workflows to adapt dynamically based on intermediate outputs or external signals, facilitating multi-hop reasoning, query reformulation, or multimodal integration.\n",
      "- **Improved Handling of Noise and Redundancy**: By introducing pre- and post-retrieval filtering modules, Modular RAG reduces the entry of irrelevant or repetitive information into generation, addressing hallucination risks prevalent in Naive RAG systems [1][4].\n",
      "\n",
      "The flexibility to compose modules with specialized operators opens unexplored opportunities for system customization and experimentation, a notable contribution detailed in Gao et al. (2024).\n",
      "\n",
      "#### 2. Production-Level Benefits\n",
      "\n",
      "Dr. Han Ji-won underscores that Modular RAG’s modular design leads to tangible advantages in real-world deployment environments:\n",
      "\n",
      "- **Operational Flexibility**: In applications like chatbots for financial services, regulations or product details frequently change. Modular RAG allows updating or retraining only the retriever module responsible for indexing relevant documents, enabling rapid adaptation without retraining the generation model or retraining the entire pipeline [Interview].\n",
      "- **Maintainability and Debugging**: Isolating modules reduces the scope for troubleshooting, enabling engineers to pinpoint and resolve issues within a single module instead of combing through a monolithic pipeline. This markedly reduces downtime and operational overhead [4].\n",
      "- **Scalability and Extensibility**: Modular RAG simplifies incorporating new functionalities such as multi-modal inputs (images, tables), external knowledge bases, or advanced query parsers. As Jin et al. (2025) explain, modular frameworks like FlashRAG provide research-driven toolkits that facilitate rapid experimentation and deployment, aligning with production needs [2].\n",
      "- **Performance Optimization**: Modular architectures enable focused optimizations—such as advanced query understanding modules or noise-aware fusion mechanisms—improving overall response relevance and factual accuracy. These techniques help mitigate the hallucinatory tendencies often encountered in Naive RAG [1][4].\n",
      "\n",
      "In contrast, Naive RAG’s monolithic design hinders such agility, often requiring complete retraining or extensive retuning when changes are made to any component. Additionally, handling more sophisticated interaction patterns, multi-turn dialogue reasoning, or multi-modal data integration is cumbersome under Naive RAG.\n",
      "\n",
      "#### 3. Technical Nuances and Advanced Features\n",
      "\n",
      "Beyond mere component segmentation, Modular RAG integrates routing and scheduling operators enabling:\n",
      "\n",
      "- **Conditional Logic and Branching**: Different retrieval or generation strategies can be selected dynamically depending on the query type or intermediate results, offering tailored processing pipelines.\n",
      "- **Looping Mechanisms**: Iterative retrieval-feedback cycles improve query refinement and answer completeness for complex, multi-hop reasoning tasks.\n",
      "- **Fusion and Aggregation**: Outputs from multiple retrieval modules or knowledge sources can be merged intelligently before feeding to generation modules, enhancing the breadth and depth of information synthesis [1].\n",
      "\n",
      "This granularity supports not only stable production services but also research innovation, allowing testing of novel architectures without system-wide disruption. Jin et al. (2025) present FlashRAG as a concrete instantiation of such a modular toolkit, demonstrating 16 pre-implemented methods and extensive benchmarking support, distinguishing it from more rigid or heavy existing frameworks like LangChain or LlamaIndex [2].\n",
      "\n",
      "Dr. Han emphasizes these design principles as foundational to future-proofing RAG deployments, given the fast evolution of retrieval and generation techniques and their rising complexity.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Dr. Han Ji-won's insights provide an authoritative perspective on the critical architectural and operational distinctions between Naive RAG and Modular RAG approaches. Modular RAG’s modular, highly configurable framework affords unparalleled flexibility, extensibility, and maintainability, crucial for deploying RAG systems at production scale in dynamic, real-world applications. This design overcomes core limitations of Naive RAG, supporting multi-modal integration, complex reasoning workflows, and efficient component upgrades essential in industrial contexts. The combination of theoretical foundations from Gao et al. (2024), practical toolkits like FlashRAG (Jin et al., 2025), and application-driven commentary cements Modular RAG as a transformative advancement in retrieval-augmented generation technologies.\n",
      "\n",
      "---\n",
      "\n",
      "### References\n",
      "\n",
      "[1] Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang, \"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks,\" arXiv:2407.21059, 2024-07-26.  \n",
      "[2] Jiajie Jin et al., \"FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research,\" arXiv:2405.13576, 2025-02-24.  \n",
      "[3] Jintao Liang et al., \"Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges,\" arXiv:2506.10408, 2025-06-12.  \n",
      "[4] 슈퍼브 블로그, \"RAG 기술의 진화: Naive에서 Modular까지 총정리,\" https://blog-ko.superb-ai.com/rag-technology-evolution-naive-advanced-modular-approaches/\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import invoke_graph\n",
    "\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=30,\n",
    "    configurable={\"thread_id\": random()},\n",
    ")\n",
    "\n",
    "topic = \"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점\"\n",
    "\n",
    "# 그래프 실행\n",
    "invoke_graph(\n",
    "    interview_graph,\n",
    "    {\n",
    "        \"topic\": topic,\n",
    "        \"analyst\": analysts[0],\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                f'그래서 당신이 이 주제에 대해서 글을 쓰고 있다고 했죠? 라고 말씀하셨죠? \"{topic}\"',\n",
    "            )\n",
    "        ],\n",
    "        \"max_num_turns\": 2,\n",
    "    },\n",
    "    config=config,\n",
    "    node_names=[\"generate_question\", \"generate_answer\", \"write_section\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ee1f7a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Architectural Distinctions and Production Advantages of Modular RAG over Naive RAG: Insights from Dr. Han Ji-won\n",
       "\n",
       "### Summary\n",
       "\n",
       "Retrieval-Augmented Generation (RAG) frameworks have substantially improved large language models (LLMs) by enabling access to external knowledge, particularly for knowledge-intensive tasks. Dr. Han Ji-won, specializing in natural language processing and retrieval-augmented generation, emphasizes the critical architectural distinctions between the traditional Naive RAG and the emergent Modular RAG approaches, highlighting how these differences impact production-level deployment. Naive RAG, characterized by a linear pipeline that sequentially processes retrieval followed by generation, struggles with flexibility and adaptability in complex, dynamic environments. It tightly couples retrieval and generation modules, limiting selective improvements on either side and reducing scalability.\n",
       "\n",
       "In contrast, Modular RAG deconstructs the overall RAG process into independently manageable modules—such as retrieval, generation, routing, scheduling, and fusion—arranged akin to LEGO blocks. This decomposition enables seamless integration, selective module replacement, and complex reasoning through conditional branching and looping workflows. This architectural modularity enhances system flexibility, allowing, for instance, rapid domain-specific retriever updates without overhauling the entire system. Notably, such features provide considerable advantages in real-world applications, including maintaining currency of information and simplifying maintenance.\n",
       "\n",
       "Among the surprising insights Dr. Han shares is the emphasis on production-level benefits derived from modularity, such as improved maintainability—since individual modules can be independently optimized or debugged—and enhanced performance via fine-grained optimization (e.g., noise filtering or query refinement at module boundaries). Additionally, Modular RAG's capacity to incorporate multi-modal inputs and support advanced control flows aligns it with growing industrial demands for robust, interpretable AI services. The synthesis of academic research (notably Gao et al., 2024; Jin et al., 2025) and practical exemplification in finance chatbot updates illustrates Modular RAG’s operational viability and extensibility beyond the limitations of Naive RAG.\n",
       "\n",
       "#### Key Sources:\n",
       "1. Yunfan Gao et al., \"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks,\" arXiv, 2024  \n",
       "2. Jiajie Jin et al., \"FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research,\" arXiv, 2025  \n",
       "3. Reasoning Agentic RAG survey, arXiv, 2025  \n",
       "4. Superb Blog, \"RAG 기술의 진화: Naive에서 Modular까지 총정리\"\n",
       "\n",
       "---\n",
       "\n",
       "### Comprehensive Analysis\n",
       "\n",
       "#### 1. Fundamental Architectural Distinctions\n",
       "\n",
       "Naive RAG operates on a simple linear pipeline: a user query prompts a retrieval of relevant documents, which are directly passed to a generation module to synthesize a response. This tight coupling of Retrieval and Generation modules means minimal flexibility in updating or optimizing independent components. Gao et al. (2024) highlight that this yields inherent limitations—chiefly, shallow semantic understanding during retrieval, excessive redundancy and noise propagation into generation, and difficulties handling complex queries or diverse data types [1].\n",
       "\n",
       "Modular RAG restructures this process by fragmenting the workflow into discrete, specialized modules interconnected through orchestrated routing, scheduling, and fusion operators. Each functional component—such as retrievers, generators, context filters, or fusion modules—is encapsulated as an independent module that can be individually replaced, updated, or extended. Conceptually likened to LEGO blocks, this modularity supports complex workflows including conditional logic, iterative loops, and branching paths beyond the linear \"retrieve-then-generate\" paradigm [1][4].\n",
       "\n",
       "This paradigm shift results in several architectural advantages:\n",
       "\n",
       "- **Modularity and Replaceability**: Each module can be developed and optimized independently; for example, a domain-specific retriever optimized for financial documents can replace a generic one without impacting other modules.\n",
       "- **Complex Control Flow Support**: Routing and scheduling operators allow workflows to adapt dynamically based on intermediate outputs or external signals, facilitating multi-hop reasoning, query reformulation, or multimodal integration.\n",
       "- **Improved Handling of Noise and Redundancy**: By introducing pre- and post-retrieval filtering modules, Modular RAG reduces the entry of irrelevant or repetitive information into generation, addressing hallucination risks prevalent in Naive RAG systems [1][4].\n",
       "\n",
       "The flexibility to compose modules with specialized operators opens unexplored opportunities for system customization and experimentation, a notable contribution detailed in Gao et al. (2024).\n",
       "\n",
       "#### 2. Production-Level Benefits\n",
       "\n",
       "Dr. Han Ji-won underscores that Modular RAG’s modular design leads to tangible advantages in real-world deployment environments:\n",
       "\n",
       "- **Operational Flexibility**: In applications like chatbots for financial services, regulations or product details frequently change. Modular RAG allows updating or retraining only the retriever module responsible for indexing relevant documents, enabling rapid adaptation without retraining the generation model or retraining the entire pipeline [Interview].\n",
       "- **Maintainability and Debugging**: Isolating modules reduces the scope for troubleshooting, enabling engineers to pinpoint and resolve issues within a single module instead of combing through a monolithic pipeline. This markedly reduces downtime and operational overhead [4].\n",
       "- **Scalability and Extensibility**: Modular RAG simplifies incorporating new functionalities such as multi-modal inputs (images, tables), external knowledge bases, or advanced query parsers. As Jin et al. (2025) explain, modular frameworks like FlashRAG provide research-driven toolkits that facilitate rapid experimentation and deployment, aligning with production needs [2].\n",
       "- **Performance Optimization**: Modular architectures enable focused optimizations—such as advanced query understanding modules or noise-aware fusion mechanisms—improving overall response relevance and factual accuracy. These techniques help mitigate the hallucinatory tendencies often encountered in Naive RAG [1][4].\n",
       "\n",
       "In contrast, Naive RAG’s monolithic design hinders such agility, often requiring complete retraining or extensive retuning when changes are made to any component. Additionally, handling more sophisticated interaction patterns, multi-turn dialogue reasoning, or multi-modal data integration is cumbersome under Naive RAG.\n",
       "\n",
       "#### 3. Technical Nuances and Advanced Features\n",
       "\n",
       "Beyond mere component segmentation, Modular RAG integrates routing and scheduling operators enabling:\n",
       "\n",
       "- **Conditional Logic and Branching**: Different retrieval or generation strategies can be selected dynamically depending on the query type or intermediate results, offering tailored processing pipelines.\n",
       "- **Looping Mechanisms**: Iterative retrieval-feedback cycles improve query refinement and answer completeness for complex, multi-hop reasoning tasks.\n",
       "- **Fusion and Aggregation**: Outputs from multiple retrieval modules or knowledge sources can be merged intelligently before feeding to generation modules, enhancing the breadth and depth of information synthesis [1].\n",
       "\n",
       "This granularity supports not only stable production services but also research innovation, allowing testing of novel architectures without system-wide disruption. Jin et al. (2025) present FlashRAG as a concrete instantiation of such a modular toolkit, demonstrating 16 pre-implemented methods and extensive benchmarking support, distinguishing it from more rigid or heavy existing frameworks like LangChain or LlamaIndex [2].\n",
       "\n",
       "Dr. Han emphasizes these design principles as foundational to future-proofing RAG deployments, given the fast evolution of retrieval and generation techniques and their rising complexity.\n",
       "\n",
       "### Conclusion\n",
       "\n",
       "Dr. Han Ji-won's insights provide an authoritative perspective on the critical architectural and operational distinctions between Naive RAG and Modular RAG approaches. Modular RAG’s modular, highly configurable framework affords unparalleled flexibility, extensibility, and maintainability, crucial for deploying RAG systems at production scale in dynamic, real-world applications. This design overcomes core limitations of Naive RAG, supporting multi-modal integration, complex reasoning workflows, and efficient component upgrades essential in industrial contexts. The combination of theoretical foundations from Gao et al. (2024), practical toolkits like FlashRAG (Jin et al., 2025), and application-driven commentary cements Modular RAG as a transformative advancement in retrieval-augmented generation technologies.\n",
       "\n",
       "---\n",
       "\n",
       "### References\n",
       "\n",
       "[1] Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang, \"Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks,\" arXiv:2407.21059, 2024-07-26.  \n",
       "[2] Jiajie Jin et al., \"FlashRAG: A Modular Toolkit for Efficient Retrieval-Augmented Generation Research,\" arXiv:2405.13576, 2025-02-24.  \n",
       "[3] Jintao Liang et al., \"Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges,\" arXiv:2506.10408, 2025-06-12.  \n",
       "[4] 슈퍼브 블로그, \"RAG 기술의 진화: Naive에서 Modular까지 총정리,\" https://blog-ko.superb-ai.com/rag-technology-evolution-naive-advanced-modular-approaches/"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "snapshot = interview_graph.get_state(config=config)\n",
    "display(Markdown(snapshot.values[\"sections\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c74d83",
   "metadata": {},
   "source": [
    "## 인터뷰를 병렬로 실행 (map-reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "32cc2a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import TypedDict, Annotated\n",
    "\n",
    "\n",
    "class ResearchGraphState(TypedDict):\n",
    "    \"\"\"ResearchGraphState 상태 정의\"\"\"\n",
    "\n",
    "    topic: Annotated[str, \"연구할 주제\"]\n",
    "    max_analysts: Annotated[int, \"생성할 분석가의 최대 수\"]\n",
    "    human_analyst_feedback: Annotated[str, \"인간 분석가로부터 받은 피드백\"]\n",
    "    analysts: Annotated[list[Analyst], operator.add, \"분석가 목록\"]\n",
    "    sections: Annotated[list, operator.add, \"보고서 섹션 리스트\"]\n",
    "    introduction: Annotated[str, \"최종 보고서의 서론\"]\n",
    "    content: Annotated[str, \"최종 보고서의 본문 내용\"]\n",
    "    conclusion: Annotated[str, \"최종 보고서의 결론\"]\n",
    "    final_report: Annotated[str, \"완성된 최종 보고서\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e884dd3",
   "metadata": {},
   "source": [
    "### 인터뷰 작성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "5f6c2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Send\n",
    "\n",
    "\n",
    "def initiate_all_interviews(state: ResearchGraphState):\n",
    "    \"\"\"인터뷰를 진행합니다.\"\"\"\n",
    "\n",
    "    topic = state[\"topic\"]\n",
    "    analysts = state[\"analysts\"]\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\")\n",
    "\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "    else:\n",
    "        inputs = {\n",
    "            \"topic\": topic,\n",
    "            \"analyst\": analyst,\n",
    "            \"messages\": [\n",
    "                HumanMessage(\n",
    "                    content=f'그래서 당신이 이 주제에 대해서 글을 쓰고 있다고 했죠? \"{topic}\"'\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "        return [Send(\"conduct_interview\", inputs) for analyst in analysts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bcd9b3",
   "metadata": {},
   "source": [
    "### 최종 보고서 작성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "6f5a752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보고서 작성 지시사항\n",
    "report_writer_instructions = \"\"\"당신은 기술 문서 작성자로서 다음 주제 전반에 대한 보고서를 작성합니다:  \n",
    "\n",
    "주제: {topic}  \n",
    "\n",
    "당신에게는 분석가 팀이 있습니다. 각 분석가는 다음 두 가지를 수행했습니다:  \n",
    "\n",
    "1. 특정 하위 주제에 대한 전문가와의 인터뷰를 진행했습니다.  \n",
    "2. 조사 결과를 메모로 작성했습니다.  \n",
    "\n",
    "## 당신의 임무:  \n",
    "\n",
    "1. 분석가들이 작성한 메모 모음을 제공받게 됩니다.  \n",
    "2. 각 메모의 통찰력을 세심하게 검토하고 분석하십시오.  \n",
    "3. 모든 메모의 핵심 아이디어를 통합한 상세하고 포괄적인 요약으로 이러한 통찰력을 정리하십시오.  \n",
    "4. 각 메모의 주요 포인트를 아래 제공된 적절한 섹션에 정리하고, 각 섹션이 논리적이고 체계적으로 구성되도록 하십시오.  \n",
    "5. 보고서에 모든 필수 섹션을 포함시키되, 각 섹션의 헤더로 `### 섹션명`을 사용하십시오.  \n",
    "6. 각 섹션당 약 250단어를 목표로 하며, 심층적인 설명, 맥락 및 지원 세부사항을 제공하십시오.  \n",
    "\n",
    "## **고려할 섹션 (심층화를 위한 선택적 섹션 포함):**\n",
    "\n",
    "- **배경**: 방법론과 결과를 이해하는 데 필요한 이론적 기초, 핵심 개념 및 예비 정보.\n",
    "- **관련 연구**: 기존 연구의 개요 및 현재 연구와의 비교 또는 연관성.\n",
    "- **문제 정의**: 본 논문이 다루려는 연구 질문 또는 문제에 대한 공식적이고 정확한 정의.\n",
    "- **방법론(또는 방법)**: 연구에 사용된 방법, 알고리즘, 모델, 데이터 수집 과정 또는 실험 설정에 대한 상세한 설명.\n",
    "- **구현 세부사항**: 소프트웨어 프레임워크, 계산 자원 또는 매개변수 설정 등 방법론이나 모델 구현의 실무적 세부사항.\n",
    "- **실험**: 방법론 검증을 위해 사용된 실험 프로토콜, 데이터셋, 평가 지표, 절차 및 구성에 대한 설명.\n",
    "- **결과**: 통계적 표, 그래프, 도표 또는 질적 분석을 동반한 실험 결과 제시.\n",
    "\n",
    "## 보고서 서식 지정 방법:\n",
    "\n",
    "1. 마크다운 서식을 사용하십시오.\n",
    "2. 보고서 서문은 포함하지 마십시오.\n",
    "3. 소제목을 사용하지 마십시오.\n",
    "4. 보고서는 단일 제목 헤더로 시작하십시오: ## 인사이트\n",
    "5. 보고서에서 분석가 이름을 언급하지 마십시오.\n",
    "6. 메모 내 인용은 그대로 유지하고 괄호([1], [2] 등)로 주석을 달아 표시하십시오.\n",
    "7. 최종 통합 출처 목록을 작성하여 `## 출처` 헤더로 출처 섹션에 추가하십시오.\n",
    "8. 출처를 순서대로 나열하고 중복하지 마십시오.\n",
    "    [1] 출처 1\n",
    "    [2] 출처 2\n",
    "\n",
    "보고서 작성에 활용할 분석가 메모는 다음과 같습니다:\n",
    "\n",
    "<Context>\n",
    "{context}\n",
    "</Context>\"\"\"\n",
    "\n",
    "\n",
    "def write_report(state: ResearchGraphState):\n",
    "    \"\"\"보고서를 작성합니다.\"\"\"\n",
    "\n",
    "    formatted_sections = \"\\n\".join([section for section in state[\"sections\"]])\n",
    "    system_message = report_writer_instructions.format(\n",
    "        topic=state[\"topic\"],\n",
    "        context=formatted_sections,\n",
    "    )\n",
    "    response = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=\"이 메모들을 바탕으로 보고서를 작성하십시오.\")]\n",
    "    )\n",
    "    return {\"content\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "89d3e345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 서론과 결론 작성 지시사항\n",
    "intro_conclusion_instructions = \"\"\"다음 주제에 관한 보고서를 마무리하는 기술 문서 작성자입니다.\n",
    "\n",
    "주제: {topic}  \n",
    "\n",
    "보고서의 모든 섹션이 제공될 것입니다.\n",
    "당신의 임무는 간결하고 설득력 있는 서론 또는 결론 섹션을 작성하는 것입니다.\n",
    "사용자가 서론 작성 여부를 지시할 것입니다.\n",
    "두 섹션 모두 서두를 포함하지 마십시오.\n",
    "약 200단어를 목표로, 보고서의 모든 섹션을 간결하게 미리 소개(서론)하거나 요약(결론)하십시오.\n",
    "마크다운 서식을 사용하십시오.\n",
    "\n",
    "- 서론의 경우, 매력적인 제목을 만들고 제목에 # 헤더를 사용하십시오.\n",
    "- 서론의 경우, 섹션 헤더로 ## 서론(Introduction)을 사용하십시오.\n",
    "- 결론의 경우, 섹션 헤더로 ## 결론(Conclusion)을 사용하십시오.\n",
    "\n",
    "작성 시 고려해야 할 섹션은 다음과 같습니다: \n",
    "<Sections>\n",
    "{sections}\n",
    "</Sections>\"\"\"\n",
    "\n",
    "\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    \"\"\"서론 작성\"\"\"\n",
    "\n",
    "    formatted_sections = \"\\n\".join([section for section in state[\"sections\"]])\n",
    "    system_message = intro_conclusion_instructions.format(\n",
    "        topic=state[\"topic\"],\n",
    "        sections=formatted_sections,\n",
    "    )\n",
    "    response = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=\"보고서 서론을 작성하십시오.\")]\n",
    "    )\n",
    "    return {\"introduction\": response.content}\n",
    "\n",
    "\n",
    "# 결론 작성 함수 정의\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    \"\"\"결론 작성\"\"\"\n",
    "\n",
    "    formatted_sections = \"\\n\".join([section for section in state[\"sections\"]])\n",
    "    system_message = intro_conclusion_instructions.format(\n",
    "        topic=state[\"topic\"],\n",
    "        sections=formatted_sections,\n",
    "    )\n",
    "    response = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=\"보고서 결론을 작성하십시오.\")]\n",
    "    )\n",
    "    return {\"conclusion\": response.content}\n",
    "\n",
    "\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    \"\"\"최종 보고서 작성\"\"\"\n",
    "\n",
    "    content = state[\"content\"]\n",
    "\n",
    "    if content.startswith(\"## 통찰\"):\n",
    "        content = content.strip(\"## 통찰\")\n",
    "\n",
    "    if \"## 출처\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## 출처\\n\")\n",
    "        except:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    final_report = (\n",
    "        state[\"introduction\"]\n",
    "        + \"\\n\\n---\\n\\n## 주요 아이디어\\n\\n\"\n",
    "        + content\n",
    "        + \"\\n\\n---\\n\\n\"\n",
    "        + state[\"conclusion\"]\n",
    "    )\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## 출처\\n\" + sources\n",
    "\n",
    "    return {\"final_report\": final_report}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "eb9e5855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.constants import START, END\n",
    "from langchain_teddynote.graphs import visualize_graph\n",
    "\n",
    "builder = StateGraph(ResearchGraphState)\n",
    "\n",
    "builder.add_node(\"create_analysts\", create_analysts)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"conduct_interview\", interview_builder.compile())\n",
    "builder.add_node(\"write_report\", write_report)\n",
    "builder.add_node(\"write_introduction\", write_introduction)\n",
    "builder.add_node(\"write_conclusion\", write_conclusion)\n",
    "builder.add_node(\"finalize_report\", finalize_report)\n",
    "\n",
    "builder.add_edge(START, \"create_analysts\")\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\",\n",
    "    initiate_all_interviews,\n",
    "    [\n",
    "        \"create_analysts\",\n",
    "        \"conduct_interview\",\n",
    "    ],\n",
    ")\n",
    "builder.add_edge(\"conduct_interview\", \"write_report\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_introduction\")\n",
    "builder.add_edge(\"conduct_interview\", \"write_conclusion\")\n",
    "builder.add_edge(\n",
    "    [\n",
    "        \"write_conclusion\",\n",
    "        \"write_report\",\n",
    "        \"write_introduction\",\n",
    "    ],\n",
    "    \"finalize_report\",  # 보고서 최종 정리\n",
    ")\n",
    "builder.add_edge(\"finalize_report\", END)\n",
    "\n",
    "graph = builder.compile(\n",
    "    interrupt_before=[\"human_feedback\"], checkpointer=InMemorySaver()\n",
    ")\n",
    "\n",
    "visualize_graph(graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c01e1",
   "metadata": {},
   "source": [
    "### 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "00e78850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mcreate_analysts\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "affiliation='AI Research Institute' name='Sejin Park' role='Modular RAG Specialist' description='Sejin Park is an expert in retrieval-augmented generation models focusing on modular architectures. Their research explores how modular RAG differs structurally and functionally from simple RAG models, analyzing the benefits of component interchangeability and scalability.'\n",
      "affiliation='Tech Innovation Lab' name='Hana Kim' role='Production Deployment Analyst' description='Hana Kim specializes in the application and integration of AI models into production environments. She studies the practical advantages of modular RAG when deployed at scale, emphasizing maintainability, update efficiency, and robustness compared to traditional RAG.'\n",
      "affiliation='Natural Language Processing Center' name='Minho Lee' role='AI Systems Strategist' description='Minho Lee investigates strategic aspects of deploying advanced AI models like modular RAG to maximize operational benefits. He evaluates the overarching themes including adaptability, customization, and performance improvements modular RAG offers over simple RAG in real-world production settings.'\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36m__interrupt__\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import random_uuid\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=30,\n",
    "    configurable={\"thread_id\": random_uuid()},\n",
    ")\n",
    "\n",
    "topic = \"모듈형 RAG가 기존의 단순 RAG와 어떻게 다른지 설명하고, 이를 생산 단계에서 활용할 때의 이점을 제시하십시오.\"\n",
    "inputs = {\"topic\": topic, \"max_analysts\": 3}\n",
    "invoke_graph(graph, inputs, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9241e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task __input__ with path () wrote to unknown channel human_feedback, ignoring it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mhuman_feedback\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 분석가\n",
      "\n",
      "안녕하세요, 저는 김필이라고 합니다. Tech Innovators Inc.에서 기업가적 분석가로 일하면서 스타트업 관점에서 혁신적인 기술들이 비즈니스 모델에 미치는 영향을 집중 분석하고 있습니다. 모듈형 RAG와 단순 RAG의 차이점에 대해 깊이 탐구하는 중인데, 먼저 간단히 질문 하나 드려도 될까요? \n",
      "\n",
      "모듈형 RAG라고 하면, 일반적으로 Retrieval-Augmented Generation 시스템에서 검색과 생성 모듈을 독립적이고 교체 가능하게 분리한 구조를 의미하는데, 이런 구조가 기존의 Naive RAG에 비해 어떤 기술적 구성이 달라지는지 전문가님께서 실제로 적용해보신 구체적인 예가 있을까요? 예를 들어, 특정 도메인에서 모듈 교체가 어떻게 성능이나 확장성에 구체적으로 영향을 미쳤는지 궁금합니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"web\" url=\"https://brunch.co.kr/@vsongyev/28\" title=\"RAG 완벽 가이드: 세 가지 패러다임으로 보는 RAG - 브런치\">RAG의 세 가지 기술 패러다임에 대해 소개하고 어떤 과정으로 진행되는지, 각 패러다임에 대한 한계점과 여기서 얻을 수 있는 UX 인사이트까지 모두 알려 Drill 게요~! 이를 통해, 인공지능 모델의 추가 학습 없이도 외부 지식을 효과적으로 활용하여 보다 정확한 정보를 제공할 수 있으며, 환각 문제를 줄일 수 있습니다. 기본 RAG부터 모듈형 RAG까지 어떻게 구성되어 있는지 위의 이미지를 보시면 한눈에 확인할 수 있습니다. 아래의 이미지를 보시게 되면 기본 RAG에서 검색 전 과정과 검색 후 과정이 추가된 것을 보실 수 있습니다. 고급 RAG는 기존 데이터의 색인화와 검색 품질 개선에 중점을 두지만, 여전히 처리할 수 있는 데이터의 양과 종류에 제한이 있을 수 있습니다. 앞에서 다뤘던 기본 RAG는 검색(retrieval)과 생성(generation)의 두 가지 모듈로 구성할 수 있습니다.</Document>\n",
      "<Document source=\"web\" url=\"https://www.promptingguide.ai/kr/research/rag\" title=\"Retrieval Augmented Generation (RAG) for LLMs\">이 문서 (opens in a new tab)에서 소개되었듯이, RAG는 아래와 같이 정의될 수 있습니다: RAG 덕분에 언어 모델은 다시 학습할 필요 없이 검색 기반 생성을 통해 최신 정보에 기반한 신뢰할 수 있는 결과물을 만들 수 있습니다. 확장된 RAG 모듈에는 검색, 메모리, 융합, 라우팅, 예측, 태스크 어댑터 등 다양한 문제 해결을 위한 모듈이 포함될 수 있으며, 이들은 특정 문제 맥락에 맞게 조정될 수 있습니다. 검색 증강은 사전 학습, 파인 튜닝, 추론과 같은 다양한 단계에 적용될 수 있습니다. | 사람의 주석에 의존하지 않고도 다양한 차원(즉, 관련 및 집중된 문맥 단락을 식별하는 검색 시스템의 능력, 그러한 단락을 충실하게 활용하는 LLM의 능력 또는 생성 자체의 품질)을 평가하는 데 사용할 수 있는 일련의 메트릭입니다.</Document>\n",
      "<Document source=\"web\" url=\"https://www.gnict.org/blog/130/%EA%B8%80/%EB%8C%80%EA%B7%9C%EB%AA%A8-%EC%96%B8%EC%96%B4-%EB%AA%A8%EB%8D%B8%EC%9D%84-%EC%9C%84%ED%95%9C-%EA%B2%80%EC%83%89-%EC%A6%9D%EA%B0%95-%EC%83%9D%EC%84%B1rag-%EA%B8%B0%EC%88%A0-%ED%98%84%ED%99%A9/\" title=\"대규모 언어 모델을 위한 검색-증강 생성(RAG) 기술 현황 - 경남 ICT협회\">기본 RAG는 RAG 연구의 초기 방법론을 지칭하며, 전통적인 인덱싱, 검색 및 생성 과정을 포함하고 있습니다. 기본 RAG의 주요 구성요소로는 인덱싱(Indexing), 검색(Retrieve), 생성(Generation)이 있습니다: 고급 RAG는 크게 검색 전 절차(Pre-Retrieval Process), 검색 후 절차(Post-Retrieval Process), RAG 파이프라인 최적화(RAG Pipeline Optimization)의 3단계로 나누어볼 수 있습니다: 이 모델은 대규모 언어 모델(Large Language Models, LLM)을 사용하여 문서 청크를 바탕으로 질문을 구성하고, 이를 통해 정밀 조정 데이터를 생성할 수 있습니다. 키워드 기반 검색, 의미론적 검색, 벡터 검색과 같은 다양한 기술을 지능적으로 혼합하여 검색하는 방법으로, RAG 시스템이 다양한 질의 유형과 정보 요구에 적응할 수 있게 하여 가장 관련성 높고 맥락이 풍부한 정보를 일관되게 검색하는 것이 목표입니다. 이러한 하이브리드 검색은 검색 전략을 강화하고 RAG 파이프라인의 전반적인 성능을 향상시킬 수 있습니다.</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
      "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
      "increasing demands of application scenarios have driven the evolution of RAG,\n",
      "leading to the integration of advanced retrievers, LLMs and other complementary\n",
      "technologies, which in turn has amplified the intricacy of RAG systems.\n",
      "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process of\n",
      "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
      "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
      "decomposing complex RAG systems into independent modules and specialized\n",
      "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a more advanced\n",
      "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
      "extensive research, this paper further identifies prevalent RAG\n",
      "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
      "analysis of their respective implementation nuances. Modular RAG presents\n",
      "innovative opportunities for the conceptualization and deployment of RAG\n",
      "systems. Finally, the paper explores the potential emergence of new operators\n",
      "and paradigms, establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment of RAG\n",
      "technologies.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstract—Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of “retrieve-then-generate”. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patterns—linear, conditional,\n",
      "branching, and looping—and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Terms—Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]–[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLM’s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, \n",
      "</Content>\n",
      "</Document>\n",
      "<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2501.00353v1\" date=\"2024-12-31\" authors=\"Wanlong Liu, Junying Chen, Ke Ji, Li Zhou, Wenyu Chen, Benyou Wang\"/>\n",
      "<Title>\n",
      "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented Instructions\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-Augmented Generation (RAG) has emerged as a key paradigm for\n",
      "enhancing large language models (LLMs) by incorporating external knowledge.\n",
      "However, current RAG methods face two limitations: (1) they only cover limited\n",
      "RAG scenarios. (2) They suffer from limited task diversity due to the lack of a\n",
      "general RAG dataset. To address these limitations, we propose RAG-Instruct, a\n",
      "general method for synthesizing diverse and high-quality RAG instruction data\n",
      "based on any source corpus. Our approach leverages (1) five RAG paradigms,\n",
      "which encompass diverse query-document relationships, and (2) instruction\n",
      "simulation, which enhances instruction diversity and quality by utilizing the\n",
      "strengths of existing instruction datasets. Using this method, we construct a\n",
      "40K instruction dataset from Wikipedia, comprehensively covering diverse RAG\n",
      "scenarios and tasks. Experiments demonstrate that RAG-Instruct effectively\n",
      "enhances LLMs' RAG capabilities, achieving strong zero-shot performance and\n",
      "significantly outperforming various RAG baselines across a diverse set of\n",
      "tasks. RAG-Instruct is publicly available at\n",
      "https://github.com/FreedomIntelligence/RAG-Instruct.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n",
      "Instructions\n",
      "Wanlong Liu2†, Junying Chen1†, Ke Ji1, Li Zhou1, Wenyu Chen2, Benyou Wang1*\n",
      "1 The Chinese University of Hong Kong, Shenzhen,\n",
      "2 University of Electronic Science and Technology of China\n",
      "wangbenyou@cuhk.edu.cn\n",
      "Abstract\n",
      "Retrieval-Augmented Generation (RAG) has\n",
      "emerged as a key paradigm for enhancing large\n",
      "language models (LLMs) by incorporating\n",
      "external knowledge. However, current RAG\n",
      "methods face two limitations: (1) they only\n",
      "cover limited RAG scenarios. (2) They suffer\n",
      "from limited task diversity due to the lack\n",
      "of a general RAG dataset. To address these\n",
      "limitations, we propose RAG-Instruct, a\n",
      "general method for synthesizing diverse and\n",
      "high-quality RAG instruction data based on\n",
      "any source corpus. Our approach leverages\n",
      "(1) five RAG paradigms, which encompass\n",
      "diverse query-document relationships, and\n",
      "(2) instruction simulation, which enhances\n",
      "instruction diversity and quality by utilizing\n",
      "the strengths of existing instruction datasets.\n",
      "Using this method, we construct a 40K\n",
      "instruction dataset from Wikipedia, compre-\n",
      "hensively covering diverse RAG scenarios\n",
      "and tasks.\n",
      "Experiments demonstrate that\n",
      "RAG-Instruct effectively enhances LLMs’\n",
      "RAG capabilities, achieving strong zero-shot\n",
      "performance and significantly outperforming\n",
      "various RAG baselines across a diverse set of\n",
      "tasks. RAG-Instruct is publicly available at\n",
      "https://github.com/FreedomIntelligence/RAG-\n",
      "Instruct.\n",
      "1\n",
      "Introduction\n",
      "Retrieval-Augmented Generation (RAG) (Guu\n",
      "et al., 2020; Asai et al., 2024b) enhances large\n",
      "language models (LLMs) by integrating exter-\n",
      "nal knowledge through document retrieval, effec-\n",
      "tively reducing hallucinations and improving per-\n",
      "formance across diverse tasks (Asai et al., 2023;\n",
      "Jin et al., 2024; Lu et al., 2022; Liu et al., 2024a).\n",
      "Since retrievers are not perfect, and consider-\n",
      "able research has shown that noisy retrieval can\n",
      "adversely impact LLM performance (Petroni et al.,\n",
      "*Corresponding author. †Equal Contribution.\n",
      "2020; Shi et al., 2023; Maekawa et al., 2024), nu-\n",
      "merous studies have focused on enhancing the ro-\n",
      "bustness of RAG in handling noisy retrieval con-\n",
      "texts (Wei et al., 2024; Chan et al., 2024). On the\n",
      "one hand, some studies involve adaptive retrieval\n",
      "based on query analysis (Asai et al., 2024a; Jeong\n",
      "et al., 2024), or query reformulation (Chan et al.,\n",
      "2024; Ma et al., 2023) to enhance the robustness\n",
      "of LLM-based RAG systems. On the other hand,\n",
      "(Zhang et al., 2024; Liu et al., 2024b; Yoran et al.,\n",
      "2024) enhance the robustness of models’ naive\n",
      "RAG capabilities by training them to adapt to irrel-\n",
      "evant and noisy documents.\n",
      "However, existing RAG methods have two limi-\n",
      "tations: (1) Limited RAG scenarios. Real-world\n",
      "RAG scenarios are complex: Given the query, the\n",
      "retrieved information may directly contain the an-\n",
      "swer, offer partial help, or be helpless. Some an-\n",
      "swers can be obtained from a single document,\n",
      "while others require multi-hop reasoning across\n",
      "multiple documents. Our preliminary study demon-\n",
      "strates existing RAG methods cannot adequately\n",
      "handle all such scenarios (Chan et al., 2024; Asai\n",
      "et al., 2024a; Liu et al., 2024b).\n",
      "(2) Limited\n",
      "task diversity. Due to the lack of a general RAG\n",
      "dataset, most current RAG methods (Wei et al.,\n",
      "2024; Zhang et al., 2024) are fine-tuned on task-\n",
      "specific datasets (e.g., NQ (Kwiatkowski et al.,\n",
      "2019), TrivialQA (Joshi et al., 2017)), which suffer\n",
      "from limited question diversity and data volume.\n",
      "To address these limitations, we propose RAG-\n",
      "Instruct, a general method for synthesizing diverse\n",
      "and high-quality RAG instruction data based on any\n",
      "source corpus. Using this method, we construct a\n",
      "40K synthetic instruction dataset from Wikipedia\n",
      "tailored for RAG. Our method emphasizes the di-\n",
      "versity in two aspects:\n",
      "1. Defining diverse RAG paradigms: we define\n",
      "five RAG query paradigms that encompass\n",
      "various query-document relationships to adapt\n",
      "arXiv:2501.00353v1  [cs.CL]  31 Dec 2024\n",
      "to different RAG scenarios, considering both\n",
      "doc\n",
      "</Content>\n",
      "</Document>\n",
      "<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2505.13006v1\" date=\"2025-05-19\" authors=\"Yuyang Li, Philip J. M. Kerbusch, Raimon H. R. Pruim, Tobias Käfer\"/>\n",
      "<Title>\n",
      "Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Airports from the top 20 in terms of annual passengers are highly dynamic\n",
      "environments with thousands of flights daily, and they aim to increase the\n",
      "degree of automation. To contribute to this, we implemented a Conversational AI\n",
      "system that enables staff in an airport to communicate with flight information\n",
      "systems. This system not only answers standard airport queries but also\n",
      "resolves airport terminology, jargon, abbreviations, and dynamic questions\n",
      "involving reasoning. In this paper, we built three different\n",
      "Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL\n",
      "RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that\n",
      "traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally\n",
      "produced hallucinations, which is risky to airport safety. In contrast, SQL RAG\n",
      "and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with\n",
      "significantly fewer hallucinations. Moreover, Graph RAG was especially\n",
      "effective for questions that involved reasoning. Based on our observations, we\n",
      "thus recommend SQL RAG and Graph RAG are better for airport environments, due\n",
      "to fewer hallucinations and the ability to handle dynamic questions.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "arXiv:2505.13006v1  [cs.CL]  19 May 2025\n",
      "Evaluating the Performance of RAG Methods for Conversational AI in the\n",
      "Airport Domain\n",
      "Yuyang Li1, Philip J.M. Kerbusch2, Raimon H.R. Pruim2, Tobias Käfer1\n",
      "1Karlsruhe Institute of Technology, 2Royal Schiphol Group\n",
      "2Royal Schiphol Group, 1Karlsruhe Institute of Technology\n",
      "yuyang.li@kit.edu,\n",
      "tobias.kaefer@kit.edu\n",
      "Abstract\n",
      "Airports from the top 20 in terms of annual\n",
      "passengers are highly dynamic environments\n",
      "with thousands of flights daily, and they aim\n",
      "to increase the degree of automation. To con-\n",
      "tribute to this, we implemented a Conversa-\n",
      "tional AI system that enables staff in an air-\n",
      "port to communicate with flight information\n",
      "systems. This system not only answers stan-\n",
      "dard airport queries but also resolves airport ter-\n",
      "minology, jargon, abbreviations, and dynamic\n",
      "questions involving reasoning. In this paper, we\n",
      "built three different Retrieval-Augmented Gen-\n",
      "eration (RAG) methods, including traditional\n",
      "RAG, SQL RAG, and Knowledge Graph-based\n",
      "RAG (Graph RAG). Experiments showed that\n",
      "traditional RAG achieved 84.84% accuracy\n",
      "using BM25 + GPT-4 but occasionally pro-\n",
      "duced hallucinations, which is risky to airport\n",
      "safety. In contrast, SQL RAG and Graph RAG\n",
      "achieved 80.85% and 91.49% accuracy respec-\n",
      "tively, with significantly fewer hallucinations.\n",
      "Moreover, Graph RAG was especially effective\n",
      "for questions that involved reasoning. Based\n",
      "on our observations, we thus recommend SQL\n",
      "RAG and Graph RAG are better for airport en-\n",
      "vironments, due to fewer hallucinations and the\n",
      "ability to handle dynamic questions.\n",
      "1\n",
      "Introduction\n",
      "Amsterdam Airport Schiphol, one of the top 20\n",
      "airports in the world, ranked by annual passenger\n",
      "numbers, handles thousands of flights each day.\n",
      "These airports rely on staff like gate planners and\n",
      "apron controllers to access and update data across\n",
      "systems. For these employees, traditional database\n",
      "queries can be complex and time-consuming for\n",
      "some employees who are not query experts when\n",
      "they need flight information. A conversational AI\n",
      "system with a natural language query (NLQ) inter-\n",
      "face allows all employees to interact with systems\n",
      "naturally, asking questions like, “Which fights are\n",
      "at ramp D07?” and receiving instant answers. This\n",
      "improves productivity, and streamlines workflows,\n",
      "especially in high-pressure areas like at the gate,\n",
      "where less educated workers require access to up-\n",
      "to-date information. By replacing strict query for-\n",
      "mats with intuitive, real-time responses, conversa-\n",
      "tional AI enhances decision-making and efficiency,\n",
      "making it a suitable solution for dynamic environ-\n",
      "ments such as airports.\n",
      "Building such a system is challenging because\n",
      "flight data is stored by experts in tables using avi-\n",
      "ation abbreviations. We need our system to un-\n",
      "derstand these datasets to answer questions from\n",
      "the airport domain. Additionally, ensuring avia-\n",
      "tion safety is a major concern; the system must\n",
      "be safe and enable employees to perform accurate\n",
      "operations. We address those challenges using two\n",
      "research questions.\n",
      "The first question is how to handle flight data so\n",
      "that our system can answer different questions. We\n",
      "divided the questions into three types:\n",
      "• Straightforward questions: Questions that\n",
      "can be directly answered from the flight data.\n",
      "• Questions involving specialized airport jar-\n",
      "gon, abbreviations, and incomplete queries:\n",
      "Operators often use shorthand or omit con-\n",
      "text. Flight “KL0123” might be referred to as\n",
      "“0123” or “123,” while gate “C05” might be\n",
      "shortened to “C5.” Abbreviations like “KLM”\n",
      "for “KLM Royal Dutch Airlines” or “Delta”\n",
      "for “Delta Air Lines” are also common. Op-\n",
      "erators frequently ask short, incomplete ques-\n",
      "tions, e. g., “Which flights are at D04?” or\n",
      "“What is the gate for that Delta airline?” With-\n",
      "out resolving missing details such, these ques-\n",
      "tions cannot be answered.\n",
      "• Dynamic questions: Questions that involve\n",
      "additional calculations and reasoning, espe-\n",
      "cially related to time.\n",
      "Examples include\n",
      "“What is the connecting flight’s onramp time\n",
      "for D\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 분석가\n",
      "\n",
      "안녕하세요, 김필입니다. 스타트업 현장에서 기술과 비즈니스 모델의 접목을 연구하는 기업가적 분석가로서, 최신 AI 기술이 어떻게 산업 현장에 적용될 수 있는지 늘 관심을 갖고 있습니다.\n",
      "\n",
      "먼저, 말씀하신 주제에 대해 제 관점에서 설명을 드리겠습니다.\n",
      "\n",
      "모듈형 RAG(Modular Retrieval-Augmented Generation)와 기존의 Naive RAG는 기본적으로 정보 검색과 생성의 결합이라는 점에서는 동일하지만, 아키텍처와 운용 방식에서 큰 차이가 있습니다.\n",
      "\n",
      "Naive RAG는 검색과 생성 단계를 일체형으로 운영하며, 일반적으로 단일 검색 모듈이 호출되어 문서 집합에서 정보를 추출하고 이를 토대로 생성 모델이 텍스트를 만듭니다. 이 과정은 간단하지만, 모든 정보 소스와 질의 유형에 대해 동일한 검색 모듈을 적용하기 때문에 특정 도메인이나 목적에 최적화하기 어렵고, 확장성과 유연성에서 한계가 있습니다.\n",
      "\n",
      "반면, 모듈형 RAG는 검색 모듈을 여러 개의 독립된 모듈로 분리하여 필요에 따라 서로 다른 검색기능을 조합하거나 교체할 수 있습니다. 예를 들어, 금융 데이터용 검색 모듈과 의료 데이터용 검색 모듈을 각각 별도로 설계하고, 적합한 질의가 들어왔을 때 해당 모듈만 호출하는 식입니다. 또한 각각의 모듈은 도메인 특화된 인덱싱과 후보 문서 추출 방식을 적용해 검색 정밀도를 높일 수 있습니다.\n",
      "\n",
      "생산 현장에서 모듈형 RAG를 쓰는 이점은 크게 세 가지로 꼽을 수 있습니다.\n",
      "\n",
      "1. **높은 유연성**: 비즈니스 요구나 데이터 소스가 변화해도 개별 모듈만 수정하거나 추가하면 되므로, 시스템 전체를 다시 만들 필요가 없습니다. 실제로 저희 스타트업도 초기에 금융과 법률 정보를 동시에 다뤄야 했는데, 모듈형으로 설계해 두어 각 도메인별 인덱스와 검색 방식을 독립적으로 개선할 수 있었습니다.\n",
      "\n",
      "2. **성능 최적화가 용이**: 각 모듈이 담당하는 도메인의 특성을 반영해 인덱싱, 랭킹, 필터링 전략을 최적화함으로써 검색 결과의 질을 크게 끌어올릴 수 있었습니다. 이는 Naive RAG에서 흔히 겪는 ‘일반화된 검색 모듈의 성능 저하’ 문제를 방지하죠.\n",
      "\n",
      "3. **유지보수 및 확장성**: 신규 사업 분야를 추가하거나 기존 데이터를 분산해서 관리할 때, 모듈별 책임 분담이 되어 있어 유지보수가 쉬워지고, 신규 서비스 론칭 속도도 빨라졌습니다.\n",
      "\n",
      "혹시 지금까지 설명한 내용 중에서 특히 더 궁금하신 부분이 있으신가요? 아니면 실제 프로덕션 단계에서 발생하는 구체적인 도전 과제에 대해 말씀드려도 좋습니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_question\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 분석가\n",
      "\n",
      "안녕하세요, 저는 김필입니다. Tech Innovators Inc.에서 기업가적 분석가로 일하며, 기술과 비즈니스가 만나는 지점에서 혁신적인 통찰을 추구하고 있습니다. 모듈형 RAG(Modular Retrieval-Augmented Generation)와 기존의 Naive RAG의 차이점, 그리고 생산 환경에서 모듈형 RAG가 주는 이점에 대해 살펴보고 싶습니다.\n",
      "\n",
      "먼저, Naive RAG는 기본적으로 검색(검색된 정보를 기반으로 텍스트 생성)과 생성(생성 모델) 기능이 한 덩어리처럼 작동하는 구조입니다. 보통 전체 지식 기반을 단일 검색기와 생성기가 같이 활용하여 답변을 만듭니다.\n",
      "\n",
      "반면 모듈형 RAG는 '모듈'이라는 개별 컴포넌트 단위를 도입해 각 모듈이 특정 역할이나 데이터 영역을 담당합니다. 예를 들어, 특정 도메인에 특화된 검색 모듈과 생성 모듈을 따로 설계해 필요에 따라 연결하거나 교체할 수 있습니다. 이런 구조 덕분에 시스템 전체의 유연성과 유지보수성이 크게 향상됩니다.\n",
      "\n",
      "좀 더 구체적으로, 제가 관찰한 바에 따르면 한 스타트업에서 고객 지원 봇에 모듈형 RAG를 도입한 사례가 있습니다. 기존 Naive RAG는 다양한 고객 질문에 포괄적으로 대응했으나, 도메인 확장이 필요할 때마다 모델 재학습과 거대 데이터베이스 업데이트가 필수였습니다. 반면 모듈형 RAG는 각 고객 분류(예: 기술지원, 결제, 배송)에 맞춘 검색 모듈과 생성 모듈을 개별적으로 관리해, 새로운 도메인이 추가되어도 전체 시스템 영향도를 줄이고 빠르게 대응할 수 있었습니다.\n",
      "\n",
      "생산 단계에서 보았을 때, 가장 큰 이점은 다음과 같습니다:\n",
      "\n",
      "1. **유연한 확장성**: 특정 모듈만 추가·교체해 기능 확장이 가능해 초기 구축과 유지보수가 경제적입니다.\n",
      "2. **비용 효율성**: 필요한 부분만 집중적으로 최적화하거나 업데이트할 수 있어 불필요한 전체 시스템 재구성이 줄어듭니다.\n",
      "3. **성능 최적화**: 도메인 특화 모듈이 세밀한 검색과 맞춤형 생성을 가능하게 해 답변 정확도와 응답 속도를 개선합니다.\n",
      "4. **빠른 배포 및 테스트**: 변경 구간만 별도로 테스트해 시간과 비용을 절약하며, 실시간 서비스에 대한 위험을 낮춥니다.\n",
      "\n",
      "김필의 관점에서, 스타트업 같은 민첩성이 중요한 환경에서 모듈형 RAG는 특히 의미가 크다고 봅니다. 급변하는 시장 상황에도 빠르게 적응하는 능력이 비즈니스 경쟁력을 좌우하기 때문입니다.\n",
      "\n",
      "이 점에 대해 좀 더 구체적인 경험이나, 활용 사례 그리고 도입 시 주의해야 할 점이 궁금합니다. 혹시 추가로 상세히 설명해 주실 수 있나요?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"web\" url=\"https://g3lu.tistory.com/42\" title=\"RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG)\">## RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG) 본문 Retrieval-Augmented Geneartion(RAG)는 외부 지식 소스로부터 추가적인 정보를 통합하여 대형 언어 모델(LLM)을 개선하는 과정이다. 이는 중요한 정보를 놓칠 수 있게 된다. * **잘못된 정보 제공** : 유용한 정보를 제공하지 않고 검색된 내용을 단순히 반복하는 결과를 초래할 수 있으며, 일관성 없는 답변을 뱉는 경우가 발생한다. Advanced RAG는 Naive RAG 방식에서 직면하고 있는 문제를 해결하기 위해 고안되었다. 위의 세 가지 고찰을 보완하기 위해 **Pre-Retreival 및 Post-Retrieval**를 기존 RAG 아키텍처에 추가한 것이 Advanced RAG이다. 하지만 검색된 청크들이 간혹 중복이 되거나 의미 없는 정보를 담는 경우 발생하게 되는데, 이는 LLM이 주어진 컨텍스트를 처리하는 방식에 영향을 미칠 수 있다. * **Prompt Compression** : 검색된 정보에 Noisy가 많을 수 있으므로, LLM에 태우기전에 관련 없는 정보를 압축하고 길이를 줄이는 것도 중요하다.</Document>\n",
      "<Document source=\"web\" url=\"https://brunch.co.kr/@@aPda/338\" title=\"12화 복잡한 RAG 분해하기-Modular RAG란? - 브런치\">모듈러 RAG는 복잡한 RAG 시스템을 마치 레고 블록처럼 여러 개의 독립적인 모듈로 나누어, 필요에 따라 각 모듈을 교체하거나 조합할 수 있게 만들어 줍니다. **모듈러 RAG는 각 모듈을 독립적으로 설계하고 이를 필요에 따라 교체하거나 결합할 수 있도록 합니다.** 예를 들어, 새로운 임베딩 모델이 등장하거나 기존의 벡터 DB가 성능을 더 높일 수 있다고 판단되면, 해당 모듈만 교체하는 방식으로 시스템을 최적화할 수 있습니다. **모듈러 RAG는 또한 동적이고 적응 가능한 시스템을 가능하게 만듭니다.** 예를 들어, 특정 작업이나 데이터 소스에 맞는 모듈을 선택하고, 이를 조합함으로써 효율적인 RAG 시스템을 구성할 수 있습니다. **또한, 모듈러 RAG는 시스템의 복잡도를 낮추고, 성능을 더욱 향상할 수 있는 가능성을 제공합니다.** 예를 들어, 특정 데이터 소스에 더 적합한 벡터 DB를 찾거나, 최신의 임베딩 모델을 도입하는 등의 작업을 통해 성능을 지속적으로 개선할 수 있습니다.</Document>\n",
      "<Document source=\"web\" url=\"https://brunch.co.kr/@vsongyev/28\" title=\"RAG 완벽 가이드: 세 가지 패러다임으로 보는 RAG - 브런치\">RAG의 세 가지 기술 패러다임에 대해 소개하고 어떤 과정으로 진행되는지, 각 패러다임에 대한 한계점과 여기서 얻을 수 있는 UX 인사이트까지 모두 알려 Drill 게요~! 이를 통해, 인공지능 모델의 추가 학습 없이도 외부 지식을 효과적으로 활용하여 보다 정확한 정보를 제공할 수 있으며, 환각 문제를 줄일 수 있습니다. 기본 RAG부터 모듈형 RAG까지 어떻게 구성되어 있는지 위의 이미지를 보시면 한눈에 확인할 수 있습니다. 아래의 이미지를 보시게 되면 기본 RAG에서 검색 전 과정과 검색 후 과정이 추가된 것을 보실 수 있습니다. 고급 RAG는 기존 데이터의 색인화와 검색 품질 개선에 중점을 두지만, 여전히 처리할 수 있는 데이터의 양과 종류에 제한이 있을 수 있습니다. 앞에서 다뤘던 기본 RAG는 검색(retrieval)과 생성(generation)의 두 가지 모듈로 구성할 수 있습니다.</Document>\n",
      "==================================================\n",
      "MuPDF error: syntax error: expected object number\n",
      "\n",
      "MuPDF error: format error: object is not a stream\n",
      "\n",
      "MuPDF error: syntax error: no XObject subtype specified\n",
      "\n",
      "MuPDF error: format error: object is not a stream\n",
      "\n",
      "MuPDF error: format error: object is not a stream\n",
      "\n",
      "MuPDF error: format error: object is not a stream\n",
      "\n",
      "MuPDF error: syntax error: no XObject subtype specified\n",
      "\n",
      "MuPDF error: syntax error: no XObject subtype specified\n",
      "\n",
      "MuPDF error: syntax error: no XObject subtype specified\n",
      "\n",
      "MuPDF error: syntax error: no XObject subtype specified\n",
      "\n",
      "MuPDF error: syntax error: no XObject subtype specified\n",
      "\n",
      "MuPDF error: syntax error: no XObject subtype specified\n",
      "\n",
      "MuPDF error: syntax error: no XObject subtype specified\n",
      "\n",
      "MuPDF error: syntax error: no XObject subtype specified\n",
      "\n",
      "MuPDF error: syntax error: no XObject subtype specified\n",
      "\n",
      "MuPDF error: syntax error: no XObject subtype specified\n",
      "\n",
      "MuPDF error: syntax error: no XObject subtype specified\n",
      "\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_arxiv\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2407.21059v1\" date=\"2024-07-26\" authors=\"Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\"/>\n",
      "<Title>\n",
      "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\n",
      "of Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\n",
      "increasing demands of application scenarios have driven the evolution of RAG,\n",
      "leading to the integration of advanced retrievers, LLMs and other complementary\n",
      "technologies, which in turn has amplified the intricacy of RAG systems.\n",
      "However, the rapid advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process of\n",
      "\"retrieve-then-generate\". In this context, this paper examines the limitations\n",
      "of the existing RAG paradigm and introduces the modular RAG framework. By\n",
      "decomposing complex RAG systems into independent modules and specialized\n",
      "operators, it facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a more advanced\n",
      "design that integrates routing, scheduling, and fusion mechanisms. Drawing on\n",
      "extensive research, this paper further identifies prevalent RAG\n",
      "patterns-linear, conditional, branching, and looping-and offers a comprehensive\n",
      "analysis of their respective implementation nuances. Modular RAG presents\n",
      "innovative opportunities for the conceptualization and deployment of RAG\n",
      "systems. Finally, the paper explores the potential emergence of new operators\n",
      "and paradigms, establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment of RAG\n",
      "technologies.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "1\n",
      "Modular RAG: Transforming RAG Systems into\n",
      "LEGO-like Reconfigurable Frameworks\n",
      "Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\n",
      "Abstract—Retrieval-augmented\n",
      "Generation\n",
      "(RAG)\n",
      "has\n",
      "markedly enhanced the capabilities of Large Language Models\n",
      "(LLMs) in tackling knowledge-intensive tasks. The increasing\n",
      "demands of application scenarios have driven the evolution\n",
      "of RAG, leading to the integration of advanced retrievers,\n",
      "LLMs and other complementary technologies, which in turn\n",
      "has amplified the intricacy of RAG systems. However, the rapid\n",
      "advancements are outpacing the foundational RAG paradigm,\n",
      "with many methods struggling to be unified under the process\n",
      "of “retrieve-then-generate”. In this context, this paper examines\n",
      "the limitations of the existing RAG paradigm and introduces\n",
      "the modular RAG framework. By decomposing complex RAG\n",
      "systems into independent modules and specialized operators, it\n",
      "facilitates a highly reconfigurable framework. Modular RAG\n",
      "transcends the traditional linear architecture, embracing a\n",
      "more advanced design that integrates routing, scheduling, and\n",
      "fusion mechanisms. Drawing on extensive research, this paper\n",
      "further identifies prevalent RAG patterns—linear, conditional,\n",
      "branching, and looping—and offers a comprehensive analysis\n",
      "of their respective implementation nuances. Modular RAG\n",
      "presents\n",
      "innovative\n",
      "opportunities\n",
      "for\n",
      "the\n",
      "conceptualization\n",
      "and deployment of RAG systems. Finally, the paper explores\n",
      "the potential emergence of new operators and paradigms,\n",
      "establishing a solid theoretical foundation and a practical\n",
      "roadmap for the continued evolution and practical deployment\n",
      "of RAG technologies.\n",
      "Index Terms—Retrieval-augmented generation, large language\n",
      "model, modular system, information retrieval\n",
      "I. INTRODUCTION\n",
      "L\n",
      "ARGE Language Models (LLMs) have demonstrated\n",
      "remarkable capabilities, yet they still face numerous\n",
      "challenges, such as hallucination and the lag in information up-\n",
      "dates [1]. Retrieval-augmented Generation (RAG), by access-\n",
      "ing external knowledge bases, provides LLMs with important\n",
      "contextual information, significantly enhancing their perfor-\n",
      "mance on knowledge-intensive tasks [2]. Currently, RAG, as\n",
      "an enhancement method, has been widely applied in various\n",
      "practical application scenarios, including knowledge question\n",
      "answering, recommendation systems, customer service, and\n",
      "personal assistants. [3]–[6]\n",
      "During the nascent stages of RAG , its core framework is\n",
      "constituted by indexing, retrieval, and generation, a paradigm\n",
      "referred to as Naive RAG [7]. However, as the complexity\n",
      "of tasks and the demands of applications have escalated, the\n",
      "Yunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\n",
      "Systems, Tongji University, Shanghai, 201210, China.\n",
      "Yun Xiong is with Shanghai Key Laboratory of Data Science, School of\n",
      "Computer Science, Fudan University, Shanghai, 200438, China.\n",
      "Meng Wang and Haofen Wang are with College of Design and Innovation,\n",
      "Tongji University, Shanghai, 20092, China. (Corresponding author: Haofen\n",
      "Wang. E-mail: carter.whfcarter@gmail.com)\n",
      "limitations of Naive RAG have become increasingly apparent.\n",
      "As depicted in Figure 1, it predominantly hinges on the\n",
      "straightforward similarity of chunks, result in poor perfor-\n",
      "mance when confronted with complex queries and chunks with\n",
      "substantial variability. The primary challenges of Naive RAG\n",
      "include: 1) Shallow Understanding of Queries. The semantic\n",
      "similarity between a query and document chunk is not always\n",
      "highly consistent. Relying solely on similarity calculations\n",
      "for retrieval lacks an in-depth exploration of the relationship\n",
      "between the query and the document [8]. 2) Retrieval Re-\n",
      "dundancy and Noise. Feeding all retrieved chunks directly\n",
      "into LLMs is not always beneficial. Research indicates that\n",
      "an excess of redundant and noisy information may interfere\n",
      "with the LLM’s identification of key information, thereby\n",
      "increasing the risk of generating erroneous and hallucinated\n",
      "responses. [9]\n",
      "To overcome the aforementioned limitations, \n",
      "</Content>\n",
      "</Document>\n",
      "<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2501.00353v1\" date=\"2024-12-31\" authors=\"Wanlong Liu, Junying Chen, Ke Ji, Li Zhou, Wenyu Chen, Benyou Wang\"/>\n",
      "<Title>\n",
      "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented Instructions\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Retrieval-Augmented Generation (RAG) has emerged as a key paradigm for\n",
      "enhancing large language models (LLMs) by incorporating external knowledge.\n",
      "However, current RAG methods face two limitations: (1) they only cover limited\n",
      "RAG scenarios. (2) They suffer from limited task diversity due to the lack of a\n",
      "general RAG dataset. To address these limitations, we propose RAG-Instruct, a\n",
      "general method for synthesizing diverse and high-quality RAG instruction data\n",
      "based on any source corpus. Our approach leverages (1) five RAG paradigms,\n",
      "which encompass diverse query-document relationships, and (2) instruction\n",
      "simulation, which enhances instruction diversity and quality by utilizing the\n",
      "strengths of existing instruction datasets. Using this method, we construct a\n",
      "40K instruction dataset from Wikipedia, comprehensively covering diverse RAG\n",
      "scenarios and tasks. Experiments demonstrate that RAG-Instruct effectively\n",
      "enhances LLMs' RAG capabilities, achieving strong zero-shot performance and\n",
      "significantly outperforming various RAG baselines across a diverse set of\n",
      "tasks. RAG-Instruct is publicly available at\n",
      "https://github.com/FreedomIntelligence/RAG-Instruct.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "RAG-Instruct: Boosting LLMs with Diverse Retrieval-Augmented\n",
      "Instructions\n",
      "Wanlong Liu2†, Junying Chen1†, Ke Ji1, Li Zhou1, Wenyu Chen2, Benyou Wang1*\n",
      "1 The Chinese University of Hong Kong, Shenzhen,\n",
      "2 University of Electronic Science and Technology of China\n",
      "wangbenyou@cuhk.edu.cn\n",
      "Abstract\n",
      "Retrieval-Augmented Generation (RAG) has\n",
      "emerged as a key paradigm for enhancing large\n",
      "language models (LLMs) by incorporating\n",
      "external knowledge. However, current RAG\n",
      "methods face two limitations: (1) they only\n",
      "cover limited RAG scenarios. (2) They suffer\n",
      "from limited task diversity due to the lack\n",
      "of a general RAG dataset. To address these\n",
      "limitations, we propose RAG-Instruct, a\n",
      "general method for synthesizing diverse and\n",
      "high-quality RAG instruction data based on\n",
      "any source corpus. Our approach leverages\n",
      "(1) five RAG paradigms, which encompass\n",
      "diverse query-document relationships, and\n",
      "(2) instruction simulation, which enhances\n",
      "instruction diversity and quality by utilizing\n",
      "the strengths of existing instruction datasets.\n",
      "Using this method, we construct a 40K\n",
      "instruction dataset from Wikipedia, compre-\n",
      "hensively covering diverse RAG scenarios\n",
      "and tasks.\n",
      "Experiments demonstrate that\n",
      "RAG-Instruct effectively enhances LLMs’\n",
      "RAG capabilities, achieving strong zero-shot\n",
      "performance and significantly outperforming\n",
      "various RAG baselines across a diverse set of\n",
      "tasks. RAG-Instruct is publicly available at\n",
      "https://github.com/FreedomIntelligence/RAG-\n",
      "Instruct.\n",
      "1\n",
      "Introduction\n",
      "Retrieval-Augmented Generation (RAG) (Guu\n",
      "et al., 2020; Asai et al., 2024b) enhances large\n",
      "language models (LLMs) by integrating exter-\n",
      "nal knowledge through document retrieval, effec-\n",
      "tively reducing hallucinations and improving per-\n",
      "formance across diverse tasks (Asai et al., 2023;\n",
      "Jin et al., 2024; Lu et al., 2022; Liu et al., 2024a).\n",
      "Since retrievers are not perfect, and consider-\n",
      "able research has shown that noisy retrieval can\n",
      "adversely impact LLM performance (Petroni et al.,\n",
      "*Corresponding author. †Equal Contribution.\n",
      "2020; Shi et al., 2023; Maekawa et al., 2024), nu-\n",
      "merous studies have focused on enhancing the ro-\n",
      "bustness of RAG in handling noisy retrieval con-\n",
      "texts (Wei et al., 2024; Chan et al., 2024). On the\n",
      "one hand, some studies involve adaptive retrieval\n",
      "based on query analysis (Asai et al., 2024a; Jeong\n",
      "et al., 2024), or query reformulation (Chan et al.,\n",
      "2024; Ma et al., 2023) to enhance the robustness\n",
      "of LLM-based RAG systems. On the other hand,\n",
      "(Zhang et al., 2024; Liu et al., 2024b; Yoran et al.,\n",
      "2024) enhance the robustness of models’ naive\n",
      "RAG capabilities by training them to adapt to irrel-\n",
      "evant and noisy documents.\n",
      "However, existing RAG methods have two limi-\n",
      "tations: (1) Limited RAG scenarios. Real-world\n",
      "RAG scenarios are complex: Given the query, the\n",
      "retrieved information may directly contain the an-\n",
      "swer, offer partial help, or be helpless. Some an-\n",
      "swers can be obtained from a single document,\n",
      "while others require multi-hop reasoning across\n",
      "multiple documents. Our preliminary study demon-\n",
      "strates existing RAG methods cannot adequately\n",
      "handle all such scenarios (Chan et al., 2024; Asai\n",
      "et al., 2024a; Liu et al., 2024b).\n",
      "(2) Limited\n",
      "task diversity. Due to the lack of a general RAG\n",
      "dataset, most current RAG methods (Wei et al.,\n",
      "2024; Zhang et al., 2024) are fine-tuned on task-\n",
      "specific datasets (e.g., NQ (Kwiatkowski et al.,\n",
      "2019), TrivialQA (Joshi et al., 2017)), which suffer\n",
      "from limited question diversity and data volume.\n",
      "To address these limitations, we propose RAG-\n",
      "Instruct, a general method for synthesizing diverse\n",
      "and high-quality RAG instruction data based on any\n",
      "source corpus. Using this method, we construct a\n",
      "40K synthetic instruction dataset from Wikipedia\n",
      "tailored for RAG. Our method emphasizes the di-\n",
      "versity in two aspects:\n",
      "1. Defining diverse RAG paradigms: we define\n",
      "five RAG query paradigms that encompass\n",
      "various query-document relationships to adapt\n",
      "arXiv:2501.00353v1  [cs.CL]  31 Dec 2024\n",
      "to different RAG scenarios, considering both\n",
      "doc\n",
      "</Content>\n",
      "</Document>\n",
      "<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2501.15098v1\" date=\"2025-01-25\" authors=\"Zihang Li, Yangdong Ruan, Wenjun Liu, Zhengyang Wang, Tong Yang\"/>\n",
      "<Title>\n",
      "CFT-RAG: An Entity Tree Based Retrieval Augmented Generation Algorithm With Cuckoo Filter\n",
      "</Title>\n",
      "\n",
      "<Summary>\n",
      "Although retrieval-augmented generation(RAG) significantly improves\n",
      "generation quality by retrieving external knowledge bases and integrating\n",
      "generated content, it faces computational efficiency bottlenecks, particularly\n",
      "in knowledge retrieval tasks involving hierarchical structures for Tree-RAG.\n",
      "This paper proposes a Tree-RAG acceleration method based on the improved Cuckoo\n",
      "Filter, which optimizes entity localization during the retrieval process to\n",
      "achieve significant performance improvements. Tree-RAG effectively organizes\n",
      "entities through the introduction of a hierarchical tree structure, while the\n",
      "Cuckoo Filter serves as an efficient data structure that supports rapid\n",
      "membership queries and dynamic updates. The experiment results demonstrate that\n",
      "our method is much faster than naive Tree-RAG while maintaining high levels of\n",
      "generative quality. When the number of trees is large, our method is hundreds\n",
      "of times faster than naive Tree-RAG. Our work is available at\n",
      "https://github.com/TUPYP7180/CFT-RAG-2025.\n",
      "</Summary>\n",
      "\n",
      "<Content>\n",
      "CFT-RAG: An Entity Tree Based Retrieval Augmented Generation Algorithm\n",
      "With Cuckoo Filter\n",
      "Zihang Li 1 Yangdong Ruan 2 Wenjun Liu 3 Zhengyang Wang 4 Tong Yang 5\n",
      "Abstract\n",
      "Although retrieval-augmented generation(RAG)\n",
      "significantly improves generation quality by re-\n",
      "trieving external knowledge bases and integrat-\n",
      "ing generated content, it faces computational effi-\n",
      "ciency bottlenecks, particularly in knowledge re-\n",
      "trieval tasks involving hierarchical structures for\n",
      "Tree-RAG. This paper proposes a Tree-RAG ac-\n",
      "celeration method based on the improved Cuckoo\n",
      "Filter, which optimizes entity localization dur-\n",
      "ing the retrieval process to achieve significant\n",
      "performance improvements.\n",
      "Tree-RAG effec-\n",
      "tively organizes entities through the introduc-\n",
      "tion of a hierarchical tree structure, while the\n",
      "Cuckoo Filter serves as an efficient data struc-\n",
      "ture that supports rapid membership queries\n",
      "and dynamic updates. The experiment results\n",
      "demonstrate that our method is much faster than\n",
      "naive Tree-RAG while maintaining high lev-\n",
      "els of generative quality.\n",
      "When the number\n",
      "of trees is large, our method is hundreds of\n",
      "times faster than naive Tree-RAG. Our work is\n",
      "available at https://github.com/TUPYP7180/CFT-\n",
      "RAG-2025.\n",
      "1. Introduction\n",
      "In the era of information explosion, Retrieval-Augmented\n",
      "Generation (RAG), a technology integrating retrieval\n",
      "mechanisms with generative models, has gained significant\n",
      "attention. It allows models to draw on external knowledge\n",
      "bases during text generation, effectively overcoming the\n",
      "limitations of traditional generative models in knowledge-\n",
      "intensive tasks (Lewis et al., 2020). The knowledge base, a\n",
      "vital part of the RAG system, stores a wealth of structured\n",
      "and unstructured knowledge, acting as the main source\n",
      "of external information for the model. However, with the\n",
      "1Peking University\n",
      "2Beihang University\n",
      "3Peking Univer-\n",
      "sity 4Peking University 5Peking University.\n",
      "Correspondence\n",
      "to:\n",
      "Tong Yang <yangtong@pku.edu.cn>, Zihang Li <liz-\n",
      "ihang@stu.pku.edu.cn>.\n",
      "Preprint.\n",
      "continuous expansion of the knowledge base and the rapid\n",
      "pace of knowledge update, the challenge of efficiently\n",
      "retrieving relevant and accurate information from it has\n",
      "become a major obstacle to improving the performance of\n",
      "RAG system. Enhancing the retrieval speed and accuracy\n",
      "of the knowledge base is crucial for boosting the overall\n",
      "performance of the RAG system. Faster and more accurate\n",
      "retrieval enables the model to access relevant knowledge\n",
      "promptly, improving response speed and the quality of\n",
      "generated content. In contrast, inefficient or inaccurate\n",
      "retrieval can result in incorrect or irrelevant outputs,\n",
      "degrading user experience and system usability. Therefore,\n",
      "exploring ways to optimize the knowledge base retrieval\n",
      "mechanism is of great theoretical and practical importance,\n",
      "and this paper will focus on this key issue.\n",
      "Knowledge\n",
      "bases\n",
      "in\n",
      "Retrieval-Augmented\n",
      "Genera-\n",
      "tion (RAG) systems are mainly of three types: text-based,\n",
      "graph-based, and tree-based.\n",
      "Text-based ones store\n",
      "information as text, easy to manage but slow in retrieval due\n",
      "to complex language processing. Graph-based knowledge\n",
      "bases represent knowledge as graphs, excelling in handling\n",
      "complex relationships with relatively fast retrieval for\n",
      "certain queries, thanks to graph neural networks. Tree-\n",
      "based knowledge bases structure knowledge hierarchically.\n",
      "Despite text-based and graph-based knowledge bases\n",
      "having made good progress, the retrieval speed of all three\n",
      "types, especially tree-based ones, needs improvement.\n",
      "For RAG systems to provide faster and more accurate\n",
      "responses, optimizing retrieval from these knowledge bases,\n",
      "particularly tree-RAG, is a key research challenge.\n",
      "Tree-RAG, an extension of RAG, improves on tradi-\n",
      "tional RAG frameworks by using a hierarchical tree\n",
      "structure to organize the retrieved knowledge, thus pro-\n",
      "viding richer context and capturing complex relationships\n",
      "among entities.\n",
      "In Tree-RAG, entities are arranged\n",
      "hierarchically, allowing the retrieval process to more\n",
      "effec\n",
      "</Content>\n",
      "</Document>\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36msearch_web\u001b[0m in [\u001b[1;33mconduct_interview\u001b[0m] 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "<Document source=\"web\" url=\"https://g3lu.tistory.com/42\" title=\"RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG)\">## RAG의 패러다임(Naive RAG, Advanced RAG, Modular RAG) 본문 Retrieval-Augmented Geneartion(RAG)는 외부 지식 소스로부터 추가적인 정보를 통합하여 대형 언어 모델(LLM)을 개선하는 과정이다. 이는 중요한 정보를 놓칠 수 있게 된다. * **잘못된 정보 제공** : 유용한 정보를 제공하지 않고 검색된 내용을 단순히 반복하는 결과를 초래할 수 있으며, 일관성 없는 답변을 뱉는 경우가 발생한다. Advanced RAG는 Naive RAG 방식에서 직면하고 있는 문제를 해결하기 위해 고안되었다. 위의 세 가지 고찰을 보완하기 위해 **Pre-Retreival 및 Post-Retrieval**를 기존 RAG 아키텍처에 추가한 것이 Advanced RAG이다. 하지만 검색된 청크들이 간혹 중복이 되거나 의미 없는 정보를 담는 경우 발생하게 되는데, 이는 LLM이 주어진 컨텍스트를 처리하는 방식에 영향을 미칠 수 있다. * **Prompt Compression** : 검색된 정보에 Noisy가 많을 수 있으므로, LLM에 태우기전에 관련 없는 정보를 압축하고 길이를 줄이는 것도 중요하다.</Document>\n",
      "<Document source=\"web\" url=\"https://brunch.co.kr/@vsongyev/28\" title=\"RAG 완벽 가이드: 세 가지 패러다임으로 보는 RAG - 브런치\">RAG의 세 가지 기술 패러다임에 대해 소개하고 어떤 과정으로 진행되는지, 각 패러다임에 대한 한계점과 여기서 얻을 수 있는 UX 인사이트까지 모두 알려 Drill 게요~! 이를 통해, 인공지능 모델의 추가 학습 없이도 외부 지식을 효과적으로 활용하여 보다 정확한 정보를 제공할 수 있으며, 환각 문제를 줄일 수 있습니다. 기본 RAG부터 모듈형 RAG까지 어떻게 구성되어 있는지 위의 이미지를 보시면 한눈에 확인할 수 있습니다. 아래의 이미지를 보시게 되면 기본 RAG에서 검색 전 과정과 검색 후 과정이 추가된 것을 보실 수 있습니다. 고급 RAG는 기존 데이터의 색인화와 검색 품질 개선에 중점을 두지만, 여전히 처리할 수 있는 데이터의 양과 종류에 제한이 있을 수 있습니다. 앞에서 다뤘던 기본 RAG는 검색(retrieval)과 생성(generation)의 두 가지 모듈로 구성할 수 있습니다.</Document>\n",
      "<Document source=\"web\" url=\"https://www.redhat.com/ko/topics/ai/what-is-retrieval-augmented-generation\" title=\"검색 증강 생성(RAG)이란? 생성형 AI의 정확도를 높이는 기술 - Red Hat\">RAG의 장점과 활용 사례 · 정확성. RAG는 사용자가 이러한 주장을 검증할 수 있도록 인용할 수 있는 소스를 LLM에 제공합니다. · 비용 효율성 · 개발자 제어</Document>\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'fitz' has no attribute 'fitz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_community/utilities/arxiv.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    221\u001b[39m                 \u001b[38;5;28;01mwith\u001b[39;00m fitz.open(doc_file_name) \u001b[38;5;28;01mas\u001b[39;00m doc_file:\n\u001b[32m    222\u001b[39m                     text: str = \u001b[33m\"\"\u001b[39m.join(page.get_text() \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;28;01min\u001b[39;00m doc_file)\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m             \u001b[38;5;28;01mexcept\u001b[39;00m (FileNotFoundError, fitz.fitz.FileDataError) \u001b[38;5;28;01mas\u001b[39;00m f_ex:\n\u001b[32m    224\u001b[39m                 logger.debug(f_ex)\n",
      "\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/pymupdf/__init__.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[39m\n\u001b[32m   3032\u001b[39m                     self.page_count2 = extra.page_count_fz\n\u001b[32m   3033\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3034\u001b[39m             JM_mupdf_show_errors = JM_mupdf_show_errors_old\n",
      "\u001b[31mFileNotFoundError\u001b[39m: no such file: './2501.00353v1.RAG_Instruct__Boosting_LLMs_with_Diverse_Retrieval_Augmented_Instructions.pdf'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[203]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuman_feedback\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_teddynote/messages.py:409\u001b[39m, in \u001b[36minvoke_graph\u001b[39m\u001b[34m(graph, inputs, config, node_names, callback)\u001b[39m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace[-\u001b[32m1\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespace) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mroot graph\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# subgraphs=True 를 통해 서브그래프의 출력도 포함\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    411\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# node_names가 비어있지 않은 경우에만 필터링\u001b[39;49;00m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnode_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_names\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2657\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2655\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2656\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2657\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2662\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2663\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2664\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2666\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2667\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:253\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:511\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    509\u001b[39m                 interrupts.append(exc)\n\u001b[32m    510\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    512\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/_executor.py:81\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2657\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2655\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2656\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2657\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2662\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2663\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2664\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2666\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2667\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:253\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    258\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:511\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    509\u001b[39m                 interrupts.append(exc)\n\u001b[32m    510\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    512\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/_executor.py:81\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     79\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:657\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:401\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    399\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[188]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36msearch_arxiv\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m      4\u001b[39m llm_with_structured = llm.with_structured_output(SearchQuery)\n\u001b[32m      6\u001b[39m response = llm_with_structured.invoke(\n\u001b[32m      7\u001b[39m     [(\u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, search_instructions)] + state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      8\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m results = \u001b[43marxiv_retriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_max_docs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mload_all_available_meta\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_full_documents\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m context = [\n\u001b[32m     18\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m<Document source=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33marxiv\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m url=\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc.metadata[\u001b[33m\"\u001b[39m\u001b[33mentry_id\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m date=\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc.metadata.get(\u001b[33m\"\u001b[39m\u001b[33mPublished\u001b[39m\u001b[33m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m authors=\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdoc.metadata.get(\u001b[33m\"\u001b[39m\u001b[33mAuthors\u001b[39m\u001b[33m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m/>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m<Title>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdoc.metadata[\u001b[33m\"\u001b[39m\u001b[33mTitle\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m</Title>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m<Summary>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdoc.metadata[\u001b[33m\"\u001b[39m\u001b[33mSummary\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m</Summary>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m<Content>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdoc.page_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m</Content>\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m</Document>\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m     20\u001b[39m ]\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: [*context]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_core/retrievers.py:263\u001b[39m, in \u001b[36mBaseRetriever.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    261\u001b[39m kwargs_ = kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._new_arg_supported:\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs_\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    267\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._get_relevant_documents(\u001b[38;5;28minput\u001b[39m, **kwargs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_community/retrievers/arxiv.py:90\u001b[39m, in \u001b[36mArxivRetriever._get_relevant_documents\u001b[39m\u001b[34m(self, query, run_manager)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_relevant_documents\u001b[39m(\n\u001b[32m     87\u001b[39m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, *, run_manager: CallbackManagerForRetrieverRun\n\u001b[32m     88\u001b[39m ) -> List[Document]:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_full_documents:\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     92\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.get_summaries_as_docs(query)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_community/utilities/arxiv.py:185\u001b[39m, in \u001b[36mArxivAPIWrapper.load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m) -> List[Document]:\n\u001b[32m    173\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[33;03m    Run Arxiv search and get the article texts plus the article meta information.\u001b[39;00m\n\u001b[32m    175\u001b[39m \u001b[33;03m    See https://lukasschwab.me/arxiv.py/index.html#Search\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    183\u001b[39m \u001b[33;03m        query: a plaintext search query\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_community/utilities/arxiv.py:223\u001b[39m, in \u001b[36mArxivAPIWrapper.lazy_load\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m fitz.open(doc_file_name) \u001b[38;5;28;01mas\u001b[39;00m doc_file:\n\u001b[32m    222\u001b[39m         text: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(page.get_text() \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m doc_file)\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mFileNotFoundError\u001b[39;00m, \u001b[43mfitz\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfitz\u001b[49m.FileDataError) \u001b[38;5;28;01mas\u001b[39;00m f_ex:\n\u001b[32m    224\u001b[39m     logger.debug(f_ex)\n\u001b[32m    225\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'fitz' has no attribute 'fitz'",
      "During task with name 'search_arxiv' and id '8d3725ec-741a-0ebd-c684-ad354cc6b4a4'",
      "During task with name 'conduct_interview' and id 'c9d37083-5999-3f8e-6920-c11da4de8828'"
     ]
    }
   ],
   "source": [
    "invoke_graph(\n",
    "    graph,\n",
    "    Command(update={\"human_feedback\": None}),\n",
    "    config,\n",
    "    node_names=[\n",
    "        \"create_analysts\",\n",
    "        \"human_feedback\",\n",
    "        \"conduct_interview\",\n",
    "        \"generate_question\",\n",
    "        \"generate_answer\",\n",
    "        \"write_section\",\n",
    "        \"write_report\",\n",
    "        \"write_introduction\",\n",
    "        \"write_conclusion\",\n",
    "        \"finalize_report\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c082b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "final_state = graph.get_state(config)\n",
    "report = final_state.values.get(\"final_report\")\n",
    "display(Markdown(report))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation-introduction-to-langgraph (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
