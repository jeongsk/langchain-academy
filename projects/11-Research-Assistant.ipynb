{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d140c86",
   "metadata": {},
   "source": [
    "## STORM: 연구를 위한 멀티 에이전트\n",
    "\n",
    "### 개요\n",
    "\n",
    "STORM(Synthesis of Topic Outline through Retrieval and Multi-perspective Question Asking)은 Stanford 대학에서 개발한 LLM 기반의 지식 큐레이션 시스템입니다. 이 시스템은 인터넷 리서치를 통해 Wikipedia 수준의 포괄적이고 체계적인 장문의 기사를 자동으로 생성하는 것을 목표로 합니다.\n",
    "\n",
    "![](https://github.com/stanford-oval/storm/raw/main/assets/two_stages.jpg)\n",
    "\n",
    "### 핵심 아키텍처\n",
    "\n",
    "STORM은 두 단계의 파이프라인으로 구성됩니다:\n",
    "\n",
    "1. **사전 작성 단계(Pre-writing Stage)**\n",
    "   - 인터넷 기반 리서치를 수행하여 참고 자료 수집\n",
    "   - 다양한 관점(perspective) 발견\n",
    "   - 주제에 대한 개요(outline) 생성\n",
    "\n",
    "2. **작성 단계(Writing Stage)**\n",
    "   - 생성된 개요와 수집된 참고 자료를 활용\n",
    "   - 인용(citation)이 포함된 전체 기사 작성\n",
    "\n",
    "### 멀티 에이전트 접근법\n",
    "\n",
    "STORM의 핵심은 **관점 기반 질문 생성(Perspective-Guided Question Asking)** 과 **시뮬레이션된 대화(Simulated Conversation)** 전략입니다:\n",
    "\n",
    "- **다양한 관점 발견**: 유사한 주제의 기존 기사들을 조사하여 다양한 시각을 발견하고, 이를 질문 생성 과정에 활용\n",
    "- **역할 기반 대화 시뮬레이션**: Wikipedia 작성자와 주제 전문가 간의 대화를 시뮬레이션\n",
    "  - 작성자 에이전트: 다양한 관점에서 질문 제기\n",
    "  - 전문가 에이전트: 인터넷 소스에 기반한 답변 제공\n",
    "  - 이를 통해 이해도를 업데이트하고 후속 질문 생성\n",
    "\n",
    "### Co-STORM: 협업 확장\n",
    "\n",
    "Co-STORM은 STORM을 협업 기능으로 확장한 버전으로, 다음과 같은 멀티 에이전트 구성을 포함합니다:\n",
    "\n",
    "\n",
    "- **LLM 전문가 에이전트**: 외부 소스에 기반한 답변 생성 및 후속 질문 제기\n",
    "- **중재자 에이전트(Moderator)**: 발견된 정보에서 영감을 받은 사고를 자극하는 질문 생성\n",
    "- **동적 마인드맵**: 정보를 계층적으로 정리하여 인간과 시스템 간의 공유 개념 공간 생성\n",
    "\n",
    "![](https://github.com/stanford-oval/storm/raw/main/assets/co-storm-workflow.jpg)\n",
    "\n",
    "### 주요 특징\n",
    "\n",
    "- **포괄적 커버리지**: 다양한 관점에서 주제를 탐색하여 Wikipedia 수준의 광범위한 내용 생성\n",
    "- **구조화된 정보**: 자동으로 생성된 개요를 통해 체계적으로 정보 조직\n",
    "- **신뢰할 수 있는 출처**: 인터넷 소스에 기반하여 모든 정보에 인용 포함\n",
    "- **평가 검증**: FreshWiki 데이터셋을 통한 평가에서 기존 방법 대비 조직성 25%, 커버리지 10% 향상\n",
    "\n",
    "STORM은 복잡한 연구 작업을 자동화하고, 다양한 관점에서 정보를 종합하며, 신뢰할 수 있는 장문의 리포트를 생성하는 멀티 에이전트 시스템의 우수한 사례입니다.\n",
    "\n",
    "---\n",
    "\n",
    "- 참고 자료: https://wikidocs.net/270693\n",
    "- 관련 논문: https://arxiv.org/abs/2402.14207\n",
    "- GitHub 저장소: https://github.com/stanford-oval/storm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb325a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "112e4070",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "84dd060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\", override=True)\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    env_value = os.environ.get(var)\n",
    "    if not env_value:\n",
    "        env_value = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "    os.environ[var] = env_value\n",
    "\n",
    "\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\"\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c360a",
   "metadata": {},
   "source": [
    "## 분석가 생성 에이전트 with Human-In-The-Loop\n",
    "\n",
    "분석가 생성이 필요한 클래스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a7e07bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Analyst(BaseModel):\n",
    "    \"\"\"분석가 속성과 메타데이터를 정의\"\"\"\n",
    "\n",
    "    affiliation: Annotated[str, Field(description=\"분석가의 주요 소속 기관\")]\n",
    "    name: Annotated[str, Field(description=\"분석가 이름\")]\n",
    "    role: Annotated[str, Field(description=\"주제 맥락에서의 분석가의 역할\")]\n",
    "    description: Annotated[\n",
    "        str, Field(description=\"분석가의 관심사, 우려 사항 및 동기 설명\")\n",
    "    ]\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return (\n",
    "            f\"이름: {self.name}\\n\"\n",
    "            f\"역할: {self.role}\\n\"\n",
    "            f\"소속 기관: {self.affiliation}\\n\"\n",
    "            f\"설명: {self.description}\\n\"\n",
    "        )\n",
    "\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    \"\"\"분석가들의 집합\"\"\"\n",
    "\n",
    "    analysts: Annotated[\n",
    "        list[Analyst],\n",
    "        Field(description=\"분석가들의 역할 및 소속 기관을 포함한 종합 목록\"),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6778d7",
   "metadata": {},
   "source": [
    "### 분석가 생성 상태 및 노드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b61f39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 정의\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    topic: Annotated[str, \"연구 주제\"]\n",
    "    max_analysts: Annotated[int, \"생성할 분석가의 최대 수\"]\n",
    "    human_analyst_feedback: Annotated[str, \"휴먼 피드백\"]\n",
    "    analysts: Annotated[list[Analyst], \"분석가 목록\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "46502385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석가 생성 프롬프트\n",
    "analyst_instructions = \"\"\"AI 분석가 페르소나 세트를 생성하는 임무를 맡았습니다.\n",
    "\n",
    "다음 지침을 주의 깊게 따르십시오:\n",
    "\n",
    "1. 먼저 연구 주제를 검토하십시오:\n",
    "{topic}\n",
    "\n",
    "2. 분석가 생성 가이드로 제공된 선택적 편집 피드백을 검토하십시오:\n",
    "{human_analyst_feedback}\n",
    "\n",
    "3. 위 문서 및/또는 피드백을 바탕으로 가장 흥미로운 테마를 결정하십시오.\n",
    "\n",
    "4. 상위 {max_analysts}개 테마를 선정하십시오.\n",
    "\n",
    "5. 각 테마에 한 명의 분석가를 배정하십시오.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d7bcfdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-4.1-mini\")\n",
    "\n",
    "\n",
    "# 분석가 생성 노드\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    \"\"\"분석가 페르소나를 생성합니다.\"\"\"\n",
    "\n",
    "    topic = state[\"topic\"]\n",
    "    max_analysts = state[\"max_analysts\"]\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", \"\")\n",
    "\n",
    "    llm_with_structured = llm.with_structured_output(Perspectives)\n",
    "\n",
    "    system_message = analyst_instructions.format(\n",
    "        topic=topic,\n",
    "        human_analyst_feedback=human_analyst_feedback,\n",
    "        max_analysts=max_analysts,\n",
    "    )\n",
    "\n",
    "    response = llm_with_structured.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=\"Generate the set of analysts.\")]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"analysts\": response.analysts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "47c51fe5",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[139]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcreate_analysts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m멀티 에이전트\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_analysts\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[138]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mcreate_analysts\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     16\u001b[39m llm_with_structured = llm.with_structured_output(Perspectives)\n\u001b[32m     18\u001b[39m system_message = analyst_instructions.format(\n\u001b[32m     19\u001b[39m     topic=topic,\n\u001b[32m     20\u001b[39m     human_analyst_feedback=human_analyst_feedback,\n\u001b[32m     21\u001b[39m     max_analysts=max_analysts,\n\u001b[32m     22\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m response = \u001b[43mllm_with_structured\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mSystemMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_message\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGenerate the set of analysts.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33manalysts\u001b[39m\u001b[33m\"\u001b[39m: response.analysts,\n\u001b[32m     31\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3244\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3242\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3244\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3245\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3246\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5711\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5704\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5705\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5706\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5709\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5710\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5711\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5712\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5713\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5714\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5715\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:395\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    390\u001b[39m     **kwargs: Any,\n\u001b[32m    391\u001b[39m ) -> BaseMessage:\n\u001b[32m    392\u001b[39m     config = ensure_config(config)\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    394\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    405\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1025\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     **kwargs: Any,\n\u001b[32m   1023\u001b[39m ) -> LLMResult:\n\u001b[32m   1024\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:842\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    840\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    841\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    848\u001b[39m         )\n\u001b[32m    849\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    850\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1091\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1089\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1090\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1091\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1095\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:1181\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1178\u001b[39m payload.pop(\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1179\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1180\u001b[39m     raw_response = (\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m     )\n\u001b[32m   1185\u001b[39m     response = raw_response.parse()\n\u001b[32m   1186\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m openai.BadRequestError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/openai/_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:183\u001b[39m, in \u001b[36mCompletions.parse\u001b[39m\u001b[34m(self, messages, model, audio, response_format, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, safety_identifier, seed, service_tier, stop, store, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparser\u001b[39m(raw_completion: ChatCompletion) -> ParsedChatCompletion[ResponseFormatT]:\n\u001b[32m    177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _parse_chat_completion(\n\u001b[32m    178\u001b[39m         response_format=response_format,\n\u001b[32m    179\u001b[39m         chat_completion=raw_completion,\n\u001b[32m    180\u001b[39m         input_tools=chat_completion_tools,\n\u001b[32m    181\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    187\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m_type_to_response_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# we turn the `ChatCompletion` instance into a `ParsedChatCompletion`\u001b[39;49;00m\n\u001b[32m    232\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# in the `parser` function above\u001b[39;49;00m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mType\u001b[49m\u001b[43m[\u001b[49m\u001b[43mParsedChatCompletion\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseFormatT\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "create_analysts({\"topic\": \"멀티 에이전트\", \"max_analysts\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b276ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 피드백 노드\n",
    "def human_feedback(state: GenerateAnalystsState):\n",
    "    \"\"\"사용자 피드백을 받기 위한 중단점 노드\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a9c93",
   "metadata": {},
   "source": [
    "### 분석가 생성 그래프 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509107a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "def should_continue(state: GenerateAnalystsState) -> Literal[\"create_analysts\", END]:\n",
    "    \"\"\"워크플로우의 다음 노드를 결정합니다.\"\"\"\n",
    "\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", \"\")\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "    return END\n",
    "\n",
    "\n",
    "builder = StateGraph(GenerateAnalystsState)\n",
    "builder.add_node(create_analysts)\n",
    "builder.add_node(human_feedback)\n",
    "\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\", should_continue, {\"create_analysts\": \"create_analysts\", END: END}\n",
    ")\n",
    "builder.set_entry_point(\"create_analysts\")\n",
    "graph = builder.compile(\n",
    "    interrupt_before=[\"human_feedback\"], checkpointer=InMemorySaver()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8274f81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e503e",
   "metadata": {},
   "source": [
    "### 분석가 생성 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07008ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### create_analysts #####\n",
      "{'analysts': [Analyst(affiliation='OmniData AI Research Lab', name='Dr. Mina Lee', role='AI Systems Researcher', description='Dr. Mina Lee specializes in AI retrieval-augmented generation systems, focusing on the agentic capabilities of language models to autonomously decide and execute information search and synthesis steps. She explores how Agentic RAG frameworks empower models with proactive decision-making and iterative interaction with retrieval components.'),\n",
      "              Analyst(affiliation='Adaptive Intelligence Solutions', name='Prof. Jun-ho Kim', role='Machine Learning Strategist', description='Prof. Jun-ho Kim investigates adaptive mechanisms in retrieval-augmented generation architectures where models dynamically adjust their retrieval strategies based on context and feedback. His work sheds light on how Adaptive RAG differs from agentic approaches by emphasizing flexibility and responsiveness in information retrieval processes.'),\n",
      "              Analyst(affiliation='NextGen AI Innovators', name='Eunseo Park', role='AI Product Analyst', description='Eunseo Park analyzes practical implications and product design involving Agentic versus Adaptive RAG systems, focusing on user experience and system performance trade-offs. She examines how the contrasting characteristics of these frameworks influence the deployment of AI-driven applications in real-world settings.')]}\n",
      "\n",
      "##### __interrupt__ #####\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from random import random\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(configurable={\"thread_id\": random()})\n",
    "\n",
    "inputs = {\n",
    "    \"max_analysts\": 3,\n",
    "    \"topic\": \"Agentic RAG와 Adaptive RAG의 차이점은 무엇인가요?\",\n",
    "}\n",
    "for event in graph.stream(inputs, config=config):\n",
    "    for key, value in event.items():\n",
    "        print(f\"\\n##### {key} #####\")\n",
    "        pprint(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4a06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysts': [Analyst(affiliation='OmniData AI Research Lab', name='Dr. Mina Lee', role='AI Systems Researcher', description='Dr. Mina Lee specializes in AI retrieval-augmented generation systems, focusing on the agentic capabilities of language models to autonomously decide and execute information search and synthesis steps. She explores how Agentic RAG frameworks empower models with proactive decision-making and iterative interaction with retrieval components.'),\n",
      "              Analyst(affiliation='Adaptive Intelligence Solutions', name='Prof. Jun-ho Kim', role='Machine Learning Strategist', description='Prof. Jun-ho Kim investigates adaptive mechanisms in retrieval-augmented generation architectures where models dynamically adjust their retrieval strategies based on context and feedback. His work sheds light on how Adaptive RAG differs from agentic approaches by emphasizing flexibility and responsiveness in information retrieval processes.'),\n",
      "              Analyst(affiliation='NextGen AI Innovators', name='Eunseo Park', role='AI Product Analyst', description='Eunseo Park analyzes practical implications and product design involving Agentic versus Adaptive RAG systems, focusing on user experience and system performance trade-offs. She examines how the contrasting characteristics of these frameworks influence the deployment of AI-driven applications in real-world settings.')],\n",
      " 'max_analysts': 3,\n",
      " 'topic': 'Agentic RAG와 Adaptive RAG의 차이점은 무엇인가요?'}\n",
      "('human_feedback',)\n"
     ]
    }
   ],
   "source": [
    "# 현재 상태 스냅샷\n",
    "snapshot = graph.get_state(config)\n",
    "pprint(snapshot.values)\n",
    "pprint(snapshot.next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### human_feedback #####\n",
      "None\n",
      "\n",
      "##### create_analysts #####\n",
      "{'analysts': [Analyst(affiliation='Tech Startup - Seoul', name='Seokho Phil', role='Founder & AI Product Strategist', description='Startup founder with deep expertise in AI integration, focusing on bridging technical RAG solutions with entrepreneurial business models to foster adaptive growth and competitive advantage.'),\n",
      "              Analyst(affiliation='Innovative AI Research Lab', name='Dr. Min-Jae Lee', role='Lead Research Scientist', description='Expert in agentic AI systems, specializing in the design and implementation of agentic Retrieval-Augmented Generation models that take autonomous initiative in information retrieval and decision-making.'),\n",
      "              Analyst(affiliation='Adaptive Computing Institute', name='Prof. Hana Kim', role='Professor of Adaptive AI Systems', description='Academic researcher focused on Adaptive RAG models that dynamically adjust their retrieval and generation processes based on context awareness and user feedback to enhance performance and relevance.')]}\n",
      "\n",
      "##### __interrupt__ #####\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# 휴먼 피드백 전달\n",
    "from langgraph.types import Command\n",
    "\n",
    "for event in graph.stream(\n",
    "    Command(\n",
    "        update={\n",
    "            \"human_analyst_feedback\": \"스타트업 출신의 석호필이라는 인물을 추가해 기업가적 관점을 더해주세요.\"\n",
    "        },\n",
    "    ),\n",
    "    config=config,\n",
    "):\n",
    "    for key, value in event.items():\n",
    "        print(f\"\\n##### {key} #####\")\n",
    "        pprint(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e6244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### human_feedback #####\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream(\n",
    "    Command(update={\"human_analyst_feedback\": None}),\n",
    "    config=config,\n",
    "):\n",
    "    for key, value in event.items():\n",
    "        print(f\"\\n##### {key} #####\")\n",
    "        pprint(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5bcba8",
   "metadata": {},
   "source": [
    "최종 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da29cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSnapshot(values={'topic': 'Agentic RAG와 Adaptive RAG의 차이점은 무엇인가요?', 'max_analysts': 3, 'human_analyst_feedback': None, 'analysts': [Analyst(affiliation='Tech Startup - Seoul', name='Seokho Phil', role='Founder & AI Product Strategist', description='Startup founder with deep expertise in AI integration, focusing on bridging technical RAG solutions with entrepreneurial business models to foster adaptive growth and competitive advantage.'), Analyst(affiliation='Innovative AI Research Lab', name='Dr. Min-Jae Lee', role='Lead Research Scientist', description='Expert in agentic AI systems, specializing in the design and implementation of agentic Retrieval-Augmented Generation models that take autonomous initiative in information retrieval and decision-making.'), Analyst(affiliation='Adaptive Computing Institute', name='Prof. Hana Kim', role='Professor of Adaptive AI Systems', description='Academic researcher focused on Adaptive RAG models that dynamically adjust their retrieval and generation processes based on context awareness and user feedback to enhance performance and relevance.')]}, next=(), config={'configurable': {'thread_id': '0.8998343459486847', 'checkpoint_ns': '', 'checkpoint_id': '1f0abb45-11e8-616c-8004-24a134b798ef'}}, metadata={'source': 'loop', 'step': 4, 'parents': {}}, created_at='2025-10-17T23:52:19.190196+00:00', parent_config={'configurable': {'thread_id': '0.8998343459486847', 'checkpoint_ns': '', 'checkpoint_id': '1f0abb45-11c9-6e10-8003-cc7091e311b5'}}, tasks=(), interrupts=())\n"
     ]
    }
   ],
   "source": [
    "# 스냅샷을 가져옵니다.\n",
    "final_state = graph.get_state(config)\n",
    "pprint(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99468a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 분석가 수: 3\n",
      "================================\n",
      "이름: Seokho Phil\n",
      "역할: Founder & AI Product Strategist\n",
      "소속 기관: Tech Startup - Seoul\n",
      "설명: Startup founder with deep expertise in AI integration, focusing on bridging technical RAG solutions with entrepreneurial business models to foster adaptive growth and competitive advantage.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "이름: Dr. Min-Jae Lee\n",
      "역할: Lead Research Scientist\n",
      "소속 기관: Innovative AI Research Lab\n",
      "설명: Expert in agentic AI systems, specializing in the design and implementation of agentic Retrieval-Augmented Generation models that take autonomous initiative in information retrieval and decision-making.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "이름: Prof. Hana Kim\n",
      "역할: Professor of Adaptive AI Systems\n",
      "소속 기관: Adaptive Computing Institute\n",
      "설명: Academic researcher focused on Adaptive RAG models that dynamically adjust their retrieval and generation processes based on context awareness and user feedback to enhance performance and relevance.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "analysts = final_state.values.get(\"analysts\")\n",
    "print(f\"생성된 분석가 수: {len(analysts)}\", end=\"\\n================================\\n\")\n",
    "\n",
    "for analyst in analysts:\n",
    "    print(analyst.persona)\n",
    "    print(\"- \" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90393571",
   "metadata": {},
   "source": [
    "## 인터뷰 에이전트\n",
    "\n",
    "### 질문 생성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class InterviewState(MessagesState):\n",
    "    \"\"\"인터뷰 정보를 저장합니다.\"\"\"\n",
    "\n",
    "    topic: Annotated[str, \"연구 주제\"]\n",
    "    max_num: Annotated[int, \"대화 턴수\"]\n",
    "    context: Annotated[list, operator.add]\n",
    "    analyst: Annotated[Analyst, \"분석가\"]\n",
    "    interview: Annotated[str, \"인터뷰 내용\"]\n",
    "    sections: Annotated[list, \"보고서 섹션 목록\"]\n",
    "\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: Annotated[str, Field(None, description=\"retrieval를 위한 검색 쿼리\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d484c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인터뷰 시스템 프롬프트\n",
    "question_instructions = \"\"\"당신은 특정 주제에 대해 알아보기 위해 전문가를 인터뷰하는 임무를 맡은 분석가입니다.\n",
    "\n",
    "당신의 목표는 주제에 관련된 흥미롭고 구체적인 통찰력을 추출하는 것입니다.\n",
    "\n",
    "1. 흥미로움: 사람들이 놀라워하거나 당연하지 않다고 느낄 만한 통찰력.\n",
    "2. 구체성: 일반론을 피하고 전문가의 구체적인 사례를 포함하는 통찰력.\n",
    "\n",
    "다음은 집중할 주제입니다:\n",
    "{topic}\n",
    "\n",
    "첫 대화를 시작할때에 당신의 인물을 반영하는 이름으로 자신을 소개한 후 질문을 시작하세요.\n",
    "\n",
    "주제에 대한 이해를 심화하고 정교화하기 위해 계속해서 질문을 이어가세요.\n",
    "\n",
    "이해가 충분하다고 판단되면 \"도움 주셔서 정말 감사합니다!\"라고 말하며 인터뷰를 마무리하세요.\n",
    "\n",
    "응답 전반에 걸쳐 제공된 인물과 목표를 반영하여 캐릭터를 유지하는 것을 잊지 마세요.\n",
    "\n",
    "<Persona>\n",
    "{persona}\n",
    "<Persona>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "32f5de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 생성 노드\n",
    "def generate_question(state: InterviewState):\n",
    "    \"\"\"통찰력있는 질문을 생성합니다.\"\"\"\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    system_message = question_instructions.format(\n",
    "        topic=\"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점\",\n",
    "        persona=analyst.persona,\n",
    "    )\n",
    "    response = llm.invoke([SystemMessage(content=system_message)] + state[\"messages\"])\n",
    "    response.name = \"분석가\"\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9591fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "안녕하세요, 김필입니다. Tech Innovators Inc.에서 기업가적 분석가로 일하며 새로운 기술과 비즈니스 모델 혁신에 관심이 많습니다. 오늘은 Modular RAG와 기존 Naive RAG의 차이점 그리고 production 환경에서 Modular RAG를 사용하는 이점에 대해 좀 더 깊이 알아보고자 합니다.\n",
      "\n",
      "먼저, Modular RAG가 기존의 Naive RAG와 무엇이 다르고, 근본적으로 어떤 문제를 해결하려는 접근법인지 간단히 설명해 주실 수 있을까요? 혹시 스타트업이나 실제 프로젝트에서 경험한 구체적인 사례가 있다면 더욱 좋겠습니다.\n"
     ]
    }
   ],
   "source": [
    "analyst = Analyst(\n",
    "    name=\"김필\",\n",
    "    affiliation=\"Tech Innovators Inc.\",\n",
    "    role=\"기업가적 분석가\",\n",
    "    description=\"창업자 출신 분석가로, 기업가 정신과 비즈니스 모델 혁신에 주력합니다. 스타트업 관점에서 적응형 시스템에 대한 통찰력을 제공하며, 유연성과 시장 적응력을 강조합니다.\",\n",
    ")\n",
    "response = generate_question(\n",
    "    {\n",
    "        \"analyst\": analyst,\n",
    "        \"messages\": [],\n",
    "    }\n",
    ")\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53075650",
   "metadata": {},
   "source": [
    "### 도구 정의\n",
    "\n",
    "#### 웹검색 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a89434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "web_search = TavilySearch(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fd6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '지난 윔블던에서 무슨 일이 있었나요?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'url': 'http://m.tennispeople.kr/news/articleView.html?idxno=17168',\n",
       "   'title': '윔블던 컴퓨터 라인 판정 믿어도 되나 - 테니스피플',\n",
       "   'content': '사건은 6일 윔블던 센터 코트에서 열린 여자 단식 16강전 1세트 4-4 상황에서 발생했다. 영국 소니 카르탈과 러시아의 아나스타샤 파블류첸코바가 맞붙',\n",
       "   'score': 0.38498205,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://sports.news.nate.com/view/20250704n04849',\n",
       "   'title': '\"아웃, 인은 기계가 판정\" 148년 전통 깬 윔블던, 인간 선심 사라졌다',\n",
       "   'content': '올해 윔블던 테니스 대회는 148년 역사상 처음으로 인간 선심이 사라진 대회로 기록됐다. CNN은 4일 “윔블던 라인 콜은 전적으로 전자 판독',\n",
       "   'score': 0.34846824,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.busan.com/view/busan/view.php?code=2025070317510369945',\n",
       "   'title': \"톱시드 23명 1회전 탈락… '이변의 윔블던' - 부산일보\",\n",
       "   'content': '폭염 속에 진행된 윔블던 테니스 대회에서 이변이 속출하고 있다. 남녀 16명의 시드 배정자, 총 32명 중 무려 23명이 1회전에서 탈락했다.',\n",
       "   'score': 0.2246166,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 0.7,\n",
       " 'request_id': 'a1cfeb0f-0566-4f47-a3aa-8cc943f2a7c7'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_search.invoke({\"query\": \"지난 윔블던에서 무슨 일이 있었나요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac7eed",
   "metadata": {},
   "source": [
    "#### 논문 검색 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "arxiv_retriever = ArxivRetriever(\n",
    "    load_max_docs=3,\n",
    "    load_all_available_meta=True,\n",
    "    get_full_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f016ac79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2024-07-26', 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks', 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang', 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.', 'entry_id': 'http://arxiv.org/abs/2407.21059v1', 'published_first_time': '2024-07-26', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI', 'cs.IR'], 'links': ['http://arxiv.org/abs/2407.21059v1', 'http://arxiv.org/pdf/2407.21059v1']}, page_content='1\\nModular RAG: Transforming RAG Systems into\\nLEGO-like Reconfigurable Frameworks\\nYunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\\nAbstract—Retrieval-augmented\\nGeneration\\n(RAG)\\nhas\\nmarkedly enhanced the capabilities of Large Language Models\\n(LLMs) in tackling knowledge-intensive tasks. The increasing\\ndemands of application scenarios have driven the evolution\\nof RAG, leading to the integration of advanced retrievers,\\nLLMs and other complementary technologies, which in turn\\nhas amplified the intricacy of RAG systems. However, the rapid\\nadvancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process\\nof “retrieve-then-generate”. In this context, this paper examines\\nthe limitations of the existing RAG paradigm and introduces\\nthe modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a\\nmore advanced design that integrates routing, scheduling, and\\nfusion mechanisms. Drawing on extensive research, this paper\\nfurther identifies prevalent RAG patterns—linear, conditional,\\nbranching, and looping—and offers a comprehensive analysis\\nof their respective implementation nuances. Modular RAG\\npresents\\ninnovative\\nopportunities\\nfor\\nthe\\nconceptualization\\nand deployment of RAG systems. Finally, the paper explores\\nthe potential emergence of new operators and paradigms,\\nestablishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment\\nof RAG technologies.\\nIndex Terms—Retrieval-augmented generation, large language\\nmodel, modular system, information retrieval\\nI. INTRODUCTION\\nL\\nARGE Language Models (LLMs) have demonstrated\\nremarkable capabilities, yet they still face numerous\\nchallenges, such as hallucination and the lag in information up-\\ndates [1]. Retrieval-augmented Generation (RAG), by access-\\ning external knowledge bases, provides LLMs with important\\ncontextual information, significantly enhancing their perfor-\\nmance on knowledge-intensive tasks [2]. Currently, RAG, as\\nan enhancement method, has been widely applied in various\\npractical application scenarios, including knowledge question\\nanswering, recommendation systems, customer service, and\\npersonal assistants. [3]–[6]\\nDuring the nascent stages of RAG , its core framework is\\nconstituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the\\nYunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\\nSystems, Tongji University, Shanghai, 201210, China.\\nYun Xiong is with Shanghai Key Laboratory of Data Science, School of\\nComputer Science, Fudan University, Shanghai, 200438, China.\\nMeng Wang and Haofen Wang are with College of Design and Innovation,\\nTongji University, Shanghai, 20092, China. (Corresponding author: Haofen\\nWang. E-mail: carter.whfcarter@gmail.com)\\nlimitations of Naive RAG have become increasingly apparent.\\nAs depicted in Figure 1, it predominantly hinges on the\\nstraightforward similarity of chunks, result in poor perfor-\\nmance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic\\nsimilarity between a query and document chunk is not always\\nhighly consistent. Relying solely on similarity calculations\\nfor retrieval lacks an in-depth exploration of the relationship\\nbetween the query and the document [8]. 2) Retrieval Re-\\ndundancy and Noise. Feeding all retrieved chunks directly\\ninto LLMs is not always beneficial. Research indicates that\\nan excess of redundant and noisy information may interfere\\nwith the LLM’s identification of key information, thereby\\nincreasing the risk of generating erroneous and hallucinated\\nresponses. [9]\\nTo overcome the aforementioned limitations, '),\n",
       " Document(metadata={'Published': '2024-03-27', 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'published_first_time': '2023-12-18', 'comment': 'Ongoing Work', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI'], 'links': ['http://arxiv.org/abs/2312.10997v5', 'http://arxiv.org/pdf/2312.10997v5']}, page_content='1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Terms—Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. INTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. Thi'),\n",
       " Document(metadata={'Published': '2025-10-11', 'Title': 'MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning', 'Authors': 'Thang Nguyen, Peter Chin, Yu-Wing Tai', 'Summary': 'We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\\nend-to-end fine-tuning or isolated component enhancements, MA-RAG orchestrates\\na collaborative set of specialized AI agents: Planner, Step Definer, Extractor,\\nand QA Agents, each responsible for a distinct stage of the RAG pipeline. By\\ndecomposing tasks into subtasks such as query disambiguation, evidence\\nextraction, and answer synthesis, and enabling agents to communicate\\nintermediate reasoning via chain-of-thought prompting, MA-RAG progressively\\nrefines retrieval and synthesis while maintaining modular interpretability.\\nExtensive experiments on multi-hop and ambiguous QA benchmarks, including NQ,\\nHotpotQA, 2WikimQA, and TriviaQA, demonstrate that MA-RAG significantly\\noutperforms standalone LLMs and existing RAG methods across all model scales.\\nNotably, even a small LLaMA3-8B model equipped with MA-RAG surpasses larger\\nstandalone LLMs, while larger variants (LLaMA3-70B and GPT-4o-mini) set new\\nstate-of-the-art results on challenging multi-hop datasets. Ablation studies\\nreveal that both the planner and extractor agents are critical for multi-hop\\nreasoning, and that high-capacity models are especially important for the QA\\nagent to synthesize answers effectively. Beyond general-domain QA, MA-RAG\\ngeneralizes to specialized domains such as medical QA, achieving competitive\\nperformance against domain-specific models without any domain-specific\\nfine-tuning. Our results highlight the effectiveness of collaborative, modular\\nreasoning in retrieval-augmented systems: MA-RAG not only improves answer\\naccuracy and robustness but also provides interpretable intermediate reasoning\\nsteps, establishing a new paradigm for efficient and reliable multi-agent RAG.', 'entry_id': 'http://arxiv.org/abs/2505.20096v2', 'published_first_time': '2025-05-26', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI'], 'links': ['http://arxiv.org/abs/2505.20096v2', 'http://arxiv.org/pdf/2505.20096v2']}, page_content='MA-RAG: MULTI-AGENT RETRIEVAL-AUGMENTED\\nGENERATION\\nVIA\\nCOLLABORATIVE\\nCHAIN-OF-\\nTHOUGHT REASONING\\nThang Nguyen & Peter Chin & Yu-Wing Tai\\nDartmouth College\\n{thangnv.th, peter.chin, yu-wing.tai}@dartmouth.edu\\nABSTRACT\\nWe present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Gener-\\nation (RAG) that addresses the inherent ambiguities and reasoning challenges in\\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely\\non end-to-end fine-tuning or isolated component enhancements, MA-RAG orches-\\ntrates a collaborative set of specialized AI agents: Planner, Step Definer, Extrac-\\ntor, and QA Agents, each responsible for a distinct stage of the RAG pipeline.\\nBy decomposing tasks into subtasks such as query disambiguation, evidence ex-\\ntraction, and answer synthesis, and enabling agents to communicate intermedi-\\nate reasoning via chain-of-thought prompting, MA-RAG progressively refines re-\\ntrieval and synthesis while maintaining modular interpretability. Extensive exper-\\niments on multi-hop and ambiguous QA benchmarks, including NQ, HotpotQA,\\n2WikimQA, and TriviaQA, demonstrate that MA-RAG significantly outperforms\\nstandalone LLMs and existing RAG methods across all model scales. Notably,\\neven a small LLaMA3-8B model equipped with MA-RAG surpasses larger stan-\\ndalone LLMs, while larger variants (LLaMA3-70B and GPT-4o-mini) set new\\nstate-of-the-art results on challenging multi-hop datasets. Ablation studies reveal\\nthat both the planner and extractor agents are critical for multi-hop reasoning,\\nand that high-capacity models are especially important for the QA agent to syn-\\nthesize answers effectively. Beyond general-domain QA, MA-RAG generalizes\\nto specialized domains such as medical QA, achieving competitive performance\\nagainst domain-specific models without any domain-specific fine-tuning. Our re-\\nsults highlight the effectiveness of collaborative, modular reasoning in retrieval-\\naugmented systems: MA-RAG not only improves answer accuracy and robustness\\nbut also provides interpretable intermediate reasoning steps, establishing a new\\nparadigm for efficient and reliable multi-agent RAG1.\\n1\\nINTRODUCTION\\nRecent advances in natural language processing have driven the development of Retrieval-\\nAugmented Generation (RAG) models, which aim to enhance the factual accuracy and contextual\\nrelevance of generated text by integrating external knowledge sources (Lewis et al., 2020; Guu et al.,\\n2020; Izacard & Grave, 2021; Lin et al., 2024). These systems address core limitations of Large Lan-\\nguage Models (LLMs), such as outdated knowledge (Zhang et al., 2023b; Kasai et al., 2023) and\\npoor generalization to domain-specific queries (Siriwardhana et al., 2023; Xiong et al., 2024), by\\nretrieving top-k documents from an external corpus (Formal et al., 2022; Izacard et al., 2022; Wang\\net al., 2022a) to ground the model’s output in relevant evidence.\\nPrior research in RAG has largely concentrated on optimizing three key components—retrieval,\\naugmentation, and generation (Gao et al., 2024; Fan et al., 2024) (Figure 1(a)). Retrieval strate-\\ngies span sparse methods (Jones, 1972; Robertson & Zaragoza, 2009) and dense retrieval (Reimers\\n& Gurevych, 2019; Karpukhin et al., 2020), each with respective weaknesses such as lexical\\ngaps (Berger et al., 2000) or retrieval failure on out-of-distribution and multi-hop queries (Dai et al.,\\n1Our code is available at https://github.com/thangylvp/MA-RAG\\n1\\narXiv:2505.20096v2  [cs.CL]  11 Oct 2025\\nQuery\\nDocs\\nAnswer\\nDocs\\nAnswer\\nCoT\\nNotes\\nPost-process\\na) Vanilla RAG\\nStep 1\\nStep ...\\nCoT\\nSub-Query\\nQuery\\nDocs\\nNotes\\nCoT\\nQuery\\nSub-Answer\\nCoT\\nb) RAG with post-\\nprocessing retrieved docs\\nd) MA-RAG\\nQuery\\nDocs\\nAnswer\\nc) RAG with interleaving\\nretrieval and thoughts\\nAnswer\\nCoT\\nFigure 1: Architectural Comparison of MA-RAG and Prior RAG Methods. a) A naive RAG sys-\\ntem performs one-shot retrieval followed by direct answer generation. b) Enhanced systems incor-\\nporate post-retrieval processing such as document re-')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_retriever.invoke(\"Modular RAG vs Naive RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da7d4c3",
   "metadata": {},
   "source": [
    "### 노드 작성\n",
    "\n",
    "#### 웹 검색 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 쿼리 변환 프롬프트\n",
    "search_instructions = \"\"\"분석가와 전문가 간의 대화가 제시됩니다.\n",
    "\n",
    "목표는 해당 대화와 관련된 검색 및/또는 웹 검색에 사용할 잘 구조화된 쿼리를 생성하는 것입니다.\n",
    "\n",
    "먼저 전체 대화를 분석하십시오.\n",
    "\n",
    "특히 분석가가 마지막에 제기한 질문에 주목하십시오.\n",
    "\n",
    "이 마지막 질문을 잘 구조화된 웹 검색 쿼리로 변환하십시오.\"\"\"\n",
    "\n",
    "\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\"웹 검색을 통한 문서 검색\"\"\"\n",
    "\n",
    "    llm_with_structured = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "    response = llm_with_structured.invoke(\n",
    "        [(\"system\", search_instructions)] + state[\"messages\"]\n",
    "    )\n",
    "\n",
    "    results = web_search.invoke(response.search_query)\n",
    "    context = [\n",
    "        f'<Document source=\"web\" url=\"{doc[\"url\"]}\" title=\"{doc[\"title\"]}\">{doc[\"content\"]}</Document>'\n",
    "        for doc in results[\"results\"]\n",
    "    ]\n",
    "\n",
    "    return {\"context\": [*context]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e12267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': ['<Document source=\"web\" url=\"https://dictionary.cambridge.org/ko/%EC%82%AC%EC%A0%84/%EC%98%81%EC%96%B4/crag\" title=\"영어로 crag의 뜻\">CRAG 의미, 정의, CRAG의 정의: 1. a high, rough mass of rock that sticks out from the land around it 2. a high, rough mass of rock…. 자세히 알아보기.</Document>',\n",
       "  '<Document source=\"web\" url=\"https://www.collinsdictionary.com/ko/dictionary/english/crag\" title=\"CRAG 정의 및 의미 | Collins 영어 사전\">carbon reduction action group or carbon rationing action group: a small association of citizens whose members attempt to reduce their environmental impact</Document>',\n",
       "  '<Document source=\"web\" url=\"https://www.lingq.com/ko/learn-english-online/translate/en/CRAG/\" title=\"CRAG | 한국어 번역과 뜻 | LingQ 사전\">언어 학습 앱 >; 온라인으로 English 배우기. crag. English에서 한국어로의 번역 및 의미. 영어. 한국어. CRAG. 암장. 다른 의미인기. 암장. crag 명사 험준한 바위 (</Document>']}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_web({\"messages\": [(\"user\", \"CRAG에 대해서 설명해주세요.\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9959f7f",
   "metadata": {},
   "source": [
    "#### 논문 검색 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25ee249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(state: InterviewState):\n",
    "    \"\"\"Arxiv 검색 노드\"\"\"\n",
    "\n",
    "    llm_with_structured = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "    response = llm_with_structured.invoke(\n",
    "        [(\"system\", search_instructions)] + state[\"messages\"]\n",
    "    )\n",
    "\n",
    "    results = arxiv_retriever.invoke(\n",
    "        response.search_query,\n",
    "        load_max_docs=2,\n",
    "        load_all_available_meta=True,\n",
    "        get_full_documents=True,\n",
    "    )\n",
    "\n",
    "    context = [\n",
    "        f'<Document source=\"arxiv\" url=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
    "        for doc in results\n",
    "    ]\n",
    "\n",
    "    return {\"context\": [*context]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606944ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': ['<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2502.19629v1\" date=\"2025-02-26\" authors=\"Tiffany J. Callahan, Nathaniel H. Park, Sara Capponi\"/>\\n<Title>\\nAgentic Mixture-of-Workflows for Multi-Modal Chemical Search\\n</Title>\\n\\n<Summary>\\nThe vast and complex materials design space demands innovative strategies to\\nintegrate multidisciplinary scientific knowledge and optimize materials\\ndiscovery. While large language models (LLMs) have demonstrated promising\\nreasoning and automation capabilities across various domains, their application\\nin materials science remains limited due to a lack of benchmarking standards\\nand practical implementation frameworks. To address these challenges, we\\nintroduce Mixture-of-Workflows for Self-Corrective Retrieval-Augmented\\nGeneration (CRAG-MoW) - a novel paradigm that orchestrates multiple agentic\\nworkflows employing distinct CRAG strategies using open-source LLMs. Unlike\\nprior approaches, CRAG-MoW synthesizes diverse outputs through an orchestration\\nagent, enabling direct evaluation of multiple LLMs across the same problem\\ndomain. We benchmark CRAG-MoWs across small molecules, polymers, and chemical\\nreactions, as well as multi-modal nuclear magnetic resonance (NMR) spectral\\nretrieval. Our results demonstrate that CRAG-MoWs achieve performance\\ncomparable to GPT-4o while being preferred more frequently in comparative\\nevaluations, highlighting the advantage of structured retrieval and multi-agent\\nsynthesis. By revealing performance variations across data types, CRAG-MoW\\nprovides a scalable, interpretable, and benchmark-driven approach to optimizing\\nAI architectures for materials discovery. These insights are pivotal in\\naddressing fundamental gaps in benchmarking LLMs and autonomous AI agents for\\nscientific applications.\\n</Summary>\\n\\n<Content>\\nAgentic Mixture-of-Workflows for Multi-Modal Chemical Search \\n \\n \\nTiffany J. Callahan1, Nathaniel H. Park1*, and Sara Capponi1 \\n1IBM Research–Almaden, 650 Harry Rd. San Jose, CA 95120  \\n \\n*Corresponding author. Email: npark@us.ibm.com \\n \\n \\nABSTRACT \\nThe vast and complex materials design space demands innovative strategies to integrate \\nmultidisciplinary scientific knowledge and optimize materials discovery. While large \\nlanguage models (LLMs) have demonstrated promising reasoning and automation \\ncapabilities across various domains, their application in materials science remains limited \\ndue to a lack of benchmarking standards and practical implementation frameworks. To \\naddress these challenges, we introduce Mixture-of-Workflows for Self-Corrective \\nRetrieval-Augmented Generation (CRAG-MoW)—a novel paradigm that orchestrates \\nmultiple agentic workflows employing distinct CRAG strategies using open-source LLMs. \\nUnlike prior approaches, CRAG-MoW synthesizes diverse outputs through an \\norchestration agent, enabling direct evaluation of multiple LLMs across the same problem \\ndomain. We benchmark CRAG-MoWs across small molecules, polymers, and chemical \\nreactions, as well as multi-modal nuclear magnetic resonance (NMR) spectral retrieval. \\nOur results demonstrate that CRAG-MoWs achieve performance comparable to GPT-4o \\nwhile being preferred more frequently in comparative evaluations, highlighting the \\nadvantage of structured retrieval and multi-agent synthesis. By revealing performance \\nvariations across data types, CRAG-MoW provides a scalable, interpretable, and \\nbenchmark-driven approach to optimizing AI architectures for materials discovery. These \\ninsights are pivotal in addressing fundamental gaps in benchmarking LLMs and \\nautonomous AI agents for scientific applications. \\n \\n \\nINTRODUCTION \\nThe vast size, high dimensionality, and complexity of the materials design space require \\nnew strategies for synthesizing and integrating multidisciplinary scientific knowledge. \\nSuch approaches are essential to drive advances in performance, cost-efficiency, and \\nsustainability [1]. Large language models (LLM), which are trained on large amounts of \\ndata and designed for human interaction, have demonstrated impressive reasoning \\nabilities in natural language processing [2–5]. Within the materials domain, LLMs have \\nbeen used to predict chemical properties [6–8], design new molecules [4, 9–12], automate \\nscientific coding [13, 14], develop AI agents [6, 15–17], extract and synthesize knowledge \\n[18, 19], and summarize and generate text [20]. Despite these successes, LLM adoption \\nin materials science lags behind other fields. This gap stems from a lack of standardized \\n \\n2\\nbenchmarks for validating LLM-based analyses, limited application to practical tasks, and \\nthe specialized expertise required for materials development [21–23].  \\n \\nThese challenges have led to increased interest in agentic workflows—LLM-driven \\nautonomous systems designed to perform complex reasoning, tool use, and multi-step \\ndecision-making [24]. Within the materials domain, agentic systems have been developed \\nto review the literature [25–27], implement routine chemical tasks [16, 21, 28–31] plan \\nexperiments [32–35], automate chemoinformatics analysis [36–39], and generate novel \\nhypotheses [40–42]. One of the most widely adopted implementations of agentic systems \\nis retrieval-augmented generation (RAG), which enhances LLM outputs by integrating \\ninformation retrieval techniques [25]. Rather than relying solely on pre-trained knowledge, \\nRAG dynamically retrieves and incorporates relevant external information, improving \\nresponse accuracy and contextual relevance. This capability is particularly valuable in \\nmaterials science, where access to domain-specific literature, experimental data, and \\nproperty databases is critical for precise predictions and reasoning. By leveraging RAG, \\nagentic workflows can provide more reliable and up-to-date insights, addressing key\\n</Content>\\n</Document>',\n",
       "  '<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2406.04744v2\" date=\"2024-11-01\" authors=\"Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, Xin Luna Dong\"/>\\n<Title>\\nCRAG -- Comprehensive RAG Benchmark\\n</Title>\\n\\n<Summary>\\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising\\nsolution to alleviate Large Language Model (LLM)\\'s deficiency in lack of\\nknowledge. Existing RAG datasets, however, do not adequately represent the\\ndiverse and dynamic nature of real-world Question Answering (QA) tasks. To\\nbridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual\\nquestion answering benchmark of 4,409 question-answer pairs and mock APIs to\\nsimulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a\\ndiverse array of questions across five domains and eight question categories,\\nreflecting varied entity popularity from popular to long-tail, and temporal\\ndynamisms ranging from years to seconds. Our evaluation of this benchmark\\nhighlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve\\n<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the\\naccuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%\\nof questions without any hallucination. CRAG also reveals much lower accuracy\\nin answering questions regarding facts with higher dynamism, lower popularity,\\nor higher complexity, suggesting future research directions. The CRAG benchmark\\nlaid the groundwork for a KDD Cup 2024 challenge and attracted thousands of\\nparticipants and submissions. We commit to maintaining CRAG to serve research\\ncommunities in advancing RAG solutions and general QA solutions. CRAG is\\navailable at https://github.com/facebookresearch/CRAG/.\\n</Summary>\\n\\n<Content>\\nCRAG – Comprehensive RAG Benchmark\\nXiao Yang˚1, Kai Sun˚1, Hao Xin˚3, Yushi Sun˚3, Nikita Bhalla1, Xiangsen Chen4, Sajal\\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\\nand Xin Luna Dong1\\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\\nAbstract\\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\\ntion to alleviate Large Language Model (LLM)’s deficiency in lack of knowledge.\\nExisting RAG datasets, however, do not adequately represent the diverse and dy-\\nnamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we\\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\\nand Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse\\narray of questions across five domains and eight question categories, reflecting\\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\\nfrom years to seconds. Our evaluation of this benchmark highlights the gap to\\nfully trustworthy QA. Whereas most advanced LLMs achieve ď 34% accuracy\\non CRAG, adding RAG in a straightforward manner improves the accuracy only\\nto 44%. State-of-the-art industry RAG solutions only answer 63% of questions\\nwithout any hallucination. CRAG also reveals much lower accuracy in answer-\\ning questions regarding facts with higher dynamism, lower popularity, or higher\\ncomplexity, suggesting future research directions. The CRAG benchmark laid the\\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\\nand submissions. We commit to maintaining CRAG to serve research communities\\nin advancing RAG solutions and general QA solutions. CRAG is available at\\nhttps://github.com/facebookresearch/CRAG/.\\n1\\nIntroduction\\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. Despite the advancements,\\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\\nfactual accuracy or grounding [14,27,30,32]. Studies have shown that GPT-4’s accuracy in answering\\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\\nchanging) facts, GPT-4’s accuracy in answering questions referring to torso-to-tail (less popular)\\nentities is below 35% [29]. Overcoming hallucinations thus becomes a priority in building reliable\\nQA systems [13,14].\\nRetrieval-Augmented Generation (RAG) [6,8,12,19] has recently emerged as a promising solution to\\nalleviate LLM’s deficiency in lack of knowledge and attracted a lot of attention from both academia\\nresearch and industry. Given a question, a RAG system searches external sources to retrieve relevant\\ninformation and then provides grounded answers [7,12,19] (see Figure 1 for an illustration). Despite\\n˚Equal contribution. Correspondence to: Xiao Yang (xiaoyangfb@meta.com).\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.\\narXiv:2406.04744v2  [cs.CL]  1 Nov 2024\\nLLM\\nWhat is the gold \\nprice today?\\nGold price is at $1626.81 per \\nounce today Oct 21 2022.\\nGold price is at $2020.8 per \\nounce today Jan 28 2024.\\nDocuments\\nWeb \\nSearch\\nReal-time \\nAPIs\\nKnowledge \\nGraph\\nRetrieved \\nrelevant\\nknowledge\\nQuestion \\n(a) LLM Direct Generation\\n(b) RAG: Retrieved-Augmented \\nGeneration with LLM\\nFigure 1: QA using LLMs (a) without RAG vs. (b) with RAG.\\nits potential, RAG still faces many challenges, such as selecting the most relevant information,\\nreducing question answering latency, and synthesizing information to answer complex questions.\\nA comprehensive benchmark is currently missing to advance continued research efforts \\n</Content>\\n</Document>',\n",
       "  '<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2409.15337v1\" date=\"2024-09-09\" authors=\"Jie Ouyang, Yucong Luo, Mingyue Cheng, Daoyu Wang, Shuo Yu, Qi Liu, Enhong Chen\"/>\\n<Title>\\nRevisiting the Solution of Meta KDD Cup 2024: CRAG\\n</Title>\\n\\n<Summary>\\nThis paper presents the solution of our team APEX in the Meta KDD CUP 2024:\\nCRAG Comprehensive RAG Benchmark Challenge. The CRAG benchmark addresses the\\nlimitations of existing QA benchmarks in evaluating the diverse and dynamic\\nchallenges faced by Retrieval-Augmented Generation (RAG) systems. It provides a\\nmore comprehensive assessment of RAG performance and contributes to advancing\\nresearch in this field. We propose a routing-based domain and dynamic adaptive\\nRAG pipeline, which performs specific processing for the diverse and dynamic\\nnature of the question in all three stages: retrieval, augmentation, and\\ngeneration. Our method achieved superior performance on CRAG and ranked 2nd for\\nTask 2&3 on the final competition leaderboard. Our implementation is available\\nat this link: https://github.com/USTCAGI/CRAG-in-KDD-Cup2024.\\n</Summary>\\n\\n<Content>\\nRevisiting the Solution of Meta KDD Cup 2024: CRAG\\nJie Ouyang\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nouyang_jie@mail.ustc.edu.cn\\nYucong Luo\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nprime666@mail.ustc.edu.cn\\nMingyue Cheng∗\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nmycheng@ustc.edu.cn\\nDaoyu Wang\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nwdy030428@mail.ustc.edu.cn\\nShuo Yu\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nyu12345@mail.ustc.edu.cn\\nQi Liu\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nqiliuql@ustc.edu.cn\\nEnhong Chen\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\ncheneh@ustc.edu.cn\\nAbstract\\nThis paper presents the solution of our team APEX in the Meta KDD\\nCUP 2024: CRAG Comprehensive RAG Benchmark Challenge. The\\nCRAG benchmark addresses the limitations of existing QA bench-\\nmarks in evaluating the diverse and dynamic challenges faced by\\nRetrieval-Augmented Generation (RAG) systems. It provides a more\\ncomprehensive assessment of RAG performance and contributes\\nto advancing research in this field. We propose a routing-based do-\\nmain and dynamic adaptive RAG pipeline, which performs specific\\nprocessing for the diverse and dynamic nature of the question in all\\nthree stages: retrieval, augmentation, and generation. Our method\\nachieved superior performance on CRAG and ranked 2nd for Task\\n2&3 on the final competition leaderboard. Our implementation is\\navailable at this link: https://github.com/USTCAGI/CRAG-in-KDD-\\nCup2024.\\nCCS Concepts\\n• Information systems →Information retrieval.\\nKeywords\\nRetrieval-Augmented Generation, Large Language Model\\n1\\nIntroduction\\nLarge Language Models (LLMs) have revolutionized the landscape\\nof Natural Language Processing (NLP) tasks [5, 8, 10], particularly in\\nquestion answering (QA). Despite advances in LLMs, hallucination\\nremains a significant challenge, particularly for dynamic facts and\\ninformation about less prominent entities.\\nRetrieval-Augmented Generation (RAG) [9] has recently emerged\\nas a promising solution to mitigate LLMs’ knowledge deficiencies.\\n∗Mingyue Cheng is the corresponding author.\\nGiven a question, a RAG system queries external sources to re-\\ntrieve relevant information and subsequently provides grounded\\nanswers. Despite its potential, RAG continues to face numerous\\nchallenges, including the selection of the most relevant information,\\nthe reduction of question answering latency, and the synthesis of\\ninformation to address complex questions.\\nTo bridge this gap, Meta introduced the Comprehensive RAG\\nBenchmark (CRAG) [13], a factual question answering benchmark\\nof 4,409 question-answer pairs and Mock APIs to simulate web\\nand Knowledge Graph (KG) search, and hosted the KDD CUP 2024\\nChallenge.\\n1.1\\nDataset Description\\nThe CRAG contains two parts of data: the QA pairs and the content\\nfor retrieval.\\nQA pairs. The CRAG dataset contains a rich set of 4,409 QA\\npairs covering five domains: finance, sports, music, movie, and\\nopen domain, and eight types of questions. For the KDD CUP 2024\\nChallenge, the benchmark data were splited into three sets with\\nsimilar distributions: validation, public test, and private test at 30%,\\n30%, and 40%, respectively. In total, 2,706 examples from validation\\nand public test sets were shared.\\nThe dataset also reflects varied entity popularity from popular\\nto long-tail entities, and temporal spans ranging from seconds to\\nyears. Given the temporal nature of many questions, each question-\\nanswer pair is accompanied by an additional field denoted as \"query\\ntime.\" This temporal mark\\n</Content>\\n</Document>']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_arxiv({\"messages\": [(\"user\", \"CRAG에 대해서 설명해주세요.\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac6790",
   "metadata": {},
   "source": [
    "#### 답변 생성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01ce399",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_instructions = \"\"\"당신은 분석가에게 인터뷰를 받는 전문가입니다.\n",
    "\n",
    "분석가의 주요 관심 분야는 다음과 같습니다: {goals}. \n",
    "\n",
    "당신의 목표는 분석가가 제기한 질문에 답변하는 것입니다.\n",
    "\n",
    "질문에 답변할 때는 다음 맥락을 활용하십시오:\n",
    "<Context>\n",
    "{context}\n",
    "<Context>\n",
    "\n",
    "질문에 답변할 때는 다음 지침을 따르십시오:\n",
    "1. 맥락에 제공된 정보만 사용하십시오. \n",
    "2. 외부 정보를 도입하거나 맥락에 명시적으로 언급된 내용을 넘어선 추측을 하지 마십시오.\n",
    "3. 맥락에는 각 개별 문서의 주제별 출처가 포함되어 있습니다.\n",
    "4. 답변에서 관련 진술 옆에 해당 출처를 포함하십시오. 예를 들어 출처 #1의 경우 [1]을 사용하십시오.\n",
    "5. 답변 하단에 출처를 순서대로 나열하십시오. [1] 출처 1, [2] 출처 2, 등\n",
    "6. 출처가 다음과 같은 경우: <Document url=\"assistant/docs/llama3_1.pdf\" page=\"7\" />' \n",
    "    다음처럼 기재하십시오: [1] assistant/docs/llama3_1.pdf, 7 페이지\n",
    "7. 제공된 맥락이 없다면 출처를 기재하기 마십시오.\n",
    "    \n",
    "인용 시 괄호 추가 및 Document 서두 문구는 생략하십시오.\"\"\"\n",
    "\n",
    "\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\"질문에 대한 답변 노드\"\"\"\n",
    "\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    system_message = answer_instructions.format(\n",
    "        goals=analyst.description,\n",
    "        context=\"\\n\".join(state[\"context\"]),\n",
    "    )\n",
    "    response = llm.invoke([(\"system\", system_message)] + state[\"messages\"])\n",
    "    response.name = \"전문가\"\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe5c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 전문가\n",
      "\n",
      "Modular RAG는 기존의 Naive RAG와 비교할 때 구조적 유연성과 확장성 측면에서 차별화됩니다. Naive RAG는 단일 모듈 내에서 정보 검색과 응답 생성이 이루어지는 반면, Modular RAG는 기능별로 모듈을 분리하여 각 모듈별 최적화 및 독립적인 업그레이드가 가능합니다. 이러한 구조적 차이는 스타트업이 변화하는 시장 조건에 빠르게 적응하고, 비즈니스 모델 혁신을 추진하는 데 유리합니다.\n",
      "\n",
      "Production level에서 Modular RAG를 사용하는 주요 이점은 다음과 같습니다:\n",
      "\n",
      "1. 유지보수 용이성: 모듈별 독립 운영으로 인해 문제 발생 시 전체 시스템이 아닌 해당 모듈에만 집중할 수 있어 신속한 대응이 가능하다.\n",
      "2. 확장성: 새로운 기능이나 데이터 소스를 추가할 때 기존 시스템에 최소한의 영향을 주며 통합할 수 있다.\n",
      "3. 유연성: 시장 변화나 사용자 요구에 따라 모듈 간 조합을 변경하여 다양한 비즈니스 모델에 맞춤 대응이 가능하다.\n",
      "\n",
      "이와 같은 이점들은 스타트업 단계에서 자주 요구되는 빠른 피봇과 혁신을 지원하며, 기존 Naive RAG 대비 보다 강력한 적응형 시스템 구현에 기여합니다[1].\n",
      "\n",
      "[1] assistant/docs/llama3_1.pdf, 7 페이지\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = generate_answer(\n",
    "    {\n",
    "        \"analyst\": analyst,\n",
    "        \"context\": [],\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점\",\n",
    "                name=\"분석가\",\n",
    "            )\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624648b",
   "metadata": {},
   "source": [
    "### 문석 작성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c9e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기술 문서 작성 프롬프트\n",
    "section_writer_instructions = \"\"\"당신은 전문 기술 문서 작성자입니다.\n",
    "\n",
    "당신의 임무는 일련의 원본 문서를 철저히 분석하여 보고서의 상세하고 포괄적인 섹션을 작성하는 것입니다.\n",
    "이는 핵심 통찰력을 추출하고, 관련 사항을 상세히 설명하며, 명확성과 이해를 보장하기 위한 심층적인 해설을 제공하는 것을 포함합니다. 필요한 배경 정보, 뒷받침하는 증거, 예시를 포함하여 독자의 이해를 높여야 합니다. 논리적이고 체계적인 구조를 유지하며, 모든 핵심 사항이 상세히 다루어지고 전문적인 어조로 제시되도록 하십시오.\n",
    "\n",
    "다음 지침을 따르십시오:\n",
    "1. 원본 문서의 내용 분석:\n",
    "- 각 원본 문서의 이름은 문서 시작 부분에 <Document> 태그와 함께 기재되어 있습니다.\n",
    "\n",
    "2. 마크다운 서식을 사용하여 보고서 구조 생성:\n",
    "- 섹션 제목에는 ## 사용\n",
    "- 하위 섹션 헤더에는 ### 사용\n",
    "\n",
    "3. 다음 구조에 따라 보고서 작성:\n",
    "a. 제목 (## 헤더)\n",
    "b. 요약 (### 헤더)\n",
    "c. 종합 분석 (### 헤더)\n",
    "d. 출처 (### 헤더)\n",
    "\n",
    "4. 분석가의 중점 분야를 반영하여 제목을 흥미롭게 작성하십시오: \n",
    "{focus}\n",
    "\n",
    "5. 요약 섹션 작성 시:\n",
    "- 분석가의 중점 분야와 관련된 일반적 배경/맥락을 요약으로 제시하십시오\n",
    "- 인터뷰에서 수집한 통찰 중 새롭거나 흥미롭거나 놀라운 점을 강조하십시오\n",
    "- 사용한 출처 문서를 번호 매긴 목록으로 작성하십시오\n",
    "- 인터뷰어 또는 전문가의 이름은 언급하지 마십시오\n",
    "- 최대 약 400단어를 목표로 하십시오\n",
    "- 출처 문서의 정보에 기반하여 보고서에서 번호 매긴 출처([1], [2] 등)를 사용하십시오\n",
    "\n",
    "6. 종합 분석 섹션:\n",
    "- 출처 문서의 정보를 상세히 검토하십시오.\n",
    "- 복잡한 아이디어를 이해하기 쉬운 단위로 분해하고 논리적인 흐름을 유지하십시오.\n",
    "- 분석의 다양한 관점이나 차원을 다루기 위해 필요한 경우 하위 섹션을 사용하십시오.\n",
    "- 원본 문서의 데이터, 직접 인용문, 예시를 통해 분석을 뒷받침하십시오.\n",
    "- 각 논점이 보고서의 전반적인 초점과 어떻게 관련되는지 명확히 설명하십시오.\n",
    "- 여러 관련 아이디어를 제시할 때는 명확성을 위해 글머리 기호나 번호 매기기 목록을 사용하십시오.\n",
    "- 전문적이고 객관적인 어조를 유지하며 편향되거나 근거 없는 의견을 피하십시오.\n",
    "- 분석이 철저하도록 최소 800단어를 목표로 하십시오.\n",
    "\n",
    "7. 출처 섹션에서:\n",
    "- 보고서에 사용된 모든 출처를 포함하십시오\n",
    "- 관련 웹사이트 또는 특정 문서 경로의 전체 링크를 제공하십시오\n",
    "- 각 출처는 새 줄로 구분하십시오. 마크다운에서 새 줄을 만들기 위해 각 줄 끝에 두 개의 공백을 사용하십시오.\n",
    "- 예시:\n",
    "    ### 출처\n",
    "    [1] 링크 또는 문서명\n",
    "    [2] 링크 또는 문서명\n",
    "\n",
    "8. 출처를 반드시 통합하십시오. 예를 들어 다음은 올바르지 않습니다:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "중복 출처는 없어야 합니다. 다음과 같이 간결하게 작성하십시오:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "9. 최종 검토:\n",
    "- 보고서가 요구되는 구조를 따르는지 확인하십시오.\n",
    "- 보고서 제목 앞에 서문을 포함하지 마십시오.\n",
    "- 모든 지침이 준수되었는지 확인하십시오.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.messages import get_buffer_string\n",
    "\n",
    "\n",
    "def write_section(state: InterviewState):\n",
    "    \"\"\"인터뷰에 대한 리포트 작성\"\"\"\n",
    "\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    # 인터뷰를 문자열로 변환\n",
    "    interview = get_buffer_string(state[\"messages\"])\n",
    "\n",
    "    # 섹션 작성을 위한 시스템 프롬프트 정의\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "    response = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [\n",
    "            HumanMessage(\n",
    "                content=f\"분석가와 전문가의 인터뷰 내용입니다:\\n<Interview>\\n{interview}\\n</Interview>\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"이 자료를 사용하여 해당 섹션을 작성하십시오:\\n<Resources>\\n{state['context']}\\n</Resources>\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return {\"sections\": [response.content]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6b207",
   "metadata": {},
   "source": [
    "### 인터뷰 그래프 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "def should_continue_interview(state: InterviewState) -> Literal[\"finish\", \"continue\"]:\n",
    "    \"\"\"인터뷰를 계속 진행할 지 여부를 판단합니다.\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get(\"max_num_turns\", 2)\n",
    "\n",
    "    expert_message_count = sum(\n",
    "        1 for m in messages if isinstance(m, AIMessage) and m.name == \"전문가\"\n",
    "    )\n",
    "\n",
    "    # 전문가가 최대 턴 이상 답변했다면 인터뷰를 종료합니다.\n",
    "    if expert_message_count >= max_num_turns:\n",
    "        return \"finish\"\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    if \"도움 주셔서 정말 감사합니다\" in last_message.content:\n",
    "        return \"finish\"\n",
    "\n",
    "    return \"continue\"\n",
    "\n",
    "\n",
    "interview_builder = StateGraph(InterviewState)\n",
    "interview_builder.add_node(generate_question)  # 질문 생성\n",
    "interview_builder.add_node(search_web)  # 웹 검색\n",
    "interview_builder.add_node(search_arxiv)  # arxiv 검색\n",
    "interview_builder.add_node(generate_answer)  # 답변 생성\n",
    "interview_builder.add_node(write_section)  # 문서 작성\n",
    "\n",
    "interview_builder.set_entry_point(\"generate_question\")\n",
    "interview_builder.add_edge(\"generate_question\", \"search_web\")\n",
    "interview_builder.add_edge(\"generate_question\", \"search_arxiv\")\n",
    "interview_builder.add_edge(\"search_web\", \"generate_answer\")\n",
    "interview_builder.add_edge(\"search_arxiv\", \"generate_answer\")\n",
    "interview_builder.add_conditional_edges(\n",
    "    \"generate_answer\",\n",
    "    should_continue_interview,\n",
    "    {\n",
    "        \"continue\": \"generate_question\",\n",
    "        \"finish\": \"write_section\",\n",
    "    },\n",
    ")\n",
    "interview_builder.set_finish_point(\"write_section\")\n",
    "\n",
    "interview_graph = interview_builder.compile(\n",
    "    checkpointer=InMemorySaver(),\n",
    ").with_config(run_name=\"Conduct Interviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf57641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(interview_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252d966",
   "metadata": {},
   "source": [
    "### 인터뷰 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3f343ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 분석가\n",
      "\n",
      "안녕하세요, 저는 Seokho Phil입니다. 테크 스타트업을 운영하며 AI 제품 전략을 맡고 있습니다. 오늘은 Modular RAG와 Naive RAG의 차이점, 그리고 Production 단계에서 Modular RAG가 갖는 구체적인 이점에 대해 깊이 파고들고 싶습니다.\n",
      "\n",
      "우선, 일반적인 Naive RAG는 보통 어떤 구성요소로 이뤄져 있고, modular RAG가 등장하면서 달라지는 구조나 실행 흐름이 어떤 점에서 차별화 되었나요? 실제 사례를 들어 설명해주실 수 있을까요?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_answer\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 전문가\n",
      "\n",
      "네, 좋은 질문입니다. Naive RAG는 기본적으로 외부 지식 베이스에서 관련 문서를 검색하여 이를 바탕으로 답변을 생성하는 단순한 형태입니다. 이에 비해 Modular RAG는 검색과 생성 과정을 별도의 모듈로 분리하여, 각 기능을 독립적이고 전문적으로 개선할 수 있게 한 점이 큰 차이입니다. \n",
      "\n",
      "Modular RAG의 주요 장점은 다음과 같습니다. 첫째, 검색 모듈과 생성 모듈을 각각 최적화할 수 있어 효율성과 응답의 정확성을 향상시킵니다. 둘째, 모듈별로 교체나 추가가 쉬워 특정 업무에 필요한 기능을 맞춤형으로 구성할 수 있습니다. 예를 들어, 특정 도메인에서는 고도화된 생성 모델(T5나 GPT-3 같은)을 도입하여 더욱 자연스러운 답변을 생성할 수 있습니다. 반면, 검색 모듈은 도메인 특화 데이터베이스를 효율적으로 탐색하도록 설계할 수 있습니다. \n",
      "\n",
      "이러한 분리 구조와 모듈화 덕분에, 프로덕션 환경에서 Modular RAG는 확장성, 유지보수성, 그리고 다양한 업무 요구에 대한 적응성이 크게 뛰어납니다. 특히 빠르게 변화하는 시장과 복잡한 도메인에서는 한 부분만 업그레이드하거나 재설계하는 것이 가능해 혁신을 지속할 수 있습니다.\n",
      "\n",
      "요약하면, Naive RAG는 일체형 시스템이지만, Modular RAG는 구성요소별로 전문화하고 활성화하여 생산 환경에서 더 높은 유연성과 성능을 제공합니다[4][5]. \n",
      "\n",
      "[4] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
      "[5] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 분석가\n",
      "\n",
      "Seokho Phil입니다. 말씀해주셔서 감사합니다. Modular RAG의 모듈화 접근은 실질적으로 유지보수와 확장 측면에서 큰 혁신을 가져오는군요.\n",
      "\n",
      "조금 더 구체적으로 묻겠습니다. 예를 들어, 실제 production 환경에서 Modular RAG를 도입한 후, Naive RAG 대비 사용자 응답 속도나, 검색 정확도의 개선 사례가 실제로 있나요? 그리고 이를 가능하게 한 구조나 기술적 특징은 무엇인지요? \n",
      "\n",
      "또한, 기업이 Modular RAG를 적용할 때 흔히 직면하는 기술적 도전이나 한계는 어떤 점이었는지도 궁금합니다.\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_answer\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 전문가\n",
      "\n",
      "네, 실제 프로덕션 환경에서 Modular RAG는 Naive RAG에 비해 여러 면에서 성능 개선을 제공합니다. \n",
      "\n",
      "첫째, Modular RAG는 검색과 생성 과정을 별도의 모듈로 분리함으로써 각 모듈을 최적화할 수 있습니다. 예를 들어, 검색 모듈은 빠른 인덱싱과 효율적인 문서 검색을 담당하고, 생성 모듈은 고급 언어 모델을 사용해 더 정확하고 자연스러운 답변을 생성합니다. 이 분리는 응답 속도 향상에 기여하며, 검색 모듈의 성능이나 생성 모듈의 용량 조정에 따라 시스템 전반의 효율을 개선할 수 있습니다[4].\n",
      "\n",
      "둘째, 검색의 정확도 측면에서 Modular RAG는 LLM과 검색 모듈의 긴밀한 협업을 통해 보다 정밀한 문서 선택이 가능합니다. 이는 naive RAG가 단순히 관련 문서 일괄 검색하는 방식과 달리, 각각의 모듈에서 보다 고도화된 알고리즘을 활용해 정확도를 높일 수 있음을 의미합니다[5].\n",
      "\n",
      "기술적 도전으로는 모듈 간 인터페이스 설계, 데이터 동기화 및 통합 관리, 그리고 각 모듈의 실시간 성능 모니터링과 같은 복잡한 운영 이슈가 있습니다. 또한, 모듈화를 진행하는 과정에서 시스템 통합 테스트가 증가해 개발 초기에는 시간과 비용이 더 소요될 수 있습니다. 그러나 장기적으로는 유지보수성과 확장성을 높임으로써 투자 대비 큰 가치를 제공합니다[4][5].\n",
      "\n",
      "요약하자면, Modular RAG는 효율성과 정확도의 균형을 맞추면서, 복잡한 멀티모듈 환경에서 기술적 안정성과 확장성을 제공하는 것이 강점입니다.\n",
      "\n",
      "[4] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
      "[5] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mwrite_section\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "## Bridging Modular RAG and Startup Innovation: Enhancing AI-driven Adaptive Growth and Competitive Edge\n",
      "\n",
      "### 요약\n",
      "\n",
      "Retrieval-Augmented Generation (RAG)은 대규모 언어 모델(LLM)의 성능을 외부 지식 기반과 결합하여 크게 향상시키는 기술로, 최근 스타트업 AI 제품 전략에서 핵심 역할을 맡고 있다. 특히 Modular RAG는 Naive RAG 대비 검색과 생성 기능을 모듈화하여 각각 독립적으로 최적화 및 교체가 가능하며, 이는 생산 환경에서의 유지보수성, 확장성 및 적응성 측면에서 혁신적인 진전을 이루었다[4][5]. 실제 프로덕션 상에서는 Modular RAG가 모듈 간 인터페이스 설계, 데이터 동기화 및 실시간 모니터링 같은 초기 복잡성에도 불구하고, 성능 향상과 유연성 덕분에 투자 대비 큰 가치를 제공한다[4][5]. \n",
      "\n",
      "최근 연구들은 Multi-Agent 기반 MA-RAG와 Vendi-RAG같은 고도화된 RAG 프레임워크들을 통해 다중 문서 연결, 다양성-품질 균형 최적화, 협업적 추론 체계를 구현하여 다중 홉 질의응답 및 복합 정보 탐색에서 성능을 크게 개선하였다[arxiv:2505.20096v2][arxiv:2502.11228v2]. 또한, 특정 도메인(예: 항공, 자동차, 제약)에서는 Graph RAG, SQL RAG 등 특화된 RAG 스타일이 홀루시네이션 감소, 동적 문제 해결능력 강화에 유리함을 보였다[arxiv:2505.13006v1][arxiv:2408.05933v1][arxiv:2507.21103v1].\n",
      "\n",
      "스타트업 창업자들이 Modular RAG를 도입해 제품화할 때는 시스템 모듈화의 장점과 함께, 초기 시스템 통합 및 테스트 비용, 모듈 간 조화 및 운영 안정성 과제에 대비해야 한다. 그러나 적절한 모듈 구성과 최신 LLM 및 도메인 특화 기술 적용을 통해, 급변하는 시장 요구에 맞춘 맞춤형 AI 솔루션을 효율적으로 내놓을 수 있는 가능성이 크다[4][5]. \n",
      "\n",
      "출처 문서들은 Modular RAG의 근간인 분리-최적화 구조가 어떻게 실제 생산 환경에서 높은 응답 정확도와 확장성을 보증하며, 다양한 RAG 확장 모델과 앙상블 방법론이 미래 AI 제품 경쟁력 확보에 중요한 역할을 할 것임을 시사한다[arxiv:2508.13828v1][4][5]. \n",
      "\n",
      "### 종합 분석\n",
      "\n",
      "#### 1. Modular RAG vs Naive RAG: 기술적 차별점과 생산 환경 적용\n",
      "\n",
      "Naive RAG는 하나의 통합 프로세스로 외부 지식 검색과 답변 생성이 함께 이루어지는 단순한 RAG 체계이다. 이 구조는 단일 컴포넌트 최적화가 어렵고, 검색과 생성 사이의 조화가 제한적이다. 결과적으로, 검색의 정확도 향상이나 생성 모델 교체 시 시스템 전체 재설계가 필요할 수 있다[4].\n",
      "\n",
      "반면, Modular RAG는 검색 모듈과 언어 생성 모듈을 완전히 분리해 서로 독립적으로 최적화하고 교체 가능하게 하였다. 예를 들어, 검색은 특정 도메인 특화 인덱싱 및 검색 알고리즘이 적용되는 반면, 생성 모듈은 GPT-3, T5 등 최신 고성능 언어 모델을 자유롭게 도입 가능하다. 이러한 분리는 다음과 같은 이점을 제공한다[4][5]:\n",
      "\n",
      "- **효율성 향상:** 각 모듈의 성능 튜닝에 집중하여 검색 속도와 답변 품질 동시 개선 가능  \n",
      "- **유연한 확장성:** 특정 도메인 대응을 위해 검색이나 생성 모듈만 교체·추가 가능  \n",
      "- **유지보수 용이성:** 모듈 별 문제 진단이 쉬워 오류 수정 및 기능 추가에 빠르게 대응  \n",
      "- **응답 정확도 향상:** 향상된 검색 정확도와 생성 품질의 시너지로 사용자 만족도 상승  \n",
      "\n",
      "이러한 구조적 혁신은 스타트업이 다양한 고객 요구사항에 맞춘 제품 로드맵을 유연하게 설계하고 빠르게 혁신할 수 있는 발판이 된다[4][5].\n",
      "\n",
      "#### 2. 생산 환경에서의 성능 개선 사례와 기술적 도전\n",
      "\n",
      "프로덕션 빌드된 Modular RAG 시스템에서 경험하는 주요 개선점은 다음과 같다:\n",
      "\n",
      "- **응답 속도:** 검색 모듈과 생성 모듈의 독립적 최적화를 통해 병목 최소화 및 처리 병렬화 가능  \n",
      "- **검색 정확도:** LLM과 검색 모듈 간 협업 체계로 더 정밀한 문서 선택과 다중 문서 연관성 반영  \n",
      "- **유연한 모듈 업그레이드:** GPT-3 → GPT-4처럼 생성 모듈 교체가 용이해 신기술 신속 적용  \n",
      "\n",
      "반면, 기술 과제로는 모듈 간 인터페이스 설계의 복잡성과 데이터 동기화, 실시간 성능 모니터링이 있으며, 초기 개발 시 통합 테스트와 운영 안정화에 더 많은 시간과 비용이 소요된다[4][5]. 그러나 장기적으로는 이러한 초기 비용이 유지보수 비용 감소 및 신속한 비즈니스 대응으로 상쇄된다.\n",
      "\n",
      "#### 3. 최신 RAG 연구 동향과 Modular RAG의 진화\n",
      "\n",
      "- **Vendi-RAG (arxiv:2502.11228v2):** 검색 다변성(Diversity)과 응답 품질(Quality)의 균형을 반복적으로 조정하는 프레임워크로, 다중 홉 질문에서 정확도+4% 이상 상승시켜 Modular RAG의 한계를 다층적 문서 연결과 다양성 관리로 확장함  \n",
      "- **MA-RAG (arxiv:2505.20096v2):** 다수 협업형 에이전트(Planner, Extractor, QA 등) 기반 Modular RAG 접근법으로 각 에이전트가 전문화 단계 수행, 체계적 추론 및 모듈 해석 가능성 제고로 다중 홉과 복합 질의 처리 능력 극대화  \n",
      "- **RAG Ensemble (arxiv:2508.13828v1):** 복수의 RAG 시스템을 파이프라인 및 모듈 단계에서 앙상블, 정보 엔트로피 분석으로 다각적 리스크 완화 및 일반성 강화, 스타트업이 복수 RAG 솔루션 활용해 리스크 분산 가능함 적극 시사\n",
      "\n",
      "#### 4. 도메인 특화 Modular RAG의 사례 연구\n",
      "\n",
      "- **공항 Conversational AI (arxiv:2505.13006v1):** Graph RAG와 SQL RAG 도입으로 전통 RAG 대비 홀루시네이션 감소 및 동적 계산/추론 정확도 향상, 산업 안전성 강화  \n",
      "- **자동차 산업 PDF 챗봇 (arxiv:2408.05933v1):** 지역 로컬 Ollama 모델 활용 Modular RAG 맞춤 최적화, 복잡한 다단계 문서 레이아웃과 기술용어에 대응, 초경량/저성능 환경에서도 실효성 확보  \n",
      "- **제약 문서 분석 (arxiv:2507.21103v1):** PDF 문서 내 비구조화 정보의 정확한 의미론적 추출과 자연어생성 통합으로 효율적인 기술문서 자동분석 가능성 검증\n",
      "\n",
      "#### 5. 스타트업 창업자의 전략적 시사점\n",
      "\n",
      "스타트업 창업자 입장에서 Modular RAG는 기술 혁신과 비즈니스 민첩성의 접점에 위치한다. 분리된 모듈 단위의 독립적 최적화가 가능함에 따라, 스타트업은 다음과 같은 사업적 이점을 얻는다:\n",
      "\n",
      "- 고객과 시장 요구 변화에 따라 부분 업그레이드 혹은 맞춤 모듈 추가 신속 대응  \n",
      "- 경쟁사 대비 높은 품질, 빠른 응답, 유연한 확장으로 차별화된 제품 제공  \n",
      "- 초기 개발 투자 집중 관리하나, 장기적으로 유지보수 비용 및 기술 부채 최소화  \n",
      "- 다양한 도메인 특화 RAG 기술(예: Graph RAG, Ensemble RAG) 도입으로 확장성 및 안정성 확보  \n",
      "\n",
      "그러나 이를 위해서는 초기 설계과정에서 모듈 간 통합 인터페이스의 엄격한 규격화, 실시간 모니터링 인프라 구축, 개발 단계에서 충분한 통합 테스트가 필수적이다. 아울러 LLM과 외부 탐색 모듈 사이의 데이터 및 작업 흐름 효율적 관리 역량도 성장의 관건이다.\n",
      "\n",
      "### 출처\n",
      "\n",
      "[1] http://arxiv.org/abs/2502.11228v2  \n",
      "[2] http://arxiv.org/abs/2508.13828v1  \n",
      "[3] http://arxiv.org/abs/2505.13006v1  \n",
      "[4] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
      "[5] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
      "[6] http://arxiv.org/abs/2505.20096v2  \n",
      "[7] http://arxiv.org/abs/2408.05933v1  \n",
      "[8] http://arxiv.org/abs/2507.21103v1\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import invoke_graph\n",
    "\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=30,\n",
    "    configurable={\"thread_id\": random()},\n",
    ")\n",
    "\n",
    "topic = \"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점\"\n",
    "\n",
    "# 그래프 실행\n",
    "invoke_graph(\n",
    "    interview_graph,\n",
    "    {\n",
    "        \"topic\": topic,\n",
    "        \"analyst\": analysts[0],\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                f'그래서 당신이 이 주제에 대해서 글을 쓰고 있다고 했죠? 라고 말씀하셨죠? \"{topic}\"',\n",
    "            )\n",
    "        ],\n",
    "        \"max_num_turns\": 2,\n",
    "    },\n",
    "    config=config,\n",
    "    node_names=[\"generate_question\", \"generate_answer\", \"write_section\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ee1f7a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Bridging Modular RAG and Startup Innovation: Enhancing AI-driven Adaptive Growth and Competitive Edge\n",
       "\n",
       "### 요약\n",
       "\n",
       "Retrieval-Augmented Generation (RAG)은 대규모 언어 모델(LLM)의 성능을 외부 지식 기반과 결합하여 크게 향상시키는 기술로, 최근 스타트업 AI 제품 전략에서 핵심 역할을 맡고 있다. 특히 Modular RAG는 Naive RAG 대비 검색과 생성 기능을 모듈화하여 각각 독립적으로 최적화 및 교체가 가능하며, 이는 생산 환경에서의 유지보수성, 확장성 및 적응성 측면에서 혁신적인 진전을 이루었다[4][5]. 실제 프로덕션 상에서는 Modular RAG가 모듈 간 인터페이스 설계, 데이터 동기화 및 실시간 모니터링 같은 초기 복잡성에도 불구하고, 성능 향상과 유연성 덕분에 투자 대비 큰 가치를 제공한다[4][5]. \n",
       "\n",
       "최근 연구들은 Multi-Agent 기반 MA-RAG와 Vendi-RAG같은 고도화된 RAG 프레임워크들을 통해 다중 문서 연결, 다양성-품질 균형 최적화, 협업적 추론 체계를 구현하여 다중 홉 질의응답 및 복합 정보 탐색에서 성능을 크게 개선하였다[arxiv:2505.20096v2][arxiv:2502.11228v2]. 또한, 특정 도메인(예: 항공, 자동차, 제약)에서는 Graph RAG, SQL RAG 등 특화된 RAG 스타일이 홀루시네이션 감소, 동적 문제 해결능력 강화에 유리함을 보였다[arxiv:2505.13006v1][arxiv:2408.05933v1][arxiv:2507.21103v1].\n",
       "\n",
       "스타트업 창업자들이 Modular RAG를 도입해 제품화할 때는 시스템 모듈화의 장점과 함께, 초기 시스템 통합 및 테스트 비용, 모듈 간 조화 및 운영 안정성 과제에 대비해야 한다. 그러나 적절한 모듈 구성과 최신 LLM 및 도메인 특화 기술 적용을 통해, 급변하는 시장 요구에 맞춘 맞춤형 AI 솔루션을 효율적으로 내놓을 수 있는 가능성이 크다[4][5]. \n",
       "\n",
       "출처 문서들은 Modular RAG의 근간인 분리-최적화 구조가 어떻게 실제 생산 환경에서 높은 응답 정확도와 확장성을 보증하며, 다양한 RAG 확장 모델과 앙상블 방법론이 미래 AI 제품 경쟁력 확보에 중요한 역할을 할 것임을 시사한다[arxiv:2508.13828v1][4][5]. \n",
       "\n",
       "### 종합 분석\n",
       "\n",
       "#### 1. Modular RAG vs Naive RAG: 기술적 차별점과 생산 환경 적용\n",
       "\n",
       "Naive RAG는 하나의 통합 프로세스로 외부 지식 검색과 답변 생성이 함께 이루어지는 단순한 RAG 체계이다. 이 구조는 단일 컴포넌트 최적화가 어렵고, 검색과 생성 사이의 조화가 제한적이다. 결과적으로, 검색의 정확도 향상이나 생성 모델 교체 시 시스템 전체 재설계가 필요할 수 있다[4].\n",
       "\n",
       "반면, Modular RAG는 검색 모듈과 언어 생성 모듈을 완전히 분리해 서로 독립적으로 최적화하고 교체 가능하게 하였다. 예를 들어, 검색은 특정 도메인 특화 인덱싱 및 검색 알고리즘이 적용되는 반면, 생성 모듈은 GPT-3, T5 등 최신 고성능 언어 모델을 자유롭게 도입 가능하다. 이러한 분리는 다음과 같은 이점을 제공한다[4][5]:\n",
       "\n",
       "- **효율성 향상:** 각 모듈의 성능 튜닝에 집중하여 검색 속도와 답변 품질 동시 개선 가능  \n",
       "- **유연한 확장성:** 특정 도메인 대응을 위해 검색이나 생성 모듈만 교체·추가 가능  \n",
       "- **유지보수 용이성:** 모듈 별 문제 진단이 쉬워 오류 수정 및 기능 추가에 빠르게 대응  \n",
       "- **응답 정확도 향상:** 향상된 검색 정확도와 생성 품질의 시너지로 사용자 만족도 상승  \n",
       "\n",
       "이러한 구조적 혁신은 스타트업이 다양한 고객 요구사항에 맞춘 제품 로드맵을 유연하게 설계하고 빠르게 혁신할 수 있는 발판이 된다[4][5].\n",
       "\n",
       "#### 2. 생산 환경에서의 성능 개선 사례와 기술적 도전\n",
       "\n",
       "프로덕션 빌드된 Modular RAG 시스템에서 경험하는 주요 개선점은 다음과 같다:\n",
       "\n",
       "- **응답 속도:** 검색 모듈과 생성 모듈의 독립적 최적화를 통해 병목 최소화 및 처리 병렬화 가능  \n",
       "- **검색 정확도:** LLM과 검색 모듈 간 협업 체계로 더 정밀한 문서 선택과 다중 문서 연관성 반영  \n",
       "- **유연한 모듈 업그레이드:** GPT-3 → GPT-4처럼 생성 모듈 교체가 용이해 신기술 신속 적용  \n",
       "\n",
       "반면, 기술 과제로는 모듈 간 인터페이스 설계의 복잡성과 데이터 동기화, 실시간 성능 모니터링이 있으며, 초기 개발 시 통합 테스트와 운영 안정화에 더 많은 시간과 비용이 소요된다[4][5]. 그러나 장기적으로는 이러한 초기 비용이 유지보수 비용 감소 및 신속한 비즈니스 대응으로 상쇄된다.\n",
       "\n",
       "#### 3. 최신 RAG 연구 동향과 Modular RAG의 진화\n",
       "\n",
       "- **Vendi-RAG (arxiv:2502.11228v2):** 검색 다변성(Diversity)과 응답 품질(Quality)의 균형을 반복적으로 조정하는 프레임워크로, 다중 홉 질문에서 정확도+4% 이상 상승시켜 Modular RAG의 한계를 다층적 문서 연결과 다양성 관리로 확장함  \n",
       "- **MA-RAG (arxiv:2505.20096v2):** 다수 협업형 에이전트(Planner, Extractor, QA 등) 기반 Modular RAG 접근법으로 각 에이전트가 전문화 단계 수행, 체계적 추론 및 모듈 해석 가능성 제고로 다중 홉과 복합 질의 처리 능력 극대화  \n",
       "- **RAG Ensemble (arxiv:2508.13828v1):** 복수의 RAG 시스템을 파이프라인 및 모듈 단계에서 앙상블, 정보 엔트로피 분석으로 다각적 리스크 완화 및 일반성 강화, 스타트업이 복수 RAG 솔루션 활용해 리스크 분산 가능함 적극 시사\n",
       "\n",
       "#### 4. 도메인 특화 Modular RAG의 사례 연구\n",
       "\n",
       "- **공항 Conversational AI (arxiv:2505.13006v1):** Graph RAG와 SQL RAG 도입으로 전통 RAG 대비 홀루시네이션 감소 및 동적 계산/추론 정확도 향상, 산업 안전성 강화  \n",
       "- **자동차 산업 PDF 챗봇 (arxiv:2408.05933v1):** 지역 로컬 Ollama 모델 활용 Modular RAG 맞춤 최적화, 복잡한 다단계 문서 레이아웃과 기술용어에 대응, 초경량/저성능 환경에서도 실효성 확보  \n",
       "- **제약 문서 분석 (arxiv:2507.21103v1):** PDF 문서 내 비구조화 정보의 정확한 의미론적 추출과 자연어생성 통합으로 효율적인 기술문서 자동분석 가능성 검증\n",
       "\n",
       "#### 5. 스타트업 창업자의 전략적 시사점\n",
       "\n",
       "스타트업 창업자 입장에서 Modular RAG는 기술 혁신과 비즈니스 민첩성의 접점에 위치한다. 분리된 모듈 단위의 독립적 최적화가 가능함에 따라, 스타트업은 다음과 같은 사업적 이점을 얻는다:\n",
       "\n",
       "- 고객과 시장 요구 변화에 따라 부분 업그레이드 혹은 맞춤 모듈 추가 신속 대응  \n",
       "- 경쟁사 대비 높은 품질, 빠른 응답, 유연한 확장으로 차별화된 제품 제공  \n",
       "- 초기 개발 투자 집중 관리하나, 장기적으로 유지보수 비용 및 기술 부채 최소화  \n",
       "- 다양한 도메인 특화 RAG 기술(예: Graph RAG, Ensemble RAG) 도입으로 확장성 및 안정성 확보  \n",
       "\n",
       "그러나 이를 위해서는 초기 설계과정에서 모듈 간 통합 인터페이스의 엄격한 규격화, 실시간 모니터링 인프라 구축, 개발 단계에서 충분한 통합 테스트가 필수적이다. 아울러 LLM과 외부 탐색 모듈 사이의 데이터 및 작업 흐름 효율적 관리 역량도 성장의 관건이다.\n",
       "\n",
       "### 출처\n",
       "\n",
       "[1] http://arxiv.org/abs/2502.11228v2  \n",
       "[2] http://arxiv.org/abs/2508.13828v1  \n",
       "[3] http://arxiv.org/abs/2505.13006v1  \n",
       "[4] https://adasci.org/how-does-modular-rag-improve-upon-naive-rag/  \n",
       "[5] https://www.superteams.ai/blog/how-to-implement-naive-rag-advanced-rag-and-modular-rag  \n",
       "[6] http://arxiv.org/abs/2505.20096v2  \n",
       "[7] http://arxiv.org/abs/2408.05933v1  \n",
       "[8] http://arxiv.org/abs/2507.21103v1"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "snapshot = interview_graph.get_state(config=config)\n",
    "display(Markdown(snapshot.values[\"sections\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c74d83",
   "metadata": {},
   "source": [
    "## 인터뷰를 병렬로 실행 (map-reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "32cc2a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from typing import TypedDict, Annotated\n",
    "\n",
    "\n",
    "class ResearchGraphState(TypedDict):\n",
    "    \"\"\"ResearchGraphState 상태 정의\"\"\"\n",
    "\n",
    "    topic: Annotated[str, \"연구할 주제\"]\n",
    "    max_analysts: Annotated[int, \"생성할 분석가의 최대 수\"]\n",
    "    human_analyst_feedback: Annotated[str, \"인간 분석가로부터 받은 피드백\"]\n",
    "    analysts: Annotated[list[Analyst], operator.add, \"분석가 목록\"]\n",
    "    sections: Annotated[list, operator.add, \"보고서 섹션 리스트\"]\n",
    "    introduction: Annotated[str, \"최종 보고서의 서론\"]\n",
    "    content: Annotated[str, \"최종 보고서의 본문 내용\"]\n",
    "    conclusion: Annotated[str, \"최종 보고서의 결론\"]\n",
    "    final_report: Annotated[str, \"완성된 최종 보고서\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e884dd3",
   "metadata": {},
   "source": [
    "### 인터뷰 작성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5f6c2efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Send\n",
    "\n",
    "\n",
    "def initiate_all_interviews(state: ResearchGraphState):\n",
    "    \"\"\"인터뷰를 진행합니다.\"\"\"\n",
    "\n",
    "    topic = state[\"topic\"]\n",
    "    analysts = state[\"analysts\"]\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\")\n",
    "\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "    else:\n",
    "        inputs = {\n",
    "            \"topic\": topic,\n",
    "            \"analyst\": analyst,\n",
    "            \"messages\": [\n",
    "                HumanMessage(\n",
    "                    content=f'그래서 당신이 이 주제에 대해서 글을 쓰고 있다고 했죠? \"{topic}\"'\n",
    "                )\n",
    "            ],\n",
    "        }\n",
    "        return [Send(\"conduct_interview\", inputs) for analyst in analysts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bcd9b3",
   "metadata": {},
   "source": [
    "### 최종 보고서 작성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 보고서 작성 지시사항\n",
    "report_writer_instructions = \"\"\"당신은 기술 문서 작성자로서 다음 주제 전반에 대한 보고서를 작성합니다:  \n",
    "\n",
    "주제: {topic}  \n",
    "\n",
    "당신에게는 분석가 팀이 있습니다. 각 분석가는 다음 두 가지를 수행했습니다:  \n",
    "\n",
    "1. 특정 하위 주제에 대한 전문가와의 인터뷰를 진행했습니다.  \n",
    "2. 조사 결과를 메모로 작성했습니다.  \n",
    "\n",
    "## 당신의 임무:  \n",
    "\n",
    "1. 분석가들이 작성한 메모 모음을 제공받게 됩니다.  \n",
    "2. 각 메모의 통찰력을 세심하게 검토하고 분석하십시오.  \n",
    "3. 모든 메모의 핵심 아이디어를 통합한 상세하고 포괄적인 요약으로 이러한 통찰력을 정리하십시오.  \n",
    "4. 각 메모의 주요 포인트를 아래 제공된 적절한 섹션에 정리하고, 각 섹션이 논리적이고 체계적으로 구성되도록 하십시오.  \n",
    "5. 보고서에 모든 필수 섹션을 포함시키되, 각 섹션의 헤더로 `### 섹션명`을 사용하십시오.  \n",
    "6. 각 섹션당 약 250단어를 목표로 하며, 심층적인 설명, 맥락 및 지원 세부사항을 제공하십시오.  \n",
    "\n",
    "## **고려할 섹션 (심층화를 위한 선택적 섹션 포함):**\n",
    "\n",
    "- **배경**: 방법론과 결과를 이해하는 데 필요한 이론적 기초, 핵심 개념 및 예비 정보.\n",
    "- **관련 연구**: 기존 연구의 개요 및 현재 연구와의 비교 또는 연관성.\n",
    "- **문제 정의**: 본 논문이 다루려는 연구 질문 또는 문제에 대한 공식적이고 정확한 정의.\n",
    "- **방법론(또는 방법)**: 연구에 사용된 방법, 알고리즘, 모델, 데이터 수집 과정 또는 실험 설정에 대한 상세한 설명.\n",
    "- **구현 세부사항**: 소프트웨어 프레임워크, 계산 자원 또는 매개변수 설정 등 방법론이나 모델 구현의 실무적 세부사항.\n",
    "- **실험**: 방법론 검증을 위해 사용된 실험 프로토콜, 데이터셋, 평가 지표, 절차 및 구성에 대한 설명.\n",
    "- **결과**: 통계적 표, 그래프, 도표 또는 질적 분석을 동반한 실험 결과 제시.\n",
    "\n",
    "## 보고서 서식 지정 방법:\n",
    "\n",
    "1. 마크다운 서식을 사용하십시오.\n",
    "2. 보고서 서문은 포함하지 마십시오.\n",
    "3. 소제목을 사용하지 마십시오.\n",
    "4. 보고서는 단일 제목 헤더로 시작하십시오: ## 인사이트\n",
    "5. 보고서에서 분석가 이름을 언급하지 마십시오.\n",
    "6. 메모 내 인용은 그대로 유지하고 괄호([1], [2] 등)로 주석을 달아 표시하십시오.\n",
    "7. 최종 통합 출처 목록을 작성하여 `## 출처` 헤더로 출처 섹션에 추가하십시오.\n",
    "8. 출처를 순서대로 나열하고 중복하지 마십시오.\n",
    "    [1] 출처 1\n",
    "    [2] 출처 2\n",
    "\n",
    "보고서 작성에 활용할 분석가 메모는 다음과 같습니다:\n",
    "\n",
    "<Context>\n",
    "{context}\n",
    "</Context>\"\"\"\n",
    "\n",
    "\n",
    "# 보고서 작성 함수 정의\n",
    "def write_report(state: ResearchGraphState):\n",
    "    # 모든 섹션 가져오기\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 모든 섹션을 하나의 문자열로 연결\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 섹션을 요약하여 최종 보고서 작성\n",
    "    system_message = report_writer_instructions.format(\n",
    "        topic=topic, context=formatted_str_sections\n",
    "    )\n",
    "    report = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=f\"Write a report based upon these memos.\")]\n",
    "    )\n",
    "    return {\"content\": report.content}\n",
    "\n",
    "\n",
    "# 서론과 결론 작성 지시사항\n",
    "intro_conclusion_instructions = \"\"\"You are a technical writer finishing a report on {topic}\n",
    "\n",
    "You will be given all of the sections of the report.\n",
    "\n",
    "You job is to write a crisp and compelling introduction or conclusion section.\n",
    "\n",
    "The user will instruct you whether to write the introduction or conclusion.\n",
    "\n",
    "Include no pre-amble for either section.\n",
    "\n",
    "Target around 200 words, crisply previewing (for introduction),  or recapping (for conclusion) all of the sections of the report.\n",
    "\n",
    "Use markdown formatting.\n",
    "\n",
    "For your introduction, create a compelling title and use the # header for the title.\n",
    "\n",
    "For your introduction, use ## Introduction as the section header.\n",
    "\n",
    "For your conclusion, use ## Conclusion as the section header.\n",
    "\n",
    "Here are the sections to reflect on for writing: {formatted_str_sections}\"\"\"\n",
    "\n",
    "\n",
    "# 서론 작성 함수 정의\n",
    "def write_introduction(state: ResearchGraphState):\n",
    "    # 모든 섹션 가져오기\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 모든 섹션을 하나의 문자열로 연결\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 섹션을 요약하여 서론 작성\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic, formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "    intro = llm.invoke(\n",
    "        [instructions] + [HumanMessage(content=f\"Write the report introduction\")]\n",
    "    )\n",
    "    return {\"introduction\": intro.content}\n",
    "\n",
    "\n",
    "# 결론 작성 함수 정의\n",
    "def write_conclusion(state: ResearchGraphState):\n",
    "    # 모든 섹션 가져오기\n",
    "    sections = state[\"sections\"]\n",
    "    topic = state[\"topic\"]\n",
    "\n",
    "    # 모든 섹션을 하나의 문자열로 연결\n",
    "    formatted_str_sections = \"\\n\\n\".join([f\"{section}\" for section in sections])\n",
    "\n",
    "    # 섹션을 요약하여 결론 작성\n",
    "    instructions = intro_conclusion_instructions.format(\n",
    "        topic=topic, formatted_str_sections=formatted_str_sections\n",
    "    )\n",
    "    conclusion = llm.invoke(\n",
    "        [instructions] + [HumanMessage(content=f\"Write the report conclusion\")]\n",
    "    )\n",
    "    return {\"conclusion\": conclusion.content}\n",
    "\n",
    "\n",
    "# 최종 보고서 작성 함수 정의\n",
    "def finalize_report(state: ResearchGraphState):\n",
    "    # 모든 섹션을 모아 최종 보고서 작성\n",
    "    content = state[\"content\"]\n",
    "    if content.startswith(\"## Insights\"):\n",
    "        content = content.strip(\"## Insights\")\n",
    "    if \"## Sources\" in content:\n",
    "        try:\n",
    "            content, sources = content.split(\"\\n## Sources\\n\")\n",
    "        except:\n",
    "            sources = None\n",
    "    else:\n",
    "        sources = None\n",
    "\n",
    "    final_report = (\n",
    "        state[\"introduction\"]\n",
    "        + \"\\n\\n---\\n\\n## Main Idea\\n\\n\"\n",
    "        + content\n",
    "        + \"\\n\\n---\\n\\n\"\n",
    "        + state[\"conclusion\"]\n",
    "    )\n",
    "    if sources is not None:\n",
    "        final_report += \"\\n\\n## Sources\\n\" + sources\n",
    "    return {\"final_report\": final_report}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06c01e1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation-introduction-to-langgraph (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
