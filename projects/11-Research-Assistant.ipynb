{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d140c86",
   "metadata": {},
   "source": [
    "## STORM: 연구를 위한 멀티 에이전트\n",
    "\n",
    "### 개요\n",
    "\n",
    "STORM(Synthesis of Topic Outline through Retrieval and Multi-perspective Question Asking)은 Stanford 대학에서 개발한 LLM 기반의 지식 큐레이션 시스템입니다. 이 시스템은 인터넷 리서치를 통해 Wikipedia 수준의 포괄적이고 체계적인 장문의 기사를 자동으로 생성하는 것을 목표로 합니다.\n",
    "\n",
    "![](https://github.com/stanford-oval/storm/raw/main/assets/two_stages.jpg)\n",
    "\n",
    "### 핵심 아키텍처\n",
    "\n",
    "STORM은 두 단계의 파이프라인으로 구성됩니다:\n",
    "\n",
    "1. **사전 작성 단계(Pre-writing Stage)**\n",
    "   - 인터넷 기반 리서치를 수행하여 참고 자료 수집\n",
    "   - 다양한 관점(perspective) 발견\n",
    "   - 주제에 대한 개요(outline) 생성\n",
    "\n",
    "2. **작성 단계(Writing Stage)**\n",
    "   - 생성된 개요와 수집된 참고 자료를 활용\n",
    "   - 인용(citation)이 포함된 전체 기사 작성\n",
    "\n",
    "### 멀티 에이전트 접근법\n",
    "\n",
    "STORM의 핵심은 **관점 기반 질문 생성(Perspective-Guided Question Asking)** 과 **시뮬레이션된 대화(Simulated Conversation)** 전략입니다:\n",
    "\n",
    "- **다양한 관점 발견**: 유사한 주제의 기존 기사들을 조사하여 다양한 시각을 발견하고, 이를 질문 생성 과정에 활용\n",
    "- **역할 기반 대화 시뮬레이션**: Wikipedia 작성자와 주제 전문가 간의 대화를 시뮬레이션\n",
    "  - 작성자 에이전트: 다양한 관점에서 질문 제기\n",
    "  - 전문가 에이전트: 인터넷 소스에 기반한 답변 제공\n",
    "  - 이를 통해 이해도를 업데이트하고 후속 질문 생성\n",
    "\n",
    "### Co-STORM: 협업 확장\n",
    "\n",
    "Co-STORM은 STORM을 협업 기능으로 확장한 버전으로, 다음과 같은 멀티 에이전트 구성을 포함합니다:\n",
    "\n",
    "\n",
    "- **LLM 전문가 에이전트**: 외부 소스에 기반한 답변 생성 및 후속 질문 제기\n",
    "- **중재자 에이전트(Moderator)**: 발견된 정보에서 영감을 받은 사고를 자극하는 질문 생성\n",
    "- **동적 마인드맵**: 정보를 계층적으로 정리하여 인간과 시스템 간의 공유 개념 공간 생성\n",
    "\n",
    "![](https://github.com/stanford-oval/storm/raw/main/assets/co-storm-workflow.jpg)\n",
    "\n",
    "### 주요 특징\n",
    "\n",
    "- **포괄적 커버리지**: 다양한 관점에서 주제를 탐색하여 Wikipedia 수준의 광범위한 내용 생성\n",
    "- **구조화된 정보**: 자동으로 생성된 개요를 통해 체계적으로 정보 조직\n",
    "- **신뢰할 수 있는 출처**: 인터넷 소스에 기반하여 모든 정보에 인용 포함\n",
    "- **평가 검증**: FreshWiki 데이터셋을 통한 평가에서 기존 방법 대비 조직성 25%, 커버리지 10% 향상\n",
    "\n",
    "STORM은 복잡한 연구 작업을 자동화하고, 다양한 관점에서 정보를 종합하며, 신뢰할 수 있는 장문의 리포트를 생성하는 멀티 에이전트 시스템의 우수한 사례입니다.\n",
    "\n",
    "---\n",
    "\n",
    "- 참고 자료: https://wikidocs.net/270693\n",
    "- 관련 논문: https://arxiv.org/abs/2402.14207\n",
    "- GitHub 저장소: https://github.com/stanford-oval/storm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cb325a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "112e4070",
   "metadata": {},
   "source": [
    "## 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84dd060d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\", override=True)\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    env_value = os.environ.get(var)\n",
    "    if not env_value:\n",
    "        env_value = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "    os.environ[var] = env_value\n",
    "\n",
    "\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\"\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945c360a",
   "metadata": {},
   "source": [
    "## 분석가 생성 에이전트 with Human-In-The-Loop\n",
    "\n",
    "분석가 생성이 필요한 클래스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a7e07bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Analyst(BaseModel):\n",
    "    \"\"\"분석가 속성과 메타데이터를 정의\"\"\"\n",
    "\n",
    "    affiliation: Annotated[str, Field(description=\"분석가의 주요 소속 기관\")]\n",
    "    name: Annotated[str, Field(description=\"분석가 이름\")]\n",
    "    role: Annotated[str, Field(description=\"주제 맥락에서의 분석가의 역할\")]\n",
    "    description: Annotated[\n",
    "        str, Field(description=\"분석가의 관심사, 우려 사항 및 동기 설명\")\n",
    "    ]\n",
    "\n",
    "    @property\n",
    "    def persona(self) -> str:\n",
    "        return f\"이름: {self.name}\\n역할: {self.role}\\n소속 기관: {self.affiliation}\\n\"\n",
    "\n",
    "\n",
    "class Perspectives(BaseModel):\n",
    "    \"\"\"분석가들의 집합\"\"\"\n",
    "\n",
    "    analysts: Annotated[\n",
    "        list[Analyst],\n",
    "        Field(description=\"분석가들의 역할 및 소속 기관을 포함한 종합 목록\"),\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6778d7",
   "metadata": {},
   "source": [
    "### 분석가 생성 상태 및 노드 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상태 정의\n",
    "class GenerateAnalystsState(TypedDict):\n",
    "    topic: Annotated[str, \"연구 주제\"]\n",
    "    max_analysts: Annotated[int, \"생성할 분석가의 최대 수\"]\n",
    "    human_analyst_feedback: Annotated[str, \"휴먼 피드백\"]\n",
    "    analysts: Annotated[list[Analyst], \"분석가 목록\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46502385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분석가 생성 프롬프트\n",
    "analyst_instructions = \"\"\"AI 분석가 페르소나 세트를 생성하는 임무를 맡았습니다.\n",
    "\n",
    "다음 지침을 주의 깊게 따르십시오:\n",
    "\n",
    "1. 먼저 연구 주제를 검토하십시오:\n",
    "{topic}\n",
    "\n",
    "2. 분석가 생성 가이드로 제공된 선택적 편집 피드백을 검토하십시오:\n",
    "{human_analyst_feedback}\n",
    "\n",
    "3. 위 문서 및/또는 피드백을 바탕으로 가장 흥미로운 테마를 결정하십시오.\n",
    "\n",
    "4. 상위 {max_analysts}개 테마를 선정하십시오.\n",
    "\n",
    "5. 각 테마에 한 명의 분석가를 배정하십시오.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bcfdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "llm = init_chat_model(\"openai:gpt-4.1-mini\")\n",
    "\n",
    "\n",
    "# 분석가 생성 노드\n",
    "def create_analysts(state: GenerateAnalystsState):\n",
    "    \"\"\"분석가 페르소나를 생성합니다.\"\"\"\n",
    "\n",
    "    topic = state[\"topic\"]\n",
    "    max_analysts = state[\"max_analysts\"]\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", \"\")\n",
    "\n",
    "    llm_with_structured = llm.with_structured_output(Perspectives)\n",
    "\n",
    "    system_message = analyst_instructions.format(\n",
    "        topic=topic,\n",
    "        human_analyst_feedback=human_analyst_feedback,\n",
    "        max_analysts=max_analysts,\n",
    "    )\n",
    "\n",
    "    response = llm_with_structured.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [HumanMessage(content=\"Generate the set of analysts.\")]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"analysts\": response.analysts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47c51fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'analysts': [Analyst(affiliation='MIT Computer Science and Artificial Intelligence Laboratory', name='Dr. Aiden Lee', role='Multi-Agent Systems Theorist', description='Specializes in the foundational theories of multi-agent interactions, coordination algorithms, and emergent behavior in decentralized systems.'),\n",
       "  Analyst(affiliation='Stanford University, Human-Centered AI Institute', name='Prof. Emily Chen', role='Human-Multi-Agent Interaction Specialist', description='Focuses on designing multi-agent systems that effectively collaborate with humans, emphasizing usability, trust, and ethical considerations.'),\n",
       "  Analyst(affiliation='OpenAI Research', name='Dr. Ravi Kumar', role='Applied Multi-Agent Reinforcement Learning Researcher', description='Expert in developing practical multi-agent reinforcement learning algorithms for complex, real-world applications including robotics and autonomous vehicles.')]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_analysts({\"topic\": \"멀티 에이전트\", \"max_analysts\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2b276ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 피드백 노드\n",
    "def human_feedback(state: GenerateAnalystsState):\n",
    "    \"\"\"사용자 피드백을 받기 위한 중단점 노드\"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3a9c93",
   "metadata": {},
   "source": [
    "### 분석가 생성 그래프 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "509107a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "\n",
    "def should_continue(state: GenerateAnalystsState) -> Literal[\"create_analysts\", END]:\n",
    "    \"\"\"워크플로우의 다음 노드를 결정합니다.\"\"\"\n",
    "\n",
    "    human_analyst_feedback = state.get(\"human_analyst_feedback\", \"\")\n",
    "    if human_analyst_feedback:\n",
    "        return \"create_analysts\"\n",
    "    return END\n",
    "\n",
    "\n",
    "builder = StateGraph(GenerateAnalystsState)\n",
    "builder.add_node(create_analysts)\n",
    "builder.add_node(human_feedback)\n",
    "\n",
    "builder.add_edge(\"create_analysts\", \"human_feedback\")\n",
    "builder.add_conditional_edges(\n",
    "    \"human_feedback\", should_continue, {\"create_analysts\": \"create_analysts\", END: END}\n",
    ")\n",
    "builder.set_entry_point(\"create_analysts\")\n",
    "graph = builder.compile(\n",
    "    interrupt_before=[\"human_feedback\"], checkpointer=InMemorySaver()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8274f81d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e503e",
   "metadata": {},
   "source": [
    "### 분석가 생성 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07008ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### create_analysts #####\n",
      "{'analysts': [Analyst(affiliation='OmniData AI Research Lab', name='Dr. Mina Lee', role='AI Systems Researcher', description='Dr. Mina Lee specializes in AI retrieval-augmented generation systems, focusing on the agentic capabilities of language models to autonomously decide and execute information search and synthesis steps. She explores how Agentic RAG frameworks empower models with proactive decision-making and iterative interaction with retrieval components.'),\n",
      "              Analyst(affiliation='Adaptive Intelligence Solutions', name='Prof. Jun-ho Kim', role='Machine Learning Strategist', description='Prof. Jun-ho Kim investigates adaptive mechanisms in retrieval-augmented generation architectures where models dynamically adjust their retrieval strategies based on context and feedback. His work sheds light on how Adaptive RAG differs from agentic approaches by emphasizing flexibility and responsiveness in information retrieval processes.'),\n",
      "              Analyst(affiliation='NextGen AI Innovators', name='Eunseo Park', role='AI Product Analyst', description='Eunseo Park analyzes practical implications and product design involving Agentic versus Adaptive RAG systems, focusing on user experience and system performance trade-offs. She examines how the contrasting characteristics of these frameworks influence the deployment of AI-driven applications in real-world settings.')]}\n",
      "\n",
      "##### __interrupt__ #####\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from random import random\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "\n",
    "config = RunnableConfig(configurable={\"thread_id\": random()})\n",
    "\n",
    "inputs = {\n",
    "    \"max_analysts\": 3,\n",
    "    \"topic\": \"Agentic RAG와 Adaptive RAG의 차이점은 무엇인가요?\",\n",
    "}\n",
    "for event in graph.stream(inputs, config=config):\n",
    "    for key, value in event.items():\n",
    "        print(f\"\\n##### {key} #####\")\n",
    "        pprint(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bab4a06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analysts': [Analyst(affiliation='OmniData AI Research Lab', name='Dr. Mina Lee', role='AI Systems Researcher', description='Dr. Mina Lee specializes in AI retrieval-augmented generation systems, focusing on the agentic capabilities of language models to autonomously decide and execute information search and synthesis steps. She explores how Agentic RAG frameworks empower models with proactive decision-making and iterative interaction with retrieval components.'),\n",
      "              Analyst(affiliation='Adaptive Intelligence Solutions', name='Prof. Jun-ho Kim', role='Machine Learning Strategist', description='Prof. Jun-ho Kim investigates adaptive mechanisms in retrieval-augmented generation architectures where models dynamically adjust their retrieval strategies based on context and feedback. His work sheds light on how Adaptive RAG differs from agentic approaches by emphasizing flexibility and responsiveness in information retrieval processes.'),\n",
      "              Analyst(affiliation='NextGen AI Innovators', name='Eunseo Park', role='AI Product Analyst', description='Eunseo Park analyzes practical implications and product design involving Agentic versus Adaptive RAG systems, focusing on user experience and system performance trade-offs. She examines how the contrasting characteristics of these frameworks influence the deployment of AI-driven applications in real-world settings.')],\n",
      " 'max_analysts': 3,\n",
      " 'topic': 'Agentic RAG와 Adaptive RAG의 차이점은 무엇인가요?'}\n",
      "('human_feedback',)\n"
     ]
    }
   ],
   "source": [
    "# 현재 상태 스냅샷\n",
    "snapshot = graph.get_state(config)\n",
    "pprint(snapshot.values)\n",
    "pprint(snapshot.next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb64c3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### human_feedback #####\n",
      "None\n",
      "\n",
      "##### create_analysts #####\n",
      "{'analysts': [Analyst(affiliation='Tech Startup - Seoul', name='Seokho Phil', role='Founder & AI Product Strategist', description='Startup founder with deep expertise in AI integration, focusing on bridging technical RAG solutions with entrepreneurial business models to foster adaptive growth and competitive advantage.'),\n",
      "              Analyst(affiliation='Innovative AI Research Lab', name='Dr. Min-Jae Lee', role='Lead Research Scientist', description='Expert in agentic AI systems, specializing in the design and implementation of agentic Retrieval-Augmented Generation models that take autonomous initiative in information retrieval and decision-making.'),\n",
      "              Analyst(affiliation='Adaptive Computing Institute', name='Prof. Hana Kim', role='Professor of Adaptive AI Systems', description='Academic researcher focused on Adaptive RAG models that dynamically adjust their retrieval and generation processes based on context awareness and user feedback to enhance performance and relevance.')]}\n",
      "\n",
      "##### __interrupt__ #####\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# 휴먼 피드백 전달\n",
    "from langgraph.types import Command\n",
    "\n",
    "for event in graph.stream(\n",
    "    Command(\n",
    "        update={\n",
    "            \"human_analyst_feedback\": \"스타트업 출신의 석호필이라는 인물을 추가해 기업가적 관점을 더해주세요.\"\n",
    "        },\n",
    "    ),\n",
    "    config=config,\n",
    "):\n",
    "    for key, value in event.items():\n",
    "        print(f\"\\n##### {key} #####\")\n",
    "        pprint(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "913e6244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "##### human_feedback #####\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for event in graph.stream(\n",
    "    Command(update={\"human_analyst_feedback\": None}),\n",
    "    config=config,\n",
    "):\n",
    "    for key, value in event.items():\n",
    "        print(f\"\\n##### {key} #####\")\n",
    "        pprint(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5bcba8",
   "metadata": {},
   "source": [
    "최종 결과를 출력합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0da29cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StateSnapshot(values={'topic': 'Agentic RAG와 Adaptive RAG의 차이점은 무엇인가요?', 'max_analysts': 3, 'human_analyst_feedback': None, 'analysts': [Analyst(affiliation='Tech Startup - Seoul', name='Seokho Phil', role='Founder & AI Product Strategist', description='Startup founder with deep expertise in AI integration, focusing on bridging technical RAG solutions with entrepreneurial business models to foster adaptive growth and competitive advantage.'), Analyst(affiliation='Innovative AI Research Lab', name='Dr. Min-Jae Lee', role='Lead Research Scientist', description='Expert in agentic AI systems, specializing in the design and implementation of agentic Retrieval-Augmented Generation models that take autonomous initiative in information retrieval and decision-making.'), Analyst(affiliation='Adaptive Computing Institute', name='Prof. Hana Kim', role='Professor of Adaptive AI Systems', description='Academic researcher focused on Adaptive RAG models that dynamically adjust their retrieval and generation processes based on context awareness and user feedback to enhance performance and relevance.')]}, next=(), config={'configurable': {'thread_id': '0.8998343459486847', 'checkpoint_ns': '', 'checkpoint_id': '1f0abb45-11e8-616c-8004-24a134b798ef'}}, metadata={'source': 'loop', 'step': 4, 'parents': {}}, created_at='2025-10-17T23:52:19.190196+00:00', parent_config={'configurable': {'thread_id': '0.8998343459486847', 'checkpoint_ns': '', 'checkpoint_id': '1f0abb45-11c9-6e10-8003-cc7091e311b5'}}, tasks=(), interrupts=())\n"
     ]
    }
   ],
   "source": [
    "# 스냅샷을 가져옵니다.\n",
    "final_state = graph.get_state(config)\n",
    "pprint(final_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99468a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성된 분석가 수: 3\n",
      "================================\n",
      "이름: Seokho Phil\n",
      "역할: Founder & AI Product Strategist\n",
      "소속 기관: Tech Startup - Seoul\n",
      "설명: Startup founder with deep expertise in AI integration, focusing on bridging technical RAG solutions with entrepreneurial business models to foster adaptive growth and competitive advantage.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "이름: Dr. Min-Jae Lee\n",
      "역할: Lead Research Scientist\n",
      "소속 기관: Innovative AI Research Lab\n",
      "설명: Expert in agentic AI systems, specializing in the design and implementation of agentic Retrieval-Augmented Generation models that take autonomous initiative in information retrieval and decision-making.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "이름: Prof. Hana Kim\n",
      "역할: Professor of Adaptive AI Systems\n",
      "소속 기관: Adaptive Computing Institute\n",
      "설명: Academic researcher focused on Adaptive RAG models that dynamically adjust their retrieval and generation processes based on context awareness and user feedback to enhance performance and relevance.\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "analysts = final_state.values.get(\"analysts\")\n",
    "print(f\"생성된 분석가 수: {len(analysts)}\", end=\"\\n================================\\n\")\n",
    "\n",
    "for analyst in analysts:\n",
    "    print(analyst.persona)\n",
    "    print(\"- \" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90393571",
   "metadata": {},
   "source": [
    "## 인터뷰 에이전트\n",
    "\n",
    "### 질문 생성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364a5fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from langgraph.graph import MessagesState\n",
    "\n",
    "\n",
    "class InterviewState(MessagesState):\n",
    "    \"\"\"인터뷰 상태를 저장합니다.\"\"\"\n",
    "\n",
    "    max_num: Annotated[int, \"대화 턴수\"]\n",
    "    context: Annotated[list, operator.add]\n",
    "    analyst: Annotated[Analyst, \"분석가\"]\n",
    "    interview: Annotated[str, \"인터뷰 내용\"]\n",
    "    sections: Annotated[list, \"보고서 섹션 목록\"]\n",
    "\n",
    "\n",
    "class SearchQuery(BaseModel):\n",
    "    search_query: Annotated[str, Field(None, description=\"retrieval를 위한 검색 쿼리\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d484c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인터뷰 시스템 프롬프트\n",
    "question_instructions = \"\"\"당신은 특정 주제에 대해 알아보기 위해 전문가를 인터뷰하는 임무를 맡은 분석가입니다.\n",
    "\n",
    "당신의 목표는 주제에 관련된 흥미롭고 구체적인 통찰력을 추출하는 것입니다.\n",
    "\n",
    "1. 흥미로움: 사람들이 놀라워하거나 당연하지 않다고 느낄 만한 통찰력.\n",
    "2. 구체성: 일반론을 피하고 전문가의 구체적인 사례를 포함하는 통찰력.\n",
    "\n",
    "다음은 집중할 주제와 목표 목록입니다:\n",
    "{goal}\n",
    "\n",
    "첫 대화를 시작할때에 당신의 인물을 반영하는 이름으로 자신을 소개한 후 질문을 시작하세요.\n",
    "\n",
    "주제에 대한 이해를 심화하고 정교화하기 위해 계속해서 질문을 이어가세요.\n",
    "\n",
    "이해가 충분하다고 판단되면 \"도움 주셔서 정말 감사합니다!\"라고 말하며 인터뷰를 마무리하세요.\n",
    "\n",
    "응답 전반에 걸쳐 제공된 인물과 목표를 반영하여 캐릭터를 유지하는 것을 잊지 마세요.\n",
    "\n",
    "<Persona>\n",
    "{persona}\n",
    "<Persona>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "32f5de37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질문 생성 노드\n",
    "def generate_question(state: InterviewState):\n",
    "    \"\"\"통찰력있는 질문을 생성합니다.\"\"\"\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    system_message = question_instructions.format(\n",
    "        goal=analyst.description,\n",
    "        persona=analyst.persona,\n",
    "    )\n",
    "    response = llm.invoke([SystemMessage(content=system_message)] + state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7b9591fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "안녕하세요, 저는 Tech Innovators Inc.에서 기업가적 분석가로 활동하고 있는 김필입니다. 창업자 출신으로서 기업가 정신과 비즈니스 모델 혁신에 특별히 관심을 두고 있으며, 스타트업 관점에서 적응형 시스템이 어떻게 유연성과 시장 적응력을 극대화하는지 탐구하고 있습니다.\n",
      "\n",
      "먼저, 창업자로서 경험하신 적응형 시스템의 구체적인 사례가 있으신가요? 특히, 이 시스템이 비즈니스 모델 혁신에 어떤 직접적인 영향을 미쳤는지 궁금합니다.\n"
     ]
    }
   ],
   "source": [
    "analyst = Analyst(\n",
    "    name=\"김필\",\n",
    "    affiliation=\"Tech Innovators Inc.\",\n",
    "    role=\"기업가적 분석가\",\n",
    "    description=\"창업자 출신 분석가로, 기업가 정신과 비즈니스 모델 혁신에 주력합니다. 스타트업 관점에서 적응형 시스템에 대한 통찰력을 제공하며, 유연성과 시장 적응력을 강조합니다.\",\n",
    ")\n",
    "response = generate_question(\n",
    "    {\n",
    "        \"analyst\": analyst,\n",
    "        \"messages\": [],\n",
    "    }\n",
    ")\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53075650",
   "metadata": {},
   "source": [
    "### 도구 정의\n",
    "\n",
    "#### 웹검색 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2a89434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "web_search = TavilySearch(max_results=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc7fd6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '지난 윔블던에서 무슨 일이 있었나요?',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'url': 'http://m.tennispeople.kr/news/articleView.html?idxno=17168',\n",
       "   'title': '윔블던 컴퓨터 라인 판정 믿어도 되나 - 테니스피플',\n",
       "   'content': '사건은 6일 윔블던 센터 코트에서 열린 여자 단식 16강전 1세트 4-4 상황에서 발생했다. 영국 소니 카르탈과 러시아의 아나스타샤 파블류첸코바가 맞붙',\n",
       "   'score': 0.38498205,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://sports.news.nate.com/view/20250704n04849',\n",
       "   'title': '\"아웃, 인은 기계가 판정\" 148년 전통 깬 윔블던, 인간 선심 사라졌다',\n",
       "   'content': '올해 윔블던 테니스 대회는 148년 역사상 처음으로 인간 선심이 사라진 대회로 기록됐다. CNN은 4일 “윔블던 라인 콜은 전적으로 전자 판독',\n",
       "   'score': 0.34846824,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.busan.com/view/busan/view.php?code=2025070317510369945',\n",
       "   'title': \"톱시드 23명 1회전 탈락… '이변의 윔블던' - 부산일보\",\n",
       "   'content': '폭염 속에 진행된 윔블던 테니스 대회에서 이변이 속출하고 있다. 남녀 16명의 시드 배정자, 총 32명 중 무려 23명이 1회전에서 탈락했다.',\n",
       "   'score': 0.2246166,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 0.7,\n",
       " 'request_id': 'a1cfeb0f-0566-4f47-a3aa-8cc943f2a7c7'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "web_search.invoke({\"query\": \"지난 윔블던에서 무슨 일이 있었나요?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac7eed",
   "metadata": {},
   "source": [
    "#### 논문 검색 도구"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f49ac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.retrievers import ArxivRetriever\n",
    "\n",
    "arxiv_retriever = ArxivRetriever(\n",
    "    load_max_docs=3,\n",
    "    load_all_available_meta=True,\n",
    "    get_full_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f016ac79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2024-07-26', 'Title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks', 'Authors': 'Yunfan Gao, Yun Xiong, Meng Wang, Haofen Wang', 'Summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.', 'entry_id': 'http://arxiv.org/abs/2407.21059v1', 'published_first_time': '2024-07-26', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI', 'cs.IR'], 'links': ['http://arxiv.org/abs/2407.21059v1', 'http://arxiv.org/pdf/2407.21059v1']}, page_content='1\\nModular RAG: Transforming RAG Systems into\\nLEGO-like Reconfigurable Frameworks\\nYunfan Gao, Yun Xiong, Meng Wang, Haofen Wang\\nAbstract—Retrieval-augmented\\nGeneration\\n(RAG)\\nhas\\nmarkedly enhanced the capabilities of Large Language Models\\n(LLMs) in tackling knowledge-intensive tasks. The increasing\\ndemands of application scenarios have driven the evolution\\nof RAG, leading to the integration of advanced retrievers,\\nLLMs and other complementary technologies, which in turn\\nhas amplified the intricacy of RAG systems. However, the rapid\\nadvancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process\\nof “retrieve-then-generate”. In this context, this paper examines\\nthe limitations of the existing RAG paradigm and introduces\\nthe modular RAG framework. By decomposing complex RAG\\nsystems into independent modules and specialized operators, it\\nfacilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a\\nmore advanced design that integrates routing, scheduling, and\\nfusion mechanisms. Drawing on extensive research, this paper\\nfurther identifies prevalent RAG patterns—linear, conditional,\\nbranching, and looping—and offers a comprehensive analysis\\nof their respective implementation nuances. Modular RAG\\npresents\\ninnovative\\nopportunities\\nfor\\nthe\\nconceptualization\\nand deployment of RAG systems. Finally, the paper explores\\nthe potential emergence of new operators and paradigms,\\nestablishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment\\nof RAG technologies.\\nIndex Terms—Retrieval-augmented generation, large language\\nmodel, modular system, information retrieval\\nI. INTRODUCTION\\nL\\nARGE Language Models (LLMs) have demonstrated\\nremarkable capabilities, yet they still face numerous\\nchallenges, such as hallucination and the lag in information up-\\ndates [1]. Retrieval-augmented Generation (RAG), by access-\\ning external knowledge bases, provides LLMs with important\\ncontextual information, significantly enhancing their perfor-\\nmance on knowledge-intensive tasks [2]. Currently, RAG, as\\nan enhancement method, has been widely applied in various\\npractical application scenarios, including knowledge question\\nanswering, recommendation systems, customer service, and\\npersonal assistants. [3]–[6]\\nDuring the nascent stages of RAG , its core framework is\\nconstituted by indexing, retrieval, and generation, a paradigm\\nreferred to as Naive RAG [7]. However, as the complexity\\nof tasks and the demands of applications have escalated, the\\nYunfan Gao is with Shanghai Research Institute for Intelligent Autonomous\\nSystems, Tongji University, Shanghai, 201210, China.\\nYun Xiong is with Shanghai Key Laboratory of Data Science, School of\\nComputer Science, Fudan University, Shanghai, 200438, China.\\nMeng Wang and Haofen Wang are with College of Design and Innovation,\\nTongji University, Shanghai, 20092, China. (Corresponding author: Haofen\\nWang. E-mail: carter.whfcarter@gmail.com)\\nlimitations of Naive RAG have become increasingly apparent.\\nAs depicted in Figure 1, it predominantly hinges on the\\nstraightforward similarity of chunks, result in poor perfor-\\nmance when confronted with complex queries and chunks with\\nsubstantial variability. The primary challenges of Naive RAG\\ninclude: 1) Shallow Understanding of Queries. The semantic\\nsimilarity between a query and document chunk is not always\\nhighly consistent. Relying solely on similarity calculations\\nfor retrieval lacks an in-depth exploration of the relationship\\nbetween the query and the document [8]. 2) Retrieval Re-\\ndundancy and Noise. Feeding all retrieved chunks directly\\ninto LLMs is not always beneficial. Research indicates that\\nan excess of redundant and noisy information may interfere\\nwith the LLM’s identification of key information, thereby\\nincreasing the risk of generating erroneous and hallucinated\\nresponses. [9]\\nTo overcome the aforementioned limitations, '),\n",
       " Document(metadata={'Published': '2024-03-27', 'Title': 'Retrieval-Augmented Generation for Large Language Models: A Survey', 'Authors': 'Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Meng Wang, Haofen Wang', 'Summary': \"Large Language Models (LLMs) showcase impressive capabilities but encounter\\nchallenges like hallucination, outdated knowledge, and non-transparent,\\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\\nemerged as a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the generation,\\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\\nupdates and integration of domain-specific information. RAG synergistically\\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\\nexternal databases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\\ntripartite foundation of RAG frameworks, which includes the retrieval, the\\ngeneration and the augmentation techniques. The paper highlights the\\nstate-of-the-art technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG systems.\\nFurthermore, this paper introduces up-to-date evaluation framework and\\nbenchmark. At the end, this article delineates the challenges currently faced\\nand points out prospective avenues for research and development.\", 'entry_id': 'http://arxiv.org/abs/2312.10997v5', 'published_first_time': '2023-12-18', 'comment': 'Ongoing Work', 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI'], 'links': ['http://arxiv.org/abs/2312.10997v5', 'http://arxiv.org/pdf/2312.10997v5']}, page_content='1\\nRetrieval-Augmented Generation for Large\\nLanguage Models: A Survey\\nYunfan Gaoa, Yun Xiongb, Xinyu Gaob, Kangxiang Jiab, Jinliu Panb, Yuxi Bic, Yi Daia, Jiawei Suna, Meng\\nWangc, and Haofen Wang a,c\\naShanghai Research Institute for Intelligent Autonomous Systems, Tongji University\\nbShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\ncCollege of Design and Innovation, Tongji University\\nAbstract—Large Language Models (LLMs) showcase impres-\\nsive capabilities but encounter challenges like hallucination,\\noutdated knowledge, and non-transparent, untraceable reasoning\\nprocesses. Retrieval-Augmented Generation (RAG) has emerged\\nas a promising solution by incorporating knowledge from external\\ndatabases. This enhances the accuracy and credibility of the\\ngeneration, particularly for knowledge-intensive tasks, and allows\\nfor continuous knowledge updates and integration of domain-\\nspecific information. RAG synergistically merges LLMs’ intrin-\\nsic knowledge with the vast, dynamic repositories of external\\ndatabases. This comprehensive review paper offers a detailed\\nexamination of the progression of RAG paradigms, encompassing\\nthe Naive RAG, the Advanced RAG, and the Modular RAG.\\nIt meticulously scrutinizes the tripartite foundation of RAG\\nframeworks, which includes the retrieval, the generation and the\\naugmentation techniques. The paper highlights the state-of-the-\\nart technologies embedded in each of these critical components,\\nproviding a profound understanding of the advancements in RAG\\nsystems. Furthermore, this paper introduces up-to-date evalua-\\ntion framework and benchmark. At the end, this article delineates\\nthe challenges currently faced and points out prospective avenues\\nfor research and development 1.\\nIndex Terms—Large language model, retrieval-augmented gen-\\neration, natural language processing, information retrieval\\nI. INTRODUCTION\\nL\\nARGE language models (LLMs) have achieved remark-\\nable success, though they still face significant limitations,\\nespecially in domain-specific or knowledge-intensive tasks [1],\\nnotably producing “hallucinations” [2] when handling queries\\nbeyond their training data or requiring current information. To\\novercome challenges, Retrieval-Augmented Generation (RAG)\\nenhances LLMs by retrieving relevant document chunks from\\nexternal knowledge base through semantic similarity calcu-\\nlation. By referencing external knowledge, RAG effectively\\nreduces the problem of generating factually incorrect content.\\nIts integration into LLMs has resulted in widespread adoption,\\nestablishing RAG as a key technology in advancing chatbots\\nand enhancing the suitability of LLMs for real-world applica-\\ntions.\\nRAG technology has rapidly developed in recent years, and\\nthe technology tree summarizing related research is shown\\nCorresponding Author.Email:haofen.wang@tongji.edu.cn\\n1Resources\\nare\\navailable\\nat\\nhttps://github.com/Tongji-KGLLM/\\nRAG-Survey\\nin Figure 1. The development trajectory of RAG in the era\\nof large models exhibits several distinct stage characteristics.\\nInitially, RAG’s inception coincided with the rise of the\\nTransformer architecture, focusing on enhancing language\\nmodels by incorporating additional knowledge through Pre-\\nTraining Models (PTM). This early stage was characterized\\nby foundational work aimed at refining pre-training techniques\\n[3]–[5].The subsequent arrival of ChatGPT [6] marked a\\npivotal moment, with LLM demonstrating powerful in context\\nlearning (ICL) capabilities. RAG research shifted towards\\nproviding better information for LLMs to answer more com-\\nplex and knowledge-intensive tasks during the inference stage,\\nleading to rapid development in RAG studies. As research\\nprogressed, the enhancement of RAG was no longer limited\\nto the inference stage but began to incorporate more with LLM\\nfine-tuning techniques.\\nThe burgeoning field of RAG has experienced swift growth,\\nyet it has not been accompanied by a systematic synthesis that\\ncould clarify its broader trajectory. Thi'),\n",
       " Document(metadata={'Published': '2025-10-11', 'Title': 'MA-RAG: Multi-Agent Retrieval-Augmented Generation via Collaborative Chain-of-Thought Reasoning', 'Authors': 'Thang Nguyen, Peter Chin, Yu-Wing Tai', 'Summary': 'We present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Generation\\n(RAG) that addresses the inherent ambiguities and reasoning challenges in\\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely on\\nend-to-end fine-tuning or isolated component enhancements, MA-RAG orchestrates\\na collaborative set of specialized AI agents: Planner, Step Definer, Extractor,\\nand QA Agents, each responsible for a distinct stage of the RAG pipeline. By\\ndecomposing tasks into subtasks such as query disambiguation, evidence\\nextraction, and answer synthesis, and enabling agents to communicate\\nintermediate reasoning via chain-of-thought prompting, MA-RAG progressively\\nrefines retrieval and synthesis while maintaining modular interpretability.\\nExtensive experiments on multi-hop and ambiguous QA benchmarks, including NQ,\\nHotpotQA, 2WikimQA, and TriviaQA, demonstrate that MA-RAG significantly\\noutperforms standalone LLMs and existing RAG methods across all model scales.\\nNotably, even a small LLaMA3-8B model equipped with MA-RAG surpasses larger\\nstandalone LLMs, while larger variants (LLaMA3-70B and GPT-4o-mini) set new\\nstate-of-the-art results on challenging multi-hop datasets. Ablation studies\\nreveal that both the planner and extractor agents are critical for multi-hop\\nreasoning, and that high-capacity models are especially important for the QA\\nagent to synthesize answers effectively. Beyond general-domain QA, MA-RAG\\ngeneralizes to specialized domains such as medical QA, achieving competitive\\nperformance against domain-specific models without any domain-specific\\nfine-tuning. Our results highlight the effectiveness of collaborative, modular\\nreasoning in retrieval-augmented systems: MA-RAG not only improves answer\\naccuracy and robustness but also provides interpretable intermediate reasoning\\nsteps, establishing a new paradigm for efficient and reliable multi-agent RAG.', 'entry_id': 'http://arxiv.org/abs/2505.20096v2', 'published_first_time': '2025-05-26', 'comment': None, 'journal_ref': None, 'doi': None, 'primary_category': 'cs.CL', 'categories': ['cs.CL', 'cs.AI'], 'links': ['http://arxiv.org/abs/2505.20096v2', 'http://arxiv.org/pdf/2505.20096v2']}, page_content='MA-RAG: MULTI-AGENT RETRIEVAL-AUGMENTED\\nGENERATION\\nVIA\\nCOLLABORATIVE\\nCHAIN-OF-\\nTHOUGHT REASONING\\nThang Nguyen & Peter Chin & Yu-Wing Tai\\nDartmouth College\\n{thangnv.th, peter.chin, yu-wing.tai}@dartmouth.edu\\nABSTRACT\\nWe present MA-RAG, a Multi-Agent framework for Retrieval-Augmented Gener-\\nation (RAG) that addresses the inherent ambiguities and reasoning challenges in\\ncomplex information-seeking tasks. Unlike conventional RAG methods that rely\\non end-to-end fine-tuning or isolated component enhancements, MA-RAG orches-\\ntrates a collaborative set of specialized AI agents: Planner, Step Definer, Extrac-\\ntor, and QA Agents, each responsible for a distinct stage of the RAG pipeline.\\nBy decomposing tasks into subtasks such as query disambiguation, evidence ex-\\ntraction, and answer synthesis, and enabling agents to communicate intermedi-\\nate reasoning via chain-of-thought prompting, MA-RAG progressively refines re-\\ntrieval and synthesis while maintaining modular interpretability. Extensive exper-\\niments on multi-hop and ambiguous QA benchmarks, including NQ, HotpotQA,\\n2WikimQA, and TriviaQA, demonstrate that MA-RAG significantly outperforms\\nstandalone LLMs and existing RAG methods across all model scales. Notably,\\neven a small LLaMA3-8B model equipped with MA-RAG surpasses larger stan-\\ndalone LLMs, while larger variants (LLaMA3-70B and GPT-4o-mini) set new\\nstate-of-the-art results on challenging multi-hop datasets. Ablation studies reveal\\nthat both the planner and extractor agents are critical for multi-hop reasoning,\\nand that high-capacity models are especially important for the QA agent to syn-\\nthesize answers effectively. Beyond general-domain QA, MA-RAG generalizes\\nto specialized domains such as medical QA, achieving competitive performance\\nagainst domain-specific models without any domain-specific fine-tuning. Our re-\\nsults highlight the effectiveness of collaborative, modular reasoning in retrieval-\\naugmented systems: MA-RAG not only improves answer accuracy and robustness\\nbut also provides interpretable intermediate reasoning steps, establishing a new\\nparadigm for efficient and reliable multi-agent RAG1.\\n1\\nINTRODUCTION\\nRecent advances in natural language processing have driven the development of Retrieval-\\nAugmented Generation (RAG) models, which aim to enhance the factual accuracy and contextual\\nrelevance of generated text by integrating external knowledge sources (Lewis et al., 2020; Guu et al.,\\n2020; Izacard & Grave, 2021; Lin et al., 2024). These systems address core limitations of Large Lan-\\nguage Models (LLMs), such as outdated knowledge (Zhang et al., 2023b; Kasai et al., 2023) and\\npoor generalization to domain-specific queries (Siriwardhana et al., 2023; Xiong et al., 2024), by\\nretrieving top-k documents from an external corpus (Formal et al., 2022; Izacard et al., 2022; Wang\\net al., 2022a) to ground the model’s output in relevant evidence.\\nPrior research in RAG has largely concentrated on optimizing three key components—retrieval,\\naugmentation, and generation (Gao et al., 2024; Fan et al., 2024) (Figure 1(a)). Retrieval strate-\\ngies span sparse methods (Jones, 1972; Robertson & Zaragoza, 2009) and dense retrieval (Reimers\\n& Gurevych, 2019; Karpukhin et al., 2020), each with respective weaknesses such as lexical\\ngaps (Berger et al., 2000) or retrieval failure on out-of-distribution and multi-hop queries (Dai et al.,\\n1Our code is available at https://github.com/thangylvp/MA-RAG\\n1\\narXiv:2505.20096v2  [cs.CL]  11 Oct 2025\\nQuery\\nDocs\\nAnswer\\nDocs\\nAnswer\\nCoT\\nNotes\\nPost-process\\na) Vanilla RAG\\nStep 1\\nStep ...\\nCoT\\nSub-Query\\nQuery\\nDocs\\nNotes\\nCoT\\nQuery\\nSub-Answer\\nCoT\\nb) RAG with post-\\nprocessing retrieved docs\\nd) MA-RAG\\nQuery\\nDocs\\nAnswer\\nc) RAG with interleaving\\nretrieval and thoughts\\nAnswer\\nCoT\\nFigure 1: Architectural Comparison of MA-RAG and Prior RAG Methods. a) A naive RAG sys-\\ntem performs one-shot retrieval followed by direct answer generation. b) Enhanced systems incor-\\nporate post-retrieval processing such as document re-')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_retriever.invoke(\"Modular RAG vs Naive RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da7d4c3",
   "metadata": {},
   "source": [
    "### 노드 작성\n",
    "\n",
    "#### 웹 검색 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ef8b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검색 쿼리 변환 프롬프트\n",
    "search_instructions = \"\"\"분석가와 전문가 간의 대화가 제시됩니다.\n",
    "\n",
    "목표는 해당 대화와 관련된 검색 및/또는 웹 검색에 사용할 잘 구조화된 쿼리를 생성하는 것입니다.\n",
    "\n",
    "먼저 전체 대화를 분석하십시오.\n",
    "\n",
    "특히 분석가가 마지막에 제기한 질문에 주목하십시오.\n",
    "\n",
    "이 마지막 질문을 잘 구조화된 웹 검색 쿼리로 변환하십시오.\"\"\"\n",
    "\n",
    "\n",
    "def search_web(state: InterviewState):\n",
    "    \"\"\"웹 검색을 통한 문서 검색\"\"\"\n",
    "\n",
    "    llm_with_structured = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "    response = llm_with_structured.invoke(\n",
    "        [(\"system\", search_instructions)] + state[\"messages\"]\n",
    "    )\n",
    "\n",
    "    results = web_search.invoke(response.search_query)\n",
    "    context = [\n",
    "        f'<Document source=\"web\" url=\"{doc[\"url\"]}\" title=\"{doc[\"title\"]}\">{doc[\"content\"]}</Document>'\n",
    "        for doc in results[\"results\"]\n",
    "    ]\n",
    "\n",
    "    return {\"context\": [*context]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f9e12267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': ['<Document source=\"web\" url=\"https://dictionary.cambridge.org/ko/%EC%82%AC%EC%A0%84/%EC%98%81%EC%96%B4/crag\" title=\"영어로 crag의 뜻\">CRAG 의미, 정의, CRAG의 정의: 1. a high, rough mass of rock that sticks out from the land around it 2. a high, rough mass of rock…. 자세히 알아보기.</Document>',\n",
       "  '<Document source=\"web\" url=\"https://www.collinsdictionary.com/ko/dictionary/english/crag\" title=\"CRAG 정의 및 의미 | Collins 영어 사전\">carbon reduction action group or carbon rationing action group: a small association of citizens whose members attempt to reduce their environmental impact</Document>',\n",
       "  '<Document source=\"web\" url=\"https://www.lingq.com/ko/learn-english-online/translate/en/CRAG/\" title=\"CRAG | 한국어 번역과 뜻 | LingQ 사전\">언어 학습 앱 >; 온라인으로 English 배우기. crag. English에서 한국어로의 번역 및 의미. 영어. 한국어. CRAG. 암장. 다른 의미인기. 암장. crag 명사 험준한 바위 (</Document>']}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_web({\"messages\": [(\"user\", \"CRAG에 대해서 설명해주세요.\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9959f7f",
   "metadata": {},
   "source": [
    "#### 논문 검색 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f25ee249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv(state: InterviewState):\n",
    "    \"\"\"Arxiv 검색 노드\"\"\"\n",
    "\n",
    "    llm_with_structured = llm.with_structured_output(SearchQuery)\n",
    "\n",
    "    response = llm_with_structured.invoke(\n",
    "        [(\"system\", search_instructions)] + state[\"messages\"]\n",
    "    )\n",
    "\n",
    "    results = arxiv_retriever.invoke(\n",
    "        response.search_query,\n",
    "        load_max_docs=2,\n",
    "        load_all_available_meta=True,\n",
    "        get_full_documents=True,\n",
    "    )\n",
    "\n",
    "    context = [\n",
    "        f'<Document source=\"arxiv\" url=\"{doc.metadata[\"entry_id\"]}\" date=\"{doc.metadata.get(\"Published\", \"\")}\" authors=\"{doc.metadata.get(\"Authors\", \"\")}\"/>\\n<Title>\\n{doc.metadata[\"Title\"]}\\n</Title>\\n\\n<Summary>\\n{doc.metadata[\"Summary\"]}\\n</Summary>\\n\\n<Content>\\n{doc.page_content}\\n</Content>\\n</Document>'\n",
    "        for doc in results\n",
    "    ]\n",
    "\n",
    "    return {\"context\": [*context]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "606944ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': ['<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2502.19629v1\" date=\"2025-02-26\" authors=\"Tiffany J. Callahan, Nathaniel H. Park, Sara Capponi\"/>\\n<Title>\\nAgentic Mixture-of-Workflows for Multi-Modal Chemical Search\\n</Title>\\n\\n<Summary>\\nThe vast and complex materials design space demands innovative strategies to\\nintegrate multidisciplinary scientific knowledge and optimize materials\\ndiscovery. While large language models (LLMs) have demonstrated promising\\nreasoning and automation capabilities across various domains, their application\\nin materials science remains limited due to a lack of benchmarking standards\\nand practical implementation frameworks. To address these challenges, we\\nintroduce Mixture-of-Workflows for Self-Corrective Retrieval-Augmented\\nGeneration (CRAG-MoW) - a novel paradigm that orchestrates multiple agentic\\nworkflows employing distinct CRAG strategies using open-source LLMs. Unlike\\nprior approaches, CRAG-MoW synthesizes diverse outputs through an orchestration\\nagent, enabling direct evaluation of multiple LLMs across the same problem\\ndomain. We benchmark CRAG-MoWs across small molecules, polymers, and chemical\\nreactions, as well as multi-modal nuclear magnetic resonance (NMR) spectral\\nretrieval. Our results demonstrate that CRAG-MoWs achieve performance\\ncomparable to GPT-4o while being preferred more frequently in comparative\\nevaluations, highlighting the advantage of structured retrieval and multi-agent\\nsynthesis. By revealing performance variations across data types, CRAG-MoW\\nprovides a scalable, interpretable, and benchmark-driven approach to optimizing\\nAI architectures for materials discovery. These insights are pivotal in\\naddressing fundamental gaps in benchmarking LLMs and autonomous AI agents for\\nscientific applications.\\n</Summary>\\n\\n<Content>\\nAgentic Mixture-of-Workflows for Multi-Modal Chemical Search \\n \\n \\nTiffany J. Callahan1, Nathaniel H. Park1*, and Sara Capponi1 \\n1IBM Research–Almaden, 650 Harry Rd. San Jose, CA 95120  \\n \\n*Corresponding author. Email: npark@us.ibm.com \\n \\n \\nABSTRACT \\nThe vast and complex materials design space demands innovative strategies to integrate \\nmultidisciplinary scientific knowledge and optimize materials discovery. While large \\nlanguage models (LLMs) have demonstrated promising reasoning and automation \\ncapabilities across various domains, their application in materials science remains limited \\ndue to a lack of benchmarking standards and practical implementation frameworks. To \\naddress these challenges, we introduce Mixture-of-Workflows for Self-Corrective \\nRetrieval-Augmented Generation (CRAG-MoW)—a novel paradigm that orchestrates \\nmultiple agentic workflows employing distinct CRAG strategies using open-source LLMs. \\nUnlike prior approaches, CRAG-MoW synthesizes diverse outputs through an \\norchestration agent, enabling direct evaluation of multiple LLMs across the same problem \\ndomain. We benchmark CRAG-MoWs across small molecules, polymers, and chemical \\nreactions, as well as multi-modal nuclear magnetic resonance (NMR) spectral retrieval. \\nOur results demonstrate that CRAG-MoWs achieve performance comparable to GPT-4o \\nwhile being preferred more frequently in comparative evaluations, highlighting the \\nadvantage of structured retrieval and multi-agent synthesis. By revealing performance \\nvariations across data types, CRAG-MoW provides a scalable, interpretable, and \\nbenchmark-driven approach to optimizing AI architectures for materials discovery. These \\ninsights are pivotal in addressing fundamental gaps in benchmarking LLMs and \\nautonomous AI agents for scientific applications. \\n \\n \\nINTRODUCTION \\nThe vast size, high dimensionality, and complexity of the materials design space require \\nnew strategies for synthesizing and integrating multidisciplinary scientific knowledge. \\nSuch approaches are essential to drive advances in performance, cost-efficiency, and \\nsustainability [1]. Large language models (LLM), which are trained on large amounts of \\ndata and designed for human interaction, have demonstrated impressive reasoning \\nabilities in natural language processing [2–5]. Within the materials domain, LLMs have \\nbeen used to predict chemical properties [6–8], design new molecules [4, 9–12], automate \\nscientific coding [13, 14], develop AI agents [6, 15–17], extract and synthesize knowledge \\n[18, 19], and summarize and generate text [20]. Despite these successes, LLM adoption \\nin materials science lags behind other fields. This gap stems from a lack of standardized \\n \\n2\\nbenchmarks for validating LLM-based analyses, limited application to practical tasks, and \\nthe specialized expertise required for materials development [21–23].  \\n \\nThese challenges have led to increased interest in agentic workflows—LLM-driven \\nautonomous systems designed to perform complex reasoning, tool use, and multi-step \\ndecision-making [24]. Within the materials domain, agentic systems have been developed \\nto review the literature [25–27], implement routine chemical tasks [16, 21, 28–31] plan \\nexperiments [32–35], automate chemoinformatics analysis [36–39], and generate novel \\nhypotheses [40–42]. One of the most widely adopted implementations of agentic systems \\nis retrieval-augmented generation (RAG), which enhances LLM outputs by integrating \\ninformation retrieval techniques [25]. Rather than relying solely on pre-trained knowledge, \\nRAG dynamically retrieves and incorporates relevant external information, improving \\nresponse accuracy and contextual relevance. This capability is particularly valuable in \\nmaterials science, where access to domain-specific literature, experimental data, and \\nproperty databases is critical for precise predictions and reasoning. By leveraging RAG, \\nagentic workflows can provide more reliable and up-to-date insights, addressing key\\n</Content>\\n</Document>',\n",
       "  '<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2406.04744v2\" date=\"2024-11-01\" authors=\"Xiao Yang, Kai Sun, Hao Xin, Yushi Sun, Nikita Bhalla, Xiangsen Chen, Sajal Choudhary, Rongze Daniel Gui, Ziran Will Jiang, Ziyu Jiang, Lingkun Kong, Brian Moran, Jiaqi Wang, Yifan Ethan Xu, An Yan, Chenyu Yang, Eting Yuan, Hanwen Zha, Nan Tang, Lei Chen, Nicolas Scheffer, Yue Liu, Nirav Shah, Rakesh Wanga, Anuj Kumar, Wen-tau Yih, Xin Luna Dong\"/>\\n<Title>\\nCRAG -- Comprehensive RAG Benchmark\\n</Title>\\n\\n<Summary>\\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising\\nsolution to alleviate Large Language Model (LLM)\\'s deficiency in lack of\\nknowledge. Existing RAG datasets, however, do not adequately represent the\\ndiverse and dynamic nature of real-world Question Answering (QA) tasks. To\\nbridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual\\nquestion answering benchmark of 4,409 question-answer pairs and mock APIs to\\nsimulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a\\ndiverse array of questions across five domains and eight question categories,\\nreflecting varied entity popularity from popular to long-tail, and temporal\\ndynamisms ranging from years to seconds. Our evaluation of this benchmark\\nhighlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve\\n<=34% accuracy on CRAG, adding RAG in a straightforward manner improves the\\naccuracy only to 44%. State-of-the-art industry RAG solutions only answer 63%\\nof questions without any hallucination. CRAG also reveals much lower accuracy\\nin answering questions regarding facts with higher dynamism, lower popularity,\\nor higher complexity, suggesting future research directions. The CRAG benchmark\\nlaid the groundwork for a KDD Cup 2024 challenge and attracted thousands of\\nparticipants and submissions. We commit to maintaining CRAG to serve research\\ncommunities in advancing RAG solutions and general QA solutions. CRAG is\\navailable at https://github.com/facebookresearch/CRAG/.\\n</Summary>\\n\\n<Content>\\nCRAG – Comprehensive RAG Benchmark\\nXiao Yang˚1, Kai Sun˚1, Hao Xin˚3, Yushi Sun˚3, Nikita Bhalla1, Xiangsen Chen4, Sajal\\nChoudhary1, Rongze Daniel Gui1, Ziran Will Jiang1, Ziyu Jiang4, Lingkun Kong1, Brian Moran1,\\nJiaqi Wang1, Yifan Ethan Xu1, An Yan1, Chenyu Yang4, Eting Yuan1, Hanwen Zha1, Nan Tang3,4,\\nLei Chen3,4, Nicolas Scheffer1, Yue Liu1, Nirav Shah1, Rakesh Wanga1, Anuj Kumar1, Wen-tau Yih2,\\nand Xin Luna Dong1\\n1Meta Reality Labs, 2 FAIR, Meta, 3 HKUST, 4 HKUST (GZ)\\nAbstract\\nRetrieval-Augmented Generation (RAG) has recently emerged as a promising solu-\\ntion to alleviate Large Language Model (LLM)’s deficiency in lack of knowledge.\\nExisting RAG datasets, however, do not adequately represent the diverse and dy-\\nnamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we\\nintroduce the Comprehensive RAG Benchmark (CRAG), a factual question an-\\nswering benchmark of 4,409 question-answer pairs and mock APIs to simulate web\\nand Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse\\narray of questions across five domains and eight question categories, reflecting\\nvaried entity popularity from popular to long-tail, and temporal dynamisms ranging\\nfrom years to seconds. Our evaluation of this benchmark highlights the gap to\\nfully trustworthy QA. Whereas most advanced LLMs achieve ď 34% accuracy\\non CRAG, adding RAG in a straightforward manner improves the accuracy only\\nto 44%. State-of-the-art industry RAG solutions only answer 63% of questions\\nwithout any hallucination. CRAG also reveals much lower accuracy in answer-\\ning questions regarding facts with higher dynamism, lower popularity, or higher\\ncomplexity, suggesting future research directions. The CRAG benchmark laid the\\ngroundwork for a KDD Cup 2024 challenge and attracted thousands of participants\\nand submissions. We commit to maintaining CRAG to serve research communities\\nin advancing RAG solutions and general QA solutions. CRAG is available at\\nhttps://github.com/facebookresearch/CRAG/.\\n1\\nIntroduction\\nLarge Language Models (LLMs) have transformed the landscape of Natural Language Processing\\n(NLP) tasks, especially in Question Answering (QA) [20, 22, 38, 39]. Despite the advancements,\\nthe issue of hallucination persists as a significant challenge; LLMs may generate answers that lack\\nfactual accuracy or grounding [14,27,30,32]. Studies have shown that GPT-4’s accuracy in answering\\nquestions referring to slow-changing or fast-changing facts is below 15% [36]; even for stable (never-\\nchanging) facts, GPT-4’s accuracy in answering questions referring to torso-to-tail (less popular)\\nentities is below 35% [29]. Overcoming hallucinations thus becomes a priority in building reliable\\nQA systems [13,14].\\nRetrieval-Augmented Generation (RAG) [6,8,12,19] has recently emerged as a promising solution to\\nalleviate LLM’s deficiency in lack of knowledge and attracted a lot of attention from both academia\\nresearch and industry. Given a question, a RAG system searches external sources to retrieve relevant\\ninformation and then provides grounded answers [7,12,19] (see Figure 1 for an illustration). Despite\\n˚Equal contribution. Correspondence to: Xiao Yang (xiaoyangfb@meta.com).\\n38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.\\narXiv:2406.04744v2  [cs.CL]  1 Nov 2024\\nLLM\\nWhat is the gold \\nprice today?\\nGold price is at $1626.81 per \\nounce today Oct 21 2022.\\nGold price is at $2020.8 per \\nounce today Jan 28 2024.\\nDocuments\\nWeb \\nSearch\\nReal-time \\nAPIs\\nKnowledge \\nGraph\\nRetrieved \\nrelevant\\nknowledge\\nQuestion \\n(a) LLM Direct Generation\\n(b) RAG: Retrieved-Augmented \\nGeneration with LLM\\nFigure 1: QA using LLMs (a) without RAG vs. (b) with RAG.\\nits potential, RAG still faces many challenges, such as selecting the most relevant information,\\nreducing question answering latency, and synthesizing information to answer complex questions.\\nA comprehensive benchmark is currently missing to advance continued research efforts \\n</Content>\\n</Document>',\n",
       "  '<Document source=\"arxiv\" url=\"http://arxiv.org/abs/2409.15337v1\" date=\"2024-09-09\" authors=\"Jie Ouyang, Yucong Luo, Mingyue Cheng, Daoyu Wang, Shuo Yu, Qi Liu, Enhong Chen\"/>\\n<Title>\\nRevisiting the Solution of Meta KDD Cup 2024: CRAG\\n</Title>\\n\\n<Summary>\\nThis paper presents the solution of our team APEX in the Meta KDD CUP 2024:\\nCRAG Comprehensive RAG Benchmark Challenge. The CRAG benchmark addresses the\\nlimitations of existing QA benchmarks in evaluating the diverse and dynamic\\nchallenges faced by Retrieval-Augmented Generation (RAG) systems. It provides a\\nmore comprehensive assessment of RAG performance and contributes to advancing\\nresearch in this field. We propose a routing-based domain and dynamic adaptive\\nRAG pipeline, which performs specific processing for the diverse and dynamic\\nnature of the question in all three stages: retrieval, augmentation, and\\ngeneration. Our method achieved superior performance on CRAG and ranked 2nd for\\nTask 2&3 on the final competition leaderboard. Our implementation is available\\nat this link: https://github.com/USTCAGI/CRAG-in-KDD-Cup2024.\\n</Summary>\\n\\n<Content>\\nRevisiting the Solution of Meta KDD Cup 2024: CRAG\\nJie Ouyang\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nouyang_jie@mail.ustc.edu.cn\\nYucong Luo\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nprime666@mail.ustc.edu.cn\\nMingyue Cheng∗\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nmycheng@ustc.edu.cn\\nDaoyu Wang\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nwdy030428@mail.ustc.edu.cn\\nShuo Yu\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nyu12345@mail.ustc.edu.cn\\nQi Liu\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\nqiliuql@ustc.edu.cn\\nEnhong Chen\\nState Key Laboratory of Cognitive\\nIntelligence, University of Science and\\nTechnology of China\\nHefei, Anhui, China\\ncheneh@ustc.edu.cn\\nAbstract\\nThis paper presents the solution of our team APEX in the Meta KDD\\nCUP 2024: CRAG Comprehensive RAG Benchmark Challenge. The\\nCRAG benchmark addresses the limitations of existing QA bench-\\nmarks in evaluating the diverse and dynamic challenges faced by\\nRetrieval-Augmented Generation (RAG) systems. It provides a more\\ncomprehensive assessment of RAG performance and contributes\\nto advancing research in this field. We propose a routing-based do-\\nmain and dynamic adaptive RAG pipeline, which performs specific\\nprocessing for the diverse and dynamic nature of the question in all\\nthree stages: retrieval, augmentation, and generation. Our method\\nachieved superior performance on CRAG and ranked 2nd for Task\\n2&3 on the final competition leaderboard. Our implementation is\\navailable at this link: https://github.com/USTCAGI/CRAG-in-KDD-\\nCup2024.\\nCCS Concepts\\n• Information systems →Information retrieval.\\nKeywords\\nRetrieval-Augmented Generation, Large Language Model\\n1\\nIntroduction\\nLarge Language Models (LLMs) have revolutionized the landscape\\nof Natural Language Processing (NLP) tasks [5, 8, 10], particularly in\\nquestion answering (QA). Despite advances in LLMs, hallucination\\nremains a significant challenge, particularly for dynamic facts and\\ninformation about less prominent entities.\\nRetrieval-Augmented Generation (RAG) [9] has recently emerged\\nas a promising solution to mitigate LLMs’ knowledge deficiencies.\\n∗Mingyue Cheng is the corresponding author.\\nGiven a question, a RAG system queries external sources to re-\\ntrieve relevant information and subsequently provides grounded\\nanswers. Despite its potential, RAG continues to face numerous\\nchallenges, including the selection of the most relevant information,\\nthe reduction of question answering latency, and the synthesis of\\ninformation to address complex questions.\\nTo bridge this gap, Meta introduced the Comprehensive RAG\\nBenchmark (CRAG) [13], a factual question answering benchmark\\nof 4,409 question-answer pairs and Mock APIs to simulate web\\nand Knowledge Graph (KG) search, and hosted the KDD CUP 2024\\nChallenge.\\n1.1\\nDataset Description\\nThe CRAG contains two parts of data: the QA pairs and the content\\nfor retrieval.\\nQA pairs. The CRAG dataset contains a rich set of 4,409 QA\\npairs covering five domains: finance, sports, music, movie, and\\nopen domain, and eight types of questions. For the KDD CUP 2024\\nChallenge, the benchmark data were splited into three sets with\\nsimilar distributions: validation, public test, and private test at 30%,\\n30%, and 40%, respectively. In total, 2,706 examples from validation\\nand public test sets were shared.\\nThe dataset also reflects varied entity popularity from popular\\nto long-tail entities, and temporal spans ranging from seconds to\\nyears. Given the temporal nature of many questions, each question-\\nanswer pair is accompanied by an additional field denoted as \"query\\ntime.\" This temporal mark\\n</Content>\\n</Document>']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_arxiv({\"messages\": [(\"user\", \"CRAG에 대해서 설명해주세요.\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac6790",
   "metadata": {},
   "source": [
    "#### 답변 생성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a01ce399",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_instructions = \"\"\"당신은 분석가에게 인터뷰를 받는 전문가입니다.\n",
    "\n",
    "분석가의 주요 관심 분야는 다음과 같습니다: {goals}. \n",
    "\n",
    "당신의 목표는 분석가가 제기한 질문에 답변하는 것입니다.\n",
    "\n",
    "질문에 답변할 때는 다음 맥락을 활용하십시오:\n",
    "<Context>\n",
    "{context}\n",
    "<Context>\n",
    "\n",
    "질문에 답변할 때는 다음 지침을 따르십시오:\n",
    "1. 맥락에 제공된 정보만 사용하십시오. \n",
    "2. 외부 정보를 도입하거나 맥락에 명시적으로 언급된 내용을 넘어선 추측을 하지 마십시오.\n",
    "3. 맥락에는 각 개별 문서의 주제별 출처가 포함되어 있습니다.\n",
    "4. 답변에서 관련 진술 옆에 해당 출처를 포함하십시오. 예를 들어 출처 #1의 경우 [1]을 사용하십시오.\n",
    "5. 답변 하단에 출처를 순서대로 나열하십시오. [1] 출처 1, [2] 출처 2, 등\n",
    "6. 출처가 다음과 같은 경우: <Document url=\"assistant/docs/llama3_1.pdf\" page=\"7\" />' \n",
    "    다음처럼 기재하십시오: [1] assistant/docs/llama3_1.pdf, 7 페이지\n",
    "7. 제공된 맥락이 없다면 출처를 기재하기 마십시오.\n",
    "    \n",
    "인용 시 괄호 추가 및 Document 서두 문구는 생략하십시오.\"\"\"\n",
    "\n",
    "\n",
    "def generate_answer(state: InterviewState):\n",
    "    \"\"\"질문에 대한 답변 노드\"\"\"\n",
    "\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    system_message = answer_instructions.format(\n",
    "        goals=analyst.description,\n",
    "        context=\"\\n\".join(state[\"context\"]),\n",
    "    )\n",
    "    response = llm.invoke([(\"system\", system_message)] + state[\"messages\"])\n",
    "    response.name = \"전문가\"\n",
    "\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "37fe5c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 전문가\n",
      "\n",
      "AI 에이전트는 기업의 비즈니스 모델 혁신과 기업가 정신에 중요한 영향을 미칩니다. 특히, 스타트업 관점에서 AI 에이전트는 적응형 시스템으로서 유연성과 시장 적응력을 강화해 줍니다. 이는 기업이 빠르게 변화하는 환경에 효과적으로 대응하고, 새로운 기회를 포착하며, 사업 운영 방식을 혁신하는 데 기여합니다. AI 기술을 활용한 자동화와 데이터 분석 능력은 기업이 비용을 절감하고 효율성을 높이는 동시에, 고객 맞춤형 서비스 제공과 신시장 개척에도 도움을 줍니다. 따라서 AI 에이전트는 기업이 지속 가능한 성장과 경쟁력을 확보하는 데 핵심적인 역할을 수행합니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "response = generate_answer(\n",
    "    {\n",
    "        \"analyst\": analyst,\n",
    "        \"context\": [],\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"AI 에이전트가 기업에 미치는 영향은?\", name=\"분석가\")\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "response[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5624648b",
   "metadata": {},
   "source": [
    "### 문석 작성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "82c9e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기술 문서 작성 프롬프트\n",
    "section_writer_instructions = \"\"\"당신은 전문 기술 문서 작성자입니다.\n",
    "\n",
    "당신의 임무는 일련의 원본 문서를 철저히 분석하여 보고서의 상세하고 포괄적인 섹션을 작성하는 것입니다.\n",
    "이는 핵심 통찰력을 추출하고, 관련 사항을 상세히 설명하며, 명확성과 이해를 보장하기 위한 심층적인 해설을 제공하는 것을 포함합니다. 필요한 배경 정보, 뒷받침하는 증거, 예시를 포함하여 독자의 이해를 높여야 합니다. 논리적이고 체계적인 구조를 유지하며, 모든 핵심 사항이 상세히 다루어지고 전문적인 어조로 제시되도록 하십시오.\n",
    "\n",
    "다음 지침을 따르십시오:\n",
    "1. 원본 문서의 내용 분석:\n",
    "- 각 원본 문서의 이름은 문서 시작 부분에 <Document> 태그와 함께 기재되어 있습니다.\n",
    "\n",
    "2. 마크다운 서식을 사용하여 보고서 구조 생성:\n",
    "- 섹션 제목에는 ## 사용\n",
    "- 하위 섹션 헤더에는 ### 사용\n",
    "\n",
    "3. 다음 구조에 따라 보고서 작성:\n",
    "a. 제목 (## 헤더)\n",
    "b. 요약 (### 헤더)\n",
    "c. 종합 분석 (### 헤더)\n",
    "d. 출처 (### 헤더)\n",
    "\n",
    "4. 분석가의 중점 분야를 반영하여 제목을 흥미롭게 작성하십시오: \n",
    "{focus}\n",
    "\n",
    "5. 요약 섹션 작성 시:\n",
    "- 분석가의 중점 분야와 관련된 일반적 배경/맥락을 요약으로 제시하십시오\n",
    "- 인터뷰에서 수집한 통찰 중 새롭거나 흥미롭거나 놀라운 점을 강조하십시오\n",
    "- 사용한 출처 문서를 번호 매긴 목록으로 작성하십시오\n",
    "- 인터뷰어 또는 전문가의 이름은 언급하지 마십시오\n",
    "- 최대 약 400단어를 목표로 하십시오\n",
    "- 출처 문서의 정보에 기반하여 보고서에서 번호 매긴 출처([1], [2] 등)를 사용하십시오\n",
    "\n",
    "6. 종합 분석 섹션:\n",
    "- 출처 문서의 정보를 상세히 검토하십시오.\n",
    "- 복잡한 아이디어를 이해하기 쉬운 단위로 분해하고 논리적인 흐름을 유지하십시오.\n",
    "- 분석의 다양한 관점이나 차원을 다루기 위해 필요한 경우 하위 섹션을 사용하십시오.\n",
    "- 원본 문서의 데이터, 직접 인용문, 예시를 통해 분석을 뒷받침하십시오.\n",
    "- 각 논점이 보고서의 전반적인 초점과 어떻게 관련되는지 명확히 설명하십시오.\n",
    "- 여러 관련 아이디어를 제시할 때는 명확성을 위해 글머리 기호나 번호 매기기 목록을 사용하십시오.\n",
    "- 전문적이고 객관적인 어조를 유지하며 편향되거나 근거 없는 의견을 피하십시오.\n",
    "- 분석이 철저하도록 최소 800단어를 목표로 하십시오.\n",
    "\n",
    "7. 출처 섹션에서:\n",
    "- 보고서에 사용된 모든 출처를 포함하십시오\n",
    "- 관련 웹사이트 또는 특정 문서 경로의 전체 링크를 제공하십시오\n",
    "- 각 출처는 새 줄로 구분하십시오. 마크다운에서 새 줄을 만들기 위해 각 줄 끝에 두 개의 공백을 사용하십시오.\n",
    "- 예시:\n",
    "    ### 출처\n",
    "    [1] 링크 또는 문서명\n",
    "    [2] 링크 또는 문서명\n",
    "\n",
    "8. 출처를 반드시 통합하십시오. 예를 들어 다음은 올바르지 않습니다:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "[4] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "중복 출처는 없어야 합니다. 다음과 같이 간결하게 작성하십시오:\n",
    "\n",
    "[3] https://ai.meta.com/blog/meta-llama-3-1/\n",
    "\n",
    "9. 최종 검토:\n",
    "- 보고서가 요구되는 구조를 따르는지 확인하십시오.\n",
    "- 보고서 제목 앞에 서문을 포함하지 마십시오.\n",
    "- 모든 지침이 준수되었는지 확인하십시오.\n",
    "\"\"\"\n",
    "\n",
    "from langchain_core.messages import get_buffer_string\n",
    "\n",
    "\n",
    "def write_section(state: InterviewState):\n",
    "    \"\"\"인터뷰에 대한 리포트 작성\"\"\"\n",
    "\n",
    "    analyst = state[\"analyst\"]\n",
    "\n",
    "    # 인터뷰를 문자열로 변환\n",
    "    interview = get_buffer_string(state[\"messages\"])\n",
    "\n",
    "    # 섹션 작성을 위한 시스템 프롬프트 정의\n",
    "    system_message = section_writer_instructions.format(focus=analyst.description)\n",
    "    response = llm.invoke(\n",
    "        [SystemMessage(content=system_message)]\n",
    "        + [\n",
    "            HumanMessage(\n",
    "                content=f\"분석가와 전문가의 인터뷰 내용입니다:\\n<Interview>\\n{interview}\\n</Interview>\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"이 자료를 사용하여 해당 섹션을 작성하십시오:\\n<Resources>\\n{state['context']}\\n</Resources>\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return {\"sections\": [response.content]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb6b207",
   "metadata": {},
   "source": [
    "### 인터뷰 그래프 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "fd3d133e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "\n",
    "def should_continue_interview(state: InterviewState) -> Literal[\"finish\", \"continue\"]:\n",
    "    \"\"\"인터뷰를 계속 진행할 지 여부를 판단합니다.\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    max_num_turns = state.get(\"max_num_turns\", 2)\n",
    "\n",
    "    expert_message_count = sum(\n",
    "        1 for m in messages if isinstance(m, AIMessage) and m.name == \"전문가\"\n",
    "    )\n",
    "\n",
    "    # 전문가가 최대 턴 이상 답변했다면 인터뷰를 종료합니다.\n",
    "    if expert_message_count >= max_num_turns:\n",
    "        return \"finish\"\n",
    "\n",
    "    last_message = messages[-1]\n",
    "    if \"도움 주셔서 정말 감사합니다\" in last_message.content:\n",
    "        return \"finish\"\n",
    "\n",
    "    return \"continue\"\n",
    "\n",
    "\n",
    "interview_builder = StateGraph(InterviewState)\n",
    "interview_builder.add_node(generate_question)  # 질문 생성\n",
    "interview_builder.add_node(search_web)  # 웹 검색\n",
    "interview_builder.add_node(search_arxiv)  # arxiv 검색\n",
    "interview_builder.add_node(generate_answer)  # 답변 생성\n",
    "interview_builder.add_node(write_section)  # 문서 작성\n",
    "\n",
    "interview_builder.set_entry_point(\"generate_question\")\n",
    "interview_builder.add_edge(\"generate_question\", \"search_web\")\n",
    "interview_builder.add_edge(\"generate_question\", \"search_arxiv\")\n",
    "interview_builder.add_edge(\"search_web\", \"generate_answer\")\n",
    "interview_builder.add_edge(\"search_arxiv\", \"generate_answer\")\n",
    "interview_builder.add_conditional_edges(\n",
    "    \"generate_answer\",\n",
    "    should_continue_interview,\n",
    "    {\n",
    "        \"continue\": \"generate_question\",\n",
    "        \"finish\": \"write_section\",\n",
    "    },\n",
    ")\n",
    "interview_builder.set_finish_point(\"write_section\")\n",
    "\n",
    "interview_graph = interview_builder.compile(\n",
    "    checkpointer=InMemorySaver(),\n",
    ").with_config(run_name=\"Conduct Interviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9cf57641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(interview_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252d966",
   "metadata": {},
   "source": [
    "### 인터뷰 그래프 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3f343ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "안녕하세요, 저는 Seokho Phil입니다. AI 통합에 중점을 둔 스타트업 창업자로서 특히 RAG(Retrieval-Augmented Generation) 기술을 비즈니스 모델과 연결해 기업의 성장과 경쟁 우위를 도모하고 있습니다.\n",
      "\n",
      "모듈러 RAG(Modular RAG)와 기존의 Naive RAG의 차이에 대해 구체적으로 설명드리자면, 전통적인 Naive RAG는 단일 형태로 설계되어 정보 검색과 생성 과정을 단순화한 반면, 모듈러 RAG는 검색, 인덱싱, 컨텍스트 융합, 생성의 각 모듈을 독립적으로 최적화할 수 있도록 설계되어 있습니다.\n",
      "\n",
      "예를 들어, 저희 스타트업에서는 금융 데이터 기반 AI 어시스턴트를 개발하며, Naive RAG 방식을 쓰면 특정 금융 보고서나 뉴스 데이터의 변동에 즉시 대응하기 어려웠습니다. 그러나 모듈러 RAG를 도입하고 나서는 검색 모듈을 별도 업데이트 해 핵심 금융 지표의 변화 탐지 성능을 빠르게 개선했고, 생성 모듈은 고객 맞춤형 설명을 제공하는 쪽으로 따로 튜닝하여 유연성이 크게 향상되었습니다.\n",
      "\n",
      "이런 모듈화 덕분에 production level에서 다음과 같은 이점이 있습니다:\n",
      "\n",
      "1. 신속한 대응과 업데이트 가능성: 각 모듈을 독립적으로 교체하거나 개선함으로써, 전체 시스템을 중단하지 않고도 최신 데이터와 비즈니스 요구에 맞추어 빠르게 적응 가능합니다.\n",
      "\n",
      "2. 비용 효율성: 예를 들어 검색 모듈에서 경량화된 벡터 인덱싱 기법을 도입해 비용을 줄이고, 생성 모듈은 더 복잡한 하드웨어에서 실행해 품질을 높이면서 자원 할당의 최적화를 가능하게 했습니다.\n",
      "\n",
      "3. 확장성: 스타트업이 초기 MVP(most viable product)를 빠르게 출시한 후, 다양한 RAG 모듈을 추가하거나 교체해 다양한 산업군 또는 고객 요구사항에 맞는 맞춤형 솔루션으로 확장할 수 있었습니다.\n",
      "\n",
      "이처럼 모듈러 RAG는 단순히 기술적인 차원뿐 아니라 스타트업의 사업 전략과 운영에도 큰 변화를 가져온 사례를 만들었습니다. 더 자세한 부분이 궁금하신가요?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_answer\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 전문가\n",
      "\n",
      "모듈러 RAG는 기존 Naive RAG와 달리 복잡한 RAG 시스템을 독립적인 모듈과 특화된 연산자로 분해해 LEGO 블록처럼 유연하게 재구성할 수 있는 구조를 가집니다. Naive RAG가 단순히 인덱싱, 검색, 생성의 직선적인 흐름에 의존하며 쿼리와 문서 청크 사이의 단순 유사도에 기반하는 반면, 모듈러 RAG는 라우팅, 스케줄링, 융합 메커니즘 등을 통합하여 더 복잡한 검색 패턴(선형, 조건부, 분기, 루프)을 지원합니다[1][4].\n",
      "\n",
      "Production 환경에서 모듈러 RAG가 주는 이점은 다음과 같습니다. 첫째, 구성 요소를 독립적으로 교체하거나 개선할 수 있어서 신속한 업데이트 및 유지보수가 가능합니다. 둘째, 중복되거나 불필요한 정보가 섞이는 것을 줄여 LLM의 노이즈 간섭을 감소시키고 결과의 정확성을 향상시킵니다[1][4]. 셋째, 유연한 설계로 새로운 검색 및 생성 방식을 맞춤형으로 도입하기 용이해 사업 확장과 성능 최적화에 최적화된 환경을 제공합니다. 넷째, 벤치마크 및 연구 도구들(예: FlashRAG)과 결합하면 다양한 RAG 방법을 평가하고 효율적 연구개발이 가능하다는 장점도 있습니다[3].\n",
      "\n",
      "따라서, 모듈러 RAG는 기술적 복잡성을 제어하면서도 생산 시스템에 반드시 필요한 유연성과 확장성을 확보하는 핵심 혁신으로서, AI 스타트업이 빠르게 진화하는 시장 요구에 대응하고 경쟁우위를 확보하는 데 매우 유용한 접근법이라 할 수 있습니다.\n",
      "\n",
      "---\n",
      "[1] arxiv.org/abs/2407.21059v1  \n",
      "[3] arxiv.org/abs/2405.13576v2  \n",
      "[4] https://blog-ko.superb-ai.com/rag-technology-evolution-naive-advanced-modular-approaches/\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_question\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "안녕하세요, 저는 Seokho Phil입니다. AI 통합에 특화된 스타트업 창업자이자, 기술적 RAG 솔루션과 창업 비즈니스 모델의 다리를 놓아 적응적 성장과 경쟁 우위를 도모하는 일을 하고 있습니다.\n",
      "\n",
      "먼저, 기존의 Naive RAG와 모듈러 RAG의 가장 큰 차이는 시스템 구성의 유연성과 확장성에 있습니다. Naive RAG는 단순히 텍스트 임베딩을 통해 쿼리와 문서 청크 간의 유사도 계산에 의존하는 반면, 모듈러 RAG는 검색, 인덱싱, 융합, 생성 각 과정을 별도의 모듈로 분리해 마치 레고 블록처럼 재조합할 수 있도록 설계되었습니다.\n",
      "\n",
      "예를 들면 저희 금융 스타트업 프로젝트에서, Naive RAG를 사용했을 때는 최신 금융 뉴스 반영이 느렸고, 고객 맞춤 설명 생성도 한계가 있었습니다. 반면, 모듈러 RAG를 도입한 이후 검색 모듈을 빠르게 업그레이드해 최신 데이터 반영률을 높였고, 생성 모듈 튜닝을 통해 각 기업 고객사마다 맞춤형 설명을 제공할 수 있었습니다. 이로써 고객 만족도와 시장 반응이 빠르게 개선되었습니다.\n",
      "\n",
      "이러한 모듈러 RAG가 프로덕션에 안겨주는 구체적 이점은 크게 세 가지입니다. 첫째, 각 모듈별 독립 업데이트를 통해 시스템 중단 없이 기능 개선이 가능해 신속한 시장 대응이 가능합니다. 둘째, 비용과 리소스를 모듈별로 최적화하며 운영 효율을 극대화했고, 셋째, 확장성이 높아 초기 MVP부터 다양한 산업군과 성장 단계에 맞춘 전략적 솔루션 제공이 가능했습니다.\n",
      "\n",
      "이처럼 모듈러 RAG는 단순 기술 넘어, 스타트업이 빠르게 변화하는 시장 요구를 충족하며 지속적 성장과 차별화된 경쟁력 확보를 가능케 하는 사업 전략의 중요한 축입니다. 혹시 더 깊이 들어가고 싶은 특정 사례나 기술적인 세부 요소가 있으신가요?\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "🔄 Node: \u001b[1;36mgenerate_answer\u001b[0m 🔄\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Name: 전문가\n",
      "\n",
      "모듈러 RAG는 기존 Naive RAG와 달리 RAG 시스템을 독립적인 모듈과 연산자로 분해하여 LEGO 블록처럼 유연하게 재구성할 수 있는 프레임워크를 제공합니다. Naive RAG는 단순히 질의와 문서 간 유사도 기반으로 검색과 생성이 직선적으로 이루어지는 반면, 모듈러 RAG는 라우팅, 스케줄링, 융합 등 복잡한 처리 흐름을 지원하며, 검색 재순위화나 요약 같은 다양한 후처리도 별도 모듈로 관리합니다. \n",
      "\n",
      "이러한 모듈화 덕분에 생산 환경에서는 각 모듈을 개별적으로 개선하거나 교체할 수 있어 유지보수가 쉽고, 필요에 따라 효율적 최적화가 가능합니다. 또한, 중복되거나 노이즈 많은 정보가 줄어들어 LLM이 더 정확한 답변을 생성할 수 있습니다. 스타트업이나 기업 환경에서는 이 유연성과 확장성 덕분에 빠른 기능 추가, 다양한 도메인별 커스텀 적용, 리소스 효율화 등이 가능해 경쟁 우위 확보에 매우 유리합니다.\n",
      "\n",
      "즉, 모듈러 RAG는 Naive RAG의 한계를 극복하며, 복잡한 요구사항과 대규모 서비스 환경에 적합한 RAG 발전의 핵심 패러다임입니다[1][4][9].\n",
      "\n",
      "[1] arxiv.org/abs/2407.21059v1  \n",
      "[4] superb-ai.com/rag-technology-evolution-naive-advanced-modular-approaches/  \n",
      "[9] wikidocs.net/270688\n",
      "==================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2657\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2656\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2657\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2658\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2659\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2660\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2661\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2662\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2663\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2664\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2666\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:224\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(futures) > (\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_waiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     done, inflight = \u001b[43mconcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_when\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFIRST_COMPLETED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmonotonic\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:305\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(fs, timeout, return_when)\u001b[39m\n\u001b[32m    303\u001b[39m     waiter = _create_and_install_waiters(fs, return_when)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m     gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[116]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m topic = \u001b[33m\"\u001b[39m\u001b[33mModular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 그래프 실행\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43minvoke_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterview_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43manalyst\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43manalysts\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m그래서 당신이 이 주제에 대해서 글을 쓰고 있다고 했죠? 라고 말씀하셨죠? \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mtopic\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_num_turns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnode_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenerate_question\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenerate_answer\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwrite_section\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain_teddynote/messages.py:409\u001b[39m, in \u001b[36minvoke_graph\u001b[39m\u001b[34m(graph, inputs, config, node_names, callback)\u001b[39m\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m namespace[-\u001b[32m1\u001b[39m].split(\u001b[33m\"\u001b[39m\u001b[33m:\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(namespace) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mroot graph\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# subgraphs=True 를 통해 서브그래프의 출력도 포함\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnamespace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    411\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_chunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# node_names가 비어있지 않은 경우에만 필터링\u001b[39;49;00m\n\u001b[32m    414\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnode_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnode_names\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2593\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2590\u001b[39m runtime = parent_runtime.merge(runtime)\n\u001b[32m   2591\u001b[39m config[CONF][CONFIG_KEY_RUNTIME] = runtime\n\u001b[32m-> \u001b[39m\u001b[32m2593\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SyncPregelLoop(\n\u001b[32m   2594\u001b[39m     \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   2595\u001b[39m     stream=StreamProtocol(stream.put, stream_modes),\n\u001b[32m   2596\u001b[39m     config=config,\n\u001b[32m   2597\u001b[39m     store=store,\n\u001b[32m   2598\u001b[39m     cache=cache,\n\u001b[32m   2599\u001b[39m     checkpointer=checkpointer,\n\u001b[32m   2600\u001b[39m     nodes=\u001b[38;5;28mself\u001b[39m.nodes,\n\u001b[32m   2601\u001b[39m     specs=\u001b[38;5;28mself\u001b[39m.channels,\n\u001b[32m   2602\u001b[39m     output_keys=output_keys,\n\u001b[32m   2603\u001b[39m     input_keys=\u001b[38;5;28mself\u001b[39m.input_channels,\n\u001b[32m   2604\u001b[39m     stream_keys=\u001b[38;5;28mself\u001b[39m.stream_channels_asis,\n\u001b[32m   2605\u001b[39m     interrupt_before=interrupt_before_,\n\u001b[32m   2606\u001b[39m     interrupt_after=interrupt_after_,\n\u001b[32m   2607\u001b[39m     manager=run_manager,\n\u001b[32m   2608\u001b[39m     durability=durability_,\n\u001b[32m   2609\u001b[39m     trigger_to_nodes=\u001b[38;5;28mself\u001b[39m.trigger_to_nodes,\n\u001b[32m   2610\u001b[39m     migrate_checkpoint=\u001b[38;5;28mself\u001b[39m._migrate_checkpoint,\n\u001b[32m   2611\u001b[39m     retry_policy=\u001b[38;5;28mself\u001b[39m.retry_policy,\n\u001b[32m   2612\u001b[39m     cache_policy=\u001b[38;5;28mself\u001b[39m.cache_policy,\n\u001b[32m   2613\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m loop:\n\u001b[32m   2614\u001b[39m     \u001b[38;5;66;03m# create runner\u001b[39;00m\n\u001b[32m   2615\u001b[39m     runner = PregelRunner(\n\u001b[32m   2616\u001b[39m         submit=config[CONF].get(\n\u001b[32m   2617\u001b[39m             CONFIG_KEY_RUNNER_SUBMIT, weakref.WeakMethod(loop.submit)\n\u001b[32m   (...)\u001b[39m\u001b[32m   2620\u001b[39m         node_finished=config[CONF].get(CONFIG_KEY_NODE_FINISHED),\n\u001b[32m   2621\u001b[39m     )\n\u001b[32m   2622\u001b[39m     \u001b[38;5;66;03m# enable subgraph streaming\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/_loop.py:1104\u001b[39m, in \u001b[36mSyncPregelLoop.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m   1097\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\n\u001b[32m   1098\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1099\u001b[39m     exc_type: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m] | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1102\u001b[39m ) -> \u001b[38;5;28mbool\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;66;03m# unwind stack\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraceback\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/contextlib.py:610\u001b[39m, in \u001b[36mExitStack.__exit__\u001b[39m\u001b[34m(self, *exc_details)\u001b[39m\n\u001b[32m    606\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    607\u001b[39m     \u001b[38;5;66;03m# bare \"raise exc_details[1]\" replaces our carefully\u001b[39;00m\n\u001b[32m    608\u001b[39m     \u001b[38;5;66;03m# set-up context\u001b[39;00m\n\u001b[32m    609\u001b[39m     fixed_ctx = exc_details[\u001b[32m1\u001b[39m].__context__\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc_details[\u001b[32m1\u001b[39m]\n\u001b[32m    611\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:\n\u001b[32m    612\u001b[39m     exc_details[\u001b[32m1\u001b[39m].__context__ = fixed_ctx\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/contextlib.py:595\u001b[39m, in \u001b[36mExitStack.__exit__\u001b[39m\u001b[34m(self, *exc_details)\u001b[39m\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m is_sync\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m595\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mcb\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mexc_details\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    596\u001b[39m         suppressed_exc = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    597\u001b[39m         pending_raise = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langgraph/pregel/_executor.py:108\u001b[39m, in \u001b[36mBackgroundExecutor.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# wait for all tasks to finish\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pending := {t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tasks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.done()}:\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[43mconcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpending\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;66;03m# shutdown the executor\u001b[39;00m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mself\u001b[39m.stack.\u001b[34m__exit__\u001b[39m(exc_type, exc_value, traceback)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:305\u001b[39m, in \u001b[36mwait\u001b[39m\u001b[34m(fs, timeout, return_when)\u001b[39m\n\u001b[32m    301\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m DoneAndNotDoneFutures(done, not_done)\n\u001b[32m    303\u001b[39m     waiter = _create_and_install_waiters(fs, return_when)\n\u001b[32m--> \u001b[39m\u001b[32m305\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fs:\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m f._condition:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.11-macos-aarch64-none/lib/python3.12/threading.py:355\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    356\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import invoke_graph\n",
    "\n",
    "\n",
    "config = RunnableConfig(\n",
    "    recursion_limit=30,\n",
    "    configurable={\"thread_id\": random()},\n",
    ")\n",
    "\n",
    "topic = \"Modular RAG 가 기존의 Naive RAG 와 어떤 차이가 있는지와 production level 에서 사용하는 이점\"\n",
    "\n",
    "# 그래프 실행\n",
    "invoke_graph(\n",
    "    interview_graph,\n",
    "    {\n",
    "        \"analyst\": analysts[0],\n",
    "        \"messages\": [\n",
    "            (\n",
    "                \"user\",\n",
    "                f'그래서 당신이 이 주제에 대해서 글을 쓰고 있다고 했죠? 라고 말씀하셨죠? \"{topic}\"',\n",
    "            )\n",
    "        ],\n",
    "        \"max_num_turns\": 2,\n",
    "    },\n",
    "    config=config,\n",
    "    node_names=[\"generate_question\", \"generate_answer\", \"write_section\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c74d83",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foundation-introduction-to-langgraph (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
