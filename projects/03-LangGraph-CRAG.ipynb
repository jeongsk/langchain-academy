{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91ab3d0b",
   "metadata": {},
   "source": [
    "## CRAG(Corrective Retrieval Augmented Generation)\n",
    "\n",
    "> - 참고 자료: https://wikidocs.net/270686\n",
    "> - 관련 논문: https://arxiv.org/pdf/2401.15884\n",
    "\n",
    "CRAG는 RAG의 개선된 버전입니다. **RAG의 한계를 보완**하기 위해 개발된 기술로, 검색된 문서의 품질을 평가하고 필요시 수정하는 메커니즘을 추가했습니다.\n",
    "\n",
    "### 주요 특징\n",
    "\n",
    "**1. 검색 평가자(Retrieval Evaluator)**\n",
    "- 검색된 문서의 관련성과 품질을 평가\n",
    "- 문서가 충분히 정확하고 관련성이 있는지 판단\n",
    "\n",
    "**2. 수정 메커니즘**\n",
    "- 검색 결과가 부정확하거나 불충분할 경우 대응\n",
    "- 추가 검색, 웹 검색, 또는 지식 정제 수행\n",
    "\n",
    "**3. 동적 처리 방식**\n",
    "검색 품질에 따라 세 가지 경로로 분기:\n",
    "- **Correct**: 검색 결과가 좋을 경우 → 그대로 사용\n",
    "- **Incorrect**: 검색 결과가 나쁠 경우 → 웹 검색 등 추가 소스 활용\n",
    "- **Ambiguous**: 애매한 경우 → 지식 정제 후 사용\n",
    "\n",
    "### RAG와의 차이점\n",
    "\n",
    "- **RAG**: 검색된 문서를 무조건 신뢰하고 사용\n",
    "- **CRAG**: 검색 품질을 평가하고, 필요시 보완하거나 다른 소스 활용\n",
    "\n",
    "이를 통해 더 정확하고 신뢰할 수 있는 답변 생성이 가능합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55637cb",
   "metadata": {},
   "source": [
    "## 환경 변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e82e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../.env\", override=True)\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    env_value = os.environ.get(var)\n",
    "    if not env_value:\n",
    "        env_value = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "    os.environ[var] = env_value\n",
    "\n",
    "\n",
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langchain-academy\"\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6435439",
   "metadata": {},
   "source": [
    "## 문서 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c871d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "\n",
    "def download_file(url, dir=\"./temp\"):\n",
    "    if not os.path.exists(dir):\n",
    "        os.makedirs(dir)\n",
    "\n",
    "    file_path = os.path.join(dir, \"doc.pdf\")\n",
    "    urllib.request.urlretrieve(url, file_path)\n",
    "\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"다운로드 완료! 파일 크기: {file_size:,}\", \"bytes\")\n",
    "\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a80649b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "다운로드 완료! 파일 크기: 1,207,479 bytes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./temp/doc.pdf'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"소프트웨어정책연구소(SPRi) - 2025년 10월호\" PDF를 다운로드합니다.\n",
    "downloaded_path = download_file(\"https://spri.kr/download/23735\")\n",
    "downloaded_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5afaafa",
   "metadata": {},
   "source": [
    "## 문서 로더"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "402fccb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 29 페이지 로드됨\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "documents = PyPDFLoader(downloaded_path).load()\n",
    "print(f\"총 {len(documents)} 페이지 로드됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b1853d",
   "metadata": {},
   "source": [
    "## 문서 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da93a99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 70개의 청크로 분할됨\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"총 {len(splits)}개의 청크로 분할됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06031a5",
   "metadata": {},
   "source": [
    "## 문서 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "454966dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anpigon/Workspace/Wantedlab/langchain-academy/venv/lib/python3.12/site-packages/langchain/embeddings/cache.py:58: UserWarning: Using default key encoder: SHA-1 is *not* collision-resistant. While acceptable for most cache scenarios, a motivated attacker can craft two different payloads that map to the same cache key. If that risk matters in your environment, supply a stronger encoder (e.g. SHA-256 or BLAKE2) via the `key_encoder` argument. If you change the key encoder, consider also creating a new cache, to avoid (the potential for) collisions with existing keys.\n",
      "  _warn_about_sha1_encoder()\n"
     ]
    }
   ],
   "source": [
    "from langchain.storage import LocalFileStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import CacheBackedEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "\n",
    "# 임베딩 모델 생성\n",
    "underlying_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "\n",
    "embeddings = CacheBackedEmbeddings.from_bytes_store(\n",
    "    underlying_embeddings,\n",
    "    document_embedding_cache=LocalFileStore(\"./cache\"),\n",
    "    namespace=underlying_embeddings.model,\n",
    "    query_embedding_cache=True,\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f567dac",
   "metadata": {},
   "source": [
    "## 리트리버 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aa6eb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 5, \"fetch_k\": 50},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f589de18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='625c5d38-8e8f-49b7-9ab2-53cf8cf54534', metadata={'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-10-02T12:57:42+09:00', 'author': 'dj', 'moddate': '2025-10-02T12:57:42+09:00', 'pdfversion': '1.4', 'source': './temp/doc.pdf', 'total_pages': 29, 'page': 6, 'page_label': '7'}, page_content='정책･법제기업･산업기술･연구인력･교육\\n5\\n미국 캘리포니아주 의회, 미국 최초로 미성년자 보호를 위한 AI 챗봇 규제 통과n미국 캘리포니아주 의회가 미국 최초로 미성년자와 취약 사용자 보호를 위해 AI 동반자 챗봇을 규제하는 SB-243 법안을 통과시켰으며, 주지사 서명 시 2026년 1월 1일부터 발효 예정n동 법안은 동반자 챗봇 운영 플랫폼을 대상으로 사용자의 자살이나 자해를 방지하기 위한 프로토콜 수립 및 미성년자 보호 조치를 요구하고, 위법으로 인한 피해자에게 민사소송 권리를 부여\\nKEY Contents'),\n",
       " Document(id='5fc0f45e-99cd-4944-a171-e9a7add59965', metadata={'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-10-02T12:57:42+09:00', 'author': 'dj', 'moddate': '2025-10-02T12:57:42+09:00', 'pdfversion': '1.4', 'source': './temp/doc.pdf', 'total_pages': 29, 'page': 9, 'page_label': '10'}, page_content='<‘제미나이 2.5 플래시 이미지’의 이미지 합성 예시>\\n출처 | Google, Nano Banana! Image editing in Gemini just got a major upgrade, 2025.08.26.         Google Deepmind, Create and edit images with Gemini'),\n",
       " Document(id='7d087b8f-86fa-4469-ade3-e87911802ea2', metadata={'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-10-02T12:57:42+09:00', 'author': 'dj', 'moddate': '2025-10-02T12:57:42+09:00', 'pdfversion': '1.4', 'source': './temp/doc.pdf', 'total_pages': 29, 'page': 2, 'page_label': '3'}, page_content='| 2025년 10월호 |\\n정책･법제'),\n",
       " Document(id='7d834587-4adf-4312-a4a3-c4cd71b8a678', metadata={'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-10-02T12:57:42+09:00', 'author': 'dj', 'moddate': '2025-10-02T12:57:42+09:00', 'pdfversion': '1.4', 'source': './temp/doc.pdf', 'total_pages': 29, 'page': 12, 'page_label': '13'}, page_content='£앤스로픽, AI 훈련을 위해 불법 복제한 50만 권의 저작물에 권당 3,000달러 지급 계획n앤스로픽이 불법복제 도서를 이용해 AI 모델을 훈련한 혐의로 제기된 저작권 침해 소송에서 작가들에게 15억 달러를 지급하기로 합의∙작가 3인이 2024년 8월 제기한 저작권 침해 소송에서 원고 측은 앤스로픽이 수백만 권의 불법복제 작품을 사용해 AI 모델을 훈련함으로써 저작권법을 위반했으며 저작권 도용을 은폐하려 했다고 주장∙샌프란시스코 연방법원의 윌리엄 알섭(William Alsup) 판사는 2025년 6월 판결에서 앤스로픽이 작가 3인의 책을 AI 모델 학습에 사용한 행위는 공정이용(Fair Use)*으로 인정* 공정이용(Fair Use): 저작권으로 보호되는 저작물을 저작권자 허락 없이도 일정 범위 내에서 이용할 수 있도록 허용∙그러나 판사는 공정이용 주장이 앤스로픽이 전자책 불법복제 사이트에서 입수한 700만 권 이상의 책에는 적용되지 않는다며, 불법복제 목록에 포함된 작가들을 대신해 작가 3인이 집단소송을 제기할 수 있다고 판결 ∙재판부는 연구 목적이라는 명분이 있더라도 정당한 방식으로 자료를 확보했어야 한다고 강조하며, 공정이용 주장이 성립하려면 초기 복제 단계부터 합법성을 확보해야 한다는 원칙을 명확히 제시∙앤스로픽은 약 50만 건의 저작물 청구에 건당 3,000달러씩 총 15억 달러를 지급하기로 합의했으며, 50만 건을 초과하는 청구에도 건당 3,000달러를 지급하기로 해, 최종 지급액은 청구 건수에 따라 달라질 전망n원고 측 변호인에 따르면 이번 합의금은 미국 저작권 소송 역사상 최대 규모로, 앤스로픽은 합의에 따라 불법적으로 다운로드한 원본 파일과 사본도 폐기 예정∙원고 측은 이번 합의는 과거 행위에 근거한 법적 청구만을 면제하며, 앤스로픽에 향후 AI 훈련에 대한 허가를 부여하거나, 2025년 8월 25일 이후 발생하는 법적 청구도 면제하지 않는다고 부연∙미국출판인협회의 마리아 팔란테(Maria Pallante) 회장은 이번 합의가 불법 복제한'),\n",
       " Document(id='ec36711f-a72b-4eab-bcb1-210d50636f47', metadata={'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-10-02T12:57:42+09:00', 'author': 'dj', 'moddate': '2025-10-02T12:57:42+09:00', 'pdfversion': '1.4', 'source': './temp/doc.pdf', 'total_pages': 29, 'page': 27, 'page_label': '28'}, page_content='6월4~5일AI & Big Data Expo North America 2025미국, 산타클라라www.ai-expo.net/northamerica9~13일 WWDC25미국, 쿠퍼티노developer.apple.com11~15일The IEEE / CVF Computer Vision and Pattern Recognition Conference 미국, 네슈빌cvpr.thecvf.com11~12일AI SUMMIT LONDON영국, 런던london.theaisummit.com11~13일(STK 2025) 스마트테크 코리아서울, 강남smarttechkorea.com18~19일AI World Congress 2025영국, 런던aiconference.london18~20일(MVEX 2025) 2025 메타버스 엑스포서울, 강남metavexpo.com7월8~11일AI for Good Global Summit 2025스위스, 제네바aiforgood.itu.int13~19일 ICML 2025캐나다, 밴쿠버icml.cc25~27일IEEE 7th International Conference on AI, CS and IP중국, 항저우www.aicsconf.cn27~1일the Association for Computational Linguistics오스트리아, 빈2025.aclweb.org8월11~13일(Ai4 2025) the Forefront of AI Innovation미국, 라스베이거스ai4.io/vegas16~22일(IJCAI 2025) International Joint Conference on Artificial Intelligence캐나다, 몬트리올2025.ijcai.org9월3~5일 2025 산업AI EXPO서울, 강서industrialaiexpo.or.kr9~11일AI Infra Summit 2025미국, 산타클라라www.ai-infra-summit.com17~18일The AI Conference미국, 샌프란시스코aiconference.com17~18일 Meta')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"미국 캘리포니아주 의회가 발의한 법안은?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266b9ab",
   "metadata": {},
   "source": [
    "## 상태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a0114d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: Annotated[str, \"사용자 질문\"]\n",
    "    generation: Annotated[str, \"LLM에서 생성된 답변\"]\n",
    "    web_search: Annotated[str, \"웹검색 필요 여부\"]\n",
    "    documents: Annotated[list[Document], \"검색된 문서들\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b04fdd",
   "metadata": {},
   "source": [
    "## 문서 검색 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1406564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_node(state: State):\n",
    "    question = state.get(\"question\")\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "df57d7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'documents': [Document(id='7625e4ff-c11a-4443-b3e5-384bfc6a1814', metadata={'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-10-02T12:57:42+09:00', 'author': 'dj', 'moddate': '2025-10-02T12:57:42+09:00', 'pdfversion': '1.4', 'source': './temp/doc.pdf', 'total_pages': 29, 'page': 9, 'page_label': '10'}, page_content='SPRi AI Brief2025년 10월호\\n8\\n구글, 이미지 편집 모델 ‘제미나이 2.5 플래시 이미지’ 공개n‘나노 바나나’라는 가칭으로 LMArena에서 1위를 차지했던 ‘제미나이 2.5 플래시 이미지’가 제미나이 앱의 이미지 편집 기능에 정식으로 채택 n제미나이 2.5 플래시 이미지는 사람이나 동물과 같은 캐릭터의 일관성에서 뛰어난 품질을 발휘하여, 배경이나 설정을 바꾸거나 여러 장의 사진을 합성해도 동일한 형상을 유지\\nKEY Contents\\n£ ‘나노 바나나’로 사전 공개된 ‘제미나이 2.5 플래시 이미지’, 제미나이 앱에 추가n구글이 제미나이 앱에 새로운 이미지 생성·편집 모델 ‘제미나이 2.5 플래시 이미지(Gemini 2.5 Flash Image)’ 기반의 이미지 편집 기능을 추가 ∙이 모델은 LMArena*에서 ‘나노 바나나(Nano Banana)’라는 가칭으로 공개되어 뛰어난 성능으로 1위를 차지하며 화제를 모았으며, 이번 업데이트로 제미나이 앱에 통합* 인간 선호도를 기반으로 AI 모델을 비교 평가하는 웹 기반 플랫폼∙이번 업데이트는 전 세계 유·무료 사용자에게 모두 제공되며, 제미나이 앱에서 생성되거나 편집된 모든 이미지에는 육안으로 식별되지 않는 워터마크가 포함되어 AI 생성 사실을 확인 가능n구글은 2025년 4월 제미나이 앱에 AI 이미지 편집 기능을 도입한 이래, 캐릭터 일관성에 중점을 두고 기술을 개선해 왔으며, 제미나이 2.5 플래시 이미지는 캐릭터 일관성 유지에서 뛰어난 품질을 달성∙사람이나 동물 사진을 업로드하면 배경이나 설정을 바꾸더라도 모든 이미지에서 동일한 모습을 유지하며, 여러 장의 사진을 업로드하고 합성하여 새로운 장면을 만들어낼 수 있도록 지원∙실내 이미지에서 벽의 색을 바꾸거나 책장, 테이블 등의 가구를 차례로 추가하는 등 여러 차례에 걸쳐 특정 부분을 수정하면서 나머지 부분을 유지하는 것도 가능하며, 한 이미지의 스타일을 다른 이미지에 적용하는 기능도 지원(예시: 나비 날개 패턴을 드레스 디자인에 합성)'),\n",
       "  Document(id='45538ebc-e16f-4dc7-9118-b9cdc5c7b54f', metadata={'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-10-02T12:57:42+09:00', 'author': 'dj', 'moddate': '2025-10-02T12:57:42+09:00', 'pdfversion': '1.4', 'source': './temp/doc.pdf', 'total_pages': 29, 'page': 23, 'page_label': '24'}, page_content='| 2025년 10월호 |\\n인력･교육'),\n",
       "  Document(id='f4392638-d762-4a4f-b199-7d85b2ecd7b9', metadata={'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-10-02T12:57:42+09:00', 'author': 'dj', 'moddate': '2025-10-02T12:57:42+09:00', 'pdfversion': '1.4', 'source': './temp/doc.pdf', 'total_pages': 29, 'page': 20, 'page_label': '21'}, page_content='<수학 벤치마크 평균 점수 기준 K2-Think와 경쟁 모델 비교>\\n출처 | MBZUAI, MBZUAI and G42 Launch K2 Think: A Leading Open-Source System for Advanced AI Reasoning, 2025.09.09.'),\n",
       "  Document(id='533586b4-c53c-4520-ba49-df1ce11b5e51', metadata={'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-10-02T12:57:42+09:00', 'author': 'dj', 'moddate': '2025-10-02T12:57:42+09:00', 'pdfversion': '1.4', 'source': './temp/doc.pdf', 'total_pages': 29, 'page': 28, 'page_label': '29'}, page_content='홈페이지 : https://spri.kr보고서와 관련된 문의는 AI정책연구실(hs.lee@spri.kr, 031-739-7333)로 연락주시기 바랍니다.경기도 성남시 분당구 대왕판교로 712번길 22 글로벌 R&D 연구동(B) 4층22, Daewangpangyo-ro 712beon-gil, Bundang-gu, Seongnam-si, Gyeonggi-do, Republic of Korea, 13488'),\n",
       "  Document(id='5af770c5-c1d2-44bd-ab75-321e283d4c62', metadata={'producer': 'Hancom PDF 1.3.0.505', 'creator': 'Hancom PDF 1.3.0.505', 'creationdate': '2025-10-02T12:57:42+09:00', 'author': 'dj', 'moddate': '2025-10-02T12:57:42+09:00', 'pdfversion': '1.4', 'source': './temp/doc.pdf', 'total_pages': 29, 'page': 15, 'page_label': '16'}, page_content='카트를 제시하면 사용자가 “카트 위임장”으로 승인해 품목과 가격을 확정∙(에이전트 단독 구매) 콘서트 티켓 판매 시작 후 구매와 같이 구매를 위임한 경우, 사용자가 사전에 상세한 구매 조건을 명시한 “의향 위임장”에 서명하여 정확한 조건 충족 시 자동으로 ‘카트 위임장’을 생성∙사용자 실시간 참여 구매 및 에이전트 단독 구매 시나리오 모두 사용자 의향부터 카트, 결제까지 전 과정에서 사용자 승인과 진위 여부를 확정하는 확실한 증거를 생성해 책임 소재를 명확히 제시n구글에 따르면 AP2는 유연한 설계 방식으로 단순한 구매 방식부터 지금까지 없던 완전히 새로운 구매 방식까지 모두 지원할 수 있는 기반을 제공∙일례로 사용자가 원하는 특정 색상의 상품이 없을 때 에이전트에게 최대 20%까지 더 지불할 의향이 있다고 밝히면 가격과 재고 현황을 모니터링해 해당 상품이 발견되면 자동으로 구매를 실행하도록 설정 가능n구글은 코인베이스(Coinbase), 이더리움 재단(Ethereum Foundation) 등과 협력해 암호화폐 기반의 에이전트 결제를 위한 개방형 확장 규격(A2A x402)도 출시해 AP2에서 스테이블코인과 암호화폐 등 다양한 결제 수단을 지원출처 | Google Cloud, Powering AI commerce with the new Agent Payments Protocol (AP2), 2025.09.17.')]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = retrieve_node({\"question\": \"구글이 공개한 이미지 편집 모델 이름은?\"})\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284d666f",
   "metadata": {},
   "source": [
    "## 답변 생성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7249d79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "\n",
    "\n",
    "def generate_node(state: State):\n",
    "    question = state.get(\"question\")\n",
    "    documents = state.get(\"documents\")\n",
    "    system_prompt = \"\"\"당신은 질문-답변(Question-Answering)을 수행하는 친절한 AI 어시스턴트입니다. 당신의 임무는 주어진 문맥(context) 에서 주어진 질문(question) 에 답하는 것입니다.\n",
    "검색된 다음 문맥(context) 을 사용하여 질문(question) 에 답하세요. 만약, 주어진 문맥(context) 에서 답을 찾을 수 없다면, 답을 모른다면 `주어진 정보에서 질문에 대한 정보를 찾을 수 없습니다` 라고 답하세요.\n",
    "한글로 답변해 주세요. 단, 기술적인 용어나 이름은 번역하지 않고 그대로 사용해 주세요.\"\"\"\n",
    "    prompt_template = \"#Question: {question}\\n\\n #Context: {context}\\n\\n #Answer:\"\n",
    "\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\n",
    "                \"user\",\n",
    "                prompt_template.format(\n",
    "                    question=question,\n",
    "                    context=documents,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return {\"generation\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa92efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'generation': '구글이 공개한 이미지 편집 모델 이름은 ‘제미나이 2.5 플래시 이미지(Gemini 2.5 Flash Image)’입니다.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_node({**results, \"question\": \"구글이 공개한 이미지 편집 모델 이름은?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9e3ffd",
   "metadata": {},
   "source": [
    "## 관련성 평가 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "da55fdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"검색된 문서의 관련성을 판단하기 위한 binary score\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        ...,\n",
    "        description=\"검색된 문서가 사용자 질문과 얼마나 관련성이 있는지 평가하는 평가자입니다.\\n\"\n",
    "        \"문서에 질문과 관련된 키워드나 의미론적 내용이 포함되어 있다면, 관련성이 있다고 평가하세요.\\n\"\n",
    "        \"문서가 질문과 관련이 있는지 여부를 나타내기 위해 'yes' 또는 'no'로 응답하세요\",\n",
    "    )\n",
    "\n",
    "\n",
    "def grade_documents_node(state: State):\n",
    "    question = state.get(\"question\")\n",
    "    documents = state.get(\"documents\")\n",
    "\n",
    "    prompt_template = (\n",
    "        \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"\n",
    "    )\n",
    "    llm_with_structured_output = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "    filtered_docs: list[Document] = []\n",
    "    for doc in documents:\n",
    "        response = llm_with_structured_output.invoke(\n",
    "            prompt_template.format(\n",
    "                question=question,\n",
    "                document=doc,\n",
    "            )\n",
    "        )\n",
    "        if response.binary_score == \"yes\":\n",
    "            filtered_docs.append(doc)\n",
    "\n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"web_search\": \"yes\" if len(filtered_docs) == 0 else \"no\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cd32b7",
   "metadata": {},
   "source": [
    "## 쿼리 재작성 노드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9692f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rewrite_node(state: State):\n",
    "    question = state.get(\"question\")\n",
    "    system_prompt = \"당신은 질문 재작성 도구입니다.\\n\"\n",
    "    \"제공된 질문을 웹 검색에 최적화된 더 좋은 질문으로 개선합니다.\\n\"\n",
    "    \"기존 질문을 살펴보고 의미적 의도/의미를 추론해 보세요.\"\n",
    "    prompt_template = \"기존 질문: {question}\"\n",
    "    response = llm.invoke(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\n",
    "                \"user\",\n",
    "                prompt_template.format(question=question),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return {\"question\": response.content}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd10db2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-academy (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
