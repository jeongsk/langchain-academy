{"config": {"lang": ["en"], "separator": "[\\s\\u200b\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "", "title": "LangGraph", "text": "<p>Klarna, Replit, Elastic \ub4f1 \uc5d0\uc774\uc804\ud2b8\uc758 \ubbf8\ub798\ub97c \ub9cc\ub4e4\uc5b4\uac00\ub294 \uae30\uc5c5\ub4e4\uc758 \uc2e0\ub8b0\ub97c \ubc1b\ub294 LangGraph\ub294 \uc624\ub798 \uc2e4\ud589\ub418\ub294 \uc0c1\ud0dc \uc800\uc7a5 \uc5d0\uc774\uc804\ud2b8\ub97c \uad6c\ucd95, \uad00\ub9ac, \ubc30\ud3ec\ud558\uae30 \uc704\ud55c \ub85c\uc6b0-\ub808\ubca8 \uc624\ucf00\uc2a4\ud2b8\ub808\uc774\uc158 \ud504\ub808\uc784\uc6cc\ud06c\uc785\ub2c8\ub2e4.</p>"}, {"location": "#_1", "title": "\uc2dc\uc791\ud558\uae30", "text": "<p>LangGraph\ub97c \uc124\uce58\ud569\ub2c8\ub2e4:</p> <pre><code>pip install -U langgraph\n</code></pre> <p>\uadf8\ub7f0 \ub2e4\uc74c, \ubbf8\ub9ac \ube4c\ub4dc\ub41c \ucef4\ud3ec\ub10c\ud2b8\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc5d0\uc774\uc804\ud2b8\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4:</p> <pre><code># \ubaa8\ub378\uc744 \ud638\ucd9c\ud558\ub824\uba74 `pip install -qU \"langchain[anthropic]\"` \uba85\ub839\uc5b4\ub97c \uc2e4\ud589\ud558\uc138\uc694.\n\nfrom langgraph.prebuilt import create_react_agent\n\ndef get_weather(city: str) -&gt; str:\n    \"\"\"\uc8fc\uc5b4\uc9c4 \ub3c4\uc2dc\uc5d0 \ub300\ud55c \ub0a0\uc528 \uc815\ubcf4\ub97c \uac00\uc838\uc635\ub2c8\ub2e4.\"\"\"\n    return f\"{city}\ub294 \ud56d\uc0c1 \ub9d1\uc74c\uc785\ub2c8\ub2e4!\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n    prompt=\"\ub2f9\uc2e0\uc740 \ub3c4\uc6c0\uc774 \ub418\ub294 \uc5b4\uc2dc\uc2a4\ud134\ud2b8\uc785\ub2c8\ub2e4\"\n)\n\n# \uc5d0\uc774\uc804\ud2b8 \uc2e4\ud589\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"sf\uc758 \ub0a0\uc528\ub294 \uc5b4\ub5a4\uac00\uc694\"}]}\n)\n</code></pre> <p>\ub354 \uc790\uc138\ud55c \uc815\ubcf4\ub294 \ube60\ub978 \uc2dc\uc791\uc744 \ucc38\uc870\ud558\uc138\uc694. \ub610\ub294, \uc0ac\uc6a9\uc790 \uc815\uc758 \uac00\ub2a5\ud55c \uc544\ud0a4\ud14d\ucc98, \uc7a5\uae30 \uba54\ubaa8\ub9ac \ubc0f \uae30\ud0c0 \ubcf5\uc7a1\ud55c \uc791\uc5c5 \ucc98\ub9ac\ub97c \uac16\ucd98 \uc5d0\uc774\uc804\ud2b8 \uc6cc\ud06c\ud50c\ub85c\uc6b0\ub97c \uad6c\ucd95\ud558\ub294 \ubc29\ubc95\uc744 \ubc30\uc6b0\ub824\uba74 LangGraph \uae30\ubcf8 \ud29c\ud1a0\ub9ac\uc5bc\uc744 \ucc38\uc870\ud558\uc138\uc694.</p>"}, {"location": "#_2", "title": "\ud575\uc2ec \uc774\uc810", "text": "<p>LangGraph\ub294 \uc624\ub798 \uc2e4\ud589\ub418\ub294 \ubaa8\ub4e0 \uc0c1\ud0dc \uc800\uc7a5 \uc6cc\ud06c\ud50c\ub85c\uc6b0\ub098 \uc5d0\uc774\uc804\ud2b8\ub97c \uc704\ud55c \ub85c\uc6b0-\ub808\ubca8 \uc9c0\uc6d0 \uc778\ud504\ub77c\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. LangGraph\ub294 \ud504\ub86c\ud504\ud2b8\ub098 \uc544\ud0a4\ud14d\ucc98\ub97c \ucd94\uc0c1\ud654\ud558\uc9c0 \uc54a\uc73c\uba70, \ub2e4\uc74c\uacfc \uac19\uc740 \ud575\uc2ec\uc801\uc778 \uc774\uc810\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4:</p> <ul> <li>\ub0b4\uad6c\uc131 \uc788\ub294 \uc2e4\ud589: \uc2e4\ud328\uc5d0\ub3c4 \uc9c0\uc18d\ub418\uace0 \uc7a5\uae30\uac04 \uc2e4\ud589\ub420 \uc218 \uc788\uc73c\uba70, \uc911\ub2e8\ub41c \uc9c0\uc810\ubd80\ud130 \uc815\ud655\ud788 \uc790\ub3d9 \uc7ac\uac1c\ub418\ub294 \uc5d0\uc774\uc804\ud2b8\ub97c \uad6c\ucd95\ud569\ub2c8\ub2e4.</li> <li>\ud734\uba3c-\uc778-\ub354-\ub8e8\ud504: \uc2e4\ud589 \uc911 \uc5b4\ub290 \uc2dc\uc810\uc5d0\uc11c\ub4e0 \uc5d0\uc774\uc804\ud2b8 \uc0c1\ud0dc\ub97c \uac80\uc0ac\ud558\uace0 \uc218\uc815\ud558\uc5ec \uc6d0\ud65c\ud558\uac8c \uc778\uac04\uc758 \uac10\ub3c5\uc744 \ud1b5\ud569\ud569\ub2c8\ub2e4.</li> <li>\ud3ec\uad04\uc801\uc778 \uba54\ubaa8\ub9ac: \uc9c4\ud589 \uc911\uc778 \ucd94\ub860\uc744 \uc704\ud55c \ub2e8\uae30 \uc791\uc5c5 \uba54\ubaa8\ub9ac\uc640 \uc138\uc158 \uac04 \uc7a5\uae30 \uc601\uad6c \uba54\ubaa8\ub9ac\ub97c \ubaa8\ub450 \uac16\ucd98 \uc9c4\uc815\ud55c \uc0c1\ud0dc \uc800\uc7a5 \uc5d0\uc774\uc804\ud2b8\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4.</li> <li>LangSmith\ub97c \uc774\uc6a9\ud55c \ub514\ubc84\uae45: \uc2e4\ud589 \uacbd\ub85c\ub97c \ucd94\uc801\ud558\uace0, \uc0c1\ud0dc \uc804\ud658\uc744 \ucea1\ucc98\ud558\uba70, \uc0c1\uc138\ud55c \ub7f0\ud0c0\uc784 \uba54\ud2b8\ub9ad\uc744 \uc81c\uacf5\ud558\ub294 \uc2dc\uac01\ud654 \ub3c4\uad6c\ub97c \ud1b5\ud574 \ubcf5\uc7a1\ud55c \uc5d0\uc774\uc804\ud2b8 \ud589\ub3d9\uc5d0 \ub300\ud55c \uae4a\uc740 \uac00\uc2dc\uc131\uc744 \ud655\ubcf4\ud569\ub2c8\ub2e4.</li> <li>\ud504\ub85c\ub355\uc158-\ub808\ub514 \ubc30\ud3ec: \uc0c1\ud0dc\ub97c \uc800\uc7a5\ud558\uace0 \uc624\ub798 \uc2e4\ud589\ub418\ub294 \uc6cc\ud06c\ud50c\ub85c\uc6b0\uc758 \uace0\uc720\ud55c \uacfc\uc81c\ub97c \ucc98\ub9ac\ud558\ub3c4\ub85d \uc124\uacc4\ub41c \ud655\uc7a5 \uac00\ub2a5\ud55c \uc778\ud504\ub77c\ub97c \ud1b5\ud574 \uc815\uad50\ud55c \uc5d0\uc774\uc804\ud2b8 \uc2dc\uc2a4\ud15c\uc744 \uc790\uc2e0 \uc788\uac8c \ubc30\ud3ec\ud569\ub2c8\ub2e4.</li> </ul>"}, {"location": "#langgraph", "title": "LangGraph \uc0dd\ud0dc\uacc4", "text": "<p>LangGraph\ub294 \ub2e8\ub3c5\uc73c\ub85c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \ubaa8\ub4e0 LangChain \uc81c\ud488\uacfc \uc6d0\ud65c\ud558\uac8c \ud1b5\ud569\ub418\uc5b4 \uac1c\ubc1c\uc790\uc5d0\uac8c \uc5d0\uc774\uc804\ud2b8 \uad6c\ucd95\uc744 \uc704\ud55c \uc644\uc804\ud55c \ub3c4\uad6c \ubaa8\uc74c\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4. LLM \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uac1c\ubc1c\uc744 \uac1c\uc120\ud558\ub824\uba74 LangGraph\ub97c \ub2e4\uc74c\uacfc \ud568\uaed8 \uc0ac\uc6a9\ud558\uc138\uc694:</p> <ul> <li>LangSmith \u2014 \uc5d0\uc774\uc804\ud2b8 \ud3c9\uac00 \ubc0f \uad00\ucc30 \uae30\ub2a5\uc5d0 \uc720\uc6a9\ud569\ub2c8\ub2e4. \uc131\ub2a5\uc774 \uc800\ud558\ub41c LLM \uc571 \uc2e4\ud589\uc744 \ub514\ubc84\uae45\ud558\uace0, \uc5d0\uc774\uc804\ud2b8 \uada4\uc801\uc744 \ud3c9\uac00\ud558\uba70, \ud504\ub85c\ub355\uc158 \ud658\uacbd\uc5d0\uc11c \uac00\uc2dc\uc131\uc744 \ud655\ubcf4\ud558\uace0, \uc2dc\uac04\uc774 \uc9c0\ub0a8\uc5d0 \ub530\ub77c \uc131\ub2a5\uc744 \uac1c\uc120\ud569\ub2c8\ub2e4.</li> <li>LangGraph \ud50c\ub7ab\ud3fc \u2014 \uc624\ub798 \uc2e4\ud589\ub418\ub294 \uc0c1\ud0dc \uc800\uc7a5 \uc6cc\ud06c\ud50c\ub85c\uc6b0\ub97c \uc704\ud574 \ud2b9\ubcc4\ud788 \uc81c\uc791\ub41c \ubc30\ud3ec \ud50c\ub7ab\ud3fc\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc5d0\uc774\uc804\ud2b8\ub97c \uc190\uc27d\uac8c \ubc30\ud3ec\ud558\uace0 \ud655\uc7a5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ud300 \uac04\uc5d0 \uc5d0\uc774\uc804\ud2b8\ub97c \ubc1c\uacac, \uc7ac\uc0ac\uc6a9, \uad6c\uc131 \ubc0f \uacf5\uc720\ud558\uace0, LangGraph Studio\uc758 \uc2dc\uac01\uc801 \ud504\ub85c\ud1a0\ud0c0\uc774\ud551\uc744 \ud1b5\ud574 \ube60\ub974\uac8c \ubc18\ubcf5 \uc791\uc5c5\uc744 \uc218\ud589\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>LangChain \u2013 LLM \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \uac1c\ubc1c\uc744 \uac04\uc18c\ud654\ud558\uae30 \uc704\ud55c \ud1b5\ud569 \ubc0f \uad6c\uc131 \uac00\ub2a5\ud55c \ucef4\ud3ec\ub10c\ud2b8\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4.</li> </ul> <p>Note</p> <p>LangGraph\uc758 JS \ubc84\uc804\uc744 \ucc3e\uace0 \uacc4\uc2e0\uac00\uc694? JS \ub9ac\ud3ec\uc9c0\ud1a0\ub9ac\uc640 JS \ubb38\uc11c\ub97c \ucc38\uc870\ud558\uc138\uc694.</p>"}, {"location": "#_3", "title": "\ucd94\uac00 \uc790\ub8cc", "text": "<ul> <li>\uac00\uc774\ub4dc: \uc2a4\ud2b8\ub9ac\ubc0d, \uba54\ubaa8\ub9ac \ubc0f \uc601\uc18d\uc131 \ucd94\uac00, \ub514\uc790\uc778 \ud328\ud134(\uc608: \ubd84\uae30, \uc11c\ube0c\uadf8\ub798\ud504 \ub4f1)\uacfc \uac19\uc740 \uc8fc\uc81c\uc5d0 \ub300\ud55c \ube60\ub974\uace0 \uc2e4\ud589 \uac00\ub2a5\ud55c \ucf54\ub4dc \uc2a4\ub2c8\ud3ab\uc785\ub2c8\ub2e4.</li> <li>\ub808\ud37c\ub7f0\uc2a4: \ud575\uc2ec \ud074\ub798\uc2a4, \uba54\uc11c\ub4dc, \uadf8\ub798\ud504 \ubc0f \uccb4\ud06c\ud3ec\uc778\ud305 API \uc0ac\uc6a9 \ubc29\ubc95, \uadf8\ub9ac\uace0 \ub354 \ub192\uc740 \uc218\uc900\uc758 \uc0ac\uc804 \ube4c\ub4dc\ub41c \ucef4\ud3ec\ub10c\ud2b8\uc5d0 \ub300\ud55c \uc0c1\uc138\ud55c \ub808\ud37c\ub7f0\uc2a4\uc785\ub2c8\ub2e4.</li> <li>\uc608\uc81c: LangGraph \uc2dc\uc791\uc5d0 \ub300\ud55c \uc548\ub0b4 \uc608\uc81c\uc785\ub2c8\ub2e4.</li> <li>LangChain \ud3ec\ub7fc: \ucee4\ubba4\ub2c8\ud2f0\uc640 \uc5f0\uacb0\ud558\uc5ec \ubaa8\ub4e0 \uae30\uc220\uc801\uc778 \uc9c8\ubb38, \uc544\uc774\ub514\uc5b4, \ud53c\ub4dc\ubc31\uc744 \uacf5\uc720\ud558\uc138\uc694.</li> <li>LangChain \uc544\uce74\ub370\ubbf8: \ubb34\ub8cc \uad6c\uc870\ud654\ub41c \uacfc\uc815\uc5d0\uc11c LangGraph\uc758 \uae30\ubcf8\uc744 \ubc30\uc6b0\uc138\uc694.</li> <li>\ud15c\ud50c\ub9bf: \ubcf5\uc81c\ud558\uace0 \uc801\uc6a9\ud560 \uc218 \uc788\ub294 \uc77c\ubc18\uc801\uc778 \uc5d0\uc774\uc804\ud2b8 \uc6cc\ud06c\ud50c\ub85c\uc6b0(\uc608: ReAct \uc5d0\uc774\uc804\ud2b8, \uba54\ubaa8\ub9ac, \uac80\uc0c9 \ub4f1)\ub97c \uc704\ud55c \uc0ac\uc804 \ube4c\ub4dc\ub41c \ucc38\uc870 \uc571\uc785\ub2c8\ub2e4.</li> <li>\uc0ac\ub840 \uc5f0\uad6c: \uc5c5\uacc4 \ub9ac\ub354\ub4e4\uc774 LangGraph\ub97c \uc0ac\uc6a9\ud558\uc5ec AI \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc744 \ub300\uaddc\ubaa8\ub85c \ucd9c\uc2dc\ud558\ub294 \ubc29\ubc95\uc744 \ub4e4\uc5b4\ubcf4\uc138\uc694.</li> </ul>"}, {"location": "#_4", "title": "\uac10\uc0ac\uc758 \ub9d0", "text": "<p>LangGraph\ub294 Pregel\uacfc Apache Beam\uc5d0\uc11c \uc601\uac10\uc744 \ubc1b\uc558\uc2b5\ub2c8\ub2e4. \uacf5\uac1c \uc778\ud130\ud398\uc774\uc2a4\ub294 NetworkX\uc5d0\uc11c \uc601\uac10\uc744 \uc5bb\uc5c8\uc2b5\ub2c8\ub2e4. LangGraph\ub294 LangChain\uc758 \uc81c\uc791\uc0ac\uc778 LangChain Inc.\uc5d0 \uc758\ud574 \ub9cc\ub4e4\uc5b4\uc84c\uc9c0\ub9cc, LangChain \uc5c6\uc774\ub3c4 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"}, {"location": "adopters/", "title": "\ud83e\udd9c\ud83d\udd78\ufe0f Case studies", "text": "<p>This list of companies using LangGraph and their success stories is compiled from public sources. If your company uses LangGraph, we'd love for you to share your story and add it to the list. You\u2019re also welcome to contribute updates based on publicly available information from other companies, such as blog posts or press releases.</p> Company Industry Use case Reference AirTop Software &amp; Technology (GenAI Native) Browser automation for AI agents Case study, 2024 AppFolio Real Estate Copilot for domain-specific task Case study, 2024 Athena Intelligence Software &amp; Technology (GenAI Native) Research &amp; summarization Case study, 2024 BlackRock Financial Services Copilot for domain-specific task Interrupt talk, 2025 Captide Software &amp; Technology (GenAI Native) Data extraction Case study, 2025 Cisco CX Software &amp; Technology Customer support Interrupt Talk, 2025 Cisco Outshift Software &amp; Technology DevOps Video story, 2025; Case study, 2025; Blog post, 2025 Cisco TAC Software &amp; Technology Customer support Video story, 2025 City of Hope Non-profit Copilot for domain-specific task Video story, 2025 C.H. Robinson Logistics Automation Case study, 2025 Definely Legal Copilot for domain-specific task Case study, 2025 Docent Pro Travel GenAI embedded product experiences Case study, 2025 Elastic Software &amp; Technology Copilot for domain-specific task Blog post, 2025 Exa Software &amp; Technology (GenAI Native) Search Case study, 2025 GitLab Software &amp; Technology Code generation Duo workflow docs Harmonic Software &amp; Technology Search Case study, 2025 Inconvo Software &amp; Technology Code generation Case study, 2025 Infor Software &amp; Technology GenAI embedded product experiences; customer support; copilot Case study, 2025 J.P. Morgan Financial Services Copilot for domain-specific task Interrupt talk, 2025 Klarna Fintech Copilot for domain-specific task Case study, 2025 Komodo Health Healthcare Copilot for domain-specific task Blog post LinkedIn Social Media Code generation; Search &amp; discovery Interrupt talk, 2025; Blog post, 2025; Blog post, 2024 Minimal E-commerce Customer support Case study, 2025 Modern Treasury Fintech GenAI embedded product experiences Video story, 2025 Monday Software &amp; Technology GenAI embedded product experiences Interrupt talk, 2025 Morningstar Financial Services Research &amp; summarization Video story, 2025 OpenRecovery Healthcare Copilot for domain-specific task Case study, 2024 Pigment Fintech GenAI embedded product experiences Video story, 2025 Prosper Fintech Customer support Video story, 2025 Qodo Software &amp; Technology (GenAI Native) Code generation Blog post, 2025 Rakuten E-commerce / Fintech Copilot for domain-specific task Video story, 2025; Blog post, 2025 Replit Software &amp; Technology Code generation Blog post, 2024; Breakout agent story, 2024; Fireside chat video, 2024 Rexera Real Estate (GenAI Native) Copilot for domain-specific task Case study, 2024 Abu Dhabi Government Government Search Case study, 2025 Tradestack Software &amp; Technology (GenAI Native) Copilot for domain-specific task Case study, 2024 Uber Transportation Developer productivity; Code generation Interrupt talk, 2025; Presentation, 2024; Video, 2024 Unify Software &amp; Technology (GenAI Native) Copilot for domain-specific task Interrupt talk, 2025; Blog post, 2024 Vizient Healthcare Copilot for domain-specific task Video story, 2025; Case study, 2025 Vodafone Telecommunications Code generation; internal search Case study, 2025 WebToon Media &amp; Entertainment Data extraction Case study, 2025 11x Software &amp; Technology (GenAI Native) Research &amp; outreach Interrupt talk, 2025"}, {"location": "llms-txt-overview/", "title": "llms.txt", "text": "<p>Below you can find a list of documentation files in the <code>llms.txt</code> format, specifically <code>llms.txt</code> and <code>llms-full.txt</code>. These files allow large language models (LLMs) and agents to access programming documentation and APIs, particularly useful within integrated development environments (IDEs).</p> Language Version llms.txt llms-full.txt LangGraph Python https://langchain-ai.github.io/langgraph/llms.txt https://langchain-ai.github.io/langgraph/llms-full.txt LangGraph JS https://langchain-ai.github.io/langgraphjs/llms.txt https://langchain-ai.github.io/langgraphjs/llms-full.txt LangChain Python https://python.langchain.com/llms.txt N/A LangChain JS https://js.langchain.com/llms.txt N/A <p>Review the output</p> <p>Even with access to up-to-date documentation, current state-of-the-art models may not always generate correct code. Treat the generated code as a starting point, and always review it before shipping code to production.</p>"}, {"location": "llms-txt-overview/#differences-between-llmstxt-and-llms-fulltxt", "title": "Differences Between <code>llms.txt</code> and <code>llms-full.txt</code>", "text": "<ul> <li> <p><code>llms.txt</code> is an index file containing links with brief descriptions of the content. An LLM or agent must follow these links to access detailed information.</p> </li> <li> <p><code>llms-full.txt</code> includes all the detailed content directly in a single file, eliminating the need for additional navigation.</p> </li> </ul> <p>A key consideration when using <code>llms-full.txt</code> is its size. For extensive documentation, this file may become too large to fit into an LLM's context window.</p>"}, {"location": "llms-txt-overview/#using-llmstxt-via-an-mcp-server", "title": "Using <code>llms.txt</code> via an MCP Server", "text": "<p>As of March 9, 2025, IDEs do not yet have robust native support for <code>llms.txt</code>. However, you can still use <code>llms.txt</code> effectively through an MCP server.</p>"}, {"location": "llms-txt-overview/#use-the-mcpdoc-server", "title": "\ud83d\ude80 Use the <code>mcpdoc</code> Server", "text": "<p>We provide an MCP server that was designed to serve documentation for LLMs and IDEs:</p> <p>\ud83d\udc49 langchain-ai/mcpdoc GitHub Repository</p> <p>This MCP server allows integrating <code>llms.txt</code> into tools like Cursor, Windsurf, Claude, and Claude Code.</p> <p>\ud83d\udcd8 Setup instructions and usage examples are available in the repository.</p>"}, {"location": "llms-txt-overview/#using-llms-fulltxt", "title": "Using <code>llms-full.txt</code>", "text": "<p>The LangGraph <code>llms-full.txt</code> file typically contains several hundred thousand tokens, exceeding the context window limitations of most LLMs. To effectively use this file:</p> <ol> <li> <p>With IDEs (e.g., Cursor, Windsurf):</p> <ul> <li>Add the <code>llms-full.txt</code> as custom documentation. The IDE will automatically chunk and index the content, implementing Retrieval-Augmented Generation (RAG).</li> </ul> </li> <li> <p>Without IDE support:</p> <ul> <li>Use a chat model with a large context window.</li> <li>Implement a RAG strategy to manage and query the documentation efficiently.</li> </ul> </li> </ol>"}, {"location": "additional-resources/", "title": "Additional resources", "text": "<p>This section contains additional resources for LangGraph.</p> <ul> <li>Community agents: A collection of prebuilt libraries that you can use in your LangGraph applications.</li> <li>LangGraph Academy: A collection of courses that teach you how to use LangGraph.</li> <li>Case studies: A collection of case studies that show how LangGraph is used in production.</li> <li>FAQ: A collection of frequently asked questions about LangGraph.</li> <li>llms.txt: A list of documentation files in the <code>llms.txt</code> format that allow LLMs and agents to access our documentation.</li> <li>LangChain Forum: A place to ask questions and get help from other LangGraph users.</li> <li>Troubleshooting: A collection of troubleshooting guides for common issues.</li> </ul>"}, {"location": "agents/agents/", "title": "LangGraph quickstart", "text": "<p>This guide shows you how to set up and use LangGraph's prebuilt, reusable components, which are designed to help you construct agentic systems quickly and reliably.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/agents/#prerequisites", "title": "Prerequisites", "text": "<p>Before you start this tutorial, ensure you have the following:</p> <ul> <li>An Anthropic API key</li> </ul>", "tags": ["agent"], "boost": 2}, {"location": "agents/agents/#1-install-dependencies", "title": "1. Install dependencies", "text": "<p>If you haven't already, install LangGraph and LangChain:</p> <p>:::python</p> <pre><code>pip install -U langgraph \"langchain[anthropic]\"\n</code></pre> <p>Info</p> <p><code>langchain[anthropic]</code> is installed so the agent can call the model.</p> <p>:::</p> <p>:::js</p> <pre><code>npm install @langchain/langgraph @langchain/core @langchain/anthropic\n</code></pre> <p>Info</p> <p><code>@langchain/core</code> <code>@langchain/anthropic</code> are installed so the agent can call the model.</p> <p>:::</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/agents/#2-create-an-agent", "title": "2. Create an agent", "text": "<p>:::python To create an agent, use @[<code>create_react_agent</code>][create_react_agent]:</p> <pre><code>from langgraph.prebuilt import create_react_agent\n\ndef get_weather(city: str) -&gt; str:  # (1)!\n    \"\"\"Get weather for a given city.\"\"\"\n    return f\"It's always sunny in {city}!\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",  # (2)!\n    tools=[get_weather],  # (3)!\n    prompt=\"You are a helpful assistant\"  # (4)!\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n</code></pre> <ol> <li>Define a tool for the agent to use. Tools can be defined as vanilla Python functions. For more advanced tool usage and customization, check the tools page.</li> <li>Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page.</li> <li>Provide a list of tools for the model to use.</li> <li>Provide a system prompt (instructions) to the language model used by the agent.    :::</li> </ol> <p>:::js To create an agent, use <code>createReactAgent</code>:</p> <pre><code>import { ChatAnthropic } from \"@langchain/anthropic\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst getWeather = tool(\n  // (1)!\n  async ({ city }) =&gt; {\n    return `It's always sunny in ${city}!`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get weather for a given city.\",\n    schema: z.object({\n      city: z.string().describe(\"The city to get weather for\"),\n    }),\n  }\n);\n\nconst agent = createReactAgent({\n  llm: new ChatAnthropic({ model: \"anthropic:claude-3-5-sonnet-latest\" }), // (2)!\n  tools: [getWeather], // (3)!\n  stateModifier: \"You are a helpful assistant\", // (4)!\n});\n\n// Run the agent\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"what is the weather in sf\" }],\n});\n</code></pre> <ol> <li>Define a tool for the agent to use. Tools can be defined using the <code>tool</code> function. For more advanced tool usage and customization, check the tools page.</li> <li>Provide a language model for the agent to use. To learn more about configuring language models for the agents, check the models page.</li> <li>Provide a list of tools for the model to use.</li> <li>Provide a system prompt (instructions) to the language model used by the agent.    :::</li> </ol>", "tags": ["agent"], "boost": 2}, {"location": "agents/agents/#3-configure-an-llm", "title": "3. Configure an LLM", "text": "<p>:::python To configure an LLM with specific parameters, such as temperature, use init_chat_model:</p> <pre><code>from langchain.chat_models import init_chat_model\nfrom langgraph.prebuilt import create_react_agent\n\n# highlight-next-line\nmodel = init_chat_model(\n    \"anthropic:claude-3-7-sonnet-latest\",\n    # highlight-next-line\n    temperature=0\n)\n\nagent = create_react_agent(\n    # highlight-next-line\n    model=model,\n    tools=[get_weather],\n)\n</code></pre> <p>:::</p> <p>:::js To configure an LLM with specific parameters, such as temperature, use a model instance:</p> <pre><code>import { ChatAnthropic } from \"@langchain/anthropic\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\n// highlight-next-line\nconst model = new ChatAnthropic({\n  model: \"claude-3-5-sonnet-latest\",\n  // highlight-next-line\n  temperature: 0,\n});\n\nconst agent = createReactAgent({\n  // highlight-next-line\n  llm: model,\n  tools: [getWeather],\n});\n</code></pre> <p>:::</p> <p>For more information on how to configure LLMs, see Models.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/agents/#4-add-a-custom-prompt", "title": "4. Add a custom prompt", "text": "<p>Prompts instruct the LLM how to behave. Add one of the following types of prompts:</p> <ul> <li>Static: A string is interpreted as a system message.</li> <li>Dynamic: A list of messages generated at runtime, based on input or configuration.</li> </ul> Static promptDynamic prompt <p>Define a fixed prompt string or list of messages:</p> <p>:::python <pre><code>from langgraph.prebuilt import create_react_agent\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n    # A static prompt that never changes\n    # highlight-next-line\n    prompt=\"Never answer questions about the weather.\"\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n</code></pre> :::</p> <p>:::js <pre><code>import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst agent = createReactAgent({\n  llm: new ChatAnthropic({ model: \"anthropic:claude-3-5-sonnet-latest\" }),\n  tools: [getWeather],\n  // A static prompt that never changes\n  // highlight-next-line\n  stateModifier: \"Never answer questions about the weather.\"\n});\n\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"what is the weather in sf\" }]\n});\n</code></pre> :::</p> <p>:::python Define a function that returns a message list based on the agent's state and configuration:</p> <pre><code>from langchain_core.messages import AnyMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.prebuilt import create_react_agent\n\n# highlight-next-line\ndef prompt(state: AgentState, config: RunnableConfig) -&gt; list[AnyMessage]:  # (1)!\n    user_name = config[\"configurable\"].get(\"user_name\")\n    system_msg = f\"You are a helpful assistant. Address the user as {user_name}.\"\n    return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n    # highlight-next-line\n    prompt=prompt\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    # highlight-next-line\n    config={\"configurable\": {\"user_name\": \"John Smith\"}}\n)\n</code></pre> <ol> <li> <p>Dynamic prompts allow including non-message context when constructing an input to the LLM, such as:</p> <ul> <li>Information passed at runtime, like a <code>user_id</code> or API credentials (using <code>config</code>).</li> <li>Internal agent state updated during a multi-step reasoning process (using <code>state</code>).</li> </ul> <p>Dynamic prompts can be defined as functions that take <code>state</code> and <code>config</code> and return a list of messages to send to the LLM.</p> </li> </ol> <p>:::</p> <p>:::js Define a function that returns messages based on the agent's state and configuration:</p> <pre><code>import { type BaseMessageLike } from \"@langchain/core/messages\";\nimport { type RunnableConfig } from \"@langchain/core/runnables\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\n// highlight-next-line\nconst dynamicPrompt = (state: { messages: BaseMessageLike[] }, config: RunnableConfig): BaseMessageLike[] =&gt; {  // (1)!\n  const userName = config.configurable?.user_name;\n  const systemMsg = `You are a helpful assistant. Address the user as ${userName}.`;\n  return [{ role: \"system\", content: systemMsg }, ...state.messages];\n};\n\nconst agent = createReactAgent({\n  llm: \"anthropic:claude-3-5-sonnet-latest\",\n  tools: [getWeather],\n  // highlight-next-line\n  stateModifier: dynamicPrompt\n});\n\nawait agent.invoke(\n  { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n  // highlight-next-line\n  { configurable: { user_name: \"John Smith\" } }\n);\n</code></pre> <ol> <li> <p>Dynamic prompts allow including non-message context when constructing an input to the LLM, such as:</p> <ul> <li>Information passed at runtime, like a <code>user_id</code> or API credentials (using <code>config</code>).</li> <li>Internal agent state updated during a multi-step reasoning process (using <code>state</code>).</li> </ul> <p>Dynamic prompts can be defined as functions that take <code>state</code> and <code>config</code> and return a list of messages to send to the LLM.</p> </li> </ol> <p>:::</p> <p>For more information, see Context.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/agents/#5-add-memory", "title": "5. Add memory", "text": "<p>To allow multi-turn conversations with an agent, you need to enable persistence by providing a checkpointer when creating an agent. At runtime, you need to provide a config containing <code>thread_id</code> \u2014 a unique identifier for the conversation (session):</p> <p>:::python</p> <pre><code>from langgraph.prebuilt import create_react_agent\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# highlight-next-line\ncheckpointer = InMemorySaver()\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n    # highlight-next-line\n    checkpointer=checkpointer  # (1)!\n)\n\n# Run the agent\n# highlight-next-line\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\nsf_response = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    # highlight-next-line\n    config  # (2)!\n)\nny_response = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what about new york?\"}]},\n    # highlight-next-line\n    config\n)\n</code></pre> <ol> <li><code>checkpointer</code> allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities.</li> <li>Pass configuration with <code>thread_id</code> to be able to resume the same conversation on future agent invocations.    :::</li> </ol> <p>:::js</p> <pre><code>import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\n// highlight-next-line\nconst checkpointer = new MemorySaver();\n\nconst agent = createReactAgent({\n  llm: \"anthropic:claude-3-5-sonnet-latest\",\n  tools: [getWeather],\n  // highlight-next-line\n  checkpointSaver: checkpointer, // (1)!\n});\n\n// Run the agent\n// highlight-next-line\nconst config = { configurable: { thread_id: \"1\" } };\nconst sfResponse = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n  // highlight-next-line\n  config // (2)!\n);\nconst nyResponse = await agent.invoke(\n  { messages: [{ role: \"user\", content: \"what about new york?\" }] },\n  // highlight-next-line\n  config\n);\n</code></pre> <ol> <li><code>checkpointSaver</code> allows the agent to store its state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities.</li> <li>Pass configuration with <code>thread_id</code> to be able to resume the same conversation on future agent invocations.    :::</li> </ol> <p>:::python When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using <code>InMemorySaver</code>). :::</p> <p>:::js When you enable the checkpointer, it stores agent state at every step in the provided checkpointer database (or in memory, if using <code>MemorySaver</code>). :::</p> <p>Note that in the above example, when the agent is invoked the second time with the same <code>thread_id</code>, the original message history from the first conversation is automatically included, together with the new user input.</p> <p>For more information, see Memory.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/agents/#6-configure-structured-output", "title": "6. Configure structured output", "text": "<p>:::python To produce structured responses conforming to a schema, use the <code>response_format</code> parameter. The schema can be defined with a <code>Pydantic</code> model or <code>TypedDict</code>. The result will be accessible via the <code>structured_response</code> field.</p> <pre><code>from pydantic import BaseModel\nfrom langgraph.prebuilt import create_react_agent\n\nclass WeatherResponse(BaseModel):\n    conditions: str\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n    # highlight-next-line\n    response_format=WeatherResponse  # (1)!\n)\n\nresponse = agent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}\n)\n\n# highlight-next-line\nresponse[\"structured_response\"]\n</code></pre> <ol> <li> <p>When <code>response_format</code> is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response.</p> <pre><code>To provide a system prompt to this LLM, use a tuple `(prompt, schema)`, e.g., `response_format=(prompt, WeatherResponse)`.\n</code></pre> <p>:::</p> </li> </ol> <p>:::js To produce structured responses conforming to a schema, use the <code>responseFormat</code> parameter. The schema can be defined with a <code>Zod</code> schema. The result will be accessible via the <code>structuredResponse</code> field.</p> <pre><code>import { z } from \"zod\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst WeatherResponse = z.object({\n  conditions: z.string(),\n});\n\nconst agent = createReactAgent({\n  llm: \"anthropic:claude-3-5-sonnet-latest\",\n  tools: [getWeather],\n  // highlight-next-line\n  responseFormat: WeatherResponse, // (1)!\n});\n\nconst response = await agent.invoke({\n  messages: [{ role: \"user\", content: \"what is the weather in sf\" }],\n});\n\n// highlight-next-line\nresponse.structuredResponse;\n</code></pre> <ol> <li> <p>When <code>responseFormat</code> is provided, a separate step is added at the end of the agent loop: agent message history is passed to an LLM with structured output to generate a structured response.</p> <pre><code>To provide a system prompt to this LLM, use an object `{ prompt, schema }`, e.g., `responseFormat: { prompt, schema: WeatherResponse }`.\n</code></pre> <p>:::</p> </li> </ol> <p>LLM post-processing</p> <p>Structured output requires an additional call to the LLM to format the response according to the schema.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/agents/#next-steps", "title": "Next steps", "text": "<ul> <li>Deploy your agent locally</li> <li>Learn more about prebuilt agents</li> <li>LangGraph Platform quickstart</li> </ul>", "tags": ["agent"], "boost": 2}, {"location": "agents/context/", "title": "Context", "text": "<p>Context engineering is the practice of building dynamic systems that provide the right information and tools, in the right format, so that an AI application can accomplish a task. Context can be characterized along two key dimensions:</p> <ol> <li>By mutability:<ul> <li>Static context: Immutable data that doesn't change during execution (e.g., user metadata, database connections, tools)</li> <li>Dynamic context: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations)</li> </ul> </li> <li>By lifetime:<ul> <li>Runtime context: Data scoped to a single run or invocation</li> <li>Cross-conversation context: Data that persists across multiple conversations or sessions</li> </ul> </li> </ol> <p>Runtime context vs LLM context</p> <p>Runtime context refers to local context: data and dependencies your code needs to run. It does not refer to:</p> <ul> <li>The LLM context, which is the data passed into the LLM's prompt.</li> <li>The \"context window\", which is the maximum number of tokens that can be passed to the LLM.</li> </ul> <p>Runtime context can be used to optimize the LLM context. For example, you can use user metadata in the runtime context to fetch user preferences and feed them into the context window.</p> <p>LangGraph provides three ways to manage context, which combines the mutability and lifetime dimensions:</p> <p>:::python</p> Context type Description Mutability Lifetime Access method Static runtime context User metadata, tools, db connections passed at startup Static Single run <code>context</code> argument to <code>invoke</code>/<code>stream</code> Dynamic runtime context (state) Mutable data that evolves during a single run Dynamic Single run LangGraph state object Dynamic cross-conversation context (store) Persistent data shared across conversations Dynamic Cross-conversation LangGraph store"}, {"location": "agents/context/#static-runtime-context", "title": "Static runtime context", "text": "<p>Static runtime context represents immutable data like user metadata, tools, and database connections that are passed to an application at the start of a run via the <code>context</code> argument to <code>invoke</code>/<code>stream</code>. This data does not change during execution.</p> <p>New in LangGraph v0.6: <code>context</code> replaces <code>config['configurable']</code></p> <p>Runtime context is now passed to the <code>context</code> argument of <code>invoke</code>/<code>stream</code>, which replaces the previous pattern of passing application configuration to <code>config['configurable']</code>.</p> <pre><code>@dataclass\nclass ContextSchema:\n    user_name: str\n\ngraph.invoke( # (1)!\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]}, # (2)!\n    # highlight-next-line\n    context={\"user_name\": \"John Smith\"} # (3)!\n)\n</code></pre> <ol> <li>This is the invocation of the agent or graph. The <code>invoke</code> method runs the underlying graph with the provided input.</li> <li>This example uses messages as an input, which is common, but your application may use different input structures.</li> <li>This is where you pass the runtime data. The <code>context</code> parameter allows you to provide additional dependencies that the agent can use during its execution.</li> </ol> Agent promptWorkflow nodeIn a tool <pre><code>from langchain_core.messages import AnyMessage\nfrom langgraph.runtime import get_runtime\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.prebuilt import create_react_agent\n\n# highlight-next-line\ndef prompt(state: AgentState) -&gt; list[AnyMessage]:\n    runtime = get_runtime(ContextSchema)\n    system_msg = f\"You are a helpful assistant. Address the user as {runtime.context.user_name}.\"\n    return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n    prompt=prompt,\n    context_schema=ContextSchema\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    # highlight-next-line\n    context={\"user_name\": \"John Smith\"}\n)\n</code></pre> <ul> <li>See Agents for details.</li> </ul> <pre><code>from langgraph.runtime import Runtime\n\n# highlight-next-line\ndef node(state: State, runtime: Runtime[ContextSchema]):\n    user_name = runtime.context.user_name\n    ...\n</code></pre> <ul> <li>See the Graph API for details.</li> </ul> <pre><code>from langgraph.runtime import get_runtime\n\n@tool\n# highlight-next-line\ndef get_user_email() -&gt; str:\n    \"\"\"Retrieve user information based on user ID.\"\"\"\n    # simulate fetching user info from a database\n    runtime = get_runtime(ContextSchema)\n    email = get_user_email_from_db(runtime.context.user_name)\n    return email\n</code></pre> <p>See the tool calling guide for details.</p> <p>Tip</p> <p>The <code>Runtime</code> object can be used to access static context and other utilities like the active store and stream writer. See the [Runtime][langgraph.runtime.Runtime] documentation for details.</p> <p>:::</p> <p>:::js</p> Context type Description Mutability Lifetime Config data passed at the start of a run Static Single run Dynamic runtime context (state) Mutable data that evolves during a single run Dynamic Single run Dynamic cross-conversation context (store) Persistent data shared across conversations Dynamic Cross-conversation"}, {"location": "agents/context/#config-static-context", "title": "Config (static context)", "text": "<p>Config is for immutable data like user metadata or API keys. Use this when you have values that don't change mid-run.</p> <p>Specify configuration using a key called \"configurable\" which is reserved for this purpose.</p> <pre><code>await graph.invoke(\n  // (1)!\n  { messages: [{ role: \"user\", content: \"hi!\" }] }, // (2)!\n  // highlight-next-line\n  { configurable: { user_id: \"user_123\" } } // (3)!\n);\n</code></pre> <p>:::</p>"}, {"location": "agents/context/#dynamic-runtime-context-state", "title": "Dynamic runtime context (state)", "text": "<p>Dynamic runtime context represents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as short-term memory during a run.</p> In an agentIn a workflow <p>Example shows how to incorporate state into an agent prompt.</p> <p>State can also be accessed by the agent's tools, which can read or update the state as needed. See tool calling guide for details.</p> <p>:::python <pre><code>from langchain_core.messages import AnyMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\n\n# highlight-next-line\nclass CustomState(AgentState): # (1)!\n    user_name: str\n\ndef prompt(\n    # highlight-next-line\n    state: CustomState\n) -&gt; list[AnyMessage]:\n    user_name = state[\"user_name\"]\n    system_msg = f\"You are a helpful assistant. User's name is {user_name}\"\n    return [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[...],\n    # highlight-next-line\n    state_schema=CustomState, # (2)!\n    prompt=prompt\n)\n\nagent.invoke({\n    \"messages\": \"hi!\",\n    \"user_name\": \"John Smith\"\n})\n</code></pre></p> <ol> <li>Define a custom state schema that extends <code>AgentState</code> or <code>MessagesState</code>.</li> <li>Pass the custom state schema to the agent. This allows the agent to access and modify the state during execution. :::</li> </ol> <p>:::js <pre><code>import type { BaseMessage } from \"@langchain/core/messages\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { MessagesZodState } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\n// highlight-next-line\nconst CustomState = z.object({ // (1)!\n  messages: MessagesZodState.shape.messages,\n  userName: z.string(),\n});\n\nconst prompt = (\n  // highlight-next-line\n  state: z.infer&lt;typeof CustomState&gt;\n): BaseMessage[] =&gt; {\n  const userName = state.userName;\n  const systemMsg = `You are a helpful assistant. User's name is ${userName}`;\n  return [{ role: \"system\", content: systemMsg }, ...state.messages];\n};\n\nconst agent = createReactAgent({\n  llm: model,\n  tools: [...],\n  // highlight-next-line\n  stateSchema: CustomState, // (2)!\n  stateModifier: prompt,\n});\n\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"hi!\" }],\n  userName: \"John Smith\",\n});\n</code></pre></p> <ol> <li>Define a custom state schema that extends <code>MessagesZodState</code> or creates a new schema.</li> <li>Pass the custom state schema to the agent. This allows the agent to access and modify the state during execution. :::</li> </ol> <p>:::python <pre><code>from typing_extensions import TypedDict\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph import StateGraph\n\n# highlight-next-line\nclass CustomState(TypedDict): # (1)!\n    messages: list[AnyMessage]\n    extra_field: int\n\n# highlight-next-line\ndef node(state: CustomState): # (2)!\n    messages = state[\"messages\"]\n    ...\n    return { # (3)!\n        # highlight-next-line\n        \"extra_field\": state[\"extra_field\"] + 1\n    }\n\nbuilder = StateGraph(State)\nbuilder.add_node(node)\nbuilder.set_entry_point(\"node\")\ngraph = builder.compile()\n</code></pre></p> <ol> <li>Define a custom state</li> <li>Access the state in any node or tool</li> <li>The Graph API is designed to work as easily as possible with state. The return value of a node represents a requested update to the state. :::</li> </ol> <p>:::js <pre><code>import type { BaseMessage } from \"@langchain/core/messages\";\nimport { StateGraph, MessagesZodState, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\n// highlight-next-line\nconst CustomState = z.object({ // (1)!\n  messages: MessagesZodState.shape.messages,\n  extraField: z.number(),\n});\n\nconst builder = new StateGraph(CustomState)\n  .addNode(\"node\", async (state) =&gt; { // (2)!\n    const messages = state.messages;\n    // ...\n    return { // (3)!\n      // highlight-next-line\n      extraField: state.extraField + 1,\n    };\n  })\n  .addEdge(START, \"node\");\n\nconst graph = builder.compile();\n</code></pre></p> <ol> <li>Define a custom state</li> <li>Access the state in any node or tool</li> <li>The Graph API is designed to work as easily as possible with state. The return value of a node represents a requested update to the state. :::</li> </ol> <p>Turning on memory</p> <p>Please see the memory guide for more details on how to enable memory. This is a powerful feature that allows you to persist the agent's state across multiple invocations. Otherwise, the state is scoped only to a single run.</p>"}, {"location": "agents/context/#dynamic-cross-conversation-context-store", "title": "Dynamic cross-conversation context (store)", "text": "<p>Dynamic cross-conversation context represents persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store. This includes user profiles, preferences, and historical interactions. The LangGraph store acts as long-term memory across multiple runs. This can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions).</p> <p>For more information, see the Memory guide.</p>"}, {"location": "agents/evals/", "title": "Evals", "text": "<p>To evaluate your agent's performance you can use <code>LangSmith</code> evaluations. You would need to first define an evaluator function to judge the results from an agent, such as final outputs or trajectory. Depending on your evaluation technique, this may or may not involve a reference output:</p> <p>:::python</p> <pre><code>def evaluator(*, outputs: dict, reference_outputs: dict):\n    # compare agent outputs against reference outputs\n    output_messages = outputs[\"messages\"]\n    reference_messages = reference_outputs[\"messages\"]\n    score = compare_messages(output_messages, reference_messages)\n    return {\"key\": \"evaluator_score\", \"score\": score}\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>type EvaluatorParams = {\n  outputs: Record&lt;string, any&gt;;\n  referenceOutputs: Record&lt;string, any&gt;;\n};\n\nfunction evaluator({ outputs, referenceOutputs }: EvaluatorParams) {\n  // compare agent outputs against reference outputs\n  const outputMessages = outputs.messages;\n  const referenceMessages = referenceOutputs.messages;\n  const score = compareMessages(outputMessages, referenceMessages);\n  return { key: \"evaluator_score\", score: score };\n}\n</code></pre> <p>:::</p> <p>To get started, you can use prebuilt evaluators from <code>AgentEvals</code> package:</p> <p>:::python</p> <pre><code>pip install -U agentevals\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>npm install agentevals\n</code></pre> <p>:::</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/evals/#create-evaluator", "title": "Create evaluator", "text": "<p>A common way to evaluate agent performance is by comparing its trajectory (the order in which it calls its tools) against a reference trajectory:</p> <p>:::python</p> <pre><code>import json\n# highlight-next-line\nfrom agentevals.trajectory.match import create_trajectory_match_evaluator\n\noutputs = [\n    {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n            {\n                \"function\": {\n                    \"name\": \"get_weather\",\n                    \"arguments\": json.dumps({\"city\": \"san francisco\"}),\n                }\n            },\n            {\n                \"function\": {\n                    \"name\": \"get_directions\",\n                    \"arguments\": json.dumps({\"destination\": \"presidio\"}),\n                }\n            }\n        ],\n    }\n]\nreference_outputs = [\n    {\n        \"role\": \"assistant\",\n        \"tool_calls\": [\n            {\n                \"function\": {\n                    \"name\": \"get_weather\",\n                    \"arguments\": json.dumps({\"city\": \"san francisco\"}),\n                }\n            },\n        ],\n    }\n]\n\n# Create the evaluator\nevaluator = create_trajectory_match_evaluator(\n    # highlight-next-line\n    trajectory_match_mode=\"superset\",  # (1)!\n)\n\n# Run the evaluator\nresult = evaluator(\n    outputs=outputs, reference_outputs=reference_outputs\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { createTrajectoryMatchEvaluator } from \"agentevals/trajectory/match\";\n\nconst outputs = [\n  {\n    role: \"assistant\",\n    tool_calls: [\n      {\n        function: {\n          name: \"get_weather\",\n          arguments: JSON.stringify({ city: \"san francisco\" }),\n        },\n      },\n      {\n        function: {\n          name: \"get_directions\",\n          arguments: JSON.stringify({ destination: \"presidio\" }),\n        },\n      },\n    ],\n  },\n];\n\nconst referenceOutputs = [\n  {\n    role: \"assistant\",\n    tool_calls: [\n      {\n        function: {\n          name: \"get_weather\",\n          arguments: JSON.stringify({ city: \"san francisco\" }),\n        },\n      },\n    ],\n  },\n];\n\n// Create the evaluator\nconst evaluator = createTrajectoryMatchEvaluator({\n  // Specify how the trajectories will be compared. `superset` will accept output trajectory as valid if it's a superset of the reference one. Other options include: strict, unordered and subset\n  trajectoryMatchMode: \"superset\", // (1)!\n});\n\n// Run the evaluator\nconst result = evaluator({\n  outputs: outputs,\n  referenceOutputs: referenceOutputs,\n});\n</code></pre> <p>:::</p> <ol> <li>Specify how the trajectories will be compared. <code>superset</code> will accept output trajectory as valid if it's a superset of the reference one. Other options include: strict, unordered and subset</li> </ol> <p>As a next step, learn more about how to customize trajectory match evaluator.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/evals/#llm-as-a-judge", "title": "LLM-as-a-judge", "text": "<p>You can use LLM-as-a-judge evaluator that uses an LLM to compare the trajectory against the reference outputs and output a score:</p> <p>:::python</p> <pre><code>import json\nfrom agentevals.trajectory.llm import (\n    # highlight-next-line\n    create_trajectory_llm_as_judge,\n    TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE\n)\n\nevaluator = create_trajectory_llm_as_judge(\n    prompt=TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n    model=\"openai:o3-mini\"\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import {\n  createTrajectoryLlmAsJudge,\n  TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n} from \"agentevals/trajectory/llm\";\n\nconst evaluator = createTrajectoryLlmAsJudge({\n  prompt: TRAJECTORY_ACCURACY_PROMPT_WITH_REFERENCE,\n  model: \"openai:o3-mini\",\n});\n</code></pre> <p>:::</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/evals/#run-evaluator", "title": "Run evaluator", "text": "<p>To run an evaluator, you will first need to create a LangSmith dataset. To use the prebuilt AgentEvals evaluators, you will need a dataset with the following schema:</p> <ul> <li>input: <code>{\"messages\": [...]}</code> input messages to call the agent with.</li> <li>output: <code>{\"messages\": [...]}</code> expected message history in the agent output. For trajectory evaluation, you can choose to keep only assistant messages.</li> </ul> <p>:::python</p> <pre><code>from langsmith import Client\nfrom langgraph.prebuilt import create_react_agent\nfrom agentevals.trajectory.match import create_trajectory_match_evaluator\n\nclient = Client()\nagent = create_react_agent(...)\nevaluator = create_trajectory_match_evaluator(...)\n\nexperiment_results = client.evaluate(\n    lambda inputs: agent.invoke(inputs),\n    # replace with your dataset name\n    data=\"&lt;Name of your dataset&gt;\",\n    evaluators=[evaluator]\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { Client } from \"langsmith\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { createTrajectoryMatchEvaluator } from \"agentevals/trajectory/match\";\n\nconst client = new Client();\nconst agent = createReactAgent({...});\nconst evaluator = createTrajectoryMatchEvaluator({...});\n\nconst experimentResults = await client.evaluate(\n    (inputs) =&gt; agent.invoke(inputs),\n    // replace with your dataset name\n    { data: \"&lt;Name of your dataset&gt;\" },\n    { evaluators: [evaluator] }\n);\n</code></pre> <p>:::</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/mcp/", "title": "Use MCP", "text": "<p>Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to language models. LangGraph agents can use tools defined on MCP servers through the <code>langchain-mcp-adapters</code> library.</p> <p></p> <p>:::python Install the <code>langchain-mcp-adapters</code> library to use MCP tools in LangGraph:</p> <pre><code>pip install langchain-mcp-adapters\n</code></pre> <p>:::</p> <p>:::js Install the <code>@langchain/mcp-adapters</code> library to use MCP tools in LangGraph:</p> <pre><code>npm install langchain-mcp-adapters\n</code></pre> <p>:::</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/mcp/#use-mcp-tools", "title": "Use MCP tools", "text": "<p>:::python The <code>langchain-mcp-adapters</code> package enables agents to use tools defined across one or more MCP servers.</p> In an agentIn a workflow Agent using tools defined on MCP servers<pre><code># highlight-next-line\nfrom langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langgraph.prebuilt import create_react_agent\n\n# highlight-next-line\nclient = MultiServerMCPClient(\n    {\n        \"math\": {\n            \"command\": \"python\",\n            # Replace with absolute path to your math_server.py file\n            \"args\": [\"/path/to/math_server.py\"],\n            \"transport\": \"stdio\",\n        },\n        \"weather\": {\n            # Ensure you start your weather server on port 8000\n            \"url\": \"http://localhost:8000/mcp\",\n            \"transport\": \"streamable_http\",\n        }\n    }\n)\n# highlight-next-line\ntools = await client.get_tools()\nagent = create_react_agent(\n    \"anthropic:claude-3-7-sonnet-latest\",\n    # highlight-next-line\n    tools\n)\nmath_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n)\nweather_response = await agent.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]}\n)\n</code></pre> Workflow using MCP tools with ToolNode<pre><code>from langchain_mcp_adapters.client import MultiServerMCPClient\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.prebuilt import ToolNode\n\n# Initialize the model\nmodel = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n\n# Set up MCP client\nclient = MultiServerMCPClient(\n    {\n        \"math\": {\n            \"command\": \"python\",\n            # Make sure to update to the full absolute path to your math_server.py file\n            \"args\": [\"./examples/math_server.py\"],\n            \"transport\": \"stdio\",\n        },\n        \"weather\": {\n            # make sure you start your weather server on port 8000\n            \"url\": \"http://localhost:8000/mcp/\",\n            \"transport\": \"streamable_http\",\n        }\n    }\n)\ntools = await client.get_tools()\n\n# Bind tools to model\nmodel_with_tools = model.bind_tools(tools)\n\n# Create ToolNode\ntool_node = ToolNode(tools)\n\ndef should_continue(state: MessagesState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\n# Define call_model function\nasync def call_model(state: MessagesState):\n    messages = state[\"messages\"]\n    response = await model_with_tools.ainvoke(messages)\n    return {\"messages\": [response]}\n\n# Build the graph\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\"call_model\", call_model)\nbuilder.add_node(\"tools\", tool_node)\n\nbuilder.add_edge(START, \"call_model\")\nbuilder.add_conditional_edges(\n    \"call_model\",\n    should_continue,\n)\nbuilder.add_edge(\"tools\", \"call_model\")\n\n# Compile the graph\ngraph = builder.compile()\n\n# Test the graph\nmath_response = await graph.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]}\n)\nweather_response = await graph.ainvoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]}\n)\n</code></pre> <p>:::</p> <p>:::js The <code>@langchain/mcp-adapters</code> package enables agents to use tools defined across one or more MCP servers.</p> In an agentIn a workflow Agent using tools defined on MCP servers<pre><code>// highlight-next-line\nimport { MultiServerMCPClient } from \"langchain-mcp-adapters/client\";\nimport { ChatAnthropic } from \"@langchain/langgraph/prebuilt\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\n// highlight-next-line\nconst client = new MultiServerMCPClient({\n  math: {\n    command: \"node\",\n    // Replace with absolute path to your math_server.js file\n    args: [\"/path/to/math_server.js\"],\n    transport: \"stdio\",\n  },\n  weather: {\n    // Ensure you start your weather server on port 8000\n    url: \"http://localhost:8000/mcp\",\n    transport: \"streamable_http\",\n  },\n});\n\n// highlight-next-line\nconst tools = await client.getTools();\nconst agent = createReactAgent({\n  llm: new ChatAnthropic({ model: \"claude-3-7-sonnet-latest\" }),\n  // highlight-next-line\n  tools,\n});\n\nconst mathResponse = await agent.invoke({\n  messages: [{ role: \"user\", content: \"what's (3 + 5) x 12?\" }],\n});\n\nconst weatherResponse = await agent.invoke({\n  messages: [{ role: \"user\", content: \"what is the weather in nyc?\" }],\n});\n</code></pre> <pre><code>import { MultiServerMCPClient } from \"langchain-mcp-adapters/client\";\nimport { StateGraph, MessagesZodState, START } from \"@langchain/langgraph\";\nimport { ToolNode } from \"@langchain/langgraph/prebuilt\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { AIMessage } from \"@langchain/core/messages\";\nimport { z } from \"zod\";\n\nconst model = new ChatOpenAI({ model: \"gpt-4\" });\n\nconst client = new MultiServerMCPClient({\n  math: {\n    command: \"node\",\n    // Make sure to update to the full absolute path to your math_server.js file\n    args: [\"./examples/math_server.js\"],\n    transport: \"stdio\",\n  },\n  weather: {\n    // make sure you start your weather server on port 8000\n    url: \"http://localhost:8000/mcp/\",\n    transport: \"streamable_http\",\n  },\n});\n\nconst tools = await client.getTools();\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"callModel\", async (state) =&gt; {\n    const response = await model.bindTools(tools).invoke(state.messages);\n    return { messages: [response] };\n  })\n  .addNode(\"tools\", new ToolNode(tools))\n  .addEdge(START, \"callModel\")\n  .addConditionalEdges(\"callModel\", (state) =&gt; {\n    const lastMessage = state.messages.at(-1) as AIMessage | undefined;\n    if (!lastMessage?.tool_calls?.length) {\n      return \"__end__\";\n    }\n    return \"tools\";\n  })\n  .addEdge(\"tools\", \"callModel\");\n\nconst graph = builder.compile();\n\nconst mathResponse = await graph.invoke({\n  messages: [{ role: \"user\", content: \"what's (3 + 5) x 12?\" }],\n});\n\nconst weatherResponse = await graph.invoke({\n  messages: [{ role: \"user\", content: \"what is the weather in nyc?\" }],\n});\n</code></pre> <p>:::</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/mcp/#custom-mcp-servers", "title": "Custom MCP servers", "text": "<p>:::python To create your own MCP servers, you can use the <code>mcp</code> library. This library provides a simple way to define tools and run them as servers.</p> <p>Install the MCP library:</p> <pre><code>pip install mcp\n</code></pre> <p>:::</p> <p>:::js To create your own MCP servers, you can use the <code>@modelcontextprotocol/sdk</code> library. This library provides a simple way to define tools and run them as servers.</p> <p>Install the MCP SDK:</p> <pre><code>npm install @modelcontextprotocol/sdk\n</code></pre> <p>:::</p> <p>Use the following reference implementations to test your agent with MCP tool servers.</p> <p>:::python</p> Example Math Server (stdio transport)<pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Math\")\n\n@mcp.tool()\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\n@mcp.tool()\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers\"\"\"\n    return a * b\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"stdio\")\n</code></pre> <p>:::</p> <p>:::js</p> Example Math Server (stdio transport)<pre><code>import { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { StdioServerTransport } from \"@modelcontextprotocol/sdk/server/stdio.js\";\nimport {\n  CallToolRequestSchema,\n  ListToolsRequestSchema,\n} from \"@modelcontextprotocol/sdk/types.js\";\n\nconst server = new Server(\n  {\n    name: \"math-server\",\n    version: \"0.1.0\",\n  },\n  {\n    capabilities: {\n      tools: {},\n    },\n  }\n);\n\nserver.setRequestHandler(ListToolsRequestSchema, async () =&gt; {\n  return {\n    tools: [\n      {\n        name: \"add\",\n        description: \"Add two numbers\",\n        inputSchema: {\n          type: \"object\",\n          properties: {\n            a: {\n              type: \"number\",\n              description: \"First number\",\n            },\n            b: {\n              type: \"number\",\n              description: \"Second number\",\n            },\n          },\n          required: [\"a\", \"b\"],\n        },\n      },\n      {\n        name: \"multiply\",\n        description: \"Multiply two numbers\",\n        inputSchema: {\n          type: \"object\",\n          properties: {\n            a: {\n              type: \"number\",\n              description: \"First number\",\n            },\n            b: {\n              type: \"number\",\n              description: \"Second number\",\n            },\n          },\n          required: [\"a\", \"b\"],\n        },\n      },\n    ],\n  };\n});\n\nserver.setRequestHandler(CallToolRequestSchema, async (request) =&gt; {\n  switch (request.params.name) {\n    case \"add\": {\n      const { a, b } = request.params.arguments as { a: number; b: number };\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: String(a + b),\n          },\n        ],\n      };\n    }\n    case \"multiply\": {\n      const { a, b } = request.params.arguments as { a: number; b: number };\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: String(a * b),\n          },\n        ],\n      };\n    }\n    default:\n      throw new Error(`Unknown tool: ${request.params.name}`);\n  }\n});\n\nasync function main() {\n  const transport = new StdioServerTransport();\n  await server.connect(transport);\n  console.error(\"Math MCP server running on stdio\");\n}\n\nmain();\n</code></pre> <p>:::</p> <p>:::python</p> Example Weather Server (Streamable HTTP transport)<pre><code>from mcp.server.fastmcp import FastMCP\n\nmcp = FastMCP(\"Weather\")\n\n@mcp.tool()\nasync def get_weather(location: str) -&gt; str:\n    \"\"\"Get weather for location.\"\"\"\n    return \"It's always sunny in New York\"\n\nif __name__ == \"__main__\":\n    mcp.run(transport=\"streamable-http\")\n</code></pre> <p>:::</p> <p>:::js</p> Example Weather Server (HTTP transport)<pre><code>import { Server } from \"@modelcontextprotocol/sdk/server/index.js\";\nimport { SSEServerTransport } from \"@modelcontextprotocol/sdk/server/sse.js\";\nimport {\n  CallToolRequestSchema,\n  ListToolsRequestSchema,\n} from \"@modelcontextprotocol/sdk/types.js\";\nimport express from \"express\";\n\nconst app = express();\napp.use(express.json());\n\nconst server = new Server(\n  {\n    name: \"weather-server\",\n    version: \"0.1.0\",\n  },\n  {\n    capabilities: {\n      tools: {},\n    },\n  }\n);\n\nserver.setRequestHandler(ListToolsRequestSchema, async () =&gt; {\n  return {\n    tools: [\n      {\n        name: \"get_weather\",\n        description: \"Get weather for location\",\n        inputSchema: {\n          type: \"object\",\n          properties: {\n            location: {\n              type: \"string\",\n              description: \"Location to get weather for\",\n            },\n          },\n          required: [\"location\"],\n        },\n      },\n    ],\n  };\n});\n\nserver.setRequestHandler(CallToolRequestSchema, async (request) =&gt; {\n  switch (request.params.name) {\n    case \"get_weather\": {\n      const { location } = request.params.arguments as { location: string };\n      return {\n        content: [\n          {\n            type: \"text\",\n            text: `It's always sunny in ${location}`,\n          },\n        ],\n      };\n    }\n    default:\n      throw new Error(`Unknown tool: ${request.params.name}`);\n  }\n});\n\napp.post(\"/mcp\", async (req, res) =&gt; {\n  const transport = new SSEServerTransport(\"/mcp\", res);\n  await server.connect(transport);\n});\n\nconst PORT = process.env.PORT || 8000;\napp.listen(PORT, () =&gt; {\n  console.log(`Weather MCP server running on port ${PORT}`);\n});\n</code></pre> <p>:::</p> <p>:::python</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/mcp/#additional-resources", "title": "Additional resources", "text": "<ul> <li>MCP documentation</li> <li>MCP Transport documentation</li> <li>langchain_mcp_adapters   :::</li> </ul> <p>:::js</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/mcp/#additional-resources_1", "title": "Additional resources", "text": "<ul> <li>MCP documentation</li> <li>MCP Transport documentation</li> <li><code>@langchain/mcp-adapters</code>   :::</li> </ul>", "tags": ["agent"], "boost": 2}, {"location": "agents/models/", "title": "Models", "text": "<p>LangGraph provides built-in support for LLMs (language models) via the LangChain library. This makes it easy to integrate various LLMs into your agents and workflows.</p>"}, {"location": "agents/models/#initialize-a-model", "title": "Initialize a model", "text": "<p>Use <code>init_chat_model</code> to initialize models:</p> OpenAIAnthropicAzureGoogle GeminiAWS Bedrock <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"openai:gpt-4.1\")\n</code></pre></p> <p>\ud83d\udc49 Read the OpenAI integration docs</p> <p><pre><code>pip install -U \"langchain[anthropic]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n</code></pre></p> <p>\ud83d\udc49 Read the Anthropic integration docs</p> <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\nllm = init_chat_model(\n    \"azure_openai:gpt-4.1\",\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n)\n</code></pre></p> <p>\ud83d\udc49 Read the Azure integration docs</p> <p><pre><code>pip install -U \"langchain[google-genai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n</code></pre></p> <p>\ud83d\udc49 Read the Google GenAI integration docs</p> <p><pre><code>pip install -U \"langchain[aws]\"\n</code></pre> <pre><code>from langchain.chat_models import init_chat_model\n\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\nllm = init_chat_model(\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    model_provider=\"bedrock_converse\",\n)\n</code></pre></p> <p>\ud83d\udc49 Read the AWS Bedrock integration docs</p> <p>:::</p> <p>:::js Use model provider classes to initialize models:</p> OpenAIAnthropicGoogleGroq <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n});\n</code></pre> <pre><code>import { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst model = new ChatAnthropic({\n  model: \"claude-3-5-sonnet-20240620\",\n  temperature: 0,\n  maxTokens: 2048,\n});\n</code></pre> <pre><code>import { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";\n\nconst model = new ChatGoogleGenerativeAI({\n  model: \"gemini-1.5-pro\",\n  temperature: 0,\n});\n</code></pre> <pre><code>import { ChatGroq } from \"@langchain/groq\";\n\nconst model = new ChatGroq({\n  model: \"llama-3.1-70b-versatile\",\n  temperature: 0,\n});\n</code></pre> <p>:::</p> <p>:::python</p>"}, {"location": "agents/models/#instantiate-a-model-directly", "title": "Instantiate a model directly", "text": "<p>If a model provider is not available via <code>init_chat_model</code>, you can instantiate the provider's model class directly. The model must implement the BaseChatModel interface and support tool calling:</p> <pre><code># Anthropic is already supported by `init_chat_model`,\n# but you can also instantiate it directly.\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(\n  model=\"claude-3-7-sonnet-latest\",\n  temperature=0,\n  max_tokens=2048\n)\n</code></pre> <p>:::</p> <p>Tool calling support</p> <p>If you are building an agent or workflow that requires the model to call external tools, ensure that the underlying language model supports tool calling. Compatible models can be found in the LangChain integrations directory.</p>"}, {"location": "agents/models/#use-in-an-agent", "title": "Use in an agent", "text": "<p>:::python When using <code>create_react_agent</code> you can specify the model by its name string, which is a shorthand for initializing the model using <code>init_chat_model</code>. This allows you to use the model without needing to import or instantiate it directly.</p> model namemodel instance <pre><code>from langgraph.prebuilt import create_react_agent\n\ncreate_react_agent(\n   # highlight-next-line\n   model=\"anthropic:claude-3-7-sonnet-latest\",\n   # other parameters\n)\n</code></pre> <pre><code>from langchain_anthropic import ChatAnthropic\nfrom langgraph.prebuilt import create_react_agent\n\nmodel = ChatAnthropic(\n    model=\"claude-3-7-sonnet-latest\",\n    temperature=0,\n    max_tokens=2048\n)\n# Alternatively\n# model = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\")\n\nagent = create_react_agent(\n  # highlight-next-line\n  model=model,\n  # other parameters\n)\n</code></pre> <p>:::</p> <p>:::js When using <code>createReactAgent</code> you can pass the model instance directly:</p> <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n});\n\nconst agent = createReactAgent({\n  llm: model,\n  tools: tools,\n});\n</code></pre> <p>:::</p> <p>:::python</p>"}, {"location": "agents/models/#dynamic-model-selection", "title": "Dynamic model selection", "text": "<p>Pass a callable function to <code>create_react_agent</code> to dynamically select the model at runtime. This is useful for scenarios where you want to choose a model based on user input, configuration settings, or other runtime conditions.</p> <p>The selector function must return a chat model. If you're using tools, you must bind the tools to the model within the selector function.</p> <p>```python from dataclasses import dataclass from typing import Literal from langchain.chat_models import init_chat_model from langchain_core.language_models import BaseChatModel from langchain_core.tools import tool from langgraph.prebuilt import create_react_agent from langgraph.prebuilt.chat_agent_executor import AgentState from langgraph.runtime import Runtime</p> <p>@tool def weather() -&gt; str:     \"\"\"Returns the current weather conditions.\"\"\"     return \"It's nice and sunny.\"</p>"}, {"location": "agents/models/#define-the-runtime-context", "title": "Define the runtime context", "text": "<p>@dataclass class CustomContext:     provider: Literal[\"anthropic\", \"openai\"]</p>"}, {"location": "agents/models/#initialize-models", "title": "Initialize models", "text": "<p>openai_model = init_chat_model(\"openai:gpt-4o\") anthropic_model = init_chat_model(\"anthropic:claude-sonnet-4-20250514\")</p>"}, {"location": "agents/models/#selector-function-for-model-choice", "title": "Selector function for model choice", "text": "<p>def select_model(state: AgentState, runtime: Runtime[CustomContext]) -&gt; BaseChatModel:     if runtime.context.provider == \"anthropic\":         model = anthropic_model     elif runtime.context.provider == \"openai\":         model = openai_model     else:         raise ValueError(f\"Unsupported provider: {runtime.context.provider}\")</p> <pre><code># With dynamic model selection, you must bind tools explicitly\nreturn model.bind_tools([weather])\n</code></pre>"}, {"location": "agents/models/#create-agent-with-dynamic-model-selection", "title": "Create agent with dynamic model selection", "text": "<p>agent = create_react_agent(select_model, tools=[weather])</p>"}, {"location": "agents/models/#invoke-with-context-to-select-model", "title": "Invoke with context to select model", "text": "<p>output = agent.invoke(     {         \"messages\": [             {                 \"role\": \"user\",                 \"content\": \"Which model is handling this?\",             }         ]     },     context=CustomContext(provider=\"openai\"), )</p> <p>print(output[\"messages\"][-1].text()) <pre><code>!!! version-added \"New in LangGraph v0.6\"\n\n:::\n\n## Advanced model configuration\n\n### Disable streaming\n\n:::python\nTo disable streaming of the individual LLM tokens, set `disable_streaming=True` when initializing the model:\n\n=== \"`init_chat_model`\"\n\n    ```python\n    from langchain.chat_models import init_chat_model\n\n    model = init_chat_model(\n        \"anthropic:claude-3-7-sonnet-latest\",\n        # highlight-next-line\n        disable_streaming=True\n    )\n    ```\n\n=== \"`ChatModel`\"\n\n    ```python\n    from langchain_anthropic import ChatAnthropic\n\n    model = ChatAnthropic(\n        model=\"claude-3-7-sonnet-latest\",\n        # highlight-next-line\n        disable_streaming=True\n    )\n    ```\n\nRefer to the [API reference](https://python.langchain.com/api_reference/core/language_models/langchain_core.language_models.chat_models.BaseChatModel.html#langchain_core.language_models.chat_models.BaseChatModel.disable_streaming) for more information on `disable_streaming`\n:::\n\n:::js\nTo disable streaming of the individual LLM tokens, set `streaming: false` when initializing the model:\n\n```typescript\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"gpt-4o\",\n  streaming: false,\n});\n</code></pre></p> <p>:::</p>"}, {"location": "agents/models/#add-model-fallbacks", "title": "Add model fallbacks", "text": "<p>:::python You can add a fallback to a different model or a different LLM provider using <code>model.with_fallbacks([...])</code>:</p> <code>init_chat_model</code><code>ChatModel</code> <pre><code>from langchain.chat_models import init_chat_model\n\nmodel_with_fallbacks = (\n    init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n    # highlight-next-line\n    .with_fallbacks([\n        init_chat_model(\"openai:gpt-4.1-mini\"),\n    ])\n)\n</code></pre> <pre><code>from langchain_anthropic import ChatAnthropic\nfrom langchain_openai import ChatOpenAI\n\nmodel_with_fallbacks = (\n    ChatAnthropic(model=\"claude-3-5-haiku-latest\")\n    # highlight-next-line\n    .with_fallbacks([\n        ChatOpenAI(model=\"gpt-4.1-mini\"),\n    ])\n)\n</code></pre> <p>See this guide for more information on model fallbacks. :::</p> <p>:::js You can add a fallback to a different model or a different LLM provider using <code>model.withFallbacks([...])</code>:</p> <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst modelWithFallbacks = new ChatOpenAI({\n  model: \"gpt-4o\",\n}).withFallbacks([\n  new ChatAnthropic({\n    model: \"claude-3-5-sonnet-20240620\",\n  }),\n]);\n</code></pre> <p>See this guide for more information on model fallbacks. :::</p> <p>:::python</p>"}, {"location": "agents/models/#use-the-built-in-rate-limiter", "title": "Use the built-in rate limiter", "text": "<p>Langchain includes a built-in in-memory rate limiter. This rate limiter is thread safe and can be shared by multiple threads in the same process.</p> <pre><code>from langchain_core.rate_limiters import InMemoryRateLimiter\nfrom langchain_anthropic import ChatAnthropic\n\nrate_limiter = InMemoryRateLimiter(\n    requests_per_second=0.1,  # &lt;-- Super slow! We can only make a request once every 10 seconds!!\n    check_every_n_seconds=0.1,  # Wake up every 100 ms to check whether allowed to make a request,\n    max_bucket_size=10,  # Controls the maximum burst size.\n)\n\nmodel = ChatAnthropic(\n   model_name=\"claude-3-opus-20240229\",\n   rate_limiter=rate_limiter\n)\n</code></pre> <p>See the LangChain docs for more information on how to handle rate limiting. :::</p>"}, {"location": "agents/models/#bring-your-own-model", "title": "Bring your own model", "text": "<p>If your desired LLM isn't officially supported by LangChain, consider these options:</p> <p>:::python</p> <ol> <li>Implement a custom LangChain chat model: Create a model conforming to the LangChain chat model interface. This enables full compatibility with LangGraph's agents and workflows but requires understanding of the LangChain framework.    :::</li> </ol> <p>:::js</p> <ol> <li> <p>Implement a custom LangChain chat model: Create a model conforming to the LangChain chat model interface. This enables full compatibility with LangGraph's agents and workflows but requires understanding of the LangChain framework.    :::</p> </li> <li> <p>Direct invocation with custom streaming: Use your model directly by adding custom streaming logic with <code>StreamWriter</code>.    Refer to the custom streaming documentation for guidance. This approach suits custom workflows where prebuilt agent integration is not necessary.</p> </li> </ol>"}, {"location": "agents/models/#additional-resources", "title": "Additional resources", "text": "<p>:::python</p> <ul> <li>Multimodal inputs</li> <li>Structured outputs</li> <li>Model integration directory</li> <li>Force model to call a specific tool</li> <li>All chat model how-to guides</li> <li>Chat model integrations   :::</li> </ul> <p>:::js</p> <ul> <li>Multimodal inputs</li> <li>Structured outputs</li> <li>Model integration directory</li> <li>Force model to call a specific tool</li> <li>All chat model how-to guides</li> <li>Chat model integrations   :::</li> </ul>"}, {"location": "agents/multi-agent/", "title": "Multi-agent", "text": "<p>A single agent might struggle if it needs to specialize in multiple domains or manage many tools. To tackle this, you can break your agent into smaller, independent agents and compose them into a multi-agent system.</p> <p>In multi-agent systems, agents need to communicate between each other. They do so via handoffs \u2014 a primitive that describes which agent to hand control to and the payload to send to that agent.</p> <p>Two of the most popular multi-agent architectures are:</p> <ul> <li>supervisor \u2014 individual agents are coordinated by a central supervisor agent. The supervisor controls all communication flow and task delegation, making decisions about which agent to invoke based on the current context and task requirements.</li> <li>swarm \u2014 agents dynamically hand off control to one another based on their specializations. The system remembers which agent was last active, ensuring that on subsequent interactions, the conversation resumes with that agent.</li> </ul>", "tags": ["agent"], "boost": 2}, {"location": "agents/multi-agent/#supervisor", "title": "Supervisor", "text": "<p>:::python Use <code>langgraph-supervisor</code> library to create a supervisor multi-agent system:</p> <pre><code>pip install langgraph-supervisor\n</code></pre> <pre><code>from langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\n# highlight-next-line\nfrom langgraph_supervisor import create_supervisor\n\ndef book_hotel(hotel_name: str):\n    \"\"\"Book a hotel\"\"\"\n    return f\"Successfully booked a stay at {hotel_name}.\"\n\ndef book_flight(from_airport: str, to_airport: str):\n    \"\"\"Book a flight\"\"\"\n    return f\"Successfully booked a flight from {from_airport} to {to_airport}.\"\n\nflight_assistant = create_react_agent(\n    model=\"openai:gpt-4o\",\n    tools=[book_flight],\n    prompt=\"You are a flight booking assistant\",\n    # highlight-next-line\n    name=\"flight_assistant\"\n)\n\nhotel_assistant = create_react_agent(\n    model=\"openai:gpt-4o\",\n    tools=[book_hotel],\n    prompt=\"You are a hotel booking assistant\",\n    # highlight-next-line\n    name=\"hotel_assistant\"\n)\n\n# highlight-next-line\nsupervisor = create_supervisor(\n    agents=[flight_assistant, hotel_assistant],\n    model=ChatOpenAI(model=\"gpt-4o\"),\n    prompt=(\n        \"You manage a hotel booking assistant and a\"\n        \"flight booking assistant. Assign work to them.\"\n    )\n).compile()\n\nfor chunk in supervisor.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\"\n            }\n        ]\n    }\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <p>:::</p> <p>:::js Use <code>@langchain/langgraph-supervisor</code> library to create a supervisor multi-agent system:</p> <pre><code>npm install @langchain/langgraph-supervisor\n</code></pre> <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n// highlight-next-line\nimport { createSupervisor } from \"langgraph-supervisor\";\n\nfunction bookHotel(hotelName: string) {\n  /**Book a hotel*/\n  return `Successfully booked a stay at ${hotelName}.`;\n}\n\nfunction bookFlight(fromAirport: string, toAirport: string) {\n  /**Book a flight*/\n  return `Successfully booked a flight from ${fromAirport} to ${toAirport}.`;\n}\n\nconst flightAssistant = createReactAgent({\n  llm: \"openai:gpt-4o\",\n  tools: [bookFlight],\n  stateModifier: \"You are a flight booking assistant\",\n  // highlight-next-line\n  name: \"flight_assistant\",\n});\n\nconst hotelAssistant = createReactAgent({\n  llm: \"openai:gpt-4o\",\n  tools: [bookHotel],\n  stateModifier: \"You are a hotel booking assistant\",\n  // highlight-next-line\n  name: \"hotel_assistant\",\n});\n\n// highlight-next-line\nconst supervisor = createSupervisor({\n  agents: [flightAssistant, hotelAssistant],\n  llm: new ChatOpenAI({ model: \"gpt-4o\" }),\n  systemPrompt:\n    \"You manage a hotel booking assistant and a \" +\n    \"flight booking assistant. Assign work to them.\",\n});\n\nfor await (const chunk of supervisor.stream({\n  messages: [\n    {\n      role: \"user\",\n      content: \"book a flight from BOS to JFK and a stay at McKittrick Hotel\",\n    },\n  ],\n})) {\n  console.log(chunk);\n  console.log(\"\\n\");\n}\n</code></pre> <p>:::</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/multi-agent/#swarm", "title": "Swarm", "text": "<p>:::python Use <code>langgraph-swarm</code> library to create a swarm multi-agent system:</p> <pre><code>pip install langgraph-swarm\n</code></pre> <pre><code>from langgraph.prebuilt import create_react_agent\n# highlight-next-line\nfrom langgraph_swarm import create_swarm, create_handoff_tool\n\ntransfer_to_hotel_assistant = create_handoff_tool(\n    agent_name=\"hotel_assistant\",\n    description=\"Transfer user to the hotel-booking assistant.\",\n)\ntransfer_to_flight_assistant = create_handoff_tool(\n    agent_name=\"flight_assistant\",\n    description=\"Transfer user to the flight-booking assistant.\",\n)\n\nflight_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    # highlight-next-line\n    tools=[book_flight, transfer_to_hotel_assistant],\n    prompt=\"You are a flight booking assistant\",\n    # highlight-next-line\n    name=\"flight_assistant\"\n)\nhotel_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    # highlight-next-line\n    tools=[book_hotel, transfer_to_flight_assistant],\n    prompt=\"You are a hotel booking assistant\",\n    # highlight-next-line\n    name=\"hotel_assistant\"\n)\n\n# highlight-next-line\nswarm = create_swarm(\n    agents=[flight_assistant, hotel_assistant],\n    default_active_agent=\"flight_assistant\"\n).compile()\n\nfor chunk in swarm.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\"\n            }\n        ]\n    }\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <p>:::</p> <p>:::js Use <code>@langchain/langgraph-swarm</code> library to create a swarm multi-agent system:</p> <pre><code>npm install @langchain/langgraph-swarm\n</code></pre> <pre><code>import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n// highlight-next-line\nimport { createSwarm, createHandoffTool } from \"@langchain/langgraph-swarm\";\n\nconst transferToHotelAssistant = createHandoffTool({\n  agentName: \"hotel_assistant\",\n  description: \"Transfer user to the hotel-booking assistant.\",\n});\n\nconst transferToFlightAssistant = createHandoffTool({\n  agentName: \"flight_assistant\",\n  description: \"Transfer user to the flight-booking assistant.\",\n});\n\nconst flightAssistant = createReactAgent({\n  llm: \"anthropic:claude-3-5-sonnet-latest\",\n  // highlight-next-line\n  tools: [bookFlight, transferToHotelAssistant],\n  stateModifier: \"You are a flight booking assistant\",\n  // highlight-next-line\n  name: \"flight_assistant\",\n});\n\nconst hotelAssistant = createReactAgent({\n  llm: \"anthropic:claude-3-5-sonnet-latest\",\n  // highlight-next-line\n  tools: [bookHotel, transferToFlightAssistant],\n  stateModifier: \"You are a hotel booking assistant\",\n  // highlight-next-line\n  name: \"hotel_assistant\",\n});\n\n// highlight-next-line\nconst swarm = createSwarm({\n  agents: [flightAssistant, hotelAssistant],\n  defaultActiveAgent: \"flight_assistant\",\n});\n\nfor await (const chunk of swarm.stream({\n  messages: [\n    {\n      role: \"user\",\n      content: \"book a flight from BOS to JFK and a stay at McKittrick Hotel\",\n    },\n  ],\n})) {\n  console.log(chunk);\n  console.log(\"\\n\");\n}\n</code></pre> <p>:::</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/multi-agent/#handoffs", "title": "Handoffs", "text": "<p>A common pattern in multi-agent interactions is handoffs, where one agent hands off control to another. Handoffs allow you to specify:</p> <ul> <li>destination: target agent to navigate to</li> <li>payload: information to pass to that agent</li> </ul> <p>:::python This is used both by <code>langgraph-supervisor</code> (supervisor hands off to individual agents) and <code>langgraph-swarm</code> (an individual agent can hand off to other agents).</p> <p>To implement handoffs with <code>create_react_agent</code>, you need to:</p> <ol> <li> <p>Create a special tool that can transfer control to a different agent</p> <pre><code>def transfer_to_bob():\n    \"\"\"Transfer to bob.\"\"\"\n    return Command(\n        # name of the agent (node) to go to\n        # highlight-next-line\n        goto=\"bob\",\n        # data to send to the agent\n        # highlight-next-line\n        update={\"messages\": [...]},\n        # indicate to LangGraph that we need to navigate to\n        # agent node in a parent graph\n        # highlight-next-line\n        graph=Command.PARENT,\n    )\n</code></pre> </li> <li> <p>Create individual agents that have access to handoff tools:</p> <pre><code>flight_assistant = create_react_agent(\n    ..., tools=[book_flight, transfer_to_hotel_assistant]\n)\nhotel_assistant = create_react_agent(\n    ..., tools=[book_hotel, transfer_to_flight_assistant]\n)\n</code></pre> </li> <li> <p>Define a parent graph that contains individual agents as nodes:</p> <pre><code>from langgraph.graph import StateGraph, MessagesState\nmulti_agent_graph = (\n    StateGraph(MessagesState)\n    .add_node(flight_assistant)\n    .add_node(hotel_assistant)\n    ...\n)\n</code></pre> </li> </ol> <p>:::</p> <p>:::js This is used both by <code>@langchain/langgraph-supervisor</code> (supervisor hands off to individual agents) and <code>@langchain/langgraph-swarm</code> (an individual agent can hand off to other agents).</p> <p>To implement handoffs with <code>createReactAgent</code>, you need to:</p> <ol> <li> <p>Create a special tool that can transfer control to a different agent</p> <pre><code>function transferToBob() {\n  /**Transfer to bob.*/\n  return new Command({\n    // name of the agent (node) to go to\n    // highlight-next-line\n    goto: \"bob\",\n    // data to send to the agent\n    // highlight-next-line\n    update: { messages: [...] },\n    // indicate to LangGraph that we need to navigate to\n    // agent node in a parent graph\n    // highlight-next-line\n    graph: Command.PARENT,\n  });\n}\n</code></pre> </li> <li> <p>Create individual agents that have access to handoff tools:</p> <pre><code>const flightAssistant = createReactAgent({\n  ..., tools: [bookFlight, transferToHotelAssistant]\n});\nconst hotelAssistant = createReactAgent({\n  ..., tools: [bookHotel, transferToFlightAssistant]\n});\n</code></pre> </li> <li> <p>Define a parent graph that contains individual agents as nodes:</p> <pre><code>import { StateGraph, MessagesZodState } from \"@langchain/langgraph\";\nconst multiAgentGraph = new StateGraph(MessagesZodState)\n  .addNode(\"flight_assistant\", flightAssistant)\n  .addNode(\"hotel_assistant\", hotelAssistant)\n  // ...\n</code></pre> <p>:::</p> </li> </ol> <p>Putting this together, here is how you can implement a simple multi-agent system with two agents \u2014 a flight booking assistant and a hotel booking assistant:</p> <p>:::python</p> <pre><code>from typing import Annotated\nfrom langchain_core.tools import tool, InjectedToolCallId\nfrom langgraph.prebuilt import create_react_agent, InjectedState\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.types import Command\n\ndef create_handoff_tool(*, agent_name: str, description: str | None = None):\n    name = f\"transfer_to_{agent_name}\"\n    description = description or f\"Transfer to {agent_name}\"\n\n    @tool(name, description=description)\n    def handoff_tool(\n        # highlight-next-line\n        state: Annotated[MessagesState, InjectedState], # (1)!\n        # highlight-next-line\n        tool_call_id: Annotated[str, InjectedToolCallId],\n    ) -&gt; Command:\n        tool_message = {\n            \"role\": \"tool\",\n            \"content\": f\"Successfully transferred to {agent_name}\",\n            \"name\": name,\n            \"tool_call_id\": tool_call_id,\n        }\n        return Command(  # (2)!\n            # highlight-next-line\n            goto=agent_name,  # (3)!\n            # highlight-next-line\n            update={\"messages\": state[\"messages\"] + [tool_message]},  # (4)!\n            # highlight-next-line\n            graph=Command.PARENT,  # (5)!\n        )\n    return handoff_tool\n\n# Handoffs\ntransfer_to_hotel_assistant = create_handoff_tool(\n    agent_name=\"hotel_assistant\",\n    description=\"Transfer user to the hotel-booking assistant.\",\n)\ntransfer_to_flight_assistant = create_handoff_tool(\n    agent_name=\"flight_assistant\",\n    description=\"Transfer user to the flight-booking assistant.\",\n)\n\n# Simple agent tools\ndef book_hotel(hotel_name: str):\n    \"\"\"Book a hotel\"\"\"\n    return f\"Successfully booked a stay at {hotel_name}.\"\n\ndef book_flight(from_airport: str, to_airport: str):\n    \"\"\"Book a flight\"\"\"\n    return f\"Successfully booked a flight from {from_airport} to {to_airport}.\"\n\n# Define agents\nflight_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    # highlight-next-line\n    tools=[book_flight, transfer_to_hotel_assistant],\n    prompt=\"You are a flight booking assistant\",\n    # highlight-next-line\n    name=\"flight_assistant\"\n)\nhotel_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    # highlight-next-line\n    tools=[book_hotel, transfer_to_flight_assistant],\n    prompt=\"You are a hotel booking assistant\",\n    # highlight-next-line\n    name=\"hotel_assistant\"\n)\n\n# Define multi-agent graph\nmulti_agent_graph = (\n    StateGraph(MessagesState)\n    .add_node(flight_assistant)\n    .add_node(hotel_assistant)\n    .add_edge(START, \"flight_assistant\")\n    .compile()\n)\n\n# Run the multi-agent graph\nfor chunk in multi_agent_graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\"\n            }\n        ]\n    }\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <ol> <li>Access agent's state</li> <li>The <code>Command</code> primitive allows specifying a state update and a node transition as a single operation, making it useful for implementing handoffs.</li> <li>Name of the agent or node to hand off to.</li> <li>Take the agent's messages and add them to the parent's state as part of the handoff. The next agent will see the parent state.</li> <li>Indicate to LangGraph that we need to navigate to agent node in a parent multi-agent graph.    :::</li> </ol> <p>:::js</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport {\n  StateGraph,\n  START,\n  MessagesZodState,\n  Command,\n} from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nfunction createHandoffTool({\n  agentName,\n  description,\n}: {\n  agentName: string;\n  description?: string;\n}) {\n  const name = `transfer_to_${agentName}`;\n  const toolDescription = description || `Transfer to ${agentName}`;\n\n  return tool(\n    async (_, config) =&gt; {\n      const toolMessage = {\n        role: \"tool\" as const,\n        content: `Successfully transferred to ${agentName}`,\n        name: name,\n        tool_call_id: config.toolCall?.id!,\n      };\n      return new Command({\n        // (2)!\n        // highlight-next-line\n        goto: agentName, // (3)!\n        // highlight-next-line\n        update: { messages: [toolMessage] }, // (4)!\n        // highlight-next-line\n        graph: Command.PARENT, // (5)!\n      });\n    },\n    {\n      name,\n      description: toolDescription,\n      schema: z.object({}),\n    }\n  );\n}\n\n// Handoffs\nconst transferToHotelAssistant = createHandoffTool({\n  agentName: \"hotel_assistant\",\n  description: \"Transfer user to the hotel-booking assistant.\",\n});\n\nconst transferToFlightAssistant = createHandoffTool({\n  agentName: \"flight_assistant\",\n  description: \"Transfer user to the flight-booking assistant.\",\n});\n\n// Simple agent tools\nconst bookHotel = tool(\n  async ({ hotelName }) =&gt; {\n    /**Book a hotel*/\n    return `Successfully booked a stay at ${hotelName}.`;\n  },\n  {\n    name: \"book_hotel\",\n    description: \"Book a hotel\",\n    schema: z.object({\n      hotelName: z.string().describe(\"Name of the hotel to book\"),\n    }),\n  }\n);\n\nconst bookFlight = tool(\n  async ({ fromAirport, toAirport }) =&gt; {\n    /**Book a flight*/\n    return `Successfully booked a flight from ${fromAirport} to ${toAirport}.`;\n  },\n  {\n    name: \"book_flight\",\n    description: \"Book a flight\",\n    schema: z.object({\n      fromAirport: z.string().describe(\"Departure airport code\"),\n      toAirport: z.string().describe(\"Arrival airport code\"),\n    }),\n  }\n);\n\n// Define agents\nconst flightAssistant = createReactAgent({\n  llm: new ChatAnthropic({ model: \"anthropic:claude-3-5-sonnet-latest\" }),\n  // highlight-next-line\n  tools: [bookFlight, transferToHotelAssistant],\n  stateModifier: \"You are a flight booking assistant\",\n  // highlight-next-line\n  name: \"flight_assistant\",\n});\n\nconst hotelAssistant = createReactAgent({\n  llm: new ChatAnthropic({ model: \"anthropic:claude-3-5-sonnet-latest\" }),\n  // highlight-next-line\n  tools: [bookHotel, transferToFlightAssistant],\n  stateModifier: \"You are a hotel booking assistant\",\n  // highlight-next-line\n  name: \"hotel_assistant\",\n});\n\n// Define multi-agent graph\nconst multiAgentGraph = new StateGraph(MessagesZodState)\n  .addNode(\"flight_assistant\", flightAssistant)\n  .addNode(\"hotel_assistant\", hotelAssistant)\n  .addEdge(START, \"flight_assistant\")\n  .compile();\n\n// Run the multi-agent graph\nfor await (const chunk of multiAgentGraph.stream({\n  messages: [\n    {\n      role: \"user\",\n      content: \"book a flight from BOS to JFK and a stay at McKittrick Hotel\",\n    },\n  ],\n})) {\n  console.log(chunk);\n  console.log(\"\\n\");\n}\n</code></pre> <ol> <li>Access agent's state</li> <li>The <code>Command</code> primitive allows specifying a state update and a node transition as a single operation, making it useful for implementing handoffs.</li> <li>Name of the agent or node to hand off to.</li> <li>Take the agent's messages and add them to the parent's state as part of the handoff. The next agent will see the parent state.</li> <li>Indicate to LangGraph that we need to navigate to agent node in a parent multi-agent graph.</li> </ol> <p>:::</p> <p>Note</p> <p>This handoff implementation assumes that:</p> <ul> <li>each agent receives overall message history (across all agents) in the multi-agent system as its input</li> <li>each agent outputs its internal messages history to the overall message history of the multi-agent system</li> </ul> <p>:::python Check out LangGraph supervisor and swarm documentation to learn how to customize handoffs. :::</p> <p>:::js Check out LangGraph supervisor and swarm documentation to learn how to customize handoffs. :::</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/overview/", "title": "Agent development using prebuilt components", "text": "<p>LangGraph provides both low-level primitives and high-level prebuilt components for building agent-based applications. This section focuses on the prebuilt, ready-to-use components designed to help you construct agentic systems quickly and reliably\u2014without the need to implement orchestration, memory, or human feedback handling from scratch.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/overview/#what-is-an-agent", "title": "What is an agent?", "text": "<p>An agent consists of three components: a large language model (LLM), a set of tools it can use, and a prompt that provides instructions.</p> <p>The LLM operates in a loop. In each iteration, it selects a tool to invoke, provides input, receives the result (an observation), and uses that observation to inform the next action. The loop continues until a stopping condition is met \u2014 typically when the agent has gathered enough information to respond to the user.</p> <p></p> Agent loop: the LLM selects tools and uses their outputs to fulfill a user request.", "tags": ["agent"], "boost": 2}, {"location": "agents/overview/#key-features", "title": "Key features", "text": "<p>LangGraph includes several capabilities essential for building robust, production-ready agentic systems:</p> <ul> <li>Memory integration: Native support for short-term (session-based) and long-term (persistent across sessions) memory, enabling stateful behaviors in chatbots and assistants.</li> <li>Human-in-the-loop control: Execution can pause indefinitely to await human feedback\u2014unlike websocket-based solutions limited to real-time interaction. This enables asynchronous approval, correction, or intervention at any point in the workflow.</li> <li>Streaming support: Real-time streaming of agent state, model tokens, tool outputs, or combined streams.</li> <li>Deployment tooling: Includes infrastructure-free deployment tools. LangGraph Platform supports testing, debugging, and deployment.</li> <li>Studio: A visual IDE for inspecting and debugging workflows.</li> <li>Supports multiple deployment options for production.</li> </ul>", "tags": ["agent"], "boost": 2}, {"location": "agents/overview/#high-level-building-blocks", "title": "High-level building blocks", "text": "<p>LangGraph comes with a set of prebuilt components that implement common agent behaviors and workflows. These abstractions are built on top of the LangGraph framework, offering a faster path to production while remaining flexible for advanced customization.</p> <p>Using LangGraph for agent development allows you to focus on your application's logic and behavior, instead of building and maintaining the supporting infrastructure for state, memory, and human feedback.</p> <p>:::python</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/overview/#package-ecosystem", "title": "Package ecosystem", "text": "<p>The high-level components are organized into several packages, each with a specific focus.</p> Package Description Installation <code>langgraph-prebuilt</code> (part of <code>langgraph</code>) Prebuilt components to create agents <code>pip install -U langgraph langchain</code> <code>langgraph-supervisor</code> Tools for building supervisor agents <code>pip install -U langgraph-supervisor</code> <code>langgraph-swarm</code> Tools for building a swarm multi-agent system <code>pip install -U langgraph-swarm</code> <code>langchain-mcp-adapters</code> Interfaces to MCP servers for tool and resource integration <code>pip install -U langchain-mcp-adapters</code> <code>langmem</code> Agent memory management: short-term and long-term <code>pip install -U langmem</code> <code>agentevals</code> Utilities to evaluate agent performance <code>pip install -U agentevals</code>", "tags": ["agent"], "boost": 2}, {"location": "agents/overview/#visualize-an-agent-graph", "title": "Visualize an agent graph", "text": "<p>Use the following tool to visualize the graph generated by @[<code>create_react_agent</code>][create_react_agent] and to view an outline of the corresponding code. It allows you to explore the infrastructure of the agent as defined by the presence of:</p> <ul> <li><code>tools</code>: A list of tools (functions, APIs, or other callable objects) that the agent can use to perform tasks.</li> <li><code>pre_model_hook</code>: A function that is called before the model is invoked. It can be used to condense messages or perform other preprocessing tasks.</li> <li><code>post_model_hook</code>: A function that is called after the model is invoked. It can be used to implement guardrails, human-in-the-loop flows, or other postprocessing tasks.</li> <li><code>response_format</code>: A data structure used to constrain the type of the final output, e.g., a <code>pydantic</code> <code>BaseModel</code>.</li> </ul> Features <code>tools</code> <code>pre_model_hook</code> <code>post_model_hook</code> <code>response_format</code> Graph <p>The following code snippet shows how to create the above agent (and underlying graph) with @[<code>create_react_agent</code>][create_react_agent]:</p> <pre><code></code></pre> <p>:::</p> <p>:::js</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/overview/#package-ecosystem_1", "title": "Package ecosystem", "text": "<p>The high-level components are organized into several packages, each with a specific focus.</p> Package Description Installation <code>langgraph</code> Prebuilt components to create agents <code>npm install @langchain/langgraph @langchain/core</code> <code>langgraph-supervisor</code> Tools for building supervisor agents <code>npm install @langchain/langgraph-supervisor</code> <code>langgraph-swarm</code> Tools for building a swarm multi-agent system <code>npm install @langchain/langgraph-swarm</code> <code>langchain-mcp-adapters</code> Interfaces to MCP servers for tool and resource integration <code>npm install @langchain/mcp-adapters</code> <code>agentevals</code> Utilities to evaluate agent performance <code>npm install agentevals</code>", "tags": ["agent"], "boost": 2}, {"location": "agents/overview/#visualize-an-agent-graph_1", "title": "Visualize an agent graph", "text": "<p>Use the following tool to visualize the graph generated by @[<code>createReactAgent</code>][create_react_agent] and to view an outline of the corresponding code. It allows you to explore the infrastructure of the agent as defined by the presence of:</p> <ul> <li><code>tools</code>: A list of tools (functions, APIs, or other callable objects) that the agent can use to perform tasks.</li> <li><code>preModelHook</code>: A function that is called before the model is invoked. It can be used to condense messages or perform other preprocessing tasks.</li> <li><code>postModelHook</code>: A function that is called after the model is invoked. It can be used to implement guardrails, human-in-the-loop flows, or other postprocessing tasks.</li> <li><code>responseFormat</code>: A data structure used to constrain the type of the final output (via Zod schemas).</li> </ul> Features <code>tools</code> <code>preModelHook</code> <code>postModelHook</code> <code>responseFormat</code> Graph <p>The following code snippet shows how to create the above agent (and underlying graph) with @[<code>createReactAgent</code>][create_react_agent]:</p> <pre><code></code></pre> <p>:::</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/run_agents/", "title": "Running agents", "text": "<p>Agents support both synchronous and asynchronous execution using either <code>.invoke()</code> / <code>await .ainvoke()</code> for full responses, or <code>.stream()</code> / <code>.astream()</code> for incremental streaming output. This section explains how to provide input, interpret output, enable streaming, and control execution limits.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/run_agents/#basic-usage", "title": "Basic usage", "text": "<p>Agents can be executed in two primary modes:</p> <p>:::python</p> <ul> <li>Synchronous using <code>.invoke()</code> or <code>.stream()</code></li> <li>Asynchronous using <code>await .ainvoke()</code> or <code>async for</code> with <code>.astream()</code>   :::</li> </ul> <p>:::js</p> <ul> <li>Synchronous using <code>.invoke()</code> or <code>.stream()</code></li> <li>Asynchronous using <code>await .invoke()</code> or <code>for await</code> with <code>.stream()</code>   :::</li> </ul> <p>:::python</p> Sync invocationAsync invocation <pre><code>from langgraph.prebuilt import create_react_agent\n\nagent = create_react_agent(...)\n\n# highlight-next-line\nresponse = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]})\n</code></pre> <pre><code>from langgraph.prebuilt import create_react_agent\n\nagent = create_react_agent(...)\n# highlight-next-line\nresponse = await agent.ainvoke({\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]})\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst agent = createReactAgent(...);\n// highlight-next-line\nconst response = await agent.invoke({\n    \"messages\": [\n        { \"role\": \"user\", \"content\": \"what is the weather in sf\" }\n    ]\n});\n</code></pre> <p>:::</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/run_agents/#inputs-and-outputs", "title": "Inputs and outputs", "text": "<p>Agents use a language model that expects a list of <code>messages</code> as an input. Therefore, agent inputs and outputs are stored as a list of <code>messages</code> under the <code>messages</code> key in the agent state.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/run_agents/#input-format", "title": "Input format", "text": "<p>Agent input must be a dictionary with a <code>messages</code> key. Supported formats are:</p> <p>:::python | Format | Example | |--------------------|-------------------------------------------------------------------------------------------------------------------------------| | String | <code>{\"messages\": \"Hello\"}</code> \u2014 Interpreted as a HumanMessage | | Message dictionary | <code>{\"messages\": {\"role\": \"user\", \"content\": \"Hello\"}}</code> | | List of messages | <code>{\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}</code> | | With custom state | <code>{\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}], \"user_name\": \"Alice\"}</code> \u2014 If using a custom <code>state_schema</code> | :::</p> <p>:::js | Format | Example | |--------------------|-------------------------------------------------------------------------------------------------------------------------------| | String | <code>{\"messages\": \"Hello\"}</code> \u2014 Interpreted as a HumanMessage | | Message dictionary | <code>{\"messages\": {\"role\": \"user\", \"content\": \"Hello\"}}</code> | | List of messages | <code>{\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}</code> | | With custom state | <code>{\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}], \"user_name\": \"Alice\"}</code> \u2014 If using a custom state definition | :::</p> <p>:::python Messages are automatically converted into LangChain's internal message format. You can read more about LangChain messages in the LangChain documentation. :::</p> <p>:::js Messages are automatically converted into LangChain's internal message format. You can read more about LangChain messages in the LangChain documentation. :::</p> <p>Using custom agent state</p> <p>:::python You can provide additional fields defined in your agent's state schema directly in the input dictionary. This allows dynamic behavior based on runtime data or prior tool outputs. See the context guide for full details. :::</p> <p>:::js You can provide additional fields defined in your agent's state directly in the state definition. This allows dynamic behavior based on runtime data or prior tool outputs. See the context guide for full details. :::</p> <p>Note</p> <p>:::python A string input for <code>messages</code> is converted to a HumanMessage. This behavior differs from the <code>prompt</code> parameter in <code>create_react_agent</code>, which is interpreted as a SystemMessage when passed as a string. :::</p> <p>:::js A string input for <code>messages</code> is converted to a HumanMessage. This behavior differs from the <code>prompt</code> parameter in <code>createReactAgent</code>, which is interpreted as a SystemMessage when passed as a string. :::</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/run_agents/#output-format", "title": "Output format", "text": "<p>:::python Agent output is a dictionary containing:</p> <ul> <li><code>messages</code>: A list of all messages exchanged during execution (user input, assistant replies, tool invocations).</li> <li>Optionally, <code>structured_response</code> if structured output is configured.</li> <li>If using a custom <code>state_schema</code>, additional keys corresponding to your defined fields may also be present in the output. These can hold updated state values from tool execution or prompt logic. :::</li> </ul> <p>:::js Agent output is a dictionary containing:</p> <ul> <li><code>messages</code>: A list of all messages exchanged during execution (user input, assistant replies, tool invocations).</li> <li>Optionally, <code>structuredResponse</code> if structured output is configured.</li> <li>If using a custom state definition, additional keys corresponding to your defined fields may also be present in the output. These can hold updated state values from tool execution or prompt logic. :::</li> </ul> <p>See the context guide for more details on working with custom state schemas and accessing context.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/run_agents/#streaming-output", "title": "Streaming output", "text": "<p>Agents support streaming responses for more responsive applications. This includes:</p> <ul> <li>Progress updates after each step</li> <li>LLM tokens as they're generated</li> <li>Custom tool messages during execution</li> </ul> <p>Streaming is available in both sync and async modes:</p> <p>:::python</p> Sync streamingAsync streaming <pre><code>for chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    stream_mode=\"updates\"\n):\n    print(chunk)\n</code></pre> <pre><code>async for chunk in agent.astream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    stream_mode=\"updates\"\n):\n    print(chunk)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>for await (const chunk of agent.stream(\n  { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n  { streamMode: \"updates\" }\n)) {\n  console.log(chunk);\n}\n</code></pre> <p>:::</p> <p>Tip</p> <p>For full details, see the streaming guide.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/run_agents/#max-iterations", "title": "Max iterations", "text": "<p>:::python To control agent execution and avoid infinite loops, set a recursion limit. This defines the maximum number of steps the agent can take before raising a <code>GraphRecursionError</code>. You can configure <code>recursion_limit</code> at runtime or when defining agent via <code>.with_config()</code>: :::</p> <p>:::js To control agent execution and avoid infinite loops, set a recursion limit. This defines the maximum number of steps the agent can take before raising a <code>GraphRecursionError</code>. You can configure <code>recursionLimit</code> at runtime or when defining agent via <code>.withConfig()</code>: :::</p> <p>:::python</p> Runtime<code>.with_config()</code> <pre><code>from langgraph.errors import GraphRecursionError\nfrom langgraph.prebuilt import create_react_agent\n\nmax_iterations = 3\n# highlight-next-line\nrecursion_limit = 2 * max_iterations + 1\nagent = create_react_agent(\n    model=\"anthropic:claude-3-5-haiku-latest\",\n    tools=[get_weather]\n)\n\ntry:\n    response = agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf\"}]},\n        # highlight-next-line\n        {\"recursion_limit\": recursion_limit},\n    )\nexcept GraphRecursionError:\n    print(\"Agent stopped due to max iterations.\")\n</code></pre> <pre><code>from langgraph.errors import GraphRecursionError\nfrom langgraph.prebuilt import create_react_agent\n\nmax_iterations = 3\n# highlight-next-line\nrecursion_limit = 2 * max_iterations + 1\nagent = create_react_agent(\n    model=\"anthropic:claude-3-5-haiku-latest\",\n    tools=[get_weather]\n)\n# highlight-next-line\nagent_with_recursion_limit = agent.with_config(recursion_limit=recursion_limit)\n\ntry:\n    response = agent_with_recursion_limit.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf\"}]},\n    )\nexcept GraphRecursionError:\n    print(\"Agent stopped due to max iterations.\")\n</code></pre> <p>:::</p> <p>:::js</p> Runtime<code>.withConfig()</code> <pre><code>import { GraphRecursionError } from \"@langchain/langgraph\";\nimport { ChatAnthropic } from \"@langchain/langgraph/prebuilt\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst maxIterations = 3;\n// highlight-next-line\nconst recursionLimit = 2 * maxIterations + 1;\nconst agent = createReactAgent({\n    llm: new ChatAnthropic({ model: \"claude-3-5-haiku-latest\" }),\n    tools: [getWeather]\n});\n\ntry {\n    const response = await agent.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf\"}]},\n        // highlight-next-line\n        { recursionLimit }\n    );\n} catch (error) {\n    if (error instanceof GraphRecursionError) {\n        console.log(\"Agent stopped due to max iterations.\");\n    }\n}\n</code></pre> <pre><code>import { GraphRecursionError } from \"@langchain/langgraph\";\nimport { ChatAnthropic } from \"@langchain/langgraph/prebuilt\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst maxIterations = 3;\n// highlight-next-line\nconst recursionLimit = 2 * maxIterations + 1;\nconst agent = createReactAgent({\n    llm: new ChatAnthropic({ model: \"claude-3-5-haiku-latest\" }),\n    tools: [getWeather]\n});\n// highlight-next-line\nconst agentWithRecursionLimit = agent.withConfig({ recursionLimit });\n\ntry {\n    const response = await agentWithRecursionLimit.invoke(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf\"}]},\n    );\n} catch (error) {\n    if (error instanceof GraphRecursionError) {\n        console.log(\"Agent stopped due to max iterations.\");\n    }\n}\n</code></pre> <p>:::</p> <p>:::python</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/run_agents/#additional-resources", "title": "Additional Resources", "text": "<ul> <li>Async programming in LangChain   :::</li> </ul>", "tags": ["agent"], "boost": 2}, {"location": "agents/ui/", "title": "UI", "text": "<p>You can use a prebuilt chat UI for interacting with any LangGraph agent through the Agent Chat UI. Using the deployed version is the quickest way to get started, and allows you to interact with both local and deployed graphs.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/ui/#run-agent-in-ui", "title": "Run agent in UI", "text": "<p>First, set up LangGraph API server locally or deploy your agent on LangGraph Platform.</p> <p>Then, navigate to Agent Chat UI, or clone the repository and run the dev server locally:</p> <p>Tip</p> <p>UI has out-of-box support for rendering tool calls, and tool result messages. To customize what messages are shown, see the Hiding Messages in the Chat section in the Agent Chat UI documentation.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/ui/#add-human-in-the-loop", "title": "Add human-in-the-loop", "text": "<p>Agent Chat UI has full support for human-in-the-loop workflows. To try it out, replace the agent code in <code>src/agent/graph.py</code> (from the deployment guide) with this agent implementation:</p> <p>Important</p> <p>Agent Chat UI works best if your LangGraph agent interrupts using the @[<code>HumanInterrupt</code> schema][HumanInterrupt]. If you do not use that schema, the Agent Chat UI will be able to render the input passed to the <code>interrupt</code> function, but it will not have full support for resuming your graph.</p>", "tags": ["agent"], "boost": 2}, {"location": "agents/ui/#generative-ui", "title": "Generative UI", "text": "<p>You can also use generative UI in the Agent Chat UI.</p> <p>Generative UI allows you to define React components, and push them to the UI from the LangGraph server. For more documentation on building generative UI LangGraph agents, read these docs.</p>", "tags": ["agent"], "boost": 2}, {"location": "cloud/reference/sdk/python_sdk_ref/", "title": "Python SDK Reference", "text": "<p>::: langgraph_sdk.client     handler: python</p> <p>::: langgraph_sdk.schema     handler: python</p> <p>::: langgraph_sdk.auth     handler: python</p> <p>::: langgraph_sdk.auth.types     handler: python</p> <p>::: langgraph_sdk.auth.exceptions     handler: python</p>"}, {"location": "concepts/agentic_concepts/", "title": "Agent architectures", "text": "<p>Many LLM applications implement a particular control flow of steps before and / or after LLM calls. As an example, RAG performs retrieval of documents relevant to a user question, and passes those documents to an LLM in order to ground the model's response in the provided document context. </p> <p>Instead of hard-coding a fixed control flow, we sometimes want LLM systems that can pick their own control flow to solve more complex problems! This is one definition of an agent: an agent is a system that uses an LLM to decide the control flow of an application. There are many ways that an LLM can control application:</p> <ul> <li>An LLM can route between two potential paths</li> <li>An LLM can decide which of many tools to call</li> <li>An LLM can decide whether the generated answer is sufficient or more work is needed</li> </ul> <p>As a result, there are many different types of agent architectures, which give an LLM varying levels of control. </p> <p></p>", "boost": 2}, {"location": "concepts/agentic_concepts/#router", "title": "Router", "text": "<p>A router allows an LLM to select a single step from a specified set of options. This is an agent architecture that exhibits a relatively limited level of control because the LLM usually focuses on making a single decision and produces a specific output from a limited set of pre-defined options. Routers typically employ a few different concepts to achieve this.</p>", "boost": 2}, {"location": "concepts/agentic_concepts/#structured-output", "title": "Structured Output", "text": "<p>Structured outputs with LLMs work by providing a specific format or schema that the LLM should follow in its response. This is similar to tool calling, but more general. While tool calling typically involves selecting and using predefined functions, structured outputs can be used for any type of formatted response. Common methods to achieve structured outputs include:</p> <ol> <li>Prompt engineering: Instructing the LLM to respond in a specific format via the system prompt.</li> <li>Output parsers: Using post-processing to extract structured data from LLM responses.</li> <li>Tool calling: Leveraging built-in tool calling capabilities of some LLMs to generate structured outputs.</li> </ol> <p>Structured outputs are crucial for routing as they ensure the LLM's decision can be reliably interpreted and acted upon by the system. Learn more about structured outputs in this how-to guide.</p>", "boost": 2}, {"location": "concepts/agentic_concepts/#tool-calling-agent", "title": "Tool-calling agent", "text": "<p>While a router allows an LLM to make a single decision, more complex agent architectures expand the LLM's control in two key ways:</p> <ol> <li>Multi-step decision making: The LLM can make a series of decisions, one after another, instead of just one.</li> <li>Tool access: The LLM can choose from and use a variety of tools to accomplish tasks.</li> </ol> <p>ReAct is a popular general purpose agent architecture that combines these expansions, integrating three core concepts. </p> <ol> <li>Tool calling: Allowing the LLM to select and use various tools as needed.</li> <li>Memory: Enabling the agent to retain and use information from previous steps.</li> <li>Planning: Empowering the LLM to create and follow multi-step plans to achieve goals.</li> </ol> <p>This architecture allows for more complex and flexible agent behaviors, going beyond simple routing to enable dynamic problem-solving with multiple steps. Unlike the original paper, today's agents rely on LLMs' tool calling capabilities and operate on a list of messages.</p> <p>In LangGraph, you can use the prebuilt agent to get started with tool-calling agents.</p>", "boost": 2}, {"location": "concepts/agentic_concepts/#tool-calling", "title": "Tool calling", "text": "<p>Tools are useful whenever you want an agent to interact with external systems. External systems (e.g., APIs) often require a particular input schema or payload, rather than natural language. When we bind an API, for example, as a tool, we give the model awareness of the required input schema. The model will choose to call a tool based upon the natural language input from the user and it will return an output that adheres to the tool's required schema. </p> <p>Many LLM providers support tool calling and tool calling interface in LangChain is simple: you can simply pass any Python <code>function</code> into <code>ChatModel.bind_tools(function)</code>.</p> <p></p>", "boost": 2}, {"location": "concepts/agentic_concepts/#memory", "title": "Memory", "text": "<p>Memory is crucial for agents, enabling them to retain and utilize information across multiple steps of problem-solving. It operates on different scales:</p> <ol> <li>Short-term memory: Allows the agent to access information acquired during earlier steps in a sequence.</li> <li>Long-term memory: Enables the agent to recall information from previous interactions, such as past messages in a conversation.</li> </ol> <p>LangGraph provides full control over memory implementation:</p> <ul> <li><code>State</code>: User-defined schema specifying the exact structure of memory to retain.</li> <li><code>Checkpointer</code>: Mechanism to store state at every step across different interactions within a session.</li> <li><code>Store</code>: Mechanism to store user-specific or application-level data across sessions.</li> </ul> <p>This flexible approach allows you to tailor the memory system to your specific agent architecture needs. Effective memory management enhances an agent's ability to maintain context, learn from past experiences, and make more informed decisions over time. For a practical guide on adding and managing memory, see Memory.</p>", "boost": 2}, {"location": "concepts/agentic_concepts/#planning", "title": "Planning", "text": "<p>In a tool-calling agent, an LLM is called repeatedly in a while-loop. At each step the agent decides which tools to call, and what the inputs to those tools should be. Those tools are then executed, and the outputs are fed back into the LLM as observations. The while-loop terminates when the agent decides it has enough information to solve the user request and it is not worth calling any more tools.</p>", "boost": 2}, {"location": "concepts/agentic_concepts/#custom-agent-architectures", "title": "Custom agent architectures", "text": "<p>While routers and tool-calling agents (like ReAct) are common, customizing agent architectures often leads to better performance for specific tasks. LangGraph offers several powerful features for building tailored agent systems:</p>", "boost": 2}, {"location": "concepts/agentic_concepts/#human-in-the-loop", "title": "Human-in-the-loop", "text": "<p>Human involvement can significantly enhance agent reliability, especially for sensitive tasks. This can involve:</p> <ul> <li>Approving specific actions</li> <li>Providing feedback to update the agent's state</li> <li>Offering guidance in complex decision-making processes</li> </ul> <p>Human-in-the-loop patterns are crucial when full automation isn't feasible or desirable. Learn more in our human-in-the-loop guide.</p>", "boost": 2}, {"location": "concepts/agentic_concepts/#parallelization", "title": "Parallelization", "text": "<p>Parallel processing is vital for efficient multi-agent systems and complex tasks. LangGraph supports parallelization through its Send API, enabling:</p> <ul> <li>Concurrent processing of multiple states</li> <li>Implementation of map-reduce-like operations</li> <li>Efficient handling of independent subtasks</li> </ul> <p>For practical implementation, see our map-reduce tutorial</p>", "boost": 2}, {"location": "concepts/agentic_concepts/#subgraphs", "title": "Subgraphs", "text": "<p>Subgraphs are essential for managing complex agent architectures, particularly in multi-agent systems. They allow:</p> <ul> <li>Isolated state management for individual agents</li> <li>Hierarchical organization of agent teams</li> <li>Controlled communication between agents and the main system</li> </ul> <p>Subgraphs communicate with the parent graph through overlapping keys in the state schema. This enables flexible, modular agent design. For implementation details, refer to our subgraph how-to guide.</p>", "boost": 2}, {"location": "concepts/agentic_concepts/#reflection", "title": "Reflection", "text": "<p>Reflection mechanisms can significantly improve agent reliability by:</p> <ol> <li>Evaluating task completion and correctness</li> <li>Providing feedback for iterative improvement</li> <li>Enabling self-correction and learning</li> </ol> <p>While often LLM-based, reflection can also use deterministic methods. For instance, in coding tasks, compilation errors can serve as feedback. This approach is demonstrated in this video using LangGraph for self-corrective code generation.</p> <p>By leveraging these features, LangGraph enables the creation of sophisticated, task-specific agent architectures that can handle complex workflows, collaborate effectively, and continuously improve their performance.</p>", "boost": 2}, {"location": "concepts/durable_execution/", "title": "Durable Execution", "text": "<p>Durable execution is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require human-in-the-loop, where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps -- even after a significant delay (e.g., a week later).</p> <p>LangGraph's built-in persistence layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted -- whether by a system failure or for human-in-the-loop interactions -- it can be resumed from its last recorded state.</p> <p>Tip</p> <p>If you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures. To make the most of durable execution, ensure that your workflow is designed to be deterministic and idempotent and wrap any side effects or non-deterministic operations inside tasks. You can use tasks from both the StateGraph (Graph API) and the Functional API.</p>", "boost": 2}, {"location": "concepts/durable_execution/#requirements", "title": "Requirements", "text": "<p>To leverage durable execution in LangGraph, you need to:</p> <ol> <li>Enable persistence in your workflow by specifying a checkpointer that will save workflow progress.</li> <li>Specify a thread identifier when executing a workflow. This will track the execution history for a particular instance of the workflow.</li> </ol> <p>:::python  3. Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside @[tasks][task] to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see Determinism and Consistent Replay. :::</p> <p>:::js  3. Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside @[tasks][task] to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see Determinism and Consistent Replay. :::</p>", "boost": 2}, {"location": "concepts/durable_execution/#determinism-and-consistent-replay", "title": "Determinism and Consistent Replay", "text": "<p>When you resume a workflow run, the code does NOT resume from the same line of code where execution stopped; instead, it will identify an appropriate starting point from which to pick up where it left off. This means that the workflow will replay all steps from the starting point until it reaches the point where it was stopped.</p> <p>As a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside tasks or nodes.</p> <p>To ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines:</p> <ul> <li>Avoid Repeating Work: If a node contains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate task. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer.</li> <li>Encapsulate Non-Deterministic Operations: Wrap any code that might yield non-deterministic results (e.g., random number generation) inside tasks or nodes. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes.</li> <li>Use Idempotent Operations: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a task starts but fails to complete successfully, the workflow's resumption will re-run the task, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution.</li> </ul> <p>:::python For some examples of pitfalls to avoid, see the Common Pitfalls section in the functional API, which shows how to structure your code using tasks to avoid these issues. The same principles apply to the @[StateGraph (Graph API)][StateGraph]. :::</p> <p>:::js For some examples of pitfalls to avoid, see the Common Pitfalls section in the functional API, which shows how to structure your code using tasks to avoid these issues. The same principles apply to the @[StateGraph (Graph API)][StateGraph]. :::</p>", "boost": 2}, {"location": "concepts/durable_execution/#durability-modes", "title": "Durability modes", "text": "<p>LangGraph supports three durability modes that allow you to balance performance and data consistency based on your application's requirements. The durability modes, from least to most durable, are as follows:</p> <ul> <li><code>\"exit\"</code></li> <li><code>\"async\"</code></li> <li><code>\"sync\"</code></li> </ul> <p>A higher durability mode add more overhead to the workflow execution.</p> <p>Added in v0.6.0</p> <p>Use the <code>durability</code> parameter instead of <code>checkpoint_during</code> (deprecated in v0.6.0) for persistence policy management:</p> <ul> <li><code>durability=\"async\"</code> replaces <code>checkpoint_during=True</code></li> <li><code>durability=\"exit\"</code> replaces <code>checkpoint_during=False</code></li> </ul> <p>for persistence policy management, with the following mapping:</p> <ul> <li><code>checkpoint_during=True</code> -&gt; <code>durability=\"async\"</code></li> <li><code>checkpoint_during=False</code> -&gt; <code>durability=\"exit\"</code></li> </ul>", "boost": 2}, {"location": "concepts/durable_execution/#exit", "title": "<code>\"exit\"</code>", "text": "<p>Changes are persisted only when graph execution completes (either successfully or with an error). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from mid-execution failures or interrupt the graph execution.</p>", "boost": 2}, {"location": "concepts/durable_execution/#async", "title": "<code>\"async\"</code>", "text": "<p>Changes are persisted asynchronously while the next step executes. This provides good performance and durability, but there's a small risk that checkpoints might not be written if the process crashes during execution.</p>", "boost": 2}, {"location": "concepts/durable_execution/#sync", "title": "<code>\"sync\"</code>", "text": "<p>Changes are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead.</p> <p>You can specify the durability mode when calling any graph execution method:</p> <p>:::python</p> <pre><code>graph.stream(\n    {\"input\": \"test\"}, \n    durability=\"sync\"\n)\n</code></pre> <p>:::</p>", "boost": 2}, {"location": "concepts/durable_execution/#using-tasks-in-nodes", "title": "Using tasks in nodes", "text": "<p>If a node contains multiple operations, you may find it easier to convert each operation into a task rather than refactor the operations into individual nodes.</p> <p>:::python</p> OriginalWith task <pre><code>from typing import NotRequired\nfrom typing_extensions import TypedDict\nimport uuid\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph import StateGraph, START, END\nimport requests\n\n# Define a TypedDict to represent the state\nclass State(TypedDict):\n    url: str\n    result: NotRequired[str]\n\ndef call_api(state: State):\n    \"\"\"Example node that makes an API request.\"\"\"\n    # highlight-next-line\n    result = requests.get(state['url']).text[:100]  # Side-effect\n    return {\n        \"result\": result\n    }\n\n# Create a StateGraph builder and add a node for the call_api function\nbuilder = StateGraph(State)\nbuilder.add_node(\"call_api\", call_api)\n\n# Connect the start and end nodes to the call_api node\nbuilder.add_edge(START, \"call_api\")\nbuilder.add_edge(\"call_api\", END)\n\n# Specify a checkpointer\ncheckpointer = InMemorySaver()\n\n# Compile the graph with the checkpointer\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Define a config with a thread ID.\nthread_id = uuid.uuid4()\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\n\n# Invoke the graph\ngraph.invoke({\"url\": \"https://www.example.com\"}, config)\n</code></pre> <pre><code>from typing import NotRequired\nfrom typing_extensions import TypedDict\nimport uuid\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.func import task\nfrom langgraph.graph import StateGraph, START, END\nimport requests\n\n# Define a TypedDict to represent the state\nclass State(TypedDict):\n    urls: list[str]\n    result: NotRequired[list[str]]\n\n\n@task\ndef _make_request(url: str):\n    \"\"\"Make a request.\"\"\"\n    # highlight-next-line\n    return requests.get(url).text[:100]\n\ndef call_api(state: State):\n    \"\"\"Example node that makes an API request.\"\"\"\n    # highlight-next-line\n    requests = [_make_request(url) for url in state['urls']]\n    results = [request.result() for request in requests]\n    return {\n        \"results\": results\n    }\n\n# Create a StateGraph builder and add a node for the call_api function\nbuilder = StateGraph(State)\nbuilder.add_node(\"call_api\", call_api)\n\n# Connect the start and end nodes to the call_api node\nbuilder.add_edge(START, \"call_api\")\nbuilder.add_edge(\"call_api\", END)\n\n# Specify a checkpointer\ncheckpointer = InMemorySaver()\n\n# Compile the graph with the checkpointer\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Define a config with a thread ID.\nthread_id = uuid.uuid4()\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\n\n# Invoke the graph\ngraph.invoke({\"urls\": [\"https://www.example.com\"]}, config)\n</code></pre> <p>:::</p> <p>:::js</p> OriginalWith task <pre><code>import { StateGraph, START, END } from \"@langchain/langgraph\";\nimport { MemorySaver } from \"@langchain/langgraph\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport { z } from \"zod\";\n\n// Define a Zod schema to represent the state\nconst State = z.object({\n  url: z.string(),\n  result: z.string().optional(),\n});\n\nconst callApi = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // highlight-next-line\n  const response = await fetch(state.url);\n  const text = await response.text();\n  const result = text.slice(0, 100); // Side-effect\n  return {\n    result,\n  };\n};\n\n// Create a StateGraph builder and add a node for the callApi function\nconst builder = new StateGraph(State)\n  .addNode(\"callApi\", callApi)\n  .addEdge(START, \"callApi\")\n  .addEdge(\"callApi\", END);\n\n// Specify a checkpointer\nconst checkpointer = new MemorySaver();\n\n// Compile the graph with the checkpointer\nconst graph = builder.compile({ checkpointer });\n\n// Define a config with a thread ID.\nconst threadId = uuidv4();\nconst config = { configurable: { thread_id: threadId } };\n\n// Invoke the graph\nawait graph.invoke({ url: \"https://www.example.com\" }, config);\n</code></pre> <pre><code>import { StateGraph, START, END } from \"@langchain/langgraph\";\nimport { MemorySaver } from \"@langchain/langgraph\";\nimport { task } from \"@langchain/langgraph\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport { z } from \"zod\";\n\n// Define a Zod schema to represent the state\nconst State = z.object({\n  urls: z.array(z.string()),\n  results: z.array(z.string()).optional(),\n});\n\nconst makeRequest = task(\"makeRequest\", async (url: string) =&gt; {\n  // highlight-next-line\n  const response = await fetch(url);\n  const text = await response.text();\n  return text.slice(0, 100);\n});\n\nconst callApi = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // highlight-next-line\n  const requests = state.urls.map((url) =&gt; makeRequest(url));\n  const results = await Promise.all(requests);\n  return {\n    results,\n  };\n};\n\n// Create a StateGraph builder and add a node for the callApi function\nconst builder = new StateGraph(State)\n  .addNode(\"callApi\", callApi)\n  .addEdge(START, \"callApi\")\n  .addEdge(\"callApi\", END);\n\n// Specify a checkpointer\nconst checkpointer = new MemorySaver();\n\n// Compile the graph with the checkpointer\nconst graph = builder.compile({ checkpointer });\n\n// Define a config with a thread ID.\nconst threadId = uuidv4();\nconst config = { configurable: { thread_id: threadId } };\n\n// Invoke the graph\nawait graph.invoke({ urls: [\"https://www.example.com\"] }, config);\n</code></pre> <p>:::</p>", "boost": 2}, {"location": "concepts/durable_execution/#resuming-workflows", "title": "Resuming Workflows", "text": "<p>Once you have enabled durable execution in your workflow, you can resume execution for the following scenarios:</p> <p>:::python</p> <ul> <li>Pausing and Resuming Workflows: Use the @[interrupt][interrupt] function to pause a workflow at specific points and the @[Command] primitive to resume it with updated state. See Human-in-the-Loop for more details.</li> <li>Recovering from Failures: Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a <code>None</code> as the input value (see this example with the functional API).   :::</li> </ul> <p>:::js</p> <ul> <li>Pausing and Resuming Workflows: Use the @[interrupt][interrupt] function to pause a workflow at specific points and the @[Command] primitive to resume it with updated state. See Human-in-the-Loop for more details.</li> <li>Recovering from Failures: Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a <code>null</code> as the input value (see this example with the functional API).   :::</li> </ul>", "boost": 2}, {"location": "concepts/durable_execution/#starting-points-for-resuming-workflows", "title": "Starting Points for Resuming Workflows", "text": "<p>:::python</p> <ul> <li>If you're using a @[StateGraph (Graph API)][StateGraph], the starting point is the beginning of the node where execution stopped.</li> <li>If you're making a subgraph call inside a node, the starting point will be the parent node that called the subgraph that was halted.   Inside the subgraph, the starting point will be the specific node where execution stopped.</li> <li>If you're using the Functional API, the starting point is the beginning of the entrypoint where execution stopped.   :::</li> </ul> <p>:::js</p> <ul> <li>If you're using a StateGraph (Graph API), the starting point is the beginning of the node where execution stopped.</li> <li>If you're making a subgraph call inside a node, the starting point will be the parent node that called the subgraph that was halted.   Inside the subgraph, the starting point will be the specific node where execution stopped.</li> <li>If you're using the Functional API, the starting point is the beginning of the entrypoint where execution stopped.   :::</li> </ul>", "boost": 2}, {"location": "concepts/functional_api/", "title": "Functional API concepts", "text": "", "boost": 2}, {"location": "concepts/functional_api/#overview", "title": "Overview", "text": "<p>The Functional API allows you to add LangGraph's key features \u2014 persistence, memory, human-in-the-loop, and streaming \u2014 to your applications with minimal changes to your existing code.</p> <p>It is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as <code>if</code> statements, <code>for</code> loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model.</p> <p>The Functional API uses two key building blocks:</p> <p>:::python</p> <ul> <li><code>@entrypoint</code> \u2013 Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts.</li> <li><code>@task</code> \u2013 Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.   :::</li> </ul> <p>:::js</p> <ul> <li><code>entrypoint</code> \u2013 An entrypoint encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts.</li> <li><code>task</code> \u2013 Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously.   :::</li> </ul> <p>This provides a minimal abstraction for building workflows with state management and streaming.</p> <p>Tip</p> <p>For information on how to use the functional API, see Use Functional API.</p>", "boost": 2}, {"location": "concepts/functional_api/#functional-api-vs-graph-api", "title": "Functional API vs. Graph API", "text": "<p>For users who prefer a more declarative approach, LangGraph's Graph API allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application.</p> <p>Here are some key differences:</p> <ul> <li>Control flow: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write.</li> <li>Short-term memory: The GraphAPI requires declaring a State and may require defining reducers to manage updates to the graph state. <code>@entrypoint</code> and <code>@tasks</code> do not require explicit state management as their state is scoped to the function and is not shared across functions.</li> <li>Checkpointing: Both APIs generate and use checkpoints. In the Graph API a new checkpoint is generated after every superstep. In the Functional API, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint.</li> <li>Visualization: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime.</li> </ul>", "boost": 2}, {"location": "concepts/functional_api/#example", "title": "Example", "text": "<p>Below we demonstrate a simple application that writes an essay and interrupts to request human review.</p> <p>:::python</p> <pre><code>from langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import interrupt\n\n@task\ndef write_essay(topic: str) -&gt; str:\n    \"\"\"Write an essay about the given topic.\"\"\"\n    time.sleep(1) # A placeholder for a long-running task.\n    return f\"An essay about topic: {topic}\"\n\n@entrypoint(checkpointer=InMemorySaver())\ndef workflow(topic: str) -&gt; dict:\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\n    essay = write_essay(\"cat\").result()\n    is_approved = interrupt({\n        # Any json-serializable payload provided to interrupt as argument.\n        # It will be surfaced on the client side as an Interrupt when streaming data\n        # from the workflow.\n        \"essay\": essay, # The essay we want reviewed.\n        # We can add any additional information that we need.\n        # For example, introduce a key called \"action\" with some instructions.\n        \"action\": \"Please approve/reject the essay\",\n    })\n\n    return {\n        \"essay\": essay, # The essay that was generated\n        \"is_approved\": is_approved, # Response from HIL\n    }\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { MemorySaver, entrypoint, task, interrupt } from \"@langchain/langgraph\";\n\nconst writeEssay = task(\"writeEssay\", async (topic: string) =&gt; {\n  // A placeholder for a long-running task.\n  await new Promise((resolve) =&gt; setTimeout(resolve, 1000));\n  return `An essay about topic: ${topic}`;\n});\n\nconst workflow = entrypoint(\n  { checkpointer: new MemorySaver(), name: \"workflow\" },\n  async (topic: string) =&gt; {\n    const essay = await writeEssay(topic);\n    const isApproved = interrupt({\n      // Any json-serializable payload provided to interrupt as argument.\n      // It will be surfaced on the client side as an Interrupt when streaming data\n      // from the workflow.\n      essay, // The essay we want reviewed.\n      // We can add any additional information that we need.\n      // For example, introduce a key called \"action\" with some instructions.\n      action: \"Please approve/reject the essay\",\n    });\n\n    return {\n      essay, // The essay that was generated\n      isApproved, // Response from HIL\n    };\n  }\n);\n</code></pre> <p>:::</p> Detailed Explanation <p>This workflow will write an essay about the topic \"cat\" and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided.</p> <p>When the workflow is resumed, it executes from the very start, but because the result of the <code>writeEssay</code> task was already saved, the task result will be loaded from the checkpoint instead of being recomputed.</p> <p>:::python <pre><code>import time\nimport uuid\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import interrupt\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\n@task\ndef write_essay(topic: str) -&gt; str:\n    \"\"\"Write an essay about the given topic.\"\"\"\n    time.sleep(1)  # This is a placeholder for a long-running task.\n    return f\"An essay about topic: {topic}\"\n\n@entrypoint(checkpointer=InMemorySaver())\ndef workflow(topic: str) -&gt; dict:\n    \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\"\n    essay = write_essay(\"cat\").result()\n    is_approved = interrupt(\n        {\n            # Any json-serializable payload provided to interrupt as argument.\n            # It will be surfaced on the client side as an Interrupt when streaming data\n            # from the workflow.\n            \"essay\": essay,  # The essay we want reviewed.\n            # We can add any additional information that we need.\n            # For example, introduce a key called \"action\" with some instructions.\n            \"action\": \"Please approve/reject the essay\",\n        }\n    )\n    return {\n        \"essay\": essay,  # The essay that was generated\n        \"is_approved\": is_approved,  # Response from HIL\n    }\n\n\nthread_id = str(uuid.uuid4())\nconfig = {\"configurable\": {\"thread_id\": thread_id}}\nfor item in workflow.stream(\"cat\", config):\n    print(item)\n# &gt; {'write_essay': 'An essay about topic: cat'}\n# &gt; {\n# &gt;     '__interrupt__': (\n# &gt;        Interrupt(\n# &gt;            value={\n# &gt;                'essay': 'An essay about topic: cat',\n# &gt;                'action': 'Please approve/reject the essay'\n# &gt;            },\n# &gt;            id='b9b2b9d788f482663ced6dc755c9e981'\n# &gt;        ),\n# &gt;    )\n# &gt; }\n</code></pre></p> <p>An essay has been written and is ready for review. Once the review is provided, we can resume the workflow:</p> <pre><code>from langgraph.types import Command\n\n# Get review from a user (e.g., via a UI)\n# In this case, we're using a bool, but this can be any json-serializable value.\nhuman_review = True\n\nfor item in workflow.stream(Command(resume=human_review), config):\n    print(item)\n</code></pre> <pre><code>{'workflow': {'essay': 'An essay about topic: cat', 'is_approved': False}}\n</code></pre> <p>The workflow has been completed and the review has been added to the essay. :::</p> <p>:::js <pre><code>import { v4 as uuidv4 } from \"uuid\";\nimport { MemorySaver, entrypoint, task, interrupt } from \"@langchain/langgraph\";\n\nconst writeEssay = task(\"writeEssay\", async (topic: string) =&gt; {\n  // This is a placeholder for a long-running task.\n  await new Promise(resolve =&gt; setTimeout(resolve, 1000));\n  return `An essay about topic: ${topic}`;\n});\n\nconst workflow = entrypoint(\n  { checkpointer: new MemorySaver(), name: \"workflow\" },\n  async (topic: string) =&gt; {\n    const essay = await writeEssay(topic);\n    const isApproved = interrupt({\n      // Any json-serializable payload provided to interrupt as argument.\n      // It will be surfaced on the client side as an Interrupt when streaming data\n      // from the workflow.\n      essay, // The essay we want reviewed.\n      // We can add any additional information that we need.\n      // For example, introduce a key called \"action\" with some instructions.\n      action: \"Please approve/reject the essay\",\n    });\n\n    return {\n      essay, // The essay that was generated\n      isApproved, // Response from HIL\n    };\n  }\n);\n\nconst threadId = uuidv4();\n\nconst config = {\n  configurable: {\n    thread_id: threadId\n  }\n};\n\nfor await (const item of workflow.stream(\"cat\", config)) {\n  console.log(item);\n}\n</code></pre></p> <pre><code>{ writeEssay: 'An essay about topic: cat' }\n{\n  __interrupt__: [{\n    value: { essay: 'An essay about topic: cat', action: 'Please approve/reject the essay' },\n    resumable: true,\n    ns: ['workflow:f7b8508b-21c0-8b4c-5958-4e8de74d2684'],\n    when: 'during'\n  }]\n}\n</code></pre> <p>An essay has been written and is ready for review. Once the review is provided, we can resume the workflow:</p> <pre><code>import { Command } from \"@langchain/langgraph\";\n\n// Get review from a user (e.g., via a UI)\n// In this case, we're using a bool, but this can be any json-serializable value.\nconst humanReview = true;\n\nfor await (const item of workflow.stream(new Command({ resume: humanReview }), config)) {\n  console.log(item);\n}\n</code></pre> <pre><code>{ workflow: { essay: 'An essay about topic: cat', isApproved: true } }\n</code></pre> <p>The workflow has been completed and the review has been added to the essay. :::</p>", "boost": 2}, {"location": "concepts/functional_api/#entrypoint", "title": "Entrypoint", "text": "<p>:::python The @[<code>@entrypoint</code>][entrypoint] decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts. :::</p> <p>:::js The @[<code>entrypoint</code>][entrypoint] function can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling long-running tasks and interrupts. :::</p>", "boost": 2}, {"location": "concepts/functional_api/#definition", "title": "Definition", "text": "<p>:::python An entrypoint is defined by decorating a function with the <code>@entrypoint</code> decorator.</p> <p>The function must accept a single positional argument, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument.</p> <p>Decorating a function with an <code>entrypoint</code> produces a @[<code>Pregel</code>][Pregel.stream] instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).</p> <p>You will usually want to pass a checkpointer to the <code>@entrypoint</code> decorator to enable persistence and use features like human-in-the-loop.</p> SyncAsync <pre><code>from langgraph.func import entrypoint\n\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(some_input: dict) -&gt; int:\n    # some logic that may involve long-running tasks like API calls,\n    # and may be interrupted for human-in-the-loop.\n    ...\n    return result\n</code></pre> <pre><code>from langgraph.func import entrypoint\n\n@entrypoint(checkpointer=checkpointer)\nasync def my_workflow(some_input: dict) -&gt; int:\n    # some logic that may involve long-running tasks like API calls,\n    # and may be interrupted for human-in-the-loop\n    ...\n    return result\n</code></pre> <p>:::</p> <p>:::js An entrypoint is defined by calling the <code>entrypoint</code> function with configuration and a function.</p> <p>The function must accept a single positional argument, which serves as the workflow input. If you need to pass multiple pieces of data, use an object as the input type for the first argument.</p> <p>Creating an entrypoint with a function produces a workflow instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing).</p> <p>You will often want to pass a checkpointer to the <code>entrypoint</code> function to enable persistence and use features like human-in-the-loop.</p> <pre><code>import { entrypoint } from \"@langchain/langgraph\";\n\nconst myWorkflow = entrypoint(\n  { checkpointer, name: \"workflow\" },\n  async (someInput: Record&lt;string, any&gt;): Promise&lt;number&gt; =&gt; {\n    // some logic that may involve long-running tasks like API calls,\n    // and may be interrupted for human-in-the-loop\n    return result;\n  }\n);\n</code></pre> <p>:::</p> <p>Serialization</p> <p>The inputs and outputs of entrypoints must be JSON-serializable to support checkpointing. Please see the serialization section for more details.</p> <p>:::python</p>", "boost": 2}, {"location": "concepts/functional_api/#injectable-parameters", "title": "Injectable parameters", "text": "<p>When declaring an <code>entrypoint</code>, you can request access to additional parameters that will be injected automatically at run time. These parameters include:</p> Parameter Description previous Access the state associated with the previous <code>checkpoint</code> for the given thread. See short-term-memory. store An instance of [BaseStore][langgraph.store.base.BaseStore]. Useful for long-term memory. writer Use to access the StreamWriter when working with Async Python &lt; 3.11. See streaming with functional API for details. config For accessing run time configuration. See RunnableConfig for information. <p>Important</p> <p>Declare the parameters with the appropriate name and type annotation.</p> Requesting Injectable Parameters <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langgraph.func import entrypoint\nfrom langgraph.store.base import BaseStore\nfrom langgraph.store.memory import InMemoryStore\n\nin_memory_store = InMemoryStore(...)  # An instance of InMemoryStore for long-term memory\n\n@entrypoint(\n    checkpointer=checkpointer,  # Specify the checkpointer\n    store=in_memory_store  # Specify the store\n)\ndef my_workflow(\n    some_input: dict,  # The input (e.g., passed via `invoke`)\n    *,\n    previous: Any = None, # For short-term memory\n    store: BaseStore,  # For long-term memory\n    writer: StreamWriter,  # For streaming custom data\n    config: RunnableConfig  # For accessing the configuration passed to the entrypoint\n) -&gt; ...:\n</code></pre> <p>:::</p>", "boost": 2}, {"location": "concepts/functional_api/#executing", "title": "Executing", "text": "<p>:::python Using the <code>@entrypoint</code> yields a @[<code>Pregel</code>][Pregel.stream] object that can be executed using the <code>invoke</code>, <code>ainvoke</code>, <code>stream</code>, and <code>astream</code> methods.</p> InvokeAsync InvokeStreamAsync Stream <pre><code>config = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\nmy_workflow.invoke(some_input, config)  # Wait for the result synchronously\n</code></pre> <pre><code>config = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\nawait my_workflow.ainvoke(some_input, config)  # Await result asynchronously\n</code></pre> <pre><code>config = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nfor chunk in my_workflow.stream(some_input, config):\n    print(chunk)\n</code></pre> <pre><code>config = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nasync for chunk in my_workflow.astream(some_input, config):\n    print(chunk)\n</code></pre> <p>:::</p> <p>:::js Using the <code>entrypoint</code> function will return an object that can be executed using the <code>invoke</code> and <code>stream</code> methods.</p> InvokeStream <pre><code>const config = {\n  configurable: {\n    thread_id: \"some_thread_id\"\n  }\n};\nawait myWorkflow.invoke(someInput, config); // Wait for the result\n</code></pre> <pre><code>const config = {\n  configurable: {\n    thread_id: \"some_thread_id\"\n  }\n};\n\nfor await (const chunk of myWorkflow.stream(someInput, config)) {\n  console.log(chunk);\n}\n</code></pre> <p>:::</p>", "boost": 2}, {"location": "concepts/functional_api/#resuming", "title": "Resuming", "text": "<p>:::python Resuming an execution after an @[interrupt][interrupt] can be done by passing a resume value to the @[Command] primitive.</p> InvokeAsync InvokeStreamAsync Stream <pre><code>from langgraph.types import Command\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nmy_workflow.invoke(Command(resume=some_resume_value), config)\n</code></pre> <pre><code>from langgraph.types import Command\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nawait my_workflow.ainvoke(Command(resume=some_resume_value), config)\n</code></pre> <pre><code>from langgraph.types import Command\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nfor chunk in my_workflow.stream(Command(resume=some_resume_value), config):\n    print(chunk)\n</code></pre> <pre><code>from langgraph.types import Command\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nasync for chunk in my_workflow.astream(Command(resume=some_resume_value), config):\n    print(chunk)\n</code></pre> <p>:::</p> <p>:::js Resuming an execution after an @[interrupt][interrupt] can be done by passing a resume value to the @[<code>Command</code>][Command] primitive.</p> InvokeStream <pre><code>import { Command } from \"@langchain/langgraph\";\n\nconst config = {\n  configurable: {\n    thread_id: \"some_thread_id\"\n  }\n};\n\nawait myWorkflow.invoke(new Command({ resume: someResumeValue }), config);\n</code></pre> <pre><code>import { Command } from \"@langchain/langgraph\";\n\nconst config = {\n  configurable: {\n    thread_id: \"some_thread_id\"\n  }\n};\n\nconst stream = await myWorkflow.stream(\n  new Command({ resume: someResumableValue }),\n  config,\n)\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n</code></pre> <p>:::</p> <p>:::python</p> <p>Resuming after an error</p> <p>To resume after an error, run the <code>entrypoint</code> with a <code>None</code> and the same thread id (config).</p> <p>This assumes that the underlying error has been resolved and execution can proceed successfully.</p> InvokeAsync InvokeStreamAsync Stream <pre><code>config = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nmy_workflow.invoke(None, config)\n</code></pre> <pre><code>config = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nawait my_workflow.ainvoke(None, config)\n</code></pre> <pre><code>config = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nfor chunk in my_workflow.stream(None, config):\n    print(chunk)\n</code></pre> <pre><code>config = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nasync for chunk in my_workflow.astream(None, config):\n    print(chunk)\n</code></pre> <p>:::</p> <p>:::js</p> <p>Resuming after an error</p> <p>To resume after an error, run the <code>entrypoint</code> with <code>null</code> and the same thread id (config).</p> <p>This assumes that the underlying error has been resolved and execution can proceed successfully.</p> InvokeStream <pre><code>const config = {\n  configurable: {\n    thread_id: \"some_thread_id\"\n  }\n};\n\nawait myWorkflow.invoke(null, config);\n</code></pre> <pre><code>const config = {\n  configurable: {\n    thread_id: \"some_thread_id\"\n  }\n};\n\nfor await (const chunk of myWorkflow.stream(null, config)) {\n  console.log(chunk);\n}\n</code></pre> <p>:::</p>", "boost": 2}, {"location": "concepts/functional_api/#short-term-memory", "title": "Short-term memory", "text": "<p>When an <code>entrypoint</code> is defined with a <code>checkpointer</code>, it stores information between successive invocations on the same thread id in checkpoints.</p> <p>:::python This allows accessing the state from the previous invocation using the <code>previous</code> parameter.</p> <p>By default, the <code>previous</code> parameter is the return value of the previous invocation.</p> <pre><code>@entrypoint(checkpointer=checkpointer)\ndef my_workflow(number: int, *, previous: Any = None) -&gt; int:\n    previous = previous or 0\n    return number + previous\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread_id\"\n    }\n}\n\nmy_workflow.invoke(1, config)  # 1 (previous was None)\nmy_workflow.invoke(2, config)  # 3 (previous was 1 from the previous invocation)\n</code></pre> <p>:::</p> <p>:::js This allows accessing the state from the previous invocation using the <code>getPreviousState</code> function.</p> <p>By default, the <code>getPreviousState</code> function returns the return value of the previous invocation.</p> <pre><code>import { entrypoint, getPreviousState } from \"@langchain/langgraph\";\n\nconst myWorkflow = entrypoint(\n  { checkpointer, name: \"workflow\" },\n  async (number: number) =&gt; {\n    const previous = getPreviousState&lt;number&gt;() ?? 0;\n    return number + previous;\n  }\n);\n\nconst config = {\n  configurable: {\n    thread_id: \"some_thread_id\",\n  },\n};\n\nawait myWorkflow.invoke(1, config); // 1 (previous was undefined)\nawait myWorkflow.invoke(2, config); // 3 (previous was 1 from the previous invocation)\n</code></pre> <p>:::</p>", "boost": 2}, {"location": "concepts/functional_api/#entrypointfinal", "title": "<code>entrypoint.final</code>", "text": "<p>:::python @[<code>entrypoint.final</code>][entrypoint.final] is a special primitive that can be returned from an entrypoint and allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint.</p> <p>The first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is <code>entrypoint.final[return_type, save_type]</code>.</p> <pre><code>@entrypoint(checkpointer=checkpointer)\ndef my_workflow(number: int, *, previous: Any = None) -&gt; entrypoint.final[int, int]:\n    previous = previous or 0\n    # This will return the previous value to the caller, saving\n    # 2 * number to the checkpoint, which will be used in the next invocation\n    # for the `previous` parameter.\n    return entrypoint.final(value=previous, save=2 * number)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"\n    }\n}\n\nmy_workflow.invoke(3, config)  # 0 (previous was None)\nmy_workflow.invoke(1, config)  # 6 (previous was 3 * 2 from the previous invocation)\n</code></pre> <p>:::</p> <p>:::js @[<code>entrypoint.final</code>][entrypoint.final] is a special primitive that can be returned from an entrypoint and allows decoupling the value that is saved in the checkpoint from the return value of the entrypoint.</p> <p>The first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint.</p> <pre><code>import { entrypoint, getPreviousState } from \"@langchain/langgraph\";\n\nconst myWorkflow = entrypoint(\n  { checkpointer, name: \"workflow\" },\n  async (number: number) =&gt; {\n    const previous = getPreviousState&lt;number&gt;() ?? 0;\n    // This will return the previous value to the caller, saving\n    // 2 * number to the checkpoint, which will be used in the next invocation\n    // for the `previous` parameter.\n    return entrypoint.final({\n      value: previous,\n      save: 2 * number,\n    });\n  }\n);\n\nconst config = {\n  configurable: {\n    thread_id: \"1\",\n  },\n};\n\nawait myWorkflow.invoke(3, config); // 0 (previous was undefined)\nawait myWorkflow.invoke(1, config); // 6 (previous was 3 * 2 from the previous invocation)\n</code></pre> <p>:::</p>", "boost": 2}, {"location": "concepts/functional_api/#task", "title": "Task", "text": "<p>A task represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics:</p> <ul> <li>Asynchronous Execution: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking.</li> <li>Checkpointing: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See persistence for more details).</li> </ul>", "boost": 2}, {"location": "concepts/functional_api/#definition_1", "title": "Definition", "text": "<p>:::python Tasks are defined using the <code>@task</code> decorator, which wraps a regular Python function.</p> <pre><code>from langgraph.func import task\n\n@task()\ndef slow_computation(input_value):\n    # Simulate a long-running operation\n    ...\n    return result\n</code></pre> <p>:::</p> <p>:::js Tasks are defined using the <code>task</code> function, which wraps a regular function.</p> <pre><code>import { task } from \"@langchain/langgraph\";\n\nconst slowComputation = task(\"slowComputation\", async (inputValue: any) =&gt; {\n  // Simulate a long-running operation\n  return result;\n});\n</code></pre> <p>:::</p> <p>Serialization</p> <p>The outputs of tasks must be JSON-serializable to support checkpointing.</p>", "boost": 2}, {"location": "concepts/functional_api/#execution", "title": "Execution", "text": "<p>Tasks can only be called from within an entrypoint, another task, or a state graph node.</p> <p>Tasks cannot be called directly from the main application code.</p> <p>:::python When you call a task, it returns immediately with a future object. A future is a placeholder for a result that will be available later.</p> <p>To obtain the result of a task, you can either wait for it synchronously (using <code>result()</code>) or await it asynchronously (using <code>await</code>).</p> Synchronous InvocationAsynchronous Invocation <pre><code>@entrypoint(checkpointer=checkpointer)\ndef my_workflow(some_input: int) -&gt; int:\n    future = slow_computation(some_input)\n    return future.result()  # Wait for the result synchronously\n</code></pre> <pre><code>@entrypoint(checkpointer=checkpointer)\nasync def my_workflow(some_input: int) -&gt; int:\n    return await slow_computation(some_input)  # Await result asynchronously\n</code></pre> <p>:::</p> <p>:::js When you call a task, it returns a Promise that can be awaited.</p> <pre><code>const myWorkflow = entrypoint(\n  { checkpointer, name: \"workflow\" },\n  async (someInput: number): Promise&lt;number&gt; =&gt; {\n    return await slowComputation(someInput);\n  }\n);\n</code></pre> <p>:::</p>", "boost": 2}, {"location": "concepts/functional_api/#when-to-use-a-task", "title": "When to use a task", "text": "<p>Tasks are useful in the following scenarios:</p> <ul> <li>Checkpointing: When you need to save the result of a long-running operation to a checkpoint, so you don't need to recompute it when resuming the workflow.</li> <li>Human-in-the-loop: If you're building a workflow that requires human intervention, you MUST use tasks to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the determinism section for more details.</li> <li>Parallel Execution: For I/O-bound tasks, tasks enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs).</li> <li>Observability: Wrapping operations in tasks provides a way to track the progress of the workflow and monitor the execution of individual operations using LangSmith.</li> <li>Retryable Work: When work needs to be retried to handle failures or inconsistencies, tasks provide a way to encapsulate and manage the retry logic.</li> </ul>", "boost": 2}, {"location": "concepts/functional_api/#serialization", "title": "Serialization", "text": "<p>There are two key aspects to serialization in LangGraph:</p> <ol> <li><code>entrypoint</code> inputs and outputs must be JSON-serializable.</li> <li><code>task</code> outputs must be JSON-serializable.</li> </ol> <p>:::python These requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable. :::</p> <p>:::js These requirements are necessary for enabling checkpointing and workflow resumption. Use primitives like objects, arrays, strings, numbers, and booleans to ensure that your inputs and outputs are serializable. :::</p> <p>Serialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution.</p> <p>Providing non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer.</p>", "boost": 2}, {"location": "concepts/functional_api/#determinism", "title": "Determinism", "text": "<p>To utilize features like human-in-the-loop, any randomness should be encapsulated inside of tasks. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same sequence of steps, even if task results are non-deterministic.</p> <p>LangGraph achieves this behavior by persisting task and subgraph results as they execute. A well-designed workflow ensures that resuming execution follows the same sequence of steps, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running tasks or tasks with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same.</p> <p>While different runs of a workflow can produce different results, resuming a specific run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up task and subgraph results that were executed prior to the graph being interrupted and avoid recomputing them.</p>", "boost": 2}, {"location": "concepts/functional_api/#idempotency", "title": "Idempotency", "text": "<p>Idempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside tasks functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a task starts, but does not complete successfully. Then, if the workflow is resumed, the task will run again. Use idempotency keys or verify existing results to avoid duplication.</p>", "boost": 2}, {"location": "concepts/functional_api/#common-pitfalls", "title": "Common Pitfalls", "text": "", "boost": 2}, {"location": "concepts/functional_api/#handling-side-effects", "title": "Handling side effects", "text": "<p>Encapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.</p> IncorrectCorrect <p>In this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow.</p> <p>:::python <pre><code>@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -&gt; int:\n    # This code will be executed a second time when resuming the workflow.\n    # Which is likely not what you want.\n    # highlight-next-line\n    with open(\"output.txt\", \"w\") as f:\n        # highlight-next-line\n        f.write(\"Side effect executed\")\n    value = interrupt(\"question\")\n    return value\n</code></pre> :::</p> <p>:::js <pre><code>import { entrypoint, interrupt } from \"@langchain/langgraph\";\nimport fs from \"fs\";\n\nconst myWorkflow = entrypoint(\n  { checkpointer, name: \"workflow },\n  async (inputs: Record&lt;string, any&gt;) =&gt; {\n    // This code will be executed a second time when resuming the workflow.\n    // Which is likely not what you want.\n    fs.writeFileSync(\"output.txt\", \"Side effect executed\");\n    const value = interrupt(\"question\");\n    return value;\n  }\n);\n</code></pre> :::</p> <p>In this example, the side effect is encapsulated in a task, ensuring consistent execution upon resumption.</p> <p>:::python <pre><code>from langgraph.func import task\n\n# highlight-next-line\n@task\n# highlight-next-line\ndef write_to_file():\n    with open(\"output.txt\", \"w\") as f:\n        f.write(\"Side effect executed\")\n\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -&gt; int:\n    # The side effect is now encapsulated in a task.\n    write_to_file().result()\n    value = interrupt(\"question\")\n    return value\n</code></pre> :::</p> <p>:::js <pre><code>import { entrypoint, task, interrupt } from \"@langchain/langgraph\";\nimport * as fs from \"fs\";\n\nconst writeToFile = task(\"writeToFile\", async () =&gt; {\n  fs.writeFileSync(\"output.txt\", \"Side effect executed\");\n});\n\nconst myWorkflow = entrypoint(\n  { checkpointer, name: \"workflow\" },\n  async (inputs: Record&lt;string, any&gt;) =&gt; {\n    // The side effect is now encapsulated in a task.\n    await writeToFile();\n    const value = interrupt(\"question\");\n    return value;\n  }\n);\n</code></pre> :::</p>", "boost": 2}, {"location": "concepts/functional_api/#non-deterministic-control-flow", "title": "Non-deterministic control flow", "text": "<p>Operations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned.</p> <ul> <li>In a task: Get random number (5) \u2192 interrupt \u2192 resume \u2192 (returns 5 again) \u2192 ...</li> <li>Not in a task: Get random number (5) \u2192 interrupt \u2192 resume \u2192 get new random number (7) \u2192 ...</li> </ul> <p>:::python This is especially important when using human-in-the-loop workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it's matched with the corresponding resume value. This matching is strictly index-based, so the order of the resume values should match the order of the interrupts. :::</p> <p>:::js This is especially important when using human-in-the-loop workflows with multiple interrupt calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it's matched with the corresponding resume value. This matching is strictly index-based, so the order of the resume values should match the order of the interrupts. :::</p> <p>If order of execution is not maintained when resuming, one <code>interrupt</code> call may be matched with the wrong <code>resume</code> value, leading to incorrect results.</p> <p>Please read the section on determinism for more details.</p> IncorrectCorrect <p>In this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed.</p> <p>:::python <pre><code>from langgraph.func import entrypoint\n\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -&gt; int:\n    t0 = inputs[\"t0\"]\n    # highlight-next-line\n    t1 = time.time()\n\n    delta_t = t1 - t0\n\n    if delta_t &gt; 1:\n        result = slow_task(1).result()\n        value = interrupt(\"question\")\n    else:\n        result = slow_task(2).result()\n        value = interrupt(\"question\")\n\n    return {\n        \"result\": result,\n        \"value\": value\n    }\n</code></pre> :::</p> <p>:::js <pre><code>import { entrypoint, interrupt } from \"@langchain/langgraph\";\n\nconst myWorkflow = entrypoint(\n  { checkpointer, name: \"workflow\" },\n  async (inputs: { t0: number }) =&gt; {\n    const t1 = Date.now();\n\n    const deltaT = t1 - inputs.t0;\n\n    if (deltaT &gt; 1000) {\n      const result = await slowTask(1);\n      const value = interrupt(\"question\");\n      return { result, value };\n    } else {\n      const result = await slowTask(2);\n      const value = interrupt(\"question\");\n      return { result, value };\n    }\n  }\n);\n</code></pre> :::</p> <p>:::python In this example, the workflow uses the input <code>t0</code> to determine which task to execute. This is deterministic because the result of the workflow depends only on the input.</p> <p><pre><code>import time\n\nfrom langgraph.func import task\n\n# highlight-next-line\n@task\n# highlight-next-line\ndef get_time() -&gt; float:\n    return time.time()\n\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -&gt; int:\n    t0 = inputs[\"t0\"]\n    # highlight-next-line\n    t1 = get_time().result()\n\n    delta_t = t1 - t0\n\n    if delta_t &gt; 1:\n        result = slow_task(1).result()\n        value = interrupt(\"question\")\n    else:\n        result = slow_task(2).result()\n        value = interrupt(\"question\")\n\n    return {\n        \"result\": result,\n        \"value\": value\n    }\n</code></pre> :::</p> <p>:::js In this example, the workflow uses the input <code>t0</code> to determine which task to execute. This is deterministic because the result of the workflow depends only on the input.</p> <p><pre><code>import { entrypoint, task, interrupt } from \"@langchain/langgraph\";\n\nconst getTime = task(\"getTime\", () =&gt; Date.now());\n\nconst myWorkflow = entrypoint(\n  { checkpointer, name: \"workflow\" },\n  async (inputs: { t0: number }): Promise&lt;any&gt; =&gt; {\n    const t1 = await getTime();\n\n    const deltaT = t1 - inputs.t0;\n\n    if (deltaT &gt; 1000) {\n      const result = await slowTask(1);\n      const value = interrupt(\"question\");\n      return { result, value };\n    } else {\n      const result = await slowTask(2);\n      const value = interrupt(\"question\");\n      return { result, value };\n    }\n  }\n);\n</code></pre> :::</p>", "boost": 2}, {"location": "concepts/human_in_the_loop/", "title": "Human-in-the-loop", "text": "<p>To review, edit, and approve tool calls in an agent or workflow, use LangGraph's human-in-the-loop features to enable human intervention at any point in a workflow. This is especially useful in large language model (LLM)-driven applications where model output may require validation, correction, or additional context.</p> <p></p> <p>Tip</p> <p>For information on how to use human-in-the-loop, see Enable human intervention and Human-in-the-loop using Server API.</p>", "tags": ["human-in-the-loop", "hil", "overview"], "boost": 2}, {"location": "concepts/human_in_the_loop/#key-capabilities", "title": "Key capabilities", "text": "<ul> <li> <p>Persistent execution state: Interrupts use LangGraph's persistence layer, which saves the graph state, to indefinitely pause graph execution until you resume. This is possible because LangGraph checkpoints the graph state after each step, which allows the system to persist execution context and later resume the workflow, continuing from where it left off. This supports asynchronous human review or input without time constraints.</p> <p>There are two ways to pause a graph:</p> <ul> <li>Dynamic interrupts: Use <code>interrupt</code> to pause a graph from inside a specific node, based on the current state of the graph.</li> <li>Static interrupts: Use <code>interrupt_before</code> and <code>interrupt_after</code> to pause the graph at pre-defined points, either before or after a node executes.</li> </ul> <p> An example graph consisting of 3 sequential steps with a breakpoint before step_3.  </p> </li> <li> <p>Flexible integration points: Human-in-the-loop logic can be introduced at any point in the workflow. This allows targeted human involvement, such as approving API calls, correcting outputs, or guiding conversations.</p> </li> </ul>", "tags": ["human-in-the-loop", "hil", "overview"], "boost": 2}, {"location": "concepts/human_in_the_loop/#patterns", "title": "Patterns", "text": "<p>There are four typical design patterns that you can implement using <code>interrupt</code> and <code>Command</code>:</p> <ul> <li>Approve or reject: Pause the graph before a critical step, such as an API call, to review and approve the action. If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action. This pattern often involves routing the graph based on the human's input.</li> <li>Edit graph state: Pause the graph to review and edit the graph state. This is useful for correcting mistakes or updating the state with additional information. This pattern often involves updating the state with the human's input.</li> <li>Review tool calls: Pause the graph to review and edit tool calls requested by the LLM before tool execution.</li> <li>Validate human input: Pause the graph to validate human input before proceeding with the next step.</li> </ul>", "tags": ["human-in-the-loop", "hil", "overview"], "boost": 2}, {"location": "concepts/low_level/", "title": "Overview", "text": "", "boost": 2}, {"location": "concepts/low_level/#graph-api", "title": "Graph API \uac1c\ub150", "text": "", "boost": 2}, {"location": "concepts/low_level/#_1", "title": "\uadf8\ub798\ud504", "text": "<p>LangGraph\ub294 \uc5d0\uc774\uc804\ud2b8 \uc6cc\ud06c\ud50c\ub85c\uc6b0\ub97c \uadf8\ub798\ud504\ub85c \ubaa8\ub378\ub9c1\ud569\ub2c8\ub2e4. \uc5d0\uc774\uc804\ud2b8\uc758 \ub3d9\uc791\uc740 \uc138 \uac00\uc9c0 \ud575\uc2ec \uad6c\uc131 \uc694\uc18c\ub85c \uc815\uc758\ub429\ub2c8\ub2e4:</p> <ol> <li><code>State</code>: \ud604\uc7ac \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc758 \uc2a4\ub0c5\uc0f7\uc744 \ub098\ud0c0\ub0b4\ub294 \uacf5\uc720 \ub370\uc774\ud130 \uad6c\uc870\uc785\ub2c8\ub2e4. \uc5b4\ub5a4 \ub370\uc774\ud130 \ud0c0\uc785\uc774\ub4e0 \ub420 \uc218 \uc788\uc9c0\ub9cc, \uc77c\ubc18\uc801\uc73c\ub85c \uacf5\uc720 \uc0c1\ud0dc \uc2a4\ud0a4\ub9c8\ub97c \uc0ac\uc6a9\ud574 \uc815\uc758\ud569\ub2c8\ub2e4.</li> <li><code>Nodes</code>: \uc5d0\uc774\uc804\ud2b8\uc758 \ub85c\uc9c1\uc744 \uc778\ucf54\ub529\ud558\ub294 \ud568\uc218\uc785\ub2c8\ub2e4. \ud604\uc7ac \uc0c1\ud0dc\ub97c \uc785\ub825\uc73c\ub85c \ubc1b\uc544 \uc5f0\uc0b0\uc774\ub098 \ubd80\uc218 \ud6a8\uacfc\ub97c \uc218\ud589\ud558\uace0, \uc5c5\ub370\uc774\ud2b8\ub41c \uc0c1\ud0dc\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4.</li> <li><code>Edges</code>: \ud604\uc7ac \uc0c1\ud0dc\uc5d0 \ub530\ub77c \ub2e4\uc74c\uc5d0 \uc2e4\ud589\ud560 Node\ub97c \uacb0\uc815\ud558\ub294 \ud568\uc218\uc785\ub2c8\ub2e4. \uc870\uac74\ubd80 \ubd84\uae30\uc774\uac70\ub098 \uace0\uc815 \uc804\ud658\uc77c \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ol> <p><code>Nodes</code>\uc640 <code>Edges</code>\ub97c \uc870\ud569\ud558\uba74 \uc2dc\uac04\uc774 \ud750\ub984\uc5d0 \ub530\ub77c \uc0c1\ud0dc\uac00 \ubcc0\ud654\ud558\ub294 \ubcf5\uc7a1\ud558\uace0 \ubc18\ubcf5\uc801\uc778 \uc6cc\ud06c\ud50c\ub85c\uc6b0\ub97c \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc2e4\uc81c \ud798\uc740 LangGraph\uac00 \uc0c1\ud0dc\ub97c \uad00\ub9ac\ud558\ub294 \ubc29\uc2dd\uc5d0 \uc788\uc2b5\ub2c8\ub2e4. \uac15\uc870\ud558\uc790\uba74, <code>Nodes</code>\uc640 <code>Edges</code>\ub294 \ub2e8\uc21c\ud788 \ud568\uc218\uc77c \ubfd0\uc774\uba70, LLM\uc744 \ud3ec\ud568\ud560 \uc218\ub3c4 \uc788\uace0 \uc21c\uc218 \ucf54\ub4dc\ub9cc \ud3ec\ud568\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc694\uc57d: \ub178\ub4dc\ub294 \uc791\uc5c5\uc744 \uc218\ud589\ud558\uace0, \uc5e3\uc9c0\ub294 \ub2e4\uc74c\uc5d0 \ubb34\uc5c7\uc744 \ud560\uc9c0 \uc54c\ub824\uc90d\ub2c8\ub2e4.</p> <p>LangGraph\uc758 \uae30\ubcf8 \uadf8\ub798\ud504 \uc54c\uace0\ub9ac\uc998\uc740 message passing\uc744 \uc0ac\uc6a9\ud574 \uc77c\ubc18 \ud504\ub85c\uadf8\ub7a8\uc744 \uc815\uc758\ud569\ub2c8\ub2e4. Node\uac00 \uc791\uc5c5\uc744 \ub9c8\uce58\uba74 \ud558\ub098 \uc774\uc0c1\uc758 \uc5e3\uc9c0\ub97c \ub530\ub77c \ub2e4\ub978 \ub178\ub4dc\uc5d0 \uba54\uc2dc\uc9c0\ub97c \ubcf4\ub0c5\ub2c8\ub2e4. \uc218\uc2e0 \ub178\ub4dc\ub294 \uc790\uc2e0\uc758 \ud568\uc218\ub97c \uc2e4\ud589\ud558\uace0, \uacb0\uacfc \uba54\uc2dc\uc9c0\ub97c \ub2e4\uc74c \ub178\ub4dc\uc5d0 \uc804\ub2ec\ud558\uba70 \uc774 \uacfc\uc815\uc774 \uacc4\uc18d\ub429\ub2c8\ub2e4. \uc774\ub294 Google\uc758 Pregel \uc2dc\uc2a4\ud15c\uc5d0\uc11c \uc601\uac10\uc744 \ubc1b\uc544, \ud504\ub85c\uadf8\ub7a8\uc774 \uc774\uc0b0\uc801\uc778 \u201c\uc288\ud37c\uc2a4\ud15d(super-steps)\u201d\uc73c\ub85c \uc9c4\ud589\ub429\ub2c8\ub2e4.</p> <p>\uc288\ud37c\uc2a4\ud15d\uc740 \uadf8\ub798\ud504 \ub178\ub4dc \uc804\uccb4\ub97c \ud55c \ubc88 \uc21c\ud68c\ud558\ub294 \ub2e8\uc77c \ubc18\ubcf5\uc73c\ub85c \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubcd1\ub82c\ub85c \uc2e4\ud589\ub418\ub294 \ub178\ub4dc\ub4e4\uc740 \uac19\uc740 \uc288\ud37c\uc2a4\ud15d\uc5d0 \uc18d\ud558\uace0, \uc21c\ucc28\uc801\uc73c\ub85c \uc2e4\ud589\ub418\ub294 \ub178\ub4dc\ub4e4\uc740 \ubcc4\ub3c4\uc758 \uc288\ud37c\uc2a4\ud15d\uc5d0 \uc18d\ud569\ub2c8\ub2e4. \uadf8\ub798\ud504 \uc2e4\ud589\uc774 \uc2dc\uc791\ub420 \ub54c \ubaa8\ub4e0 \ub178\ub4dc\ub294 <code>inactive</code> \uc0c1\ud0dc\uc5d0\uc11c \uc2dc\uc791\ud569\ub2c8\ub2e4. \ub178\ub4dc\ub294 \ub4e4\uc5b4\uc624\ub294 \uc5e3\uc9c0(\ub610\ub294 \u201c\ucc44\ub110\u201d) \uc911 \ud558\ub098\uc5d0\uc11c \uc0c8\ub85c\uc6b4 \uba54\uc2dc\uc9c0(\uc0c1\ud0dc)\ub97c \ubc1b\uc73c\uba74 <code>active</code>\uac00 \ub418\uace0, \ud568\uc218\ub97c \uc2e4\ud589\ud574 \uc5c5\ub370\uc774\ud2b8\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4. \uac01 \uc288\ud37c\uc2a4\ud15d\uc774 \ub05d\ub0a0 \ub54c, \ub4e4\uc5b4\uc624\ub294 \uba54\uc2dc\uc9c0\uac00 \uc5c6\ub294 \ub178\ub4dc\ub4e4\uc740 <code>halt</code>\ub97c \uc120\ud0dd\ud574 \uc790\uc2e0\uc744 <code>inactive</code>\ub85c \ud45c\uc2dc\ud569\ub2c8\ub2e4. \ubaa8\ub4e0 \ub178\ub4dc\uac00 <code>inactive</code>\uc774\uace0 \uc804\uc1a1 \uc911\uc778 \uba54\uc2dc\uc9c0\uac00 \uc5c6\uc744 \ub54c \uadf8\ub798\ud504 \uc2e4\ud589\uc774 \uc885\ub8cc\ub429\ub2c8\ub2e4.</p>", "boost": 2}, {"location": "concepts/low_level/#stategraph", "title": "StateGraph", "text": "<p><code>StateGraph</code> \ud074\ub798\uc2a4\ub294 \uc0ac\uc6a9\ud574\uc57c \ud560 \uc8fc\uc694 \uadf8\ub798\ud504 \ud074\ub798\uc2a4\uc785\ub2c8\ub2e4. \uc774\ub294 \uc0ac\uc6a9\uc790\uac00 \uc815\uc758\ud55c State \uac1d\uccb4\ub97c \ub9e4\uac1c\ubcc0\uc218\ub85c \ubc1b\uc2b5\ub2c8\ub2e4.</p>", "boost": 2}, {"location": "concepts/low_level/#_2", "title": "\uadf8\ub798\ud504 \ucef4\ud30c\uc77c", "text": "<p>\uadf8\ub798\ud504\ub97c \ub9cc\ub4e4\ub824\uba74 \uba3c\uc800 State\ub97c \uc815\uc758\ud558\uace0, Nodes\uc640 Edges\ub97c \ucd94\uac00\ud55c \ub4a4 \ucef4\ud30c\uc77c\ud569\ub2c8\ub2e4. \ucef4\ud30c\uc77c\uc774 \uc815\ud655\ud788 \ubb34\uc5c7\uc774\uba70 \uc65c \ud544\uc694\ud55c\uac00\uc694?</p> <p>\ucef4\ud30c\uc77c\uc740 \ub9e4\uc6b0 \uac04\ub2e8\ud55c \ub2e8\uacc4\uc785\ub2c8\ub2e4. \uadf8\ub798\ud504 \uad6c\uc870\uc5d0 \ub300\ud55c \uae30\ubcf8 \uac80\uc0ac\ub97c \uc218\ud589\ud569\ub2c8\ub2e4(\uace0\uc544 \ub178\ub4dc\uac00 \uc5c6\ub294\uc9c0 \ub4f1). \ub610\ud55c \uc5ec\uae30\uc11c \uccb4\ud06c\ud3ec\uc778\ud130\uc640 \ube0c\ub808\uc774\ud06c\ud3ec\uc778\ud2b8 \uac19\uc740 \ub7f0\ud0c0\uc784 \uc778\uc790\ub97c \uc9c0\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub798\ud504\ub97c \ucef4\ud30c\uc77c\ud558\ub824\uba74 <code>.compile</code> \uba54\uc11c\ub4dc\ub97c \ud638\ucd9c\ud558\uba74 \ub429\ub2c8\ub2e4:</p> <pre><code>graph = graph_builder.compile(...)\n</code></pre> <p>\uadf8\ub798\ud504\ub97c \uc0ac\uc6a9\ud558\uae30 \uc804\uc5d0 \ubc18\ub4dc\uc2dc \ucef4\ud30c\uc77c\ud574\uc57c \ud569\ub2c8\ub2e4.</p>", "boost": 2}, {"location": "concepts/low_level/#state", "title": "State", "text": "<p>\uadf8\ub798\ud504\ub97c \uc815\uc758\ud560 \ub54c \uac00\uc7a5 \uba3c\uc800 \ud558\ub294 \uc77c\uc740 \uadf8\ub798\ud504\uc758 State\ub97c \uc815\uc758\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. State\ub294 \uadf8\ub798\ud504\uc758 \uc2a4\ud0a4\ub9c8\uc640 \uc5c5\ub370\uc774\ud2b8\ub97c \uc801\uc6a9\ud558\ub294 \ubc29\ubc95\uc744 \uc9c0\uc815\ud558\ub294 <code>reducer</code> \ud568\uc218\ub4e4\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. State \uc2a4\ud0a4\ub9c8\ub294 \ubaa8\ub4e0 Node\uc640 Edge\uc5d0 \ub300\ud55c \uc785\ub825 \uc2a4\ud0a4\ub9c8\uac00 \ub418\uba70, <code>TypedDict</code> \ud639\uc740 <code>Pydantic</code> \ubaa8\ub378 \uc911 \ud558\ub098\ub85c \uad6c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubaa8\ub4e0 Node\ub294 State\uc5d0 \uc5c5\ub370\uc774\ud2b8\ub97c \ub0b4\ubcf4\ub0b4\uba70, \uc9c0\uc815\ub41c <code>reducer</code> \ud568\uc218\uac00 \uc774\ub97c \uc801\uc6a9\ud569\ub2c8\ub2e4.</p>", "boost": 2}, {"location": "concepts/low_level/#schema", "title": "Schema", "text": "<p>\uadf8\ub798\ud504 \uc2a4\ud0a4\ub9c8\ub97c \uc9c0\uc815\ud558\ub294 \uac00\uc7a5 \uc77c\ubc18\uc801\uc778 \ubc29\ubc95\uc740 <code>TypedDict</code>\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc785\ub2c8\ub2e4. \uae30\ubcf8\uac12\uc744 \uc81c\uacf5\ud558\ub824\uba74 <code>dataclass</code>\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc7ac\uadc0\uc801\uc778 \ub370\uc774\ud130 \uac80\uc99d\uc774 \ud544\uc694\ud558\uba74 Pydantic\uc758 BaseModel\uc744 \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc9c0\ub9cc, \uc131\ub2a5\uc740 <code>TypedDict</code>\ub098 <code>dataclass</code>\ubcf4\ub2e4 \ub0ae\uc2b5\ub2c8\ub2e4.</p> <p>\uae30\ubcf8\uc801\uc73c\ub85c \uadf8\ub798\ud504\ub294 \ub3d9\uc77c\ud55c \uc785\ub825\u00b7\ucd9c\ub825 \uc2a4\ud0a4\ub9c8\ub97c \uac00\uc9d1\ub2c8\ub2e4. \ud544\uc694\uc5d0 \ub530\ub77c \uba85\uc2dc\uc801\uc778 \uc785\ub825\u00b7\ucd9c\ub825 \uc2a4\ud0a4\ub9c8\ub97c \ubcc4\ub3c4\ub85c \uc9c0\uc815\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ud0a4\uac00 \ub9ce\uace0, \uc77c\ubd80\ub294 \uc785\ub825 \uc804\uc6a9, \uc77c\ubd80\ub294 \ucd9c\ub825 \uc804\uc6a9\uc77c \ub54c \uc720\uc6a9\ud569\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uac00\uc774\ub4dc\ub97c \ucc38\uace0\ud558\uc138\uc694.</p>", "boost": 2}, {"location": "concepts/low_level/#multiple-schemas", "title": "Multiple schemas", "text": "<p>\ub300\ubd80\ubd84\uc758 \uadf8\ub798\ud504 \ub178\ub4dc\ub294 \ub2e8\uc77c \uc2a4\ud0a4\ub9c8\ub97c \uacf5\uc720\ud569\ub2c8\ub2e4. \uc989, \ub3d9\uc77c\ud55c \uc0c1\ud0dc \ucc44\ub110\uc744 \uc77d\uace0 \uc501\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \ub2e4\uc74c\uacfc \uac19\uc740 \uacbd\uc6b0\uc5d0\ub294 \ub354 \uc138\ubc00\ud55c \uc81c\uc5b4\uac00 \ud544\uc694\ud569\ub2c8\ub2e4:</p> <ul> <li>\ub0b4\ubd80 \ub178\ub4dc\ub294 \uadf8\ub798\ud504 \uc785\ub825\u00b7\ucd9c\ub825\uc5d0 \ud544\uc694\ud558\uc9c0 \uc54a\uc740 \uc815\ubcf4\ub97c \uc804\ub2ec\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\uadf8\ub798\ud504\uc5d0 \ubcc4\ub3c4\uc758 \uc785\ub825\u00b7\ucd9c\ub825 \uc2a4\ud0a4\ub9c8\ub97c \uc0ac\uc6a9\ud558\uace0 \uc2f6\uc744 \ub54c, \ucd9c\ub825\uc740 \uc608\ub97c \ub4e4\uc5b4 \ub2e8\uc77c \uad00\ub828 \ud0a4\ub9cc \ud3ec\ud568\ud558\ub3c4\ub85d \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul> <p>\ub178\ub4dc\uac00 \ub0b4\ubd80\uc801\uc73c\ub85c\ub9cc \uc0ac\uc6a9\ud558\ub294 <code>PrivateState</code>\uc640 \uac19\uc740 \ud504\ub77c\uc774\ube57 \uc2a4\ud0a4\ub9c8\ub97c \uc815\uc758\ud574 \ube44\uacf5\uac1c \ucc44\ub110\uc5d0 \uc4f0\ub294 \uac83\ub3c4 \uac00\ub2a5\ud569\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uacbd\uc6b0, \uc804\uccb4 \uc2a4\ud0a4\ub9c8\ub294 <code>InternalState</code>\uc774\uba70, <code>input</code> \ubc0f <code>output</code> \uc2a4\ud0a4\ub9c8\ub294 InternalState\uc758 \ubd80\ubd84\uc9d1\ud569\uc73c\ub85c \uc81c\ud55c\ub429\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uac00\uc774\ub4dc\ub97c \ud655\uc778\ud558\uc138\uc694.</p> <p>\uc608\uc2dc\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4:</p> <pre><code>class InputState(TypedDict):\n    user_input: str\n\nclass OutputState(TypedDict):\n    graph_output: str\n\nclass OverallState(TypedDict):\n    foo: str\n    user_input: str\n    graph_output: str\n\nclass PrivateState(TypedDict):\n    bar: str\n\ndef node_1(state: InputState) -&gt; OverallState:\n    # OverallState\uc5d0 \uc4f0\uae30\n    return {\"foo\": state[\"user_input\"] + \" name\"}\n\ndef node_2(state: OverallState) -&gt; PrivateState:\n    # OverallState\ub97c \uc77d\uace0 PrivateState\uc5d0 \uc4f0\uae30\n    return {\"bar\": state[\"foo\"] + \" is\"}\n\ndef node_3(state: PrivateState) -&gt; OutputState:\n    # PrivateState\ub97c \uc77d\uace0 OutputState\uc5d0 \uc4f0\uae30\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\n\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\nbuilder.add_edge(\"node_2\", \"node_3\")\nbuilder.add_edge(\"node_3\", END)\n\ngraph = builder.compile()\ngraph.invoke({\"user_input\":\"My\"})\n# {'graph_output': 'My name is Lance'}\n</code></pre> <p>\ub450 \uac00\uc9c0 \uc911\uc694\ud55c \ud3ec\uc778\ud2b8:</p> <ol> <li><code>node_1</code>\uc740 <code>state: InputState</code>\ub97c \uc785\ub825 \uc2a4\ud0a4\ub9c8\ub85c \ubc1b\uc9c0\ub9cc, OverallState\uc5d0 \uc788\ub294 <code>foo</code> \ucc44\ub110\uc5d0 \uc501\ub2c8\ub2e4. \uc774\ub294 \ub178\ub4dc\uac00 \uadf8\ub798\ud504 \uc804\uccb4\uc5d0 \uc815\uc758\ub41c \uc5b4\ub5a4 \uc0c1\ud0dc \ucc44\ub110\uc5d0\ub3c4 \uc4f8 \uc218 \uc788\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \uadf8\ub798\ud504 \uc0c1\ud0dc\ub294 \ucd08\uae30\ud654 \uc2dc \uc815\uc758\ub41c \ubaa8\ub4e0 \ucc44\ub110(OverallState, InputState, OutputState)\uc758 \ud569\uc9d1\ud569\uc785\ub2c8\ub2e4.</li> <li><code>node_2</code>\uc5d0\uc11c <code>PrivateState</code>\ub97c \uc0ac\uc6a9\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. <code>StateGraph</code> \ucd08\uae30\ud654 \uc2dc <code>PrivateState</code>\ub97c \uba85\uc2dc\ud558\uc9c0 \uc54a\uc558\uc9c0\ub9cc, \ud574\ub2f9 \uc2a4\ud0a4\ub9c8\uac00 \uc874\uc7ac\ud558\uae30 \ub54c\ubb38\uc5d0 \ub178\ub4dc\uac00 \ucd94\uac00 \ucc44\ub110\uc744 \uc120\uc5b8\ud558\uace0 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ol>", "boost": 2}, {"location": "concepts/low_level/#reducers", "title": "Reducers", "text": "<p>Reducer\ub294 \ub178\ub4dc\uc5d0\uc11c \ubc18\ud658\ub41c \uc5c5\ub370\uc774\ud2b8\uac00 State\uc5d0 \uc5b4\ub5bb\uac8c \uc801\uc6a9\ub418\ub294\uc9c0\ub97c \uc774\ud574\ud558\ub294 \ud575\uc2ec \uc694\uc18c\uc785\ub2c8\ub2e4. State\uc758 \uac01 \ud0a4\ub294 \ub3c5\ub9bd\uc801\uc778 reducer \ud568\uc218\ub97c \uac00\uc9d1\ub2c8\ub2e4. \uba85\uc2dc\uc801\uc778 reducer\uac00 \uc5c6\uc73c\uba74 \uae30\ubcf8\uc801\uc73c\ub85c \ud574\ub2f9 \ud0a4\uc758 \uac12\uc744 **\ub36e\uc5b4\uc4f0\uae30**\ud569\ub2c8\ub2e4. \uae30\ubcf8 reducer \uc678\uc5d0\ub3c4 \uc5ec\ub7ec \uc885\ub958\uac00 \uc788\uc2b5\ub2c8\ub2e4.</p>", "boost": 2}, {"location": "concepts/low_level/#reducer", "title": "\uae30\ubcf8 Reducer", "text": "<p>\ub2e4\uc74c \uc608\uc2dc\ub294 \uae30\ubcf8 reducer \uc0ac\uc6a9\ubc95\uc744 \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.</p> <p>\uc608\uc2dc A:</p> <pre><code>from typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: int\n    bar: list[str]\n</code></pre> <p>\uc785\ub825\uc774 <code>{\"foo\": 1, \"bar\": [\"hi\"]}</code>\uc774\uace0, \uccab \ubc88\uc9f8 Node\uac00 <code>{\"foo\": 2}</code>\ub97c \ubc18\ud658\ud558\uba74 \uc0c1\ud0dc\ub294 <code>{\"foo\": 2, \"bar\": [\"hi\"]}</code>\uac00 \ub429\ub2c8\ub2e4. \ub450 \ubc88\uc9f8 \ub178\ub4dc\uac00 <code>{\"bar\": [\"bye\"]}</code>\ub97c \ubc18\ud658\ud558\uba74 \ucd5c\uc885 \uc0c1\ud0dc\ub294 <code>{\"foo\": 2, \"bar\": [\"bye\"]}</code>\uac00 \ub429\ub2c8\ub2e4.</p> <p>\uc608\uc2dc B:</p> <pre><code>from typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n</code></pre> <p>\uc5ec\uae30\uc11c\ub294 <code>bar</code> \ud0a4\uc5d0 <code>operator.add</code> reducer\ub97c \uc9c0\uc815\ud588\uc2b5\ub2c8\ub2e4. \uc785\ub825\uc774 <code>{\"foo\": 1, \"bar\": [\"hi\"]}</code>\uc774\uace0, \uccab \ubc88\uc9f8 \ub178\ub4dc\uac00 <code>{\"foo\": 2}</code>\ub97c \ubc18\ud658\ud558\uba74 \uc0c1\ud0dc\ub294 <code>{\"foo\": 2, \"bar\": [\"hi\"]}</code>\uac00 \ub429\ub2c8\ub2e4. \ub450 \ubc88\uc9f8 \ub178\ub4dc\uac00 <code>{\"bar\": [\"bye\"]}</code>\ub97c \ubc18\ud658\ud558\uba74 <code>bar</code> \ub9ac\uc2a4\ud2b8\uac00 \ud569\uccd0\uc838 <code>{\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}</code>\uac00 \ub429\ub2c8\ub2e4.</p>", "boost": 2}, {"location": "concepts/low_level/#graph-state", "title": "Graph State\uc5d0\uc11c \uba54\uc2dc\uc9c0 \ub2e4\ub8e8\uae30", "text": "", "boost": 2}, {"location": "concepts/low_level/#_3", "title": "\uc65c \uba54\uc2dc\uc9c0\ub97c \uc0ac\uc6a9\ud558\ub098\uc694?", "text": "<p>\ub300\ubd80\ubd84\uc758 \ucd5c\uc2e0 LLM \uc81c\uacf5\uc790\ub294 \ub9ac\uc2a4\ud2b8 \ud615\ud0dc\uc758 \uba54\uc2dc\uc9c0\ub97c \uc785\ub825\uc73c\ub85c \ubc1b\ub294 \ucc57 \ubaa8\ub378 \uc778\ud130\ud398\uc774\uc2a4\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4. LangChain\uc758 <code>ChatModel</code>\uc740 \ud2b9\ud788 <code>Message</code> \uac1d\uccb4 \ub9ac\uc2a4\ud2b8\ub97c \uc785\ub825\uc73c\ub85c \ubc1b\uc2b5\ub2c8\ub2e4. \uc774\ub7ec\ud55c \uba54\uc2dc\uc9c0\ub294 <code>HumanMessage</code>(\uc0ac\uc6a9\uc790 \uc785\ub825)\ub098 <code>AIMessage</code>(LLM \uc751\ub2f5) \ub4f1 \ub2e4\uc591\ud55c \ud615\ud0dc\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ud574\ub2f9 \uac00\uc774\ub4dc\ub97c \ucc38\uace0\ud558\uc138\uc694.</p>", "boost": 2}, {"location": "concepts/low_level/#_4", "title": "\uadf8\ub798\ud504\uc5d0 \uba54\uc2dc\uc9c0 \uc800\uc7a5\ud558\uae30", "text": "<p>\ub300\ud654 \uae30\ub85d\uc744 \uadf8\ub798\ud504 \uc0c1\ud0dc\uc5d0 \ub9ac\uc2a4\ud2b8 \ud615\ud0dc\ub85c \uc800\uc7a5\ud558\uace0 \uc2f6\ub2e4\uba74, \ud574\ub2f9 \ud0a4\uc5d0 reducer\ub97c \uc9c0\uc815\ud574\uc57c \ud569\ub2c8\ub2e4. reducer\uac00 \uc5c6\uc73c\uba74 \ucd5c\uc2e0 \uba54\uc2dc\uc9c0 \ub9ac\uc2a4\ud2b8\uac00 \uc804\uccb4\ub97c \ub36e\uc5b4\uc501\ub2c8\ub2e4. \ub9ac\uc2a4\ud2b8\uc5d0 \uba54\uc2dc\uc9c0\ub97c **\ucd94\uac00**\ud558\uace0 \uc2f6\ub2e4\uba74 <code>operator.add</code>\ub97c reducer\ub85c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc218\ub3d9\uc73c\ub85c \uba54\uc2dc\uc9c0\ub97c \uc5c5\ub370\uc774\ud2b8\ud560 \ub54c\ub294 <code>operator.add</code>\uac00 \uae30\uc874 \ub9ac\uc2a4\ud2b8\uc5d0 \uc0c8 \uba54\uc2dc\uc9c0\ub97c **\ucd94\uac00**\ud558\uc9c0\ub9cc, \uae30\uc874 \uba54\uc2dc\uc9c0\ub97c **\ub36e\uc5b4\uc4f0\uae30**\ud558\ub824\uba74 \uba54\uc2dc\uc9c0 ID\ub97c \ucd94\uc801\ud558\ub294 reducer\uac00 \ud544\uc694\ud569\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574 \ubbf8\ub9ac \uc81c\uacf5\ub418\ub294 <code>add_messages</code> \ud568\uc218\ub97c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc0c8 \uba54\uc2dc\uc9c0\ub294 \ub9ac\uc2a4\ud2b8\uc5d0 \ucd94\uac00\ud558\uace0, \uae30\uc874 \uba54\uc2dc\uc9c0\ub294 \uc62c\ubc14\ub974\uac8c \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4.</p>", "boost": 2}, {"location": "concepts/low_level/#serialization", "title": "Serialization", "text": "<p>\uba54\uc2dc\uc9c0 ID\ub97c \ucd94\uc801\ud558\ub294 \uac83 \uc678\uc5d0\ub3c4, <code>add_messages</code> \ud568\uc218\ub294 <code>messages</code> \ucc44\ub110\uc5d0\uc11c \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8\ub97c \uc218\uc2e0\ud560 \ub54c\ub9c8\ub2e4 \uba54\uc2dc\uc9c0\ub97c LangChain\uc758 <code>Message</code> \uac1d\uccb4\ub85c \uc5ed\uc9c1\ub82c\ud654\ud558\ub824\uace0 \uc2dc\ub3c4\ud569\ub2c8\ub2e4. LangChain \uc9c1\ub82c\ud654/\uc5ed\uc9c1\ub82c\ud654\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uc5ec\uae30\ub97c \ucc38\uc870\ud558\uc138\uc694. \uc774\ub97c \ud1b5\ud574 \uadf8\ub798\ud504 \uc785\ub825/\uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8\ub97c \ub2e4\uc74c\uacfc \uac19\uc740 \ud615\uc2dd\uc73c\ub85c \uc804\uc1a1\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code># \uc9c0\uc6d0\ub418\ub294 \ud615\ud0dc\n{\"messages\": [HumanMessage(content=\"message\")]}\n\n# \ub610\ud55c \uc9c0\uc6d0\ub418\ub294 \ud615\ud0dc\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\n</code></pre> <p>\uc5ed\uc9c1\ub82c\ud654\ub41c \uba54\uc2dc\uc9c0\ub294 <code>state[\"messages\"][-1].content</code>\uc640 \uac19\uc774 \uc810 \ud45c\uae30\ubc95\uc73c\ub85c \uc811\uadfc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc544\ub798 \uc608\uc2dc\ub294 <code>add_messages</code>\ub97c reducer\ub85c \uc0ac\uc6a9\ud558\ub294 \uadf8\ub798\ud504\ub97c \ubcf4\uc5ec\uc90d\ub2c8\ub2e4.</p> <pre><code>from langchain_core.messages import AnyMessage\nfrom langgraph.graph.message import add_messages\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nclass GraphState(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n</code></pre>", "boost": 2}, {"location": "concepts/low_level/#messagesstate", "title": "MessagesState", "text": "<p>\uba54\uc2dc\uc9c0 \ub9ac\uc2a4\ud2b8\ub97c \uc0c1\ud0dc\uc5d0 \ud3ec\ud568\ud558\ub294 \uacbd\uc6b0\uac00 \ud754\ud558\uae30 \ub54c\ubb38\uc5d0, <code>MessagesState</code>\ub77c\ub294 \uc0ac\uc804 \uc815\uc758\ub41c \uc0c1\ud0dc\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 <code>messages</code> \ud0a4 \ud558\ub098\ub9cc \uac00\uc9c0\uace0 \uc788\uc73c\uba70, <code>add_messages</code> reducer\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ubcf4\ud1b5 \uc5ec\uae30\uc11c \ucd94\uac00 \ud544\ub4dc\ub97c \uc815\uc758\ud574 \uc0ac\uc6a9\ud569\ub2c8\ub2e4:</p> <pre><code>from langgraph.graph import MessagesState\n\nclass State(MessagesState):\n    documents: list[str]\n</code></pre>", "boost": 2}, {"location": "concepts/low_level/#nodes", "title": "Nodes", "text": "<p>LangGraph\uc5d0\uc11c \ub178\ub4dc\ub294 Python \ud568\uc218(\ub3d9\uae30 \ub610\ub294 \ube44\ub3d9\uae30)\uc774\uba70, \ub2e4\uc74c \uc778\uc790\ub97c \ubc1b\uc2b5\ub2c8\ub2e4:</p> <ol> <li><code>state</code>: \uadf8\ub798\ud504\uc758 state</li> <li><code>config</code>: <code>RunnableConfig</code> \uac1d\uccb4(\uc608: <code>thread_id</code>, <code>tags</code> \ub4f1)</li> <li><code>runtime</code>: <code>Runtime</code> \uac1d\uccb4(\uc608: <code>store</code>, <code>stream_writer</code> \ub4f1)</li> </ol> <p>\ub178\ub4dc\ub97c \uadf8\ub798\ud504\uc5d0 \ucd94\uac00\ud558\ub824\uba74 <code>add_node</code> \uba54\uc11c\ub4dc\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4:</p> <pre><code>from dataclasses import dataclass\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph\nfrom langgraph.runtime import Runtime\n\nclass State(TypedDict):\n    input: str\n    results: str\n\n@dataclass\nclass Context:\n    user_id: str\n\nbuilder = StateGraph(State)\n\ndef plain_node(state: State):\n    return state\n\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\n    print(\"In node: \", runtime.context.user_id)\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\ndef node_with_config(state: State, config: RunnableConfig):\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\nbuilder.add_node(\"plain_node\", plain_node)\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\nbuilder.add_node(\"node_with_config\", node_with_config)\n</code></pre> <p>\ud568\uc218\ub294 \ub0b4\ubd80\uc801\uc73c\ub85c <code>RunnableLambda</code>\ub85c \ubcc0\ud658\ub418\uc5b4 \ubc30\uce58 \ubc0f \ube44\ub3d9\uae30 \uc9c0\uc6d0, \ud2b8\ub808\uc774\uc2f1, \ub514\ubc84\uae45 \uae30\ub2a5\uc744 \uc5bb\uac8c \ub429\ub2c8\ub2e4. \uc774\ub984\uc744 \uc9c0\uc815\ud558\uc9c0 \uc54a\uc73c\uba74 \ud568\uc218 \uc774\ub984\uc774 \uae30\ubcf8 \ub178\ub4dc \uc774\ub984\uc774 \ub429\ub2c8\ub2e4.</p>", "boost": 2}, {"location": "concepts/low_level/#start-node", "title": "START Node", "text": "<p><code>START</code> \ub178\ub4dc\ub294 \uc0ac\uc6a9\uc790 \uc785\ub825\uc744 \uadf8\ub798\ud504\uc5d0 \uc804\ub2ec\ud558\ub294 \ud2b9\uc218 \ub178\ub4dc\uc774\uba70, \uadf8\ub798\ud504 \uc9c4\uc785\uc810\uc744 \uc9c0\uc815\ud558\ub294 \ub370 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.</p> <pre><code>from langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n</code></pre>", "boost": 2}, {"location": "concepts/low_level/#end-node", "title": "END Node", "text": "<p><code>END</code> \ub178\ub4dc\ub294 \uc885\ub8cc \ub178\ub4dc\uc774\uba70, \uc791\uc5c5\uc774 \ub05d\ub09c \ud6c4 \ub354 \uc774\uc0c1 \uc2e4\ud589\ud560 \uc5e3\uc9c0\uac00 \uc5c6\uc74c\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.</p> <pre><code>from langgraph.graph import END\n\ngraph.add_edge(\"node_a\", END)\n</code></pre>", "boost": 2}, {"location": "concepts/low_level/#node-caching", "title": "Node Caching", "text": "<p>LangGraph\ub294 \ub178\ub4dc \uc785\ub825\uc5d0 \uae30\ubc18\ud55c \uc791\uc5c5/\ub178\ub4dc \uce90\uc2f1\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \ub178\ub4dc \uacb0\uacfc\ub97c \uce90\uc2dc\ud558\ub824\uba74 \uadf8\ub798\ud504 \ucef4\ud30c\uc77c \uc2dc \uce90\uc2dc\ub97c \uc9c0\uc815\ud558\uace0, \uac01 \ub178\ub4dc\uc5d0 \uce90\uc2dc \uc815\ucc45\uc744 \uc124\uc815\ud569\ub2c8\ub2e4:</p> <ul> <li>\uadf8\ub798\ud504 \ucef4\ud30c\uc77c \uc2dc \uce90\uc2dc\ub97c \uc9c0\uc815\ud569\ub2c8\ub2e4 (\ub610\ub294 \uc5d4\ud2b8\ub9ac\ud3ec\uc778\ud2b8 \uc9c0\uc815 \uc2dc)</li> <li>\ub178\ub4dc\uc5d0 \ub300\ud55c \uce90\uc2dc \uc815\ucc45\uc744 \uc9c0\uc815\ud569\ub2c8\ub2e4. \uac01 \uce90\uc2dc \uc815\ucc45\uc740 \ub2e4\uc74c\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4:</li> <li><code>key_func</code>: \ub178\ub4dc \uc785\ub825\uac12\uc744 \uae30\ubc18\uc73c\ub85c \uce90\uc2dc \ud0a4\ub97c \uc0dd\uc131\ud558\ub294 \ud568\uc218\ub85c, \uae30\ubcf8\uac12\uc740 \uc785\ub825\uac12\uc758 \ud53c\ud074(pickle) \ud574\uc2dc\uc785\ub2c8\ub2e4.</li> <li><code>ttl</code>: \uce90\uc2dc\uc758 \uc720\ud6a8 \uae30\uac04(\ucd08 \ub2e8\uc704). \uc9c0\uc815\ud558\uc9c0 \uc54a\uc73c\uba74 \uce90\uc2dc\ub294 \uc601\uad6c\uc801\uc73c\ub85c \uc720\uc9c0\ub429\ub2c8\ub2e4.</li> </ul> <p>\uc608\ub97c \ub4e4\uc5b4:</p> <pre><code>import time\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph\nfrom langgraph.cache.memory import InMemoryCache\nfrom langgraph.types import CachePolicy\n\nclass State(TypedDict):\n    x: int\n    result: int\n\nbuilder = StateGraph(State)\n\ndef expensive_node(state: State) -&gt; dict[str, int]:\n    # \ube44\uc6a9\uc774 \ub9ce\uc774 \ub4dc\ub294 \uc5f0\uc0b0\n    time.sleep(2)\n    return {\"result\": state[\"x\"] * 2}\n\nbuilder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\nbuilder.set_entry_point(\"expensive_node\")\nbuilder.set_finish_point(\"expensive_node\")\n\ngraph = builder.compile(cache=InMemoryCache())\n\nprint(graph.invoke({\"x\": 5}, stream_mode='updates')) # (1)!\n# [{'expensive_node': {'result': 10}}]\nprint(graph.invoke({\"x\": 5}, stream_mode='updates')) # (2)!\n# [{'expensive_node': {'result': 10}, '__metadata__': {'cached': True}}]\n</code></pre> <ol> <li>\uccab \uc2e4\ud589\uc740 \ubaa8\uc758 \uad6c\ud604\ub41c \ube44\uc6a9\uc774 \ub9ce\uc774 \ub4dc\ub294 \uacc4\uc0b0\uc73c\ub85c \uc778\ud574 2\ucd08\uac00 \uc18c\uc694\ub429\ub2c8\ub2e4.</li> <li>\ub450 \ubc88\uc9f8 \uc2e4\ud589\uc740 \uce90\uc2dc\ub97c \ud65c\uc6a9\ud558\uc5ec \ube60\ub974\uac8c \uacb0\uacfc\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4.</li> </ol>", "boost": 2}, {"location": "concepts/low_level/#edges", "title": "Edges", "text": "<p>Edges\ub294 \ub85c\uc9c1 \ud750\ub984\uc744 \uc815\uc758\ud558\uace0 \uadf8\ub798\ud504\uac00 \uc5b8\uc81c \uba48\ucd9c\uc9c0\ub97c \uacb0\uc815\ud569\ub2c8\ub2e4. \uc8fc\uc694 \uc720\ud615\uc740 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:</p> <ul> <li>Normal Edges: \ud55c \ub178\ub4dc\uc5d0\uc11c \ub2e4\uc74c \ub178\ub4dc\ub85c \uc9c1\uc811 \uc5f0\uacb0\ud569\ub2c8\ub2e4.</li> <li>Conditional Edges: \ud568\uc218\ub97c \ud638\ucd9c\ud574 \ub2e4\uc74c \ub178\ub4dc(\ub4e4)\ub97c \uacb0\uc815\ud569\ub2c8\ub2e4.</li> <li>Entry Point: \uc0ac\uc6a9\uc790 \uc785\ub825\uc774 \ub4e4\uc5b4\uc62c \ub54c \ucc98\uc74c \ud638\ucd9c\ub418\ub294 \ub178\ub4dc.</li> <li>Conditional Entry Point: \uc0ac\uc6a9\uc790 \uc785\ub825\uc5d0 \ub530\ub77c \uc2dc\uc791 \ub178\ub4dc\ub97c \uc120\ud0dd\ud569\ub2c8\ub2e4.</li> </ul> <p>\ub178\ub4dc\ub294 \uc5ec\ub7ec \uac1c\uc758 outgoing edge\ub97c \uac00\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\ub7ec \uac1c\uc758 outgoing edge\uac00 \uc788\uc73c\uba74 \ud574\ub2f9 \ubaa9\uc801\uc9c0 \ub178\ub4dc\ub4e4\uc774 **\ubcd1\ub82c**\ub85c \ub2e4\uc74c \uc288\ud37c\uc2a4\ud15d\uc5d0\uc11c \uc2e4\ud589\ub429\ub2c8\ub2e4.</p>", "boost": 2}, {"location": "concepts/low_level/#normal-edges", "title": "Normal Edges", "text": "<p>\ud56d\uc0c1 A \ub178\ub4dc\uc5d0\uc11c B \ub178\ub4dc\ub85c \uc774\ub3d9\ud558\uace0 \uc2f6\ub2e4\uba74 <code>add_edge</code>\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4:</p> <pre><code>graph.add_edge(\"node_a\", \"node_b\")\n</code></pre>", "boost": 2}, {"location": "concepts/low_level/#conditional-edges", "title": "Conditional Edges", "text": "<p>\uc120\ud0dd\uc801\uc73c\ub85c 1\uac1c \uc774\uc0c1\uc758 \uc5e3\uc9c0(\ub610\ub294 \uc885\ub8cc)\ub97c \ub77c\uc6b0\ud305\ud558\uace0 \uc2f6\ub2e4\uba74 <code>add_conditional_edges</code>\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4:</p> <pre><code>graph.add_conditional_edges(\"node_a\", routing_function)\n</code></pre> <p><code>routing_function</code>\uc740 \ud604\uc7ac <code>state</code>\ub97c \ubc1b\uc544 \ub2e4\uc74c\uc5d0 \ubcf4\ub0bc \ub178\ub4dc \uc774\ub984(\ub610\ub294 \ub9ac\uc2a4\ud2b8)\uc744 \ubc18\ud658\ud569\ub2c8\ub2e4. \ubc18\ud658\uac12\uc774 \ubc14\ub85c \ub2e4\uc74c \ub178\ub4dc \uc774\ub984\uc774 \ub418\uba70, \uc5ec\ub7ec \ub178\ub4dc\uac00 \ub3d9\uc2dc\uc5d0 \uc2e4\ud589\ub429\ub2c8\ub2e4.</p> <p>\ub515\uc154\ub108\ub9ac\ub97c \uc81c\uacf5\ud574 \ubc18\ud658\uac12\uc744 \ub178\ub4dc \uc774\ub984\uc5d0 \ub9e4\ud551\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>graph.add_conditional_edges(\n    \"node_a\",\n    routing_function,\n    {True: \"node_b\", False: \"node_c\"}\n)\n</code></pre>", "boost": 2}, {"location": "concepts/low_level/#_5", "title": "\ud301", "text": "<p>\uc870\uac74\ubd80 \ud750\ub984\uacfc \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8\ub97c \ub3d9\uc2dc\uc5d0 \ud558\uace0 \uc2f6\ub2e4\uba74 <code>Command</code>\ub97c \uc0ac\uc6a9\ud558\uc138\uc694. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uc544\ub798 <code>Command</code> \uc139\uc158\uc744 \ucc38\uace0\ud558\uc2ed\uc2dc\uc624.</p>", "boost": 2}, {"location": "concepts/low_level/#entry-point", "title": "Entry Point", "text": "<p>\uccab \ubc88\uc9f8 \uc2e4\ud589 \ub178\ub4dc\ub97c \uc9c0\uc815\ud558\ub824\uba74 <code>START</code> \ub178\ub4dc\uc5d0\uc11c \uc2e4\uc81c \ub178\ub4dc\ub85c \uc5e3\uc9c0\ub97c \uc5f0\uacb0\ud569\ub2c8\ub2e4:</p> <pre><code>from langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n</code></pre>", "boost": 2}, {"location": "concepts/low_level/#conditional-entry-point", "title": "Conditional Entry Point", "text": "<p>\uc2dc\uc791 \uc2dc\uc810\uc5d0 \ub2e4\ub978 \ub178\ub4dc\ub85c \uc9c4\uc785\ud558\uace0 \uc2f6\ub2e4\uba74 <code>add_conditional_edges</code>\ub97c <code>START</code>\uc640 \ud568\uaed8 \uc0ac\uc6a9\ud569\ub2c8\ub2e4:</p> <pre><code>from langgraph.graph import START\n\ngraph.add_conditional_edges(START, routing_function)\n</code></pre> <p>\ud544\uc694\uc5d0 \ub530\ub77c \ubc18\ud658\uac12\uc744 \ub178\ub4dc \uc774\ub984\uc5d0 \ub9e4\ud551\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>graph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})\n</code></pre>", "boost": 2}, {"location": "concepts/low_level/#send", "title": "Send", "text": "<p>\ub300\ubd80\ubd84\uc758 \ub178\ub4dc\uc640 \uc5e3\uc9c0\ub294 \uc0ac\uc804\uc5d0 \uc815\uc758\ub41c \uc0c1\ud0dc\uc640 \ub3d9\uc77c\ud55c \uc0c1\ud0dc\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uacbd\uc6b0\uc5d0 \ub530\ub77c \ub3d9\uc801\uc73c\ub85c \uc5e3\uc9c0\ub97c \uc0dd\uc131\ud558\uac70\ub098, \uac01 \uac1d\uccb4\ub9c8\ub2e4 \ubcc4\ub3c4\uc758 State\uac00 \ud544\uc694\ud560 \ub54c\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, map-reduce \ud328\ud134\uc5d0\uc11c\ub294 \uccab \ubc88\uc9f8 \ub178\ub4dc\uac00 \uac1d\uccb4 \ub9ac\uc2a4\ud2b8\ub97c \uc0dd\uc131\ud558\uace0, \uadf8 \ub9ac\uc2a4\ud2b8\uc758 \uac01 \ud56d\ubaa9\uc5d0 \ub300\ud574 \ubcc4\ub3c4\uc758 \ub178\ub4dc\ub97c \uc801\uc6a9\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\ub54c <code>Send</code> \uac1d\uccb4\ub97c \uc0ac\uc6a9\ud574 \ub3d9\uc801\uc73c\ub85c \uc5e3\uc9c0\ub97c \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <pre><code>def continue_to_jokes(state: OverallState):\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state['subjects']]\n\ngraph.add_conditional_edges(\"node_a\", continue_to_jokes)\n</code></pre>", "boost": 2}, {"location": "concepts/low_level/#command", "title": "Command", "text": "<p><code>Command</code>\ub294 **\uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8**\uc640 **\uc81c\uc5b4 \ud750\ub984**\uc744 \ud558\ub098\uc758 \ub178\ub4dc\uc5d0\uc11c \ub3d9\uc2dc\uc5d0 \uc218\ud589\ud558\uace0 \uc2f6\uc744 \ub54c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.</p> <pre><code>def my_node(state: State) -&gt; Command[Literal[\"my_other_node\"]]:\n    return Command(\n        # \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8\n        update={\"foo\": \"bar\"},\n        # \uc81c\uc5b4 \ud750\ub984\n        goto=\"my_other_node\"\n    )\n</code></pre> <p>\uc870\uac74\ubd80 \ub85c\uc9c1\uc744 \ud3ec\ud568\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>def my_node(state: State) -&gt; Command[Literal[\"my_other_node\"]]:\n    if state[\"foo\"] == \"bar\":\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")\n</code></pre> <p>\uc8fc\uc758: <code>Command</code>\ub97c \ubc18\ud658\ud558\ub294 \ud568\uc218\ub294 \ubc18\ud658 \ud0c0\uc785\uc5d0 \ub77c\uc6b0\ud305\ub420 \ub178\ub4dc \uc774\ub984 \ub9ac\uc2a4\ud2b8\ub97c \uba85\uc2dc\ud574\uc57c \ud569\ub2c8\ub2e4(<code>Command[Literal[\"my_other_node\"]]</code>). \uc774\ub294 \uadf8\ub798\ud504 \ub80c\ub354\ub9c1\uacfc \ud0c0\uc785 \uac80\uc99d\uc5d0 \ud544\uc694\ud569\ub2c8\ub2e4.</p> <p><code>Command</code> \uc0ac\uc6a9\ubc95\uc5d0 \ub300\ud55c \uc804\uccb4\uc801\uc778 \uc608\uc2dc\ub294 \uc774 \uc0ac\uc6a9 \ubc29\ubc95 \uac00\uc774\ub4dc\ub97c \ucc38\uace0\ud558\uc138\uc694.</p>", "boost": 2}, {"location": "concepts/low_level/#command-conditional-edges", "title": "\uc5b8\uc81c Command\ub97c \uc0ac\uc6a9\ud558\uace0 Conditional Edges\ub97c \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc744\uae4c?", "text": "<ul> <li>Command: \uc0c1\ud0dc\ub97c \uc5c5\ub370\uc774\ud2b8\ud558\uba74\uc11c \ub3d9\uc2dc\uc5d0 \ub2e4\ub978 \ub178\ub4dc\ub85c \uc774\ub3d9\ud574\uc57c \ud560 \ub54c(\uc608: multi-agent handoffs).</li> <li>Conditional Edges: \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8 \uc5c6\uc774 \uc21c\uc218\ud558\uac8c \ud750\ub984\ub9cc \uc81c\uc5b4\ud558\uace0 \uc2f6\uc744 \ub54c.</li> </ul>", "boost": 2}, {"location": "concepts/low_level/#_6", "title": "\ubd80\ubaa8 \uadf8\ub798\ud504\uc5d0\uc11c \ub178\ub4dc \uc774\ub3d9\ud558\uae30", "text": "<p>\uc11c\ube0c\uadf8\ub798\ud504\ub97c \uc0ac\uc6a9\ud560 \ub54c, \uc11c\ube0c\uadf8\ub798\ud504 \ub0b4\ubd80 \ub178\ub4dc\uc5d0\uc11c **\ubd80\ubaa8 \uadf8\ub798\ud504**\uc758 \ub2e4\ub978 \ub178\ub4dc\ub85c \uc774\ub3d9\ud558\uace0 \uc2f6\ub2e4\uba74 <code>Command</code>\uc5d0 <code>graph=Command.PARENT</code>\ub97c \uc9c0\uc815\ud569\ub2c8\ub2e4:</p> <pre><code>def my_node(state: State) -&gt; Command[Literal[\"other_subgraph\"]]:\n    return Command(\n        update={\"foo\": \"bar\"},\n        goto=\"other_subgraph\",   # \ubd80\ubaa8 \uadf8\ub798\ud504\uc5d0 \uc788\ub294 \ub178\ub4dc\n        graph=Command.PARENT\n    )\n</code></pre> <p>\ubd80\ubaa8 \uadf8\ub798\ud504\uc5d0 \uc788\ub294 \ud0a4\ub97c \uc5c5\ub370\uc774\ud2b8\ud558\ub824\uba74 \ud574\ub2f9 \ud0a4\uc5d0 \ub300\ud55c **reducer**\ub97c \ubd80\ubaa8 \uadf8\ub798\ud504\uc758 State\uc5d0 \uc815\uc758\ud574\uc57c \ud569\ub2c8\ub2e4.</p> <p>\uc774\ub294 \ud2b9\ud788 \ub2e4\uc911 \uc5d0\uc774\uc804\ud2b8 \ud578\ub4dc\uc624\ud504\ub97c \uad6c\ud604\ud560 \ub54c \uc720\uc6a9\ud569\ub2c8\ub2e4.</p> <p>\uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uc774 \uac00\uc774\ub4dc\ub97c \ucc38\uc870\ud558\uc138\uc694.</p>", "boost": 2}, {"location": "concepts/low_level/#_7", "title": "\ub3c4\uad6c \ub0b4\ubd80\uc5d0\uc11c \uc0ac\uc6a9\ud558\uae30", "text": "<p>\ud234 \ub0b4\ubd80\uc5d0\uc11c \uadf8\ub798\ud504 \uc0c1\ud0dc\ub97c \uc5c5\ub370\uc774\ud2b8\ud558\ub294 \uacbd\uc6b0\uac00 \ub9ce\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uace0\uac1d \uc9c0\uc6d0 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0\uc11c \ucd08\uae30 \ub2e8\uacc4\uc5d0 \uace0\uac1d \uc815\ubcf4\ub97c \uc870\ud68c\ud558\uace0 \uc2f6\uc744 \ub54c \ud234\uc744 \ud638\ucd9c\ud574 \uc0c1\ud0dc\ub97c \uc5c5\ub370\uc774\ud2b8\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ud574\ub2f9 \uac00\uc774\ub4dc\ub97c \ucc38\uace0\ud558\uc138\uc694.</p>", "boost": 2}, {"location": "concepts/low_level/#humanintheloop", "title": "Human\u2011in\u2011the\u2011loop", "text": "<p><code>Command</code>\ub294 \ud734\uba3c\u2011\uc778\u2011\ub8e8\ud504 \uc6cc\ud06c\ud50c\ub85c\uc6b0\uc5d0\uc11c\ub3c4 \uc911\uc694\ud55c \uc5ed\ud560\uc744 \ud569\ub2c8\ub2e4. <code>interrupt()</code>\ub85c \uc0ac\uc6a9\uc790 \uc785\ub825\uc744 \ubc1b\uace0, \uadf8 \uc785\ub825\uc744 <code>Command(resume=\"User input\")</code>\uc73c\ub85c \uc804\ub2ec\ud574 \uc2e4\ud589\uc744 \uc7ac\uac1c\ud569\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uac1c\ub150 \uac00\uc774\ub4dc\ub97c \ud655\uc778\ud558\uc138\uc694.</p>", "boost": 2}, {"location": "concepts/low_level/#graph-migrations", "title": "Graph Migrations", "text": "<p>LangGraph\ub294 \uccb4\ud06c\ud3ec\uc778\ud130\ub97c \uc0ac\uc6a9\ud574 \uadf8\ub798\ud504 \uc815\uc758(\ub178\ub4dc, \uc5e3\uc9c0, \uc0c1\ud0dc)\ub97c **\ub9c8\uc774\uadf8\ub808\uc774\uc158**\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <ul> <li>\uc644\ub8cc\ub41c \uc2a4\ub808\ub4dc: \uc804\uccb4 \ud1a0\ud3f4\ub85c\uc9c0\ub97c \uc790\uc720\ub86d\uac8c \ubcc0\uacbd \uac00\ub2a5(\ub178\ub4dc \ucd94\uac00\u00b7\uc0ad\uc81c\u00b7\uc774\ub984 \ubcc0\uacbd \ub4f1).</li> <li>\uc911\ub2e8\ub41c \uc2a4\ub808\ub4dc: \ub178\ub4dc \uc0ad\uc81c\u00b7\uc774\ub984 \ubcc0\uacbd\uc740 \ubd88\uac00\ub2a5(\ud604\uc7ac \uc9c4\ud589 \uc911\uc778 \ub178\ub4dc\uac00 \uc0ac\ub77c\uc9c8 \uc704\ud5d8).</li> <li>\uc0c1\ud0dc: \ud0a4 \ucd94\uac00\u00b7\uc0ad\uc81c\ub294 \uc591\ubc29\ud5a5 \ud638\ud658\uc131\uc744 \uac00\uc9d0. \ud0a4 \uc774\ub984\uc744 \ubc14\uafb8\uba74 \uae30\uc874 \uc2a4\ub808\ub4dc\uc758 \uc800\uc7a5\ub41c \uc0c1\ud0dc\ub294 \uc190\uc2e4\ub428. \ud0c0\uc785\uc774 \ud638\ud658\ub418\uc9c0 \uc54a\uac8c \ubc14\ub00c\uba74 \uae30\uc874 \uc2a4\ub808\ub4dc\uc5d0 \ubb38\uc81c\uac00 \ubc1c\uc0dd\ud560 \uc218 \uc788\uc74c.</li> </ul>", "boost": 2}, {"location": "concepts/low_level/#runtime-context", "title": "Runtime Context", "text": "<p>\uadf8\ub798\ud504\ub97c \ub9cc\ub4e4 \ub54c <code>context_schema</code>\ub97c \uc9c0\uc815\ud574 \ub7f0\ud0c0\uc784 \ucee8\ud14d\uc2a4\ud2b8\ub97c \ub178\ub4dc\uc5d0 \uc804\ub2ec\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \ubaa8\ub378 \uc774\ub984\uc774\ub098 \ub370\uc774\ud130\ubca0\uc774\uc2a4 \uc5f0\uacb0 \uac19\uc740, \uadf8\ub798\ud504 \uc0c1\ud0dc\uc640\ub294 \ubcc4\uac1c\uc778 \uc815\ubcf4\ub97c \uc804\ub2ec\ud560 \ub54c \uc720\uc6a9\ud569\ub2c8\ub2e4.</p> <pre><code>from dataclasses import dataclass\n\n@dataclass\nclass ContextSchema:\n    llm_provider: str = \"openai\"\n\ngraph = StateGraph(State, context_schema=ContextSchema)\n</code></pre> <p>\ub7f0\ud0c0\uc784 \uc2dc <code>invoke</code>\uc5d0 <code>context</code> \ud30c\ub77c\ubbf8\ud130\ub97c \uc804\ub2ec\ud558\uba74 \ub178\ub4dc \ub0b4\ubd80\uc5d0\uc11c \uc811\uadfc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>graph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\n</code></pre> <p>\ub178\ub4dc\uc5d0\uc11c \ucee8\ud14d\uc2a4\ud2b8\ub97c \uc0ac\uc6a9\ud558\ub294 \uc608\uc2dc:</p> <pre><code>from langgraph.runtime import Runtime\n\ndef node_a(state: State, runtime: Runtime[ContextSchema]):\n    llm = get_llm(runtime.context.llm_provider)\n    ...\n</code></pre> <p>\uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uad6c\uc131 \uac00\uc774\ub4dc\ub97c \ucc38\uace0\ud558\uc138\uc694.</p>", "boost": 2}, {"location": "concepts/low_level/#recursion-limit", "title": "Recursion Limit", "text": "<p>\uc7ac\uadc0 \uc81c\ud55c\uc740 \uadf8\ub798\ud504\uac00 \ub2e8\uc77c \uc2e4\ud589 \ub3d9\uc548 \uc218\ud589\ud560 \uc218 \uc788\ub294 \ucd5c\ub300 \uc288\ud37c\uc2a4\ud15d \uc218\ub97c \uc815\uc758\ud569\ub2c8\ub2e4. \uae30\ubcf8\uac12\uc740 25\uc774\uba70, <code>invoke</code>\u00b7<code>stream</code> \ud638\ucd9c \uc2dc <code>config</code> \ub515\uc154\ub108\ub9ac\uc758 <code>recursion_limit</code> \ud0a4\ub85c \uc870\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uac12\uc740 <code>configurable</code> \ud0a4 \uc548\uc774 \uc544\ub2c8\ub77c \ucd5c\uc0c1\uc704 <code>config</code>\uc5d0 \uc9c1\uc811 \ub123\uc5b4\uc57c \ud569\ub2c8\ub2e4.</p> <pre><code>graph.invoke(inputs, config={\"recursion_limit\": 5}, context={\"llm\": \"anthropic\"})\n</code></pre> <p>\uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uc7ac\uadc0 \uc81c\ud55c \uac00\uc774\ub4dc\ub97c \ud655\uc778\ud558\uc138\uc694.</p>", "boost": 2}, {"location": "concepts/low_level/#visualization", "title": "Visualization", "text": "<p>\ubcf5\uc7a1\ud55c \uadf8\ub798\ud504\ub294 \uc2dc\uac01\ud654\uac00 \ub3c4\uc6c0\uc774 \ub429\ub2c8\ub2e4. LangGraph\ub294 \uc5ec\ub7ec \ub0b4\uc7a5 \uc2dc\uac01\ud654 \ubc29\ubc95\uc744 \uc81c\uacf5\ud558\ub2c8, \uc2dc\uac01\ud654 \uac00\uc774\ub4dc\ub97c \ucc38\uace0\ud574 \ubcf4\uc138\uc694.</p>", "boost": 2}, {"location": "concepts/mcp/", "title": "MCP", "text": "<p>Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to language models. LangGraph agents can use tools defined on MCP servers through the <code>langchain-mcp-adapters</code> library.</p> <p></p> <p>Install the <code>langchain-mcp-adapters</code> library to use MCP tools in LangGraph:</p> <p>:::python <pre><code>pip install langchain-mcp-adapters\n</code></pre> :::</p> <p>:::js <pre><code>npm install @langchain/mcp-adapters\n</code></pre> :::</p>"}, {"location": "concepts/memory/", "title": "Memory", "text": "<p>Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction.</p> <p>This conceptual guide covers two types of memory, based on their recall scope:</p> <ul> <li> <p>Short-term memory, or thread-scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent's state. State is persisted to a database using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step.</p> </li> <li> <p>Long-term memory stores user-specific or application-level data across sessions and is shared across conversational threads. It can be recalled at any time and in any thread. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides stores (reference doc) to let you save and recall long-term memories.</p> </li> </ul> <p></p>", "boost": 2}, {"location": "concepts/memory/#short-term-memory", "title": "Short-term memory", "text": "<p>Short-term memory lets your application remember previous interactions within a single thread or conversation. A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.</p> <p>LangGraph manages short-term memory as part of the agent's state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph's state, the bot can access the full context for a given conversation while maintaining separation between different threads.</p>", "boost": 2}, {"location": "concepts/memory/#manage-short-term-memory", "title": "Manage short-term memory", "text": "<p>Conversation history is the most common form of short-term memory, and long conversations pose a challenge to today's LLMs. A full history may not fit inside an LLM's context window, resulting in an irrecoverable error. Even if your LLM supports the full context length, most LLMs still perform poorly over long contexts. They get \"distracted\" by stale or off-topic content, all while suffering from slower response times and higher costs.</p> <p>Chat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information.</p> <p></p> <p>For more information on common techniques for managing messages, see the Add and manage memory guide.</p>", "boost": 2}, {"location": "concepts/memory/#long-term-memory", "title": "Long-term memory", "text": "<p>Long-term memory in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is thread-scoped, long-term memory is saved within custom \"namespaces.\"</p> <p>Long-term memory is a complex challenge without a one-size-fits-all solution. However, the following questions provide a framework to help you navigate the different techniques:</p> <ul> <li> <p>What is the type of memory? Humans use memories to remember facts (semantic memory), experiences (episodic memory), and rules (procedural memory). AI agents can use memory in the same ways. For example, AI agents can use memory to remember specific facts about a user to accomplish a task.</p> </li> <li> <p>When do you want to update memories? Memory can be updated as part of an agent's application logic (e.g., \"on the hot path\"). In this case, the agent typically decides to remember facts before responding to a user. Alternatively, memory can be updated as a background task (logic that runs in the background / asynchronously and generates memories). We explain the tradeoffs between these approaches in the section below.</p> </li> </ul>", "boost": 2}, {"location": "concepts/memory/#memory-types", "title": "Memory types", "text": "<p>Different applications require various types of memory. Although the analogy isn't perfect, examining human memory types can be insightful. Some research (e.g., the CoALA paper) have even mapped these human memory types to those used in AI agents.</p> Memory Type What is Stored Human Example Agent Example Semantic Facts Things I learned in school Facts about a user Episodic Experiences Things I did Past agent actions Procedural Instructions Instincts or motor skills Agent system prompt", "boost": 2}, {"location": "concepts/memory/#semantic-memory", "title": "Semantic memory", "text": "<p>Semantic memory, both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions. </p> <p>Note</p> <p>Semantic memory is different from \"semantic search,\" which is a technique for finding similar content using \"meaning\" (usually as embeddings). Semantic memory is a term from psychology, referring to storing facts and knowledge, while semantic search is a method for retrieving information based on meaning rather than exact matches.</p>", "boost": 2}, {"location": "concepts/memory/#profile", "title": "Profile", "text": "<p>Semantic memories can be managed in different ways. For example, memories can be a single, continuously updated \"profile\" of well-scoped and specific information about a user, organization, or other entity (including the agent itself). A profile is generally just a JSON document with various key-value pairs you've selected to represent your domain. </p> <p>When remembering a profile, you will want to make sure that you are updating the profile each time. As a result, you will want to pass in the previous profile and ask the model to generate a new profile (or some JSON patch to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or strict decoding when generating documents to ensure the memory schemas remains valid.</p> <p></p>", "boost": 2}, {"location": "concepts/memory/#collection", "title": "Collection", "text": "<p>Alternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you're less likely to lose information over time. It's easier for an LLM to generate new objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to higher recall downstream.</p> <p>However, this shifts some complexity memory updating. The model must now delete or update existing items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the Trustcall package for one way to manage this and consider evaluation (e.g., with a tool like LangSmith) to help you tune the behavior.</p> <p>Working with document collections also shifts complexity to memory search over the list. The <code>Store</code> currently supports both semantic search and filtering by content.</p> <p>Finally, using a collection of memories can make it challenging to provide comprehensive context to the model. While individual memories may follow a specific schema, this structure might not capture the full context or relationships between memories. As a result, when using these memories to generate responses, the model may lack important contextual information that would be more readily available in a unified profile approach.</p> <p></p> <p>Regardless of memory management approach, the central point is that the agent will use the semantic memories to ground its responses, which often leads to more personalized and relevant interactions.</p>", "boost": 2}, {"location": "concepts/memory/#episodic-memory", "title": "Episodic memory", "text": "<p>Episodic memory, in both humans and AI agents, involves recalling past events or actions. The CoALA paper frames this well: facts can be written to semantic memory, whereas experiences can be written to episodic memory. For AI agents, episodic memory is often used to help an agent remember how to accomplish a task. </p> <p>:::python In practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. Sometimes it's easier to \"show\" than \"tell\" and LLMs learn well from examples. Few-shot learning lets you \"program\" your LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various best-practices can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input. :::</p> <p>:::js In practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. Sometimes it's easier to \"show\" than \"tell\" and LLMs learn well from examples. Few-shot learning lets you \"program\" your LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various best-practices can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input. :::</p> <p>:::python Note that the memory store is just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a LangSmith Dataset to store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity (using a BM25-like algorithm for keyword based similarity). </p> <p>See this how-to video for example usage of dynamic few-shot example selection in LangSmith. Also, see this blog post showcasing few-shot prompting to improve tool calling performance and this blog post using few-shot example to align an LLMs to human preferences. :::</p> <p>:::js Note that the memory store is just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a LangSmith Dataset to store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity.</p> <p>See this how-to video for example usage of dynamic few-shot example selection in LangSmith. Also, see this blog post showcasing few-shot prompting to improve tool calling performance and this blog post using few-shot example to align an LLMs to human preferences. :::</p>", "boost": 2}, {"location": "concepts/memory/#procedural-memory", "title": "Procedural memory", "text": "<p>Procedural memory, in both humans and AI agents, involves remembering the rules used to perform tasks. In humans, procedural memory is like the internalized knowledge of how to perform tasks, such as riding a bike via basic motor skills and balance. Episodic memory, on the other hand, involves recalling specific experiences, such as the first time you successfully rode a bike without training wheels or a memorable bike ride through a scenic route. For AI agents, procedural memory is a combination of model weights, agent code, and agent's prompt that collectively determine the agent's functionality. </p> <p>In practice, it is fairly uncommon for agents to modify their model weights or rewrite their code. However, it is more common for agents to modify their own prompts. </p> <p>One effective approach to refining an agent's instructions is through \"Reflection\" or meta-prompting. This involves prompting the agent with its current instructions (e.g., the system prompt) along with recent conversations or explicit user feedback. The agent then refines its own instructions based on this input. This method is particularly useful for tasks where instructions are challenging to specify upfront, as it allows the agent to learn and adapt from its interactions.</p> <p>For example, we built a Tweet generator using external feedback and prompt re-writing to produce high-quality paper summaries for Twitter. In this case, the specific summarization prompt was difficult to specify a priori, but it was fairly easy for a user to critique the generated Tweets and provide feedback on how to improve the summarization process. </p> <p>The below pseudo-code shows how you might implement this with the LangGraph memory store, using the store to save a prompt, the <code>update_instructions</code> node to get the current prompt (as well as feedback from the conversation with the user captured in <code>state[\"messages\"]</code>), update the prompt, and save the new prompt back to the store. Then, the <code>call_model</code> get the updated prompt from the store and uses it to generate a response.</p> <p>:::python <pre><code># Node that *uses* the instructions\ndef call_model(state: State, store: BaseStore):\n    namespace = (\"agent_instructions\", )\n    instructions = store.get(namespace, key=\"agent_a\")[0]\n    # Application logic\n    prompt = prompt_template.format(instructions=instructions.value[\"instructions\"])\n    ...\n\n# Node that updates instructions\ndef update_instructions(state: State, store: BaseStore):\n    namespace = (\"instructions\",)\n    current_instructions = store.search(namespace)[0]\n    # Memory logic\n    prompt = prompt_template.format(instructions=current_instructions.value[\"instructions\"], conversation=state[\"messages\"])\n    output = llm.invoke(prompt)\n    new_instructions = output['new_instructions']\n    store.put((\"agent_instructions\",), \"agent_a\", {\"instructions\": new_instructions})\n    ...\n</code></pre> :::</p> <p>:::js <pre><code>// Node that *uses* the instructions\nconst callModel = async (state: State, store: BaseStore) =&gt; {\n    const namespace = [\"agent_instructions\"];\n    const instructions = await store.get(namespace, \"agent_a\");\n    // Application logic\n    const prompt = promptTemplate.format({ \n        instructions: instructions[0].value.instructions \n    });\n    // ...\n};\n\n// Node that updates instructions\nconst updateInstructions = async (state: State, store: BaseStore) =&gt; {\n    const namespace = [\"instructions\"];\n    const currentInstructions = await store.search(namespace);\n    // Memory logic\n    const prompt = promptTemplate.format({ \n        instructions: currentInstructions[0].value.instructions, \n        conversation: state.messages \n    });\n    const output = await llm.invoke(prompt);\n    const newInstructions = output.new_instructions;\n    await store.put([\"agent_instructions\"], \"agent_a\", { \n        instructions: newInstructions \n    });\n    // ...\n};\n</code></pre> :::</p> <p></p>", "boost": 2}, {"location": "concepts/memory/#writing-memories", "title": "Writing memories", "text": "<p>There are two primary methods for agents to write memories: \"in the hot path\" and \"in the background\".</p> <p></p>", "boost": 2}, {"location": "concepts/memory/#in-the-hot-path", "title": "In the hot path", "text": "<p>Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored.</p> <p>However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created.</p> <p>As an example, ChatGPT uses a save_memories tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our memory-agent template as an reference implementation.</p>", "boost": 2}, {"location": "concepts/memory/#in-the-background", "title": "In the background", "text": "<p>Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work.</p> <p>However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic.</p> <p>See our memory-service template as an reference implementation.</p>", "boost": 2}, {"location": "concepts/memory/#memory-storage", "title": "Memory storage", "text": "<p>LangGraph stores long-term memories as JSON documents in a store. Each memory is organized under a custom <code>namespace</code> (similar to a folder) and a distinct <code>key</code> (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters.</p> <p>:::python <pre><code>from langgraph.store.memory import InMemoryStore\n\n\ndef embed(texts: list[str]) -&gt; list[list[float]]:\n    # Replace with an actual embedding function or LangChain embeddings object\n    return [[1.0, 2.0] * len(texts)]\n\n\n# InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nstore = InMemoryStore(index={\"embed\": embed, \"dims\": 2})\nuser_id = \"my-user\"\napplication_context = \"chitchat\"\nnamespace = (user_id, application_context)\nstore.put(\n    namespace,\n    \"a-memory\",\n    {\n        \"rules\": [\n            \"User likes short, direct language\",\n            \"User only speaks English &amp; python\",\n        ],\n        \"my-key\": \"my-value\",\n    },\n)\n# get the \"memory\" by ID\nitem = store.get(namespace, \"a-memory\")\n# search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\nitems = store.search(\n    namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\"\n)\n</code></pre> :::</p> <p>:::js <pre><code>import { InMemoryStore } from \"@langchain/langgraph\";\n\nconst embed = (texts: string[]): number[][] =&gt; {\n    // Replace with an actual embedding function or LangChain embeddings object\n    return texts.map(() =&gt; [1.0, 2.0]);\n};\n\n// InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use.\nconst store = new InMemoryStore({ index: { embed, dims: 2 } });\nconst userId = \"my-user\";\nconst applicationContext = \"chitchat\";\nconst namespace = [userId, applicationContext];\n\nawait store.put(\n    namespace,\n    \"a-memory\",\n    {\n        rules: [\n            \"User likes short, direct language\",\n            \"User only speaks English &amp; TypeScript\",\n        ],\n        \"my-key\": \"my-value\",\n    }\n);\n\n// get the \"memory\" by ID\nconst item = await store.get(namespace, \"a-memory\");\n\n// search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity\nconst items = await store.search(\n    namespace, \n    { \n        filter: { \"my-key\": \"my-value\" }, \n        query: \"language preferences\" \n    }\n);\n</code></pre> :::</p> <p>For more information about the memory store, see the Persistence guide.</p>", "boost": 2}, {"location": "concepts/multi_agent/", "title": "Multi-agent systems", "text": "<p>An agent is a system that uses an LLM to decide the control flow of an application. As you develop these systems, they might grow more complex over time, making them harder to manage and scale. For example, you might run into the following problems:</p> <ul> <li>agent has too many tools at its disposal and makes poor decisions about which tool to call next</li> <li>context grows too complex for a single agent to keep track of</li> <li>there is a need for multiple specialization areas in the system (e.g. planner, researcher, math expert, etc.)</li> </ul> <p>To tackle these, you might consider breaking your application into multiple smaller, independent agents and composing them into a multi-agent system. These independent agents can be as simple as a prompt and an LLM call, or as complex as a ReAct agent (and more!).</p> <p>The primary benefits of using multi-agent systems are:</p> <ul> <li>Modularity: Separate agents make it easier to develop, test, and maintain agentic systems.</li> <li>Specialization: You can create expert agents focused on specific domains, which helps with the overall system performance.</li> <li>Control: You can explicitly control how agents communicate (as opposed to relying on function calling).</li> </ul>"}, {"location": "concepts/multi_agent/#multi-agent-architectures", "title": "Multi-agent architectures", "text": "<p>There are several ways to connect agents in a multi-agent system:</p> <ul> <li>Network: each agent can communicate with every other agent. Any agent can decide which other agent to call next.</li> <li>Supervisor: each agent communicates with a single supervisor agent. Supervisor agent makes decisions on which agent should be called next.</li> <li>Supervisor (tool-calling): this is a special case of supervisor architecture. Individual agents can be represented as tools. In this case, a supervisor agent uses a tool-calling LLM to decide which of the agent tools to call, as well as the arguments to pass to those agents.</li> <li>Hierarchical: you can define a multi-agent system with a supervisor of supervisors. This is a generalization of the supervisor architecture and allows for more complex control flows.</li> <li>Custom multi-agent workflow: each agent communicates with only a subset of agents. Parts of the flow are deterministic, and only some agents can decide which other agents to call next.</li> </ul>"}, {"location": "concepts/multi_agent/#handoffs", "title": "Handoffs", "text": "<p>In multi-agent architectures, agents can be represented as graph nodes. Each agent node executes its step(s) and decides whether to finish execution or route to another agent, including potentially routing to itself (e.g., running in a loop). A common pattern in multi-agent interactions is handoffs, where one agent hands off control to another. Handoffs allow you to specify:</p> <ul> <li>destination: target agent to navigate to (e.g., name of the node to go to)</li> <li>payload: information to pass to that agent (e.g., state update)</li> </ul> <p>To implement handoffs in LangGraph, agent nodes can return <code>Command</code> object that allows you to combine both control flow and state updates:</p> <p>:::python</p> <pre><code>def agent(state) -&gt; Command[Literal[\"agent\", \"another_agent\"]]:\n    # the condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\n    goto = get_next_agent(...)  # 'agent' / 'another_agent'\n    return Command(\n        # Specify which agent to call next\n        goto=goto,\n        # Update the graph state\n        update={\"my_state_key\": \"my_state_value\"}\n    )\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>graph.addNode((state) =&gt; {\n    // the condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\n    const goto = getNextAgent(...); // 'agent' / 'another_agent'\n    return new Command({\n      // Specify which agent to call next\n      goto,\n      // Update the graph state\n      update: { myStateKey: \"myStateValue\" }\n    });\n})\n</code></pre> <p>:::</p> <p>:::python In a more complex scenario where each agent node is itself a graph (i.e., a subgraph), a node in one of the agent subgraphs might want to navigate to a different agent. For example, if you have two agents, <code>alice</code> and <code>bob</code> (subgraph nodes in a parent graph), and <code>alice</code> needs to navigate to <code>bob</code>, you can set <code>graph=Command.PARENT</code> in the <code>Command</code> object:</p> <pre><code>def some_node_inside_alice(state):\n    return Command(\n        goto=\"bob\",\n        update={\"my_state_key\": \"my_state_value\"},\n        # specify which graph to navigate to (defaults to the current graph)\n        graph=Command.PARENT,\n    )\n</code></pre> <p>:::</p> <p>:::js In a more complex scenario where each agent node is itself a graph (i.e., a subgraph), a node in one of the agent subgraphs might want to navigate to a different agent. For example, if you have two agents, <code>alice</code> and <code>bob</code> (subgraph nodes in a parent graph), and <code>alice</code> needs to navigate to <code>bob</code>, you can set <code>graph: Command.PARNT</code> in the <code>Command</code> object:</p> <pre><code>alice.addNode((state) =&gt; {\n  return new Command({\n    goto: \"bob\",\n    update: { myStateKey: \"myStateValue\" },\n    // specify which graph to navigate to (defaults to the current graph)\n    graph: Command.PARENT,\n  });\n});\n</code></pre> <p>:::</p> <p>Note</p> <p>:::python</p> <p>If you need to support visualization for subgraphs communicating using <code>Command(graph=Command.PARENT)</code> you would need to wrap them in a node function with <code>Command</code> annotation: Instead of this:</p> <pre><code>builder.add_node(alice)\n</code></pre> <p>you would need to do this:</p> <pre><code>def call_alice(state) -&gt; Command[Literal[\"bob\"]]:\n    return alice.invoke(state)\n\nbuilder.add_node(\"alice\", call_alice)\n</code></pre> <p>:::</p> <p>:::js If you need to support visualization for subgraphs communicating using/ <code>Command({ graph: Command.PARENT })</code> you would need to wrap them in a node function with <code>Command</code> annotation:</p> <p>Instead of this:</p> <pre><code>builder.addNode(\"alice\", alice);\n</code></pre> <p>you would need to do this:</p> <pre><code>builder.addNode(\"alice\", (state) =&gt; alice.invoke(state), { ends: [\"bob\"] });\n</code></pre> <p>:::</p>"}, {"location": "concepts/multi_agent/#handoffs-as-tools", "title": "Handoffs as tools", "text": "<p>One of the most common agent types is a tool-calling agent. For those types of agents, a common pattern is wrapping a handoff in a tool call:</p> <p>:::python</p> <pre><code>from langchain_core.tools import tool\n\n@tool\ndef transfer_to_bob():\n    \"\"\"Transfer to bob.\"\"\"\n    return Command(\n        # name of the agent (node) to go to\n        goto=\"bob\",\n        # data to send to the agent\n        update={\"my_state_key\": \"my_state_value\"},\n        # indicate to LangGraph that we need to navigate to\n        # agent node in a parent graph\n        graph=Command.PARENT,\n    )\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { Command } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst transferToBob = tool(\n  async () =&gt; {\n    return new Command({\n      // name of the agent (node) to go to\n      goto: \"bob\",\n      // data to send to the agent\n      update: { myStateKey: \"myStateValue\" },\n      // indicate to LangGraph that we need to navigate to\n      // agent node in a parent graph\n      graph: Command.PARENT,\n    });\n  },\n  {\n    name: \"transfer_to_bob\",\n    description: \"Transfer to bob.\",\n    schema: z.object({}),\n  }\n);\n</code></pre> <p>:::</p> <p>This is a special case of updating the graph state from tools where, in addition to the state update, the control flow is included as well.</p> <p>Important</p> <p>:::python   If you want to use tools that return <code>Command</code>, you can use the prebuilt @[<code>create_react_agent</code>][create_react_agent] / @[<code>ToolNode</code>][ToolNode] components, or else implement your own logic:</p> <p><pre><code>def call_tools(state):\n    ...\n    commands = [tools_by_name[tool_call[\"name\"]].invoke(tool_call) for tool_call in tool_calls]\n    return commands\n</code></pre>   :::</p> <p>:::js   If you want to use tools that return <code>Command</code>, you can use the prebuilt @[<code>createReactAgent</code>][create_react_agent] / @[ToolNode] components, or else implement your own logic:</p> <p><pre><code>graph.addNode(\"call_tools\", async (state) =&gt; {\n  // ... tool execution logic\n  const commands = toolCalls.map((toolCall) =&gt;\n    toolsByName[toolCall.name].invoke(toolCall)\n  );\n  return commands;\n});\n</code></pre>   :::</p> <p>Let's now take a closer look at the different multi-agent architectures.</p>"}, {"location": "concepts/multi_agent/#network", "title": "Network", "text": "<p>In this architecture, agents are defined as graph nodes. Each agent can communicate with every other agent (many-to-many connections) and can decide which agent to call next. This architecture is good for problems that do not have a clear hierarchy of agents or a specific sequence in which agents should be called.</p> <p>:::python</p> <pre><code>from typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.types import Command\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\nmodel = ChatOpenAI()\n\ndef agent_1(state: MessagesState) -&gt; Command[Literal[\"agent_2\", \"agent_3\", END]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # to determine which agent to call next. a common pattern is to call the model\n    # with a structured output (e.g. force it to return an output with a \"next_agent\" field)\n    response = model.invoke(...)\n    # route to one of the agents or exit based on the LLM's decision\n    # if the LLM returns \"__end__\", the graph will finish execution\n    return Command(\n        goto=response[\"next_agent\"],\n        update={\"messages\": [response[\"content\"]]},\n    )\n\ndef agent_2(state: MessagesState) -&gt; Command[Literal[\"agent_1\", \"agent_3\", END]]:\n    response = model.invoke(...)\n    return Command(\n        goto=response[\"next_agent\"],\n        update={\"messages\": [response[\"content\"]]},\n    )\n\ndef agent_3(state: MessagesState) -&gt; Command[Literal[\"agent_1\", \"agent_2\", END]]:\n    ...\n    return Command(\n        goto=response[\"next_agent\"],\n        update={\"messages\": [response[\"content\"]]},\n    )\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(agent_1)\nbuilder.add_node(agent_2)\nbuilder.add_node(agent_3)\n\nbuilder.add_edge(START, \"agent_1\")\nnetwork = builder.compile()\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MessagesZodState, START, END } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { Command } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst model = new ChatOpenAI();\n\nconst agent1 = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  // you can pass relevant parts of the state to the LLM (e.g., state.messages)\n  // to determine which agent to call next. a common pattern is to call the model\n  // with a structured output (e.g. force it to return an output with a \"next_agent\" field)\n  const response = await model.invoke(...);\n  // route to one of the agents or exit based on the LLM's decision\n  // if the LLM returns \"__end__\", the graph will finish execution\n  return new Command({\n    goto: response.nextAgent,\n    update: { messages: [response.content] },\n  });\n};\n\nconst agent2 = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const response = await model.invoke(...);\n  return new Command({\n    goto: response.nextAgent,\n    update: { messages: [response.content] },\n  });\n};\n\nconst agent3 = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  // ...\n  return new Command({\n    goto: response.nextAgent,\n    update: { messages: [response.content] },\n  });\n};\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"agent1\", agent1, {\n    ends: [\"agent2\", \"agent3\", END]\n  })\n  .addNode(\"agent2\", agent2, {\n    ends: [\"agent1\", \"agent3\", END]\n  })\n  .addNode(\"agent3\", agent3, {\n    ends: [\"agent1\", \"agent2\", END]\n  })\n  .addEdge(START, \"agent1\");\n\nconst network = builder.compile();\n</code></pre> <p>:::</p>"}, {"location": "concepts/multi_agent/#supervisor", "title": "Supervisor", "text": "<p>In this architecture, we define agents as nodes and add a supervisor node (LLM) that decides which agent nodes should be called next. We use <code>Command</code> to route execution to the appropriate agent node based on supervisor's decision. This architecture also lends itself well to running multiple agents in parallel or using map-reduce pattern.</p> <p>:::python</p> <pre><code>from typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.types import Command\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\nmodel = ChatOpenAI()\n\ndef supervisor(state: MessagesState) -&gt; Command[Literal[\"agent_1\", \"agent_2\", END]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # to determine which agent to call next. a common pattern is to call the model\n    # with a structured output (e.g. force it to return an output with a \"next_agent\" field)\n    response = model.invoke(...)\n    # route to one of the agents or exit based on the supervisor's decision\n    # if the supervisor returns \"__end__\", the graph will finish execution\n    return Command(goto=response[\"next_agent\"])\n\ndef agent_1(state: MessagesState) -&gt; Command[Literal[\"supervisor\"]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # and add any additional logic (different models, custom prompts, structured output, etc.)\n    response = model.invoke(...)\n    return Command(\n        goto=\"supervisor\",\n        update={\"messages\": [response]},\n    )\n\ndef agent_2(state: MessagesState) -&gt; Command[Literal[\"supervisor\"]]:\n    response = model.invoke(...)\n    return Command(\n        goto=\"supervisor\",\n        update={\"messages\": [response]},\n    )\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(supervisor)\nbuilder.add_node(agent_1)\nbuilder.add_node(agent_2)\n\nbuilder.add_edge(START, \"supervisor\")\n\nsupervisor = builder.compile()\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MessagesZodState, Command, START, END } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { z } from \"zod\";\n\nconst model = new ChatOpenAI();\n\nconst supervisor = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  // you can pass relevant parts of the state to the LLM (e.g., state.messages)\n  // to determine which agent to call next. a common pattern is to call the model\n  // with a structured output (e.g. force it to return an output with a \"next_agent\" field)\n  const response = await model.invoke(...);\n  // route to one of the agents or exit based on the supervisor's decision\n  // if the supervisor returns \"__end__\", the graph will finish execution\n  return new Command({ goto: response.nextAgent });\n};\n\nconst agent1 = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  // you can pass relevant parts of the state to the LLM (e.g., state.messages)\n  // and add any additional logic (different models, custom prompts, structured output, etc.)\n  const response = await model.invoke(...);\n  return new Command({\n    goto: \"supervisor\",\n    update: { messages: [response] },\n  });\n};\n\nconst agent2 = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const response = await model.invoke(...);\n  return new Command({\n    goto: \"supervisor\",\n    update: { messages: [response] },\n  });\n};\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"supervisor\", supervisor, {\n    ends: [\"agent1\", \"agent2\", END]\n  })\n  .addNode(\"agent1\", agent1, {\n    ends: [\"supervisor\"]\n  })\n  .addNode(\"agent2\", agent2, {\n    ends: [\"supervisor\"]\n  })\n  .addEdge(START, \"supervisor\");\n\nconst supervisorGraph = builder.compile();\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MessagesZodState, Command, START, END } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { z } from \"zod\";\n\nconst model = new ChatOpenAI();\n\nconst supervisor = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  // you can pass relevant parts of the state to the LLM (e.g., state.messages)\n  // to determine which agent to call next. a common pattern is to call the model\n  // with a structured output (e.g. force it to return an output with a \"next_agent\" field)\n  const response = await model.invoke(...);\n  // route to one of the agents or exit based on the supervisor's decision\n  // if the supervisor returns \"__end__\", the graph will finish execution\n  return new Command({ goto: response.nextAgent });\n};\n\nconst agent1 = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  // you can pass relevant parts of the state to the LLM (e.g., state.messages)\n  // and add any additional logic (different models, custom prompts, structured output, etc.)\n  const response = await model.invoke(...);\n  return new Command({\n    goto: \"supervisor\",\n    update: { messages: [response] },\n  });\n};\n\nconst agent2 = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const response = await model.invoke(...);\n  return new Command({\n    goto: \"supervisor\",\n    update: { messages: [response] },\n  });\n};\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"supervisor\", supervisor, {\n    ends: [\"agent1\", \"agent2\", END]\n  })\n  .addNode(\"agent1\", agent1, {\n    ends: [\"supervisor\"]\n  })\n  .addNode(\"agent2\", agent2, {\n    ends: [\"supervisor\"]\n  })\n  .addEdge(START, \"supervisor\");\n\nconst supervisorGraph = builder.compile();\n</code></pre> <p>:::</p> <p>Check out this tutorial for an example of supervisor multi-agent architecture.</p>"}, {"location": "concepts/multi_agent/#supervisor-tool-calling", "title": "Supervisor (tool-calling)", "text": "<p>In this variant of the supervisor architecture, we define a supervisor agent which is responsible for calling sub-agents. The sub-agents are exposed to the supervisor as tools, and the supervisor agent decides which tool to call next. The supervisor agent follows a standard implementation as an LLM running in a while loop calling tools until it decides to stop.</p> <p>:::python</p> <pre><code>from typing import Annotated\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import InjectedState, create_react_agent\n\nmodel = ChatOpenAI()\n\n# this is the agent function that will be called as tool\n# notice that you can pass the state to the tool via InjectedState annotation\ndef agent_1(state: Annotated[dict, InjectedState]):\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # and add any additional logic (different models, custom prompts, structured output, etc.)\n    response = model.invoke(...)\n    # return the LLM response as a string (expected tool response format)\n    # this will be automatically turned to ToolMessage\n    # by the prebuilt create_react_agent (supervisor)\n    return response.content\n\ndef agent_2(state: Annotated[dict, InjectedState]):\n    response = model.invoke(...)\n    return response.content\n\ntools = [agent_1, agent_2]\n# the simplest way to build a supervisor w/ tool-calling is to use prebuilt ReAct agent graph\n# that consists of a tool-calling LLM node (i.e. supervisor) and a tool-executing node\nsupervisor = create_react_agent(model, tools)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst model = new ChatOpenAI();\n\n// this is the agent function that will be called as tool\n// notice that you can pass the state to the tool via config parameter\nconst agent1 = tool(\n  async (_, config) =&gt; {\n    const state = config.configurable?.state;\n    // you can pass relevant parts of the state to the LLM (e.g., state.messages)\n    // and add any additional logic (different models, custom prompts, structured output, etc.)\n    const response = await model.invoke(...);\n    // return the LLM response as a string (expected tool response format)\n    // this will be automatically turned to ToolMessage\n    // by the prebuilt createReactAgent (supervisor)\n    return response.content;\n  },\n  {\n    name: \"agent1\",\n    description: \"Agent 1 description\",\n    schema: z.object({}),\n  }\n);\n\nconst agent2 = tool(\n  async (_, config) =&gt; {\n    const state = config.configurable?.state;\n    const response = await model.invoke(...);\n    return response.content;\n  },\n  {\n    name: \"agent2\",\n    description: \"Agent 2 description\",\n    schema: z.object({}),\n  }\n);\n\nconst tools = [agent1, agent2];\n// the simplest way to build a supervisor w/ tool-calling is to use prebuilt ReAct agent graph\n// that consists of a tool-calling LLM node (i.e. supervisor) and a tool-executing node\nconst supervisor = createReactAgent({ llm: model, tools });\n</code></pre> <p>:::</p>"}, {"location": "concepts/multi_agent/#hierarchical", "title": "Hierarchical", "text": "<p>As you add more agents to your system, it might become too hard for the supervisor to manage all of them. The supervisor might start making poor decisions about which agent to call next, or the context might become too complex for a single supervisor to keep track of. In other words, you end up with the same problems that motivated the multi-agent architecture in the first place.</p> <p>To address this, you can design your system hierarchically. For example, you can create separate, specialized teams of agents managed by individual supervisors, and a top-level supervisor to manage the teams.</p> <p>:::python</p> <pre><code>from typing import Literal\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, MessagesState, START, END\nfrom langgraph.types import Command\nmodel = ChatOpenAI()\n\n# define team 1 (same as the single supervisor example above)\n\ndef team_1_supervisor(state: MessagesState) -&gt; Command[Literal[\"team_1_agent_1\", \"team_1_agent_2\", END]]:\n    response = model.invoke(...)\n    return Command(goto=response[\"next_agent\"])\n\ndef team_1_agent_1(state: MessagesState) -&gt; Command[Literal[\"team_1_supervisor\"]]:\n    response = model.invoke(...)\n    return Command(goto=\"team_1_supervisor\", update={\"messages\": [response]})\n\ndef team_1_agent_2(state: MessagesState) -&gt; Command[Literal[\"team_1_supervisor\"]]:\n    response = model.invoke(...)\n    return Command(goto=\"team_1_supervisor\", update={\"messages\": [response]})\n\nteam_1_builder = StateGraph(Team1State)\nteam_1_builder.add_node(team_1_supervisor)\nteam_1_builder.add_node(team_1_agent_1)\nteam_1_builder.add_node(team_1_agent_2)\nteam_1_builder.add_edge(START, \"team_1_supervisor\")\nteam_1_graph = team_1_builder.compile()\n\n# define team 2 (same as the single supervisor example above)\nclass Team2State(MessagesState):\n    next: Literal[\"team_2_agent_1\", \"team_2_agent_2\", \"__end__\"]\n\ndef team_2_supervisor(state: Team2State):\n    ...\n\ndef team_2_agent_1(state: Team2State):\n    ...\n\ndef team_2_agent_2(state: Team2State):\n    ...\n\nteam_2_builder = StateGraph(Team2State)\n...\nteam_2_graph = team_2_builder.compile()\n\n\n# define top-level supervisor\n\nbuilder = StateGraph(MessagesState)\ndef top_level_supervisor(state: MessagesState) -&gt; Command[Literal[\"team_1_graph\", \"team_2_graph\", END]]:\n    # you can pass relevant parts of the state to the LLM (e.g., state[\"messages\"])\n    # to determine which team to call next. a common pattern is to call the model\n    # with a structured output (e.g. force it to return an output with a \"next_team\" field)\n    response = model.invoke(...)\n    # route to one of the teams or exit based on the supervisor's decision\n    # if the supervisor returns \"__end__\", the graph will finish execution\n    return Command(goto=response[\"next_team\"])\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(top_level_supervisor)\nbuilder.add_node(\"team_1_graph\", team_1_graph)\nbuilder.add_node(\"team_2_graph\", team_2_graph)\nbuilder.add_edge(START, \"top_level_supervisor\")\nbuilder.add_edge(\"team_1_graph\", \"top_level_supervisor\")\nbuilder.add_edge(\"team_2_graph\", \"top_level_supervisor\")\ngraph = builder.compile()\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MessagesZodState, Command, START, END } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { z } from \"zod\";\n\nconst model = new ChatOpenAI();\n\n// define team 1 (same as the single supervisor example above)\n\nconst team1Supervisor = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const response = await model.invoke(...);\n  return new Command({ goto: response.nextAgent });\n};\n\nconst team1Agent1 = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const response = await model.invoke(...);\n  return new Command({\n    goto: \"team1Supervisor\",\n    update: { messages: [response] }\n  });\n};\n\nconst team1Agent2 = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const response = await model.invoke(...);\n  return new Command({\n    goto: \"team1Supervisor\",\n    update: { messages: [response] }\n  });\n};\n\nconst team1Builder = new StateGraph(MessagesZodState)\n  .addNode(\"team1Supervisor\", team1Supervisor, {\n    ends: [\"team1Agent1\", \"team1Agent2\", END]\n  })\n  .addNode(\"team1Agent1\", team1Agent1, {\n    ends: [\"team1Supervisor\"]\n  })\n  .addNode(\"team1Agent2\", team1Agent2, {\n    ends: [\"team1Supervisor\"]\n  })\n  .addEdge(START, \"team1Supervisor\");\nconst team1Graph = team1Builder.compile();\n\n// define team 2 (same as the single supervisor example above)\nconst team2Supervisor = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  // ...\n};\n\nconst team2Agent1 = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  // ...\n};\n\nconst team2Agent2 = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  // ...\n};\n\nconst team2Builder = new StateGraph(MessagesZodState);\n// ... build team2Graph\nconst team2Graph = team2Builder.compile();\n\n// define top-level supervisor\n\nconst topLevelSupervisor = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  // you can pass relevant parts of the state to the LLM (e.g., state.messages)\n  // to determine which team to call next. a common pattern is to call the model\n  // with a structured output (e.g. force it to return an output with a \"next_team\" field)\n  const response = await model.invoke(...);\n  // route to one of the teams or exit based on the supervisor's decision\n  // if the supervisor returns \"__end__\", the graph will finish execution\n  return new Command({ goto: response.nextTeam });\n};\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"topLevelSupervisor\", topLevelSupervisor, {\n    ends: [\"team1Graph\", \"team2Graph\", END]\n  })\n  .addNode(\"team1Graph\", team1Graph)\n  .addNode(\"team2Graph\", team2Graph)\n  .addEdge(START, \"topLevelSupervisor\")\n  .addEdge(\"team1Graph\", \"topLevelSupervisor\")\n  .addEdge(\"team2Graph\", \"topLevelSupervisor\");\n\nconst graph = builder.compile();\n</code></pre> <p>:::</p>"}, {"location": "concepts/multi_agent/#custom-multi-agent-workflow", "title": "Custom multi-agent workflow", "text": "<p>In this architecture we add individual agents as graph nodes and define the order in which agents are called ahead of time, in a custom workflow. In LangGraph the workflow can be defined in two ways:</p> <ul> <li> <p>Explicit control flow (normal edges): LangGraph allows you to explicitly define the control flow of your application (i.e. the sequence of how agents communicate) explicitly, via normal graph edges. This is the most deterministic variant of this architecture above \u2014 we always know which agent will be called next ahead of time.</p> </li> <li> <p>Dynamic control flow (Command): in LangGraph you can allow LLMs to decide parts of your application control flow. This can be achieved by using <code>Command</code>. A special case of this is a supervisor tool-calling architecture. In that case, the tool-calling LLM powering the supervisor agent will make decisions about the order in which the tools (agents) are being called.</p> </li> </ul> <p>:::python</p> <pre><code>from langchain_openai import ChatOpenAI\nfrom langgraph.graph import StateGraph, MessagesState, START\n\nmodel = ChatOpenAI()\n\ndef agent_1(state: MessagesState):\n    response = model.invoke(...)\n    return {\"messages\": [response]}\n\ndef agent_2(state: MessagesState):\n    response = model.invoke(...)\n    return {\"messages\": [response]}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(agent_1)\nbuilder.add_node(agent_2)\n# define the flow explicitly\nbuilder.add_edge(START, \"agent_1\")\nbuilder.add_edge(\"agent_1\", \"agent_2\")\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MessagesZodState, START } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { z } from \"zod\";\n\nconst model = new ChatOpenAI();\n\nconst agent1 = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const response = await model.invoke(...);\n  return { messages: [response] };\n};\n\nconst agent2 = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const response = await model.invoke(...);\n  return { messages: [response] };\n};\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"agent1\", agent1)\n  .addNode(\"agent2\", agent2)\n  // define the flow explicitly\n  .addEdge(START, \"agent1\")\n  .addEdge(\"agent1\", \"agent2\");\n</code></pre> <p>:::</p>"}, {"location": "concepts/multi_agent/#communication-and-state-management", "title": "Communication and state management", "text": "<p>The most important thing when building multi-agent systems is figuring out how the agents communicate.</p> <p>A common, generic way for agents to communicate is via a list of messages. This opens up the following questions:</p> <ul> <li>Do agents communicate via handoffs or via tool calls?</li> <li>What messages are passed from one agent to the next?</li> <li>How are handoffs represented in the list of messages?</li> <li>How do you manage state for subagents?</li> </ul> <p>Additionally, if you are dealing with more complex agents or wish to keep individual agent state separate from the multi-agent system state, you may need to use different state schemas.</p>"}, {"location": "concepts/multi_agent/#handoffs-vs-tool-calls", "title": "Handoffs vs tool calls", "text": "<p>What is the \"payload\" that is being passed around between agents? In most of the architectures discussed above, the agents communicate via handoffs and pass the graph state as part of the handoff payload. Specifically, agents pass around lists of messages as part of the graph state. In the case of the supervisor with tool-calling, the payloads are tool call arguments.</p> <p></p>"}, {"location": "concepts/multi_agent/#message-passing-between-agents", "title": "Message passing between agents", "text": "<p>The most common way for agents to communicate is via a shared state channel, typically a list of messages. This assumes that there is always at least a single channel (key) in the state that is shared by the agents (e.g., <code>messages</code>). When communicating via a shared message list, there is an additional consideration: should the agents share the full history of their thought process or only the final result?</p> <p></p>"}, {"location": "concepts/multi_agent/#sharing-full-thought-process", "title": "Sharing full thought process", "text": "<p>Agents can share the full history of their thought process (i.e., \"scratchpad\") with all other agents. This \"scratchpad\" would typically look like a list of messages. The benefit of sharing the full thought process is that it might help other agents make better decisions and improve reasoning ability for the system as a whole. The downside is that as the number of agents and their complexity grows, the \"scratchpad\" will grow quickly and might require additional strategies for memory management.</p>"}, {"location": "concepts/multi_agent/#sharing-only-final-results", "title": "Sharing only final results", "text": "<p>Agents can have their own private \"scratchpad\" and only share the final result with the rest of the agents. This approach might work better for systems with many agents or agents that are more complex. In this case, you would need to define agents with different state schemas.</p> <p>For agents called as tools, the supervisor determines the inputs based on the tool schema. Additionally, LangGraph allows passing state to individual tools at runtime, so subordinate agents can access parent state, if needed.</p>"}, {"location": "concepts/multi_agent/#indicating-agent-name-in-messages", "title": "Indicating agent name in messages", "text": "<p>It can be helpful to indicate which agent a particular AI message is from, especially for long message histories. Some LLM providers (like OpenAI) support adding a <code>name</code> parameter to messages \u2014 you can use that to attach the agent name to the message. If that is not supported, you can consider manually injecting the agent name into the message content, e.g., <code>&lt;agent&gt;alice&lt;/agent&gt;&lt;message&gt;message from alice&lt;/message&gt;</code>.</p>"}, {"location": "concepts/multi_agent/#representing-handoffs-in-message-history", "title": "Representing handoffs in message history", "text": "<p>:::python Handoffs are typically done via the LLM calling a dedicated handoff tool. This is represented as an AI message with tool calls that is passed to the next agent (LLM). Most LLM providers don't support receiving AI messages with tool calls without corresponding tool messages. :::</p> <p>:::js Handoffs are typically done via the LLM calling a dedicated handoff tool. This is represented as an AI message with tool calls that is passed to the next agent (LLM). Most LLM providers don't support receiving AI messages with tool calls without corresponding tool messages. :::</p> <p>You therefore have two options:</p> <p>:::python</p> <ol> <li>Add an extra tool message to the message list, e.g., \"Successfully transferred to agent X\"</li> <li>Remove the AI message with the tool calls    :::</li> </ol> <p>:::js</p> <ol> <li>Add an extra tool message to the message list, e.g., \"Successfully transferred to agent X\"</li> <li>Remove the AI message with the tool calls :::</li> </ol> <p>In practice, we see that most developers opt for option (1).</p>"}, {"location": "concepts/multi_agent/#state-management-for-subagents", "title": "State management for subagents", "text": "<p>A common practice is to have multiple agents communicating on a shared message list, but only adding their final messages to the list. This means that any intermediate messages (e.g., tool calls) are not saved in this list.</p> <p>What if you do want to save these messages so that if this particular subagent is invoked in the future you can pass those back in?</p> <p>There are two high-level approaches to achieve that:</p> <p>:::python</p> <ol> <li>Store these messages in the shared message list, but filter the list before passing it to the subagent LLM. For example, you can choose to filter out all tool calls from other agents.</li> <li>Store a separate message list for each agent (e.g., <code>alice_messages</code>) in the subagent's graph state. This would be their \"view\" of what the message history looks like. :::</li> </ol> <p>:::js</p> <ol> <li>Store these messages in the shared message list, but filter the list before passing it to the subagent LLM. For example, you can choose to filter out all tool calls from other agents.</li> <li>Store a separate message list for each agent (e.g., <code>aliceMessages</code>) in the subagent's graph state. This would be their \"view\" of what the message history looks like. :::</li> </ol>"}, {"location": "concepts/multi_agent/#using-different-state-schemas", "title": "Using different state schemas", "text": "<p>An agent might need to have a different state schema from the rest of the agents. For example, a search agent might only need to keep track of queries and retrieved documents. There are two ways to achieve this in LangGraph:</p> <ul> <li>Define subgraph agents with a separate state schema. If there are no shared state keys (channels) between the subgraph and the parent graph, it's important to add input / output transformations so that the parent graph knows how to communicate with the subgraphs.</li> <li>Define agent node functions with a private input state schema that is distinct from the overall graph state schema. This allows passing information that is only needed for executing that particular agent.</li> </ul>"}, {"location": "concepts/persistence/", "title": "Persistence", "text": "<p>LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a <code>checkpoint</code> of the graph state at every super-step. Those checkpoints are saved to a <code>thread</code>, which can be accessed after graph execution. Because <code>threads</code> allow access to graph's state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we'll discuss each of these concepts in more detail.</p> <p></p> <p>LangGraph API handles checkpointing automatically</p> <p>When using the LangGraph API, you don't need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes.</p>", "boost": 2}, {"location": "concepts/persistence/#threads", "title": "Threads", "text": "<p>A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of runs. When a run is executed, the state of the underlying graph of the assistant will be persisted to the thread.</p> <p>When invoking a graph with a checkpointer, you must specify a <code>thread_id</code> as part of the <code>configurable</code> portion of the config:</p> <p>:::python</p> <pre><code>{\"configurable\": {\"thread_id\": \"1\"}}\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>{\n  configurable: {\n    thread_id: \"1\";\n  }\n}\n</code></pre> <p>:::</p> <p>A thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangGraph Platform API provides several endpoints for creating and managing threads and thread state. See the API reference for more details.</p>", "boost": 2}, {"location": "concepts/persistence/#checkpoints", "title": "Checkpoints", "text": "<p>The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by <code>StateSnapshot</code> object with the following key properties:</p> <ul> <li><code>config</code>: Config associated with this checkpoint.</li> <li><code>metadata</code>: Metadata associated with this checkpoint.</li> <li><code>values</code>: Values of the state channels at this point in time.</li> <li><code>next</code> A tuple of the node names to execute next in the graph.</li> <li><code>tasks</code>: A tuple of <code>PregelTask</code> objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts.</li> </ul> <p>Checkpoints are persisted and can be used to restore the state of a thread at a later time.</p> <p>Let's see what checkpoints are saved when a simple graph is invoked as follows:</p> <p>:::python</p> <pre><code>from langgraph.graph import StateGraph, START, END\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: str\n    bar: Annotated[list[str], add]\n\ndef node_a(state: State):\n    return {\"foo\": \"a\", \"bar\": [\"a\"]}\n\ndef node_b(state: State):\n    return {\"foo\": \"b\", \"bar\": [\"b\"]}\n\n\nworkflow = StateGraph(State)\nworkflow.add_node(node_a)\nworkflow.add_node(node_b)\nworkflow.add_edge(START, \"node_a\")\nworkflow.add_edge(\"node_a\", \"node_b\")\nworkflow.add_edge(\"node_b\", END)\n\ncheckpointer = InMemorySaver()\ngraph = workflow.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"foo\": \"\"}, config)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, START, END, MemoryServer } from \"@langchain/langgraph\";\nimport { withLangGraph } from \"@langchain/langgraph/zod\";\nimport { z } from \"zod\";\n\nconst State = z.object({\n  foo: z.string(),\n  bar: withLangGraph(z.array(z.string()), {\n    reducer: {\n      fn: (x, y) =&gt; x.concat(y),\n    },\n    default: () =&gt; [],\n  }),\n});\n\nconst workflow = new StateGraph(State)\n  .addNode(\"nodeA\", (state) =&gt; {\n    return { foo: \"a\", bar: [\"a\"] };\n  })\n  .addNode(\"nodeB\", (state) =&gt; {\n    return { foo: \"b\", bar: [\"b\"] };\n  })\n  .addEdge(START, \"nodeA\")\n  .addEdge(\"nodeA\", \"nodeB\")\n  .addEdge(\"nodeB\", END);\n\nconst checkpointer = new MemorySaver();\nconst graph = workflow.compile({ checkpointer });\n\nconst config = { configurable: { thread_id: \"1\" } };\nawait graph.invoke({ foo: \"\" }, config);\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, START, END, MemoryServer } from \"@langchain/langgraph\";\nimport { withLangGraph } from \"@langchain/langgraph/zod\";\nimport { z } from \"zod\";\n\nconst State = z.object({\n  foo: z.string(),\n  bar: withLangGraph(z.array(z.string()), {\n    reducer: {\n      fn: (x, y) =&gt; x.concat(y),\n    },\n    default: () =&gt; [],\n  }),\n});\n\nconst workflow = new StateGraph(State)\n  .addNode(\"nodeA\", (state) =&gt; {\n    return { foo: \"a\", bar: [\"a\"] };\n  })\n  .addNode(\"nodeB\", (state) =&gt; {\n    return { foo: \"b\", bar: [\"b\"] };\n  })\n  .addEdge(START, \"nodeA\")\n  .addEdge(\"nodeA\", \"nodeB\")\n  .addEdge(\"nodeB\", END);\n\nconst checkpointer = new MemorySaver();\nconst graph = workflow.compile({ checkpointer });\n\nconst config = { configurable: { thread_id: \"1\" } };\nawait graph.invoke({ foo: \"\" }, config);\n</code></pre> <p>:::</p> <p>:::python</p> <p>After we run the graph, we expect to see exactly 4 checkpoints:</p> <ul> <li>empty checkpoint with <code>START</code> as the next node to be executed</li> <li>checkpoint with the user input <code>{'foo': '', 'bar': []}</code> and <code>node_a</code> as the next node to be executed</li> <li>checkpoint with the outputs of <code>node_a</code> <code>{'foo': 'a', 'bar': ['a']}</code> and <code>node_b</code> as the next node to be executed</li> <li>checkpoint with the outputs of <code>node_b</code> <code>{'foo': 'b', 'bar': ['a', 'b']}</code> and no next nodes to be executed</li> </ul> <p>Note that we <code>bar</code> channel values contain outputs from both nodes as we have a reducer for <code>bar</code> channel.</p> <p>:::</p> <p>:::js</p> <p>After we run the graph, we expect to see exactly 4 checkpoints:</p> <ul> <li>empty checkpoint with <code>START</code> as the next node to be executed</li> <li>checkpoint with the user input <code>{'foo': '', 'bar': []}</code> and <code>nodeA</code> as the next node to be executed</li> <li>checkpoint with the outputs of <code>nodeA</code> <code>{'foo': 'a', 'bar': ['a']}</code> and <code>nodeB</code> as the next node to be executed</li> <li>checkpoint with the outputs of <code>nodeB</code> <code>{'foo': 'b', 'bar': ['a', 'b']}</code> and no next nodes to be executed</li> </ul> <p>Note that the <code>bar</code> channel values contain outputs from both nodes as we have a reducer for the <code>bar</code> channel. :::</p>", "boost": 2}, {"location": "concepts/persistence/#get-state", "title": "Get state", "text": "<p>:::python When interacting with the saved graph state, you must specify a thread identifier. You can view the latest state of the graph by calling <code>graph.get_state(config)</code>. This will return a <code>StateSnapshot</code> object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.</p> <pre><code># get the latest state snapshot\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.get_state(config)\n\n# get a state snapshot for a specific checkpoint_id\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"1ef663ba-28fe-6528-8002-5a559208592c\"}}\ngraph.get_state(config)\n</code></pre> <p>:::</p> <p>:::js When interacting with the saved graph state, you must specify a thread identifier. You can view the latest state of the graph by calling <code>graph.getState(config)</code>. This will return a <code>StateSnapshot</code> object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided.</p> <pre><code>// get the latest state snapshot\nconst config = { configurable: { thread_id: \"1\" } };\nawait graph.getState(config);\n\n// get a state snapshot for a specific checkpoint_id\nconst config = {\n  configurable: {\n    thread_id: \"1\",\n    checkpoint_id: \"1ef663ba-28fe-6528-8002-5a559208592c\",\n  },\n};\nawait graph.getState(config);\n</code></pre> <p>:::</p> <p>:::python In our example, the output of <code>get_state</code> will look like this:</p> <pre><code>StateSnapshot(\n    values={'foo': 'b', 'bar': ['a', 'b']},\n    next=(),\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\n    metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\n    created_at='2024-08-29T19:19:38.821749+00:00',\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=()\n)\n</code></pre> <p>:::</p> <p>:::js In our example, the output of <code>getState</code> will look like this:</p> <pre><code>StateSnapshot {\n  values: { foo: 'b', bar: ['a', 'b'] },\n  next: [],\n  config: {\n    configurable: {\n      thread_id: '1',\n      checkpoint_ns: '',\n      checkpoint_id: '1ef663ba-28fe-6528-8002-5a559208592c'\n    }\n  },\n  metadata: {\n    source: 'loop',\n    writes: { nodeB: { foo: 'b', bar: ['b'] } },\n    step: 2\n  },\n  createdAt: '2024-08-29T19:19:38.821749+00:00',\n  parentConfig: {\n    configurable: {\n      thread_id: '1',\n      checkpoint_ns: '',\n      checkpoint_id: '1ef663ba-28f9-6ec4-8001-31981c2c39f8'\n    }\n  },\n  tasks: []\n}\n</code></pre> <p>:::</p>", "boost": 2}, {"location": "concepts/persistence/#get-state-history", "title": "Get state history", "text": "<p>:::python You can get the full history of the graph execution for a given thread by calling <code>graph.get_state_history(config)</code>. This will return a list of <code>StateSnapshot</code> objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / <code>StateSnapshot</code> being the first in the list.</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"1\"}}\nlist(graph.get_state_history(config))\n</code></pre> <p>:::</p> <p>:::js You can get the full history of the graph execution for a given thread by calling <code>graph.getStateHistory(config)</code>. This will return a list of <code>StateSnapshot</code> objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / <code>StateSnapshot</code> being the first in the list.</p> <pre><code>const config = { configurable: { thread_id: \"1\" } };\nfor await (const state of graph.getStateHistory(config)) {\n  console.log(state);\n}\n</code></pre> <p>:::</p> <p>:::python In our example, the output of <code>get_state_history</code> will look like this:</p> <pre><code>[\n    StateSnapshot(\n        values={'foo': 'b', 'bar': ['a', 'b']},\n        next=(),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28fe-6528-8002-5a559208592c'}},\n        metadata={'source': 'loop', 'writes': {'node_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2},\n        created_at='2024-08-29T19:19:38.821749+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\n        tasks=(),\n    ),\n    StateSnapshot(\n        values={'foo': 'a', 'bar': ['a']},\n        next=('node_b',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}},\n        metadata={'source': 'loop', 'writes': {'node_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1},\n        created_at='2024-08-29T19:19:38.819946+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\n        tasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node_b', error=None, interrupts=()),),\n    ),\n    StateSnapshot(\n        values={'foo': '', 'bar': []},\n        next=('node_a',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}},\n        metadata={'source': 'loop', 'writes': None, 'step': 0},\n        created_at='2024-08-29T19:19:38.817813+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\n        tasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node_a', error=None, interrupts=()),),\n    ),\n    StateSnapshot(\n        values={'bar': []},\n        next=('__start__',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}},\n        metadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1},\n        created_at='2024-08-29T19:19:38.816205+00:00',\n        parent_config=None,\n        tasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='__start__', error=None, interrupts=()),),\n    )\n]\n</code></pre> <p>:::</p> <p>:::js In our example, the output of <code>getStateHistory</code> will look like this:</p> <pre><code>[\n  StateSnapshot {\n    values: { foo: 'b', bar: ['a', 'b'] },\n    next: [],\n    config: {\n      configurable: {\n        thread_id: '1',\n        checkpoint_ns: '',\n        checkpoint_id: '1ef663ba-28fe-6528-8002-5a559208592c'\n      }\n    },\n    metadata: {\n      source: 'loop',\n      writes: { nodeB: { foo: 'b', bar: ['b'] } },\n      step: 2\n    },\n    createdAt: '2024-08-29T19:19:38.821749+00:00',\n    parentConfig: {\n      configurable: {\n        thread_id: '1',\n        checkpoint_ns: '',\n        checkpoint_id: '1ef663ba-28f9-6ec4-8001-31981c2c39f8'\n      }\n    },\n    tasks: []\n  },\n  StateSnapshot {\n    values: { foo: 'a', bar: ['a'] },\n    next: ['nodeB'],\n    config: {\n      configurable: {\n        thread_id: '1',\n        checkpoint_ns: '',\n        checkpoint_id: '1ef663ba-28f9-6ec4-8001-31981c2c39f8'\n      }\n    },\n    metadata: {\n      source: 'loop',\n      writes: { nodeA: { foo: 'a', bar: ['a'] } },\n      step: 1\n    },\n    createdAt: '2024-08-29T19:19:38.819946+00:00',\n    parentConfig: {\n      configurable: {\n        thread_id: '1',\n        checkpoint_ns: '',\n        checkpoint_id: '1ef663ba-28f4-6b4a-8000-ca575a13d36a'\n      }\n    },\n    tasks: [\n      PregelTask {\n        id: '6fb7314f-f114-5413-a1f3-d37dfe98ff44',\n        name: 'nodeB',\n        error: null,\n        interrupts: []\n      }\n    ]\n  },\n  StateSnapshot {\n    values: { foo: '', bar: [] },\n    next: ['node_a'],\n    config: {\n      configurable: {\n        thread_id: '1',\n        checkpoint_ns: '',\n        checkpoint_id: '1ef663ba-28f4-6b4a-8000-ca575a13d36a'\n      }\n    },\n    metadata: {\n      source: 'loop',\n      writes: null,\n      step: 0\n    },\n    createdAt: '2024-08-29T19:19:38.817813+00:00',\n    parentConfig: {\n      configurable: {\n        thread_id: '1',\n        checkpoint_ns: '',\n        checkpoint_id: '1ef663ba-28f0-6c66-bfff-6723431e8481'\n      }\n    },\n    tasks: [\n      PregelTask {\n        id: 'f1b14528-5ee5-579c-949b-23ef9bfbed58',\n        name: 'node_a',\n        error: null,\n        interrupts: []\n      }\n    ]\n  },\n  StateSnapshot {\n    values: { bar: [] },\n    next: ['__start__'],\n    config: {\n      configurable: {\n        thread_id: '1',\n        checkpoint_ns: '',\n        checkpoint_id: '1ef663ba-28f0-6c66-bfff-6723431e8481'\n      }\n    },\n    metadata: {\n      source: 'input',\n      writes: { foo: '' },\n      step: -1\n    },\n    createdAt: '2024-08-29T19:19:38.816205+00:00',\n    parentConfig: null,\n    tasks: [\n      PregelTask {\n        id: '6d27aa2e-d72b-5504-a36f-8620e54a76dd',\n        name: '__start__',\n        error: null,\n        interrupts: []\n      }\n    ]\n  }\n]\n</code></pre> <p>:::</p> <p></p>", "boost": 2}, {"location": "concepts/persistence/#replay", "title": "Replay", "text": "<p>It's also possible to play-back a prior graph execution. If we <code>invoke</code> a graph with a <code>thread_id</code> and a <code>checkpoint_id</code>, then we will re-play the previously executed steps before a checkpoint that corresponds to the <code>checkpoint_id</code>, and only execute the steps after the checkpoint.</p> <ul> <li><code>thread_id</code> is the ID of a thread.</li> <li><code>checkpoint_id</code> is an identifier that refers to a specific checkpoint within a thread.</li> </ul> <p>You must pass these when invoking the graph as part of the <code>configurable</code> portion of the config:</p> <p>:::python</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"1\", \"checkpoint_id\": \"0c62ca34-ac19-445d-bbb0-5b4984975b2a\"}}\ngraph.invoke(None, config=config)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const config = {\n  configurable: {\n    thread_id: \"1\",\n    checkpoint_id: \"0c62ca34-ac19-445d-bbb0-5b4984975b2a\",\n  },\n};\nawait graph.invoke(null, config);\n</code></pre> <p>:::</p> <p>Importantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply re-plays that particular step in the graph and does not re-execute the step, but only for the steps before the provided <code>checkpoint_id</code>. All of the steps after <code>checkpoint_id</code> will be executed (i.e., a new fork), even if they have been executed previously. See this how to guide on time-travel to learn more about replaying.</p> <p></p>", "boost": 2}, {"location": "concepts/persistence/#update-state", "title": "Update state", "text": "<p>:::python</p> <p>In addition to re-playing the graph from specific <code>checkpoints</code>, we can also edit the graph state. We do this using <code>graph.update_state()</code>. This method accepts three different arguments:</p> <p>:::</p> <p>:::js</p> <p>In addition to re-playing the graph from specific <code>checkpoints</code>, we can also edit the graph state. We do this using <code>graph.updateState()</code>. This method accepts three different arguments:</p> <p>:::</p>", "boost": 2}, {"location": "concepts/persistence/#config", "title": "<code>config</code>", "text": "<p>The config should contain <code>thread_id</code> specifying which thread to update. When only the <code>thread_id</code> is passed, we update (or fork) the current state. Optionally, if we include <code>checkpoint_id</code> field, then we fork that selected checkpoint.</p>", "boost": 2}, {"location": "concepts/persistence/#values", "title": "<code>values</code>", "text": "<p>These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the reducer functions, if they are defined for some of the channels in the graph state. This means that <code>update_state</code> does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let's walk through an example.</p> <p>Let's assume you have defined the state of your graph with the following schema (see full example above):</p> <p>:::python</p> <pre><code>from typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { withLangGraph } from \"@langchain/langgraph/zod\";\nimport { z } from \"zod\";\n\nconst State = z.object({\n  foo: z.number(),\n  bar: withLangGraph(z.array(z.string()), {\n    reducer: {\n      fn: (x, y) =&gt; x.concat(y),\n    },\n    default: () =&gt; [],\n  }),\n});\n</code></pre> <p>:::</p> <p>Let's now assume the current state of the graph is</p> <p>:::python</p> <pre><code>{\"foo\": 1, \"bar\": [\"a\"]}\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>{ foo: 1, bar: [\"a\"] }\n</code></pre> <p>:::</p> <p>If you update the state as below:</p> <p>:::python</p> <pre><code>graph.update_state(config, {\"foo\": 2, \"bar\": [\"b\"]})\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>await graph.updateState(config, { foo: 2, bar: [\"b\"] });\n</code></pre> <p>:::</p> <p>Then the new state of the graph will be:</p> <p>:::python</p> <pre><code>{\"foo\": 2, \"bar\": [\"a\", \"b\"]}\n</code></pre> <p>The <code>foo</code> key (channel) is completely changed (because there is no reducer specified for that channel, so <code>update_state</code> overwrites it). However, there is a reducer specified for the <code>bar</code> key, and so it appends <code>\"b\"</code> to the state of <code>bar</code>. :::</p> <p>:::js</p> <pre><code>{ foo: 2, bar: [\"a\", \"b\"] }\n</code></pre> <p>The <code>foo</code> key (channel) is completely changed (because there is no reducer specified for that channel, so <code>updateState</code> overwrites it). However, there is a reducer specified for the <code>bar</code> key, and so it appends <code>\"b\"</code> to the state of <code>bar</code>. :::</p>", "boost": 2}, {"location": "concepts/persistence/#as_node", "title": "<code>as_node</code>", "text": "<p>:::python The final thing you can optionally specify when calling <code>update_state</code> is <code>as_node</code>. If you provided it, the update will be applied as if it came from node <code>as_node</code>. If <code>as_node</code> is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state. :::</p> <p>:::js The final thing you can optionally specify when calling <code>updateState</code> is <code>asNode</code>. If you provide it, the update will be applied as if it came from node <code>asNode</code>. If <code>asNode</code> is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this how to guide on time-travel to learn more about forking state. :::</p> <p></p>", "boost": 2}, {"location": "concepts/persistence/#memory-store", "title": "Memory Store", "text": "<p>A state schema specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence.</p> <p>But, what if we want to retain some information across threads? Consider the case of a chatbot where we want to retain specific information about the user across all chat conversations (e.g., threads) with that user!</p> <p>With checkpointers alone, we cannot share information across threads. This motivates the need for the <code>Store</code> interface. As an illustration, we can define an <code>InMemoryStore</code> to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new <code>in_memory_store</code> variable.</p> <p>LangGraph API handles stores automatically</p> <p>When using the LangGraph API, you don't need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.</p>", "boost": 2}, {"location": "concepts/persistence/#basic-usage", "title": "Basic Usage", "text": "<p>First, let's showcase this in isolation without using LangGraph.</p> <p>:::python</p> <pre><code>from langgraph.store.memory import InMemoryStore\nin_memory_store = InMemoryStore()\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { MemoryStore } from \"@langchain/langgraph\";\n\nconst memoryStore = new MemoryStore();\n</code></pre> <p>:::</p> <p>Memories are namespaced by a <code>tuple</code>, which in this specific example will be <code>(&lt;user_id&gt;, \"memories\")</code>. The namespace can be any length and represent anything, does not have to be user specific.</p> <p>:::python</p> <pre><code>user_id = \"1\"\nnamespace_for_memory = (user_id, \"memories\")\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const userId = \"1\";\nconst namespaceForMemory = [userId, \"memories\"];\n</code></pre> <p>:::</p> <p>We use the <code>store.put</code> method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (<code>memory_id</code>) and the value (a dictionary) is the memory itself.</p> <p>:::python</p> <pre><code>memory_id = str(uuid.uuid4())\nmemory = {\"food_preference\" : \"I like pizza\"}\nin_memory_store.put(namespace_for_memory, memory_id, memory)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { v4 as uuidv4 } from \"uuid\";\n\nconst memoryId = uuidv4();\nconst memory = { food_preference: \"I like pizza\" };\nawait memoryStore.put(namespaceForMemory, memoryId, memory);\n</code></pre> <p>:::</p> <p>We can read out memories in our namespace using the <code>store.search</code> method, which will return all memories for a given user as a list. The most recent memory is the last in the list.</p> <p>:::python</p> <pre><code>memories = in_memory_store.search(namespace_for_memory)\nmemories[-1].dict()\n{'value': {'food_preference': 'I like pizza'},\n 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n 'namespace': ['1', 'memories'],\n 'created_at': '2024-10-02T17:22:31.590602+00:00',\n 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n</code></pre> <p>Each memory type is a Python class (<code>Item</code>) with certain attributes. We can access it as a dictionary by converting via <code>.dict</code> as above.</p> <p>The attributes it has are:</p> <ul> <li><code>value</code>: The value (itself a dictionary) of this memory</li> <li><code>key</code>: A unique key for this memory in this namespace</li> <li><code>namespace</code>: A list of strings, the namespace of this memory type</li> <li><code>created_at</code>: Timestamp for when this memory was created</li> <li><code>updated_at</code>: Timestamp for when this memory was updated</li> </ul> <p>:::</p> <p>:::js</p> <pre><code>const memories = await memoryStore.search(namespaceForMemory);\nmemories[memories.length - 1];\n\n// {\n//   value: { food_preference: 'I like pizza' },\n//   key: '07e0caf4-1631-47b7-b15f-65515d4c1843',\n//   namespace: ['1', 'memories'],\n//   createdAt: '2024-10-02T17:22:31.590602+00:00',\n//   updatedAt: '2024-10-02T17:22:31.590605+00:00'\n// }\n</code></pre> <p>The attributes it has are:</p> <ul> <li><code>value</code>: The value of this memory</li> <li><code>key</code>: A unique key for this memory in this namespace</li> <li><code>namespace</code>: A list of strings, the namespace of this memory type</li> <li><code>createdAt</code>: Timestamp for when this memory was created</li> <li><code>updatedAt</code>: Timestamp for when this memory was updated</li> </ul> <p>:::</p>", "boost": 2}, {"location": "concepts/persistence/#semantic-search", "title": "Semantic Search", "text": "<p>Beyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model:</p> <p>:::python</p> <pre><code>from langchain.embeddings import init_embeddings\n\nstore = InMemoryStore(\n    index={\n        \"embed\": init_embeddings(\"openai:text-embedding-3-small\"),  # Embedding provider\n        \"dims\": 1536,                              # Embedding dimensions\n        \"fields\": [\"food_preference\", \"$\"]              # Fields to embed\n    }\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { OpenAIEmbeddings } from \"@langchain/openai\";\n\nconst store = new InMemoryStore({\n  index: {\n    embeddings: new OpenAIEmbeddings({ model: \"text-embedding-3-small\" }),\n    dims: 1536,\n    fields: [\"food_preference\", \"$\"], // Fields to embed\n  },\n});\n</code></pre> <p>:::</p> <p>Now when searching, you can use natural language queries to find relevant memories:</p> <p>:::python</p> <pre><code># Find memories about food preferences\n# (This can be done after putting memories into the store)\nmemories = store.search(\n    namespace_for_memory,\n    query=\"What does the user like to eat?\",\n    limit=3  # Return top 3 matches\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>// Find memories about food preferences\n// (This can be done after putting memories into the store)\nconst memories = await store.search(namespaceForMemory, {\n  query: \"What does the user like to eat?\",\n  limit: 3, // Return top 3 matches\n});\n</code></pre> <p>:::</p> <p>You can control which parts of your memories get embedded by configuring the <code>fields</code> parameter or by specifying the <code>index</code> parameter when storing memories:</p> <p>:::python</p> <pre><code># Store with specific fields to embed\nstore.put(\n    namespace_for_memory,\n    str(uuid.uuid4()),\n    {\n        \"food_preference\": \"I love Italian cuisine\",\n        \"context\": \"Discussing dinner plans\"\n    },\n    index=[\"food_preference\"]  # Only embed \"food_preferences\" field\n)\n\n# Store without embedding (still retrievable, but not searchable)\nstore.put(\n    namespace_for_memory,\n    str(uuid.uuid4()),\n    {\"system_info\": \"Last updated: 2024-01-01\"},\n    index=False\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>// Store with specific fields to embed\nawait store.put(\n  namespaceForMemory,\n  uuidv4(),\n  {\n    food_preference: \"I love Italian cuisine\",\n    context: \"Discussing dinner plans\",\n  },\n  { index: [\"food_preference\"] } // Only embed \"food_preferences\" field\n);\n\n// Store without embedding (still retrievable, but not searchable)\nawait store.put(\n  namespaceForMemory,\n  uuidv4(),\n  { system_info: \"Last updated: 2024-01-01\" },\n  { index: false }\n);\n</code></pre> <p>:::</p>", "boost": 2}, {"location": "concepts/persistence/#using-in-langgraph", "title": "Using in LangGraph", "text": "<p>:::python With this all in place, we use the <code>in_memory_store</code> in LangGraph. The <code>in_memory_store</code> works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the <code>in_memory_store</code> allows us to store arbitrary information for access across threads. We compile the graph with both the checkpointer and the <code>in_memory_store</code> as follows.</p> <pre><code>from langgraph.checkpoint.memory import InMemorySaver\n\n# We need this because we want to enable threads (conversations)\ncheckpointer = InMemorySaver()\n\n# ... Define the graph ...\n\n# Compile the graph with the checkpointer and store\ngraph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\n</code></pre> <p>:::</p> <p>:::js With this all in place, we use the <code>memoryStore</code> in LangGraph. The <code>memoryStore</code> works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the <code>memoryStore</code> allows us to store arbitrary information for access across threads. We compile the graph with both the checkpointer and the <code>memoryStore</code> as follows.</p> <pre><code>import { MemorySaver } from \"@langchain/langgraph\";\n\n// We need this because we want to enable threads (conversations)\nconst checkpointer = new MemorySaver();\n\n// ... Define the graph ...\n\n// Compile the graph with the checkpointer and store\nconst graph = workflow.compile({ checkpointer, store: memoryStore });\n</code></pre> <p>:::</p> <p>We invoke the graph with a <code>thread_id</code>, as before, and also with a <code>user_id</code>, which we'll use to namespace our memories to this particular user as we showed above.</p> <p>:::python</p> <pre><code># Invoke the graph\nuser_id = \"1\"\nconfig = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": user_id}}\n\n# First let's just say hi to the AI\nfor update in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream_mode=\"updates\"\n):\n    print(update)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>// Invoke the graph\nconst userId = \"1\";\nconst config = { configurable: { thread_id: \"1\", user_id: userId } };\n\n// First let's just say hi to the AI\nfor await (const update of await graph.stream(\n  { messages: [{ role: \"user\", content: \"hi\" }] },\n  { ...config, streamMode: \"updates\" }\n)) {\n  console.log(update);\n}\n</code></pre> <p>:::</p> <p>:::python We can access the <code>in_memory_store</code> and the <code>user_id</code> in any node by passing <code>store: BaseStore</code> and <code>config: RunnableConfig</code> as node arguments. Here's how we might use semantic search in a node to find relevant memories:</p> <pre><code>def update_memory(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n\n    # Get the user id from the config\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Namespace the memory\n    namespace = (user_id, \"memories\")\n\n    # ... Analyze conversation and create a new memory\n\n    # Create a new memory ID\n    memory_id = str(uuid.uuid4())\n\n    # We create a new memory\n    store.put(namespace, memory_id, {\"memory\": memory})\n</code></pre> <p>:::</p> <p>:::js We can access the <code>memoryStore</code> and the <code>user_id</code> in any node by accessing <code>config</code> and <code>store</code> as node arguments. Here's how we might use semantic search in a node to find relevant memories:</p> <pre><code>import {\n  LangGraphRunnableConfig,\n  BaseStore,\n  MessagesZodState,\n} from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst updateMemory = async (\n  state: z.infer&lt;typeof MessagesZodState&gt;,\n  config: LangGraphRunnableConfig,\n  store: BaseStore\n) =&gt; {\n  // Get the user id from the config\n  const userId = config.configurable?.user_id;\n\n  // Namespace the memory\n  const namespace = [userId, \"memories\"];\n\n  // ... Analyze conversation and create a new memory\n\n  // Create a new memory ID\n  const memoryId = uuidv4();\n\n  // We create a new memory\n  await store.put(namespace, memoryId, { memory });\n};\n</code></pre> <p>:::</p> <p>As we showed above, we can also access the store in any node and use the <code>store.search</code> method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary.</p> <p>:::python</p> <pre><code>memories[-1].dict()\n{'value': {'food_preference': 'I like pizza'},\n 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843',\n 'namespace': ['1', 'memories'],\n 'created_at': '2024-10-02T17:22:31.590602+00:00',\n 'updated_at': '2024-10-02T17:22:31.590605+00:00'}\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>memories[memories.length - 1];\n// {\n//   value: { food_preference: 'I like pizza' },\n//   key: '07e0caf4-1631-47b7-b15f-65515d4c1843',\n//   namespace: ['1', 'memories'],\n//   createdAt: '2024-10-02T17:22:31.590602+00:00',\n//   updatedAt: '2024-10-02T17:22:31.590605+00:00'\n// }\n</code></pre> <p>:::</p> <p>We can access the memories and use them in our model call.</p> <p>:::python</p> <pre><code>def call_model(state: MessagesState, config: RunnableConfig, *, store: BaseStore):\n    # Get the user id from the config\n    user_id = config[\"configurable\"][\"user_id\"]\n\n    # Namespace the memory\n    namespace = (user_id, \"memories\")\n\n    # Search based on the most recent message\n    memories = store.search(\n        namespace,\n        query=state[\"messages\"][-1].content,\n        limit=3\n    )\n    info = \"\\n\".join([d.value[\"memory\"] for d in memories])\n\n    # ... Use memories in the model call\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const callModel = async (\n  state: z.infer&lt;typeof MessagesZodState&gt;,\n  config: LangGraphRunnableConfig,\n  store: BaseStore\n) =&gt; {\n  // Get the user id from the config\n  const userId = config.configurable?.user_id;\n\n  // Namespace the memory\n  const namespace = [userId, \"memories\"];\n\n  // Search based on the most recent message\n  const memories = await store.search(namespace, {\n    query: state.messages[state.messages.length - 1].content,\n    limit: 3,\n  });\n  const info = memories.map((d) =&gt; d.value.memory).join(\"\\n\");\n\n  // ... Use memories in the model call\n};\n</code></pre> <p>:::</p> <p>If we create a new thread, we can still access the same memories so long as the <code>user_id</code> is the same.</p> <p>:::python</p> <pre><code># Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}}\n\n# Let's say hi again\nfor update in graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi, tell me about my memories\"}]}, config, stream_mode=\"updates\"\n):\n    print(update)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>// Invoke the graph\nconst config = { configurable: { thread_id: \"2\", user_id: \"1\" } };\n\n// Let's say hi again\nfor await (const update of await graph.stream(\n  { messages: [{ role: \"user\", content: \"hi, tell me about my memories\" }] },\n  { ...config, streamMode: \"updates\" }\n)) {\n  console.log(update);\n}\n</code></pre> <p>:::</p> <p>When we use the LangGraph Platform, either locally (e.g., in LangGraph Studio) or with LangGraph Platform, the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you do need to configure the indexing settings in your <code>langgraph.json</code> file. For example:</p> <pre><code>{\n    ...\n    \"store\": {\n        \"index\": {\n            \"embed\": \"openai:text-embeddings-3-small\",\n            \"dims\": 1536,\n            \"fields\": [\"$\"]\n        }\n    }\n}\n</code></pre> <p>See the deployment guide for more details and configuration options.</p>", "boost": 2}, {"location": "concepts/persistence/#checkpointer-libraries", "title": "Checkpointer libraries", "text": "<p>Under the hood, checkpointing is powered by checkpointer objects that conform to @[BaseCheckpointSaver] interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries:</p> <p>:::python</p> <ul> <li><code>langgraph-checkpoint</code>: The base interface for checkpointer savers (@[BaseCheckpointSaver]) and serialization/deserialization interface (@[SerializerProtocol][SerializerProtocol]). Includes in-memory checkpointer implementation (@[InMemorySaver][InMemorySaver]) for experimentation. LangGraph comes with <code>langgraph-checkpoint</code> included.</li> <li><code>langgraph-checkpoint-sqlite</code>: An implementation of LangGraph checkpointer that uses SQLite database (@[SqliteSaver][SqliteSaver] / @[AsyncSqliteSaver]). Ideal for experimentation and local workflows. Needs to be installed separately.</li> <li><code>langgraph-checkpoint-postgres</code>: An advanced checkpointer that uses Postgres database (@[PostgresSaver][PostgresSaver] / @[AsyncPostgresSaver]), used in LangGraph Platform. Ideal for using in production. Needs to be installed separately.</li> </ul> <p>:::</p> <p>:::js</p> <ul> <li><code>@langchain/langgraph-checkpoint</code>: The base interface for checkpointer savers (@[BaseCheckpointSaver][BaseCheckpointSaver]) and serialization/deserialization interface (@[SerializerProtocol][SerializerProtocol]). Includes in-memory checkpointer implementation (@[MemorySaver]) for experimentation. LangGraph comes with <code>@langchain/langgraph-checkpoint</code> included.</li> <li><code>@langchain/langgraph-checkpoint-sqlite</code>: An implementation of LangGraph checkpointer that uses SQLite database (@[SqliteSaver]). Ideal for experimentation and local workflows. Needs to be installed separately.</li> <li><code>@langchain/langgraph-checkpoint-postgres</code>: An advanced checkpointer that uses Postgres database (@[PostgresSaver]), used in LangGraph Platform. Ideal for using in production. Needs to be installed separately.</li> </ul> <p>:::</p>", "boost": 2}, {"location": "concepts/persistence/#checkpointer-interface", "title": "Checkpointer interface", "text": "<p>:::python Each checkpointer conforms to @[BaseCheckpointSaver] interface and implements the following methods:</p> <ul> <li><code>.put</code> - Store a checkpoint with its configuration and metadata.</li> <li><code>.put_writes</code> - Store intermediate writes linked to a checkpoint (i.e. pending writes).</li> <li><code>.get_tuple</code> - Fetch a checkpoint tuple using for a given configuration (<code>thread_id</code> and <code>checkpoint_id</code>). This is used to populate <code>StateSnapshot</code> in <code>graph.get_state()</code>.</li> <li><code>.list</code> - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in <code>graph.get_state_history()</code></li> </ul> <p>If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via <code>.ainvoke</code>, <code>.astream</code>, <code>.abatch</code>), asynchronous versions of the above methods will be used (<code>.aput</code>, <code>.aput_writes</code>, <code>.aget_tuple</code>, <code>.alist</code>).</p> <p>Note</p> <p>For running your graph asynchronously, you can use <code>InMemorySaver</code>, or async versions of Sqlite/Postgres checkpointers -- <code>AsyncSqliteSaver</code> / <code>AsyncPostgresSaver</code> checkpointers.</p> <p>:::</p> <p>:::js Each checkpointer conforms to the @[BaseCheckpointSaver][BaseCheckpointSaver] interface and implements the following methods:</p> <ul> <li><code>.put</code> - Store a checkpoint with its configuration and metadata.</li> <li><code>.putWrites</code> - Store intermediate writes linked to a checkpoint (i.e. pending writes).</li> <li><code>.getTuple</code> - Fetch a checkpoint tuple using for a given configuration (<code>thread_id</code> and <code>checkpoint_id</code>). This is used to populate <code>StateSnapshot</code> in <code>graph.getState()</code>.</li> <li><code>.list</code> - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in <code>graph.getStateHistory()</code>   :::</li> </ul>", "boost": 2}, {"location": "concepts/persistence/#serializer", "title": "Serializer", "text": "<p>When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects.</p> <p>:::python <code>langgraph_checkpoint</code> defines @[protocol][SerializerProtocol] for implementing serializers provides a default implementation (@[JsonPlusSerializer][JsonPlusSerializer]) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more.</p>", "boost": 2}, {"location": "concepts/persistence/#serialization-with-pickle", "title": "Serialization with <code>pickle</code>", "text": "<p>The default serializer, @[<code>JsonPlusSerializer</code>][JsonPlusSerializer], uses ormsgpack and JSON under the hood, which is not suitable for all types of objects.</p> <p>If you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes), you can use the <code>pickle_fallback</code> argument of the <code>JsonPlusSerializer</code>:</p> <pre><code>from langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer\n\n# ... Define the graph ...\ngraph.compile(\n    checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle_fallback=True))\n)\n</code></pre>", "boost": 2}, {"location": "concepts/persistence/#encryption", "title": "Encryption", "text": "<p>Checkpointers can optionally encrypt all persisted state. To enable this, pass an instance of @[<code>EncryptedSerializer</code>][EncryptedSerializer] to the <code>serde</code> argument of any <code>BaseCheckpointSaver</code> implementation. The easiest way to create an encrypted serializer is via @[<code>from_pycryptodome_aes</code>][from_pycryptodome_aes], which reads the AES key from the <code>LANGGRAPH_AES_KEY</code> environment variable (or accepts a <code>key</code> argument):</p> <pre><code>import sqlite3\n\nfrom langgraph.checkpoint.serde.encrypted import EncryptedSerializer\nfrom langgraph.checkpoint.sqlite import SqliteSaver\n\nserde = EncryptedSerializer.from_pycryptodome_aes()  # reads LANGGRAPH_AES_KEY\ncheckpointer = SqliteSaver(sqlite3.connect(\"checkpoint.db\"), serde=serde)\n</code></pre> <pre><code>from langgraph.checkpoint.serde.encrypted import EncryptedSerializer\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nserde = EncryptedSerializer.from_pycryptodome_aes()\ncheckpointer = PostgresSaver.from_conn_string(\"postgresql://...\", serde=serde)\ncheckpointer.setup()\n</code></pre> <p>When running on LangGraph Platform, encryption is automatically enabled whenever <code>LANGGRAPH_AES_KEY</code> is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing @[<code>CipherProtocol</code>][CipherProtocol] and supplying it to <code>EncryptedSerializer</code>. :::</p> <p>:::js <code>@langchain/langgraph-checkpoint</code> defines protocol for implementing serializers and provides a default implementation that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more. :::</p>", "boost": 2}, {"location": "concepts/persistence/#capabilities", "title": "Capabilities", "text": "", "boost": 2}, {"location": "concepts/persistence/#human-in-the-loop", "title": "Human-in-the-loop", "text": "<p>First, checkpointers facilitate human-in-the-loop workflows workflows by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See the how-to guides for examples.</p>", "boost": 2}, {"location": "concepts/persistence/#memory", "title": "Memory", "text": "<p>Second, checkpointers allow for \"memory\" between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See Add memory for information on how to add and manage conversation memory using checkpointers.</p>", "boost": 2}, {"location": "concepts/persistence/#time-travel", "title": "Time Travel", "text": "<p>Third, checkpointers allow for \"time travel\", allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories.</p>", "boost": 2}, {"location": "concepts/persistence/#fault-tolerance", "title": "Fault-tolerance", "text": "<p>Lastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.</p>", "boost": 2}, {"location": "concepts/persistence/#pending-writes", "title": "Pending writes", "text": "<p>Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes.</p>", "boost": 2}, {"location": "concepts/pregel/", "title": "LangGraph runtime", "text": "<p>:::python @[Pregel] implements LangGraph's runtime, managing the execution of LangGraph applications.</p> <p>Compiling a @[StateGraph][StateGraph] or creating an @[entrypoint][entrypoint] produces a @[Pregel] instance that can be invoked with input. :::</p> <p>:::js @[Pregel] implements LangGraph's runtime, managing the execution of LangGraph applications.</p> <p>Compiling a @[StateGraph][StateGraph] or creating an @[entrypoint][entrypoint] produces a @[Pregel] instance that can be invoked with input. :::</p> <p>This guide explains the runtime at a high level and provides instructions for directly implementing applications with Pregel.</p> <p>:::python</p> <p>Note: The @[Pregel] runtime is named after Google's Pregel algorithm, which describes an efficient method for large-scale parallel computation using graphs.</p> <p>:::</p> <p>:::js</p> <p>Note: The @[Pregel] runtime is named after Google's Pregel algorithm, which describes an efficient method for large-scale parallel computation using graphs.</p> <p>:::</p>", "boost": 2}, {"location": "concepts/pregel/#overview", "title": "Overview", "text": "<p>In LangGraph, Pregel combines actors and channels into a single application. Actors read data from channels and write data to channels. Pregel organizes the execution of the application into multiple steps, following the Pregel Algorithm/Bulk Synchronous Parallel model.</p> <p>Each step consists of three phases:</p> <ul> <li>Plan: Determine which actors to execute in this step. For example, in the first step, select the actors that subscribe to the special input channels; in subsequent steps, select the actors that subscribe to channels updated in the previous step.</li> <li>Execution: Execute all selected actors in parallel, until all complete, or one fails, or a timeout is reached. During this phase, channel updates are invisible to actors until the next step.</li> <li>Update: Update the channels with the values written by the actors in this step.</li> </ul> <p>Repeat until no actors are selected for execution, or a maximum number of steps is reached.</p>", "boost": 2}, {"location": "concepts/pregel/#actors", "title": "Actors", "text": "<p>An actor is a <code>PregelNode</code>. It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an actor in the Pregel algorithm. <code>PregelNodes</code> implement LangChain's Runnable interface.</p>", "boost": 2}, {"location": "concepts/pregel/#channels", "title": "Channels", "text": "<p>Channels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function \u2013 which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels:</p> <p>:::python</p> <ul> <li>@[LastValue][LastValue]: The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next.</li> <li>@[Topic][Topic]: A configurable PubSub Topic, useful for sending multiple values between actors, or for accumulating output. Can be configured to deduplicate values or to accumulate values over the course of multiple steps.</li> <li>@[BinaryOperatorAggregate][BinaryOperatorAggregate]: stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps; e.g.,<code>total = BinaryOperatorAggregate(int, operator.add)</code>   :::</li> </ul> <p>:::js</p> <ul> <li>@[LastValue]: The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next.</li> <li>@[Topic]: A configurable PubSub Topic, useful for sending multiple values between actors, or for accumulating output. Can be configured to deduplicate values or to accumulate values over the course of multiple steps.</li> <li>@[BinaryOperatorAggregate]: stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps; e.g.,<code>total = BinaryOperatorAggregate(int, operator.add)</code>   :::</li> </ul>", "boost": 2}, {"location": "concepts/pregel/#examples", "title": "Examples", "text": "<p>:::python While most users will interact with Pregel through the @[StateGraph][StateGraph] API or the @[entrypoint][entrypoint] decorator, it is possible to interact with Pregel directly. :::</p> <p>:::js While most users will interact with Pregel through the @[StateGraph] API or the @[entrypoint] decorator, it is possible to interact with Pregel directly. :::</p> <p>Below are a few different examples to give you a sense of the Pregel API.</p> Single nodeMultiple nodesTopicBinaryOperatorAggregateCycle <p>:::python <pre><code>from langgraph.channels import EphemeralValue\nfrom langgraph.pregel import Pregel, NodeBuilder\n\nnode1 = (\n    NodeBuilder().subscribe_only(\"a\")\n    .do(lambda x: x + x)\n    .write_to(\"b\")\n)\n\napp = Pregel(\n    nodes={\"node1\": node1},\n    channels={\n        \"a\": EphemeralValue(str),\n        \"b\": EphemeralValue(str),\n    },\n    input_channels=[\"a\"],\n    output_channels=[\"b\"],\n)\n\napp.invoke({\"a\": \"foo\"})\n</code></pre></p> <p><pre><code>{'b': 'foofoo'}\n</code></pre> :::</p> <p>:::js <pre><code>import { EphemeralValue } from \"@langchain/langgraph/channels\";\nimport { Pregel, NodeBuilder } from \"@langchain/langgraph/pregel\";\n\nconst node1 = new NodeBuilder()\n  .subscribeOnly(\"a\")\n  .do((x: string) =&gt; x + x)\n  .writeTo(\"b\");\n\nconst app = new Pregel({\n  nodes: { node1 },\n  channels: {\n    a: new EphemeralValue&lt;string&gt;(),\n    b: new EphemeralValue&lt;string&gt;(),\n  },\n  inputChannels: [\"a\"],\n  outputChannels: [\"b\"],\n});\n\nawait app.invoke({ a: \"foo\" });\n</code></pre></p> <p><pre><code>{ b: 'foofoo' }\n</code></pre> :::</p> <p>:::python <pre><code>from langgraph.channels import LastValue, EphemeralValue\nfrom langgraph.pregel import Pregel, NodeBuilder\n\nnode1 = (\n    NodeBuilder().subscribe_only(\"a\")\n    .do(lambda x: x + x)\n    .write_to(\"b\")\n)\n\nnode2 = (\n    NodeBuilder().subscribe_only(\"b\")\n    .do(lambda x: x + x)\n    .write_to(\"c\")\n)\n\n\napp = Pregel(\n    nodes={\"node1\": node1, \"node2\": node2},\n    channels={\n        \"a\": EphemeralValue(str),\n        \"b\": LastValue(str),\n        \"c\": EphemeralValue(str),\n    },\n    input_channels=[\"a\"],\n    output_channels=[\"b\", \"c\"],\n)\n\napp.invoke({\"a\": \"foo\"})\n</code></pre></p> <p><pre><code>{'b': 'foofoo', 'c': 'foofoofoofoo'}\n</code></pre> :::</p> <p>:::js <pre><code>import { LastValue, EphemeralValue } from \"@langchain/langgraph/channels\";\nimport { Pregel, NodeBuilder } from \"@langchain/langgraph/pregel\";\n\nconst node1 = new NodeBuilder()\n  .subscribeOnly(\"a\")\n  .do((x: string) =&gt; x + x)\n  .writeTo(\"b\");\n\nconst node2 = new NodeBuilder()\n  .subscribeOnly(\"b\")\n  .do((x: string) =&gt; x + x)\n  .writeTo(\"c\");\n\nconst app = new Pregel({\n  nodes: { node1, node2 },\n  channels: {\n    a: new EphemeralValue&lt;string&gt;(),\n    b: new LastValue&lt;string&gt;(),\n    c: new EphemeralValue&lt;string&gt;(),\n  },\n  inputChannels: [\"a\"],\n  outputChannels: [\"b\", \"c\"],\n});\n\nawait app.invoke({ a: \"foo\" });\n</code></pre></p> <p><pre><code>{ b: 'foofoo', c: 'foofoofoofoo' }\n</code></pre> :::</p> <p>:::python <pre><code>from langgraph.channels import EphemeralValue, Topic\nfrom langgraph.pregel import Pregel, NodeBuilder\n\nnode1 = (\n    NodeBuilder().subscribe_only(\"a\")\n    .do(lambda x: x + x)\n    .write_to(\"b\", \"c\")\n)\n\nnode2 = (\n    NodeBuilder().subscribe_to(\"b\")\n    .do(lambda x: x[\"b\"] + x[\"b\"])\n    .write_to(\"c\")\n)\n\napp = Pregel(\n    nodes={\"node1\": node1, \"node2\": node2},\n    channels={\n        \"a\": EphemeralValue(str),\n        \"b\": EphemeralValue(str),\n        \"c\": Topic(str, accumulate=True),\n    },\n    input_channels=[\"a\"],\n    output_channels=[\"c\"],\n)\n\napp.invoke({\"a\": \"foo\"})\n</code></pre></p> <p><pre><code>{'c': ['foofoo', 'foofoofoofoo']}\n</code></pre> :::</p> <p>:::js <pre><code>import { EphemeralValue, Topic } from \"@langchain/langgraph/channels\";\nimport { Pregel, NodeBuilder } from \"@langchain/langgraph/pregel\";\n\nconst node1 = new NodeBuilder()\n  .subscribeOnly(\"a\")\n  .do((x: string) =&gt; x + x)\n  .writeTo(\"b\", \"c\");\n\nconst node2 = new NodeBuilder()\n  .subscribeTo(\"b\")\n  .do((x: { b: string }) =&gt; x.b + x.b)\n  .writeTo(\"c\");\n\nconst app = new Pregel({\n  nodes: { node1, node2 },\n  channels: {\n    a: new EphemeralValue&lt;string&gt;(),\n    b: new EphemeralValue&lt;string&gt;(),\n    c: new Topic&lt;string&gt;({ accumulate: true }),\n  },\n  inputChannels: [\"a\"],\n  outputChannels: [\"c\"],\n});\n\nawait app.invoke({ a: \"foo\" });\n</code></pre></p> <p><pre><code>{ c: ['foofoo', 'foofoofoofoo'] }\n</code></pre> :::</p> <p>This examples demonstrates how to use the BinaryOperatorAggregate channel to implement a reducer.</p> <p>:::python <pre><code>from langgraph.channels import EphemeralValue, BinaryOperatorAggregate\nfrom langgraph.pregel import Pregel, NodeBuilder\n\n\nnode1 = (\n    NodeBuilder().subscribe_only(\"a\")\n    .do(lambda x: x + x)\n    .write_to(\"b\", \"c\")\n)\n\nnode2 = (\n    NodeBuilder().subscribe_only(\"b\")\n    .do(lambda x: x + x)\n    .write_to(\"c\")\n)\n\ndef reducer(current, update):\n    if current:\n        return current + \" | \" + update\n    else:\n        return update\n\napp = Pregel(\n    nodes={\"node1\": node1, \"node2\": node2},\n    channels={\n        \"a\": EphemeralValue(str),\n        \"b\": EphemeralValue(str),\n        \"c\": BinaryOperatorAggregate(str, operator=reducer),\n    },\n    input_channels=[\"a\"],\n    output_channels=[\"c\"],\n)\n\napp.invoke({\"a\": \"foo\"})\n</code></pre> :::</p> <p>:::js <pre><code>import { EphemeralValue, BinaryOperatorAggregate } from \"@langchain/langgraph/channels\";\nimport { Pregel, NodeBuilder } from \"@langchain/langgraph/pregel\";\n\nconst node1 = new NodeBuilder()\n  .subscribeOnly(\"a\")\n  .do((x: string) =&gt; x + x)\n  .writeTo(\"b\", \"c\");\n\nconst node2 = new NodeBuilder()\n  .subscribeOnly(\"b\")\n  .do((x: string) =&gt; x + x)\n  .writeTo(\"c\");\n\nconst reducer = (current: string, update: string) =&gt; {\n  if (current) {\n    return current + \" | \" + update;\n  } else {\n    return update;\n  }\n};\n\nconst app = new Pregel({\n  nodes: { node1, node2 },\n  channels: {\n    a: new EphemeralValue&lt;string&gt;(),\n    b: new EphemeralValue&lt;string&gt;(),\n    c: new BinaryOperatorAggregate&lt;string&gt;({ operator: reducer }),\n  },\n  inputChannels: [\"a\"],\n  outputChannels: [\"c\"],\n});\n\nawait app.invoke({ a: \"foo\" });\n</code></pre> :::</p> <p>:::python</p> <p>This example demonstrates how to introduce a cycle in the graph, by having a chain write to a channel it subscribes to. Execution will continue until a <code>None</code> value is written to the channel.</p> <pre><code>from langgraph.channels import EphemeralValue\nfrom langgraph.pregel import Pregel, NodeBuilder, ChannelWriteEntry\n\nexample_node = (\n    NodeBuilder().subscribe_only(\"value\")\n    .do(lambda x: x + x if len(x) &lt; 10 else None)\n    .write_to(ChannelWriteEntry(\"value\", skip_none=True))\n)\n\napp = Pregel(\n    nodes={\"example_node\": example_node},\n    channels={\n        \"value\": EphemeralValue(str),\n    },\n    input_channels=[\"value\"],\n    output_channels=[\"value\"],\n)\n\napp.invoke({\"value\": \"a\"})\n</code></pre> <p><pre><code>{'value': 'aaaaaaaaaaaaaaaa'}\n</code></pre> :::</p> <p>:::js</p> <p>This example demonstrates how to introduce a cycle in the graph, by having a chain write to a channel it subscribes to. Execution will continue until a <code>null</code> value is written to the channel.</p> <pre><code>import { EphemeralValue } from \"@langchain/langgraph/channels\";\nimport { Pregel, NodeBuilder, ChannelWriteEntry } from \"@langchain/langgraph/pregel\";\n\nconst exampleNode = new NodeBuilder()\n  .subscribeOnly(\"value\")\n  .do((x: string) =&gt; x.length &lt; 10 ? x + x : null)\n  .writeTo(new ChannelWriteEntry(\"value\", { skipNone: true }));\n\nconst app = new Pregel({\n  nodes: { exampleNode },\n  channels: {\n    value: new EphemeralValue&lt;string&gt;(),\n  },\n  inputChannels: [\"value\"],\n  outputChannels: [\"value\"],\n});\n\nawait app.invoke({ value: \"a\" });\n</code></pre> <p><pre><code>{ value: 'aaaaaaaaaaaaaaaa' }\n</code></pre> :::</p>", "boost": 2}, {"location": "concepts/pregel/#high-level-api", "title": "High-level API", "text": "<p>LangGraph provides two high-level APIs for creating a Pregel application: the StateGraph (Graph API) and the Functional API.</p> StateGraph (Graph API)Functional API <p>:::python</p> <p>The @[StateGraph (Graph API)][StateGraph] is a higher-level abstraction that simplifies the creation of Pregel applications. It allows you to define a graph of nodes and edges. When you compile the graph, the StateGraph API automatically creates the Pregel application for you.</p> <p><pre><code>from typing import TypedDict, Optional\n\nfrom langgraph.constants import START\nfrom langgraph.graph import StateGraph\n\nclass Essay(TypedDict):\n    topic: str\n    content: Optional[str]\n    score: Optional[float]\n\ndef write_essay(essay: Essay):\n    return {\n        \"content\": f\"Essay about {essay['topic']}\",\n    }\n\ndef score_essay(essay: Essay):\n    return {\n        \"score\": 10\n    }\n\nbuilder = StateGraph(Essay)\nbuilder.add_node(write_essay)\nbuilder.add_node(score_essay)\nbuilder.add_edge(START, \"write_essay\")\n\n# Compile the graph.\n# This will return a Pregel instance.\ngraph = builder.compile()\n</code></pre> :::</p> <p>:::js</p> <p>The @[StateGraph (Graph API)][StateGraph] is a higher-level abstraction that simplifies the creation of Pregel applications. It allows you to define a graph of nodes and edges. When you compile the graph, the StateGraph API automatically creates the Pregel application for you.</p> <p><pre><code>import { START, StateGraph } from \"@langchain/langgraph\";\n\ninterface Essay {\n  topic: string;\n  content?: string;\n  score?: number;\n}\n\nconst writeEssay = (essay: Essay) =&gt; {\n  return {\n    content: `Essay about ${essay.topic}`,\n  };\n};\n\nconst scoreEssay = (essay: Essay) =&gt; {\n  return {\n    score: 10\n  };\n};\n\nconst builder = new StateGraph&lt;Essay&gt;({\n  channels: {\n    topic: null,\n    content: null,\n    score: null,\n  }\n})\n  .addNode(\"writeEssay\", writeEssay)\n  .addNode(\"scoreEssay\", scoreEssay)\n  .addEdge(START, \"writeEssay\");\n\n// Compile the graph.\n// This will return a Pregel instance.\nconst graph = builder.compile();\n</code></pre> :::</p> <p>The compiled Pregel instance will be associated with a list of nodes and channels. You can inspect the nodes and channels by printing them.</p> <p>:::python <pre><code>print(graph.nodes)\n</code></pre></p> <p>You will see something like this:</p> <pre><code>{'__start__': &lt;langgraph.pregel.read.PregelNode at 0x7d05e3ba1810&gt;,\n 'write_essay': &lt;langgraph.pregel.read.PregelNode at 0x7d05e3ba14d0&gt;,\n 'score_essay': &lt;langgraph.pregel.read.PregelNode at 0x7d05e3ba1710&gt;}\n</code></pre> <pre><code>print(graph.channels)\n</code></pre> <p>You should see something like this</p> <p><pre><code>{'topic': &lt;langgraph.channels.last_value.LastValue at 0x7d05e3294d80&gt;,\n 'content': &lt;langgraph.channels.last_value.LastValue at 0x7d05e3295040&gt;,\n 'score': &lt;langgraph.channels.last_value.LastValue at 0x7d05e3295980&gt;,\n '__start__': &lt;langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e3297e00&gt;,\n 'write_essay': &lt;langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e32960c0&gt;,\n 'score_essay': &lt;langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8ab80&gt;,\n 'branch:__start__:__self__:write_essay': &lt;langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e32941c0&gt;,\n 'branch:__start__:__self__:score_essay': &lt;langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d88800&gt;,\n 'branch:write_essay:__self__:write_essay': &lt;langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e3295ec0&gt;,\n 'branch:write_essay:__self__:score_essay': &lt;langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8ac00&gt;,\n 'branch:score_essay:__self__:write_essay': &lt;langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d89700&gt;,\n 'branch:score_essay:__self__:score_essay': &lt;langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8b400&gt;,\n 'start:write_essay': &lt;langgraph.channels.ephemeral_value.EphemeralValue at 0x7d05e2d8b280&gt;}\n</code></pre> :::</p> <p>:::js <pre><code>console.log(graph.nodes);\n</code></pre></p> <p>You will see something like this:</p> <pre><code>{\n  __start__: PregelNode { ... },\n  writeEssay: PregelNode { ... },\n  scoreEssay: PregelNode { ... }\n}\n</code></pre> <pre><code>console.log(graph.channels);\n</code></pre> <p>You should see something like this</p> <p><pre><code>{\n  topic: LastValue { ... },\n  content: LastValue { ... },\n  score: LastValue { ... },\n  __start__: EphemeralValue { ... },\n  writeEssay: EphemeralValue { ... },\n  scoreEssay: EphemeralValue { ... },\n  'branch:__start__:__self__:writeEssay': EphemeralValue { ... },\n  'branch:__start__:__self__:scoreEssay': EphemeralValue { ... },\n  'branch:writeEssay:__self__:writeEssay': EphemeralValue { ... },\n  'branch:writeEssay:__self__:scoreEssay': EphemeralValue { ... },\n  'branch:scoreEssay:__self__:writeEssay': EphemeralValue { ... },\n  'branch:scoreEssay:__self__:scoreEssay': EphemeralValue { ... },\n  'start:writeEssay': EphemeralValue { ... }\n}\n</code></pre> :::</p> <p>:::python</p> <p>In the Functional API, you can use an @[<code>entrypoint</code>][entrypoint] to create a Pregel application. The <code>entrypoint</code> decorator allows you to define a function that takes input and returns output.</p> <pre><code>from typing import TypedDict, Optional\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.func import entrypoint\n\nclass Essay(TypedDict):\n    topic: str\n    content: Optional[str]\n    score: Optional[float]\n\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef write_essay(essay: Essay):\n    return {\n        \"content\": f\"Essay about {essay['topic']}\",\n    }\n\nprint(\"Nodes: \")\nprint(write_essay.nodes)\nprint(\"Channels: \")\nprint(write_essay.channels)\n</code></pre> <p><pre><code>Nodes:\n{'write_essay': &lt;langgraph.pregel.read.PregelNode object at 0x7d05e2f9aad0&gt;}\nChannels:\n{'__start__': &lt;langgraph.channels.ephemeral_value.EphemeralValue object at 0x7d05e2c906c0&gt;, '__end__': &lt;langgraph.channels.last_value.LastValue object at 0x7d05e2c90c40&gt;, '__previous__': &lt;langgraph.channels.last_value.LastValue object at 0x7d05e1007280&gt;}\n</code></pre> :::</p> <p>:::js</p> <p>In the Functional API, you can use an @[<code>entrypoint</code>][entrypoint] to create a Pregel application. The <code>entrypoint</code> decorator allows you to define a function that takes input and returns output.</p> <pre><code>import { MemorySaver } from \"@langchain/langgraph\";\nimport { entrypoint } from \"@langchain/langgraph/func\";\n\ninterface Essay {\n  topic: string;\n  content?: string;\n  score?: number;\n}\n\nconst checkpointer = new MemorySaver();\n\nconst writeEssay = entrypoint(\n  { checkpointer, name: \"writeEssay\" },\n  async (essay: Essay) =&gt; {\n    return {\n      content: `Essay about ${essay.topic}`,\n    };\n  }\n);\n\nconsole.log(\"Nodes: \");\nconsole.log(writeEssay.nodes);\nconsole.log(\"Channels: \");\nconsole.log(writeEssay.channels);\n</code></pre> <p><pre><code>Nodes:\n{ writeEssay: PregelNode { ... } }\nChannels:\n{\n  __start__: EphemeralValue { ... },\n  __end__: LastValue { ... },\n  __previous__: LastValue { ... }\n}\n</code></pre> :::</p>", "boost": 2}, {"location": "concepts/streaming/", "title": "Streaming", "text": "<p>LangGraph implements a streaming system to surface real-time updates, allowing for responsive and transparent user experiences.</p> <p>LangGraph\u2019s streaming system lets you surface live feedback from graph runs to your app. There are three main categories of data you can stream:</p> <ol> <li>Workflow progress \u2014 get state updates after each graph node is executed.</li> <li>LLM tokens \u2014 stream language model tokens as they\u2019re generated.</li> <li>Custom updates \u2014 emit user-defined signals (e.g., \u201cFetched 10/100 records\u201d).</li> </ol>"}, {"location": "concepts/streaming/#whats-possible-with-langgraph-streaming", "title": "What\u2019s possible with LangGraph streaming", "text": "<ul> <li>Stream LLM tokens \u2014 capture token streams from anywhere: inside nodes, subgraphs, or tools.</li> <li>Emit progress notifications from tools \u2014 send custom updates or progress signals directly from tool functions.</li> <li>Stream from subgraphs \u2014 include outputs from both the parent graph and any nested subgraphs.</li> <li>Use any LLM \u2014 stream tokens from any LLM, even if it's not a LangChain model using the <code>custom</code> streaming mode.</li> <li>Use multiple streaming modes \u2014 choose from <code>values</code> (full state), <code>updates</code> (state deltas), <code>messages</code> (LLM tokens + metadata), <code>custom</code> (arbitrary user data), or <code>debug</code> (detailed traces).</li> </ul>"}, {"location": "concepts/subgraphs/", "title": "Subgraphs", "text": "<p>A subgraph is a graph that is used as a node in another graph \u2014 this is the concept of encapsulation applied to LangGraph. Subgraphs allow you to build complex systems with multiple components that are themselves graphs.</p> <p></p> <p>Some reasons for using subgraphs are:</p> <ul> <li>building multi-agent systems</li> <li>when you want to reuse a set of nodes in multiple graphs</li> <li>when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph</li> </ul> <p>The main question when adding subgraphs is how the parent graph and subgraph communicate, i.e. how they pass the state between each other during the graph execution. There are two scenarios:</p> <ul> <li>parent and subgraph have shared state keys in their state schemas. In this case, you can include the subgraph as a node in the parent graph</li> </ul> <p>:::python</p> <pre><code>from langgraph.graph import StateGraph, MessagesState, START\n\n# Subgraph\n\ndef call_model(state: MessagesState):\n    response = model.invoke(state[\"messages\"])\n    return {\"messages\": response}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(call_model)\n...\n# highlight-next-line\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\n# highlight-next-line\nbuilder.add_node(\"subgraph_node\", subgraph)\nbuilder.add_edge(START, \"subgraph_node\")\ngraph = builder.compile()\n...\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MessagesZodState, START } from \"@langchain/langgraph\";\n\n// Subgraph\n\nconst subgraphBuilder = new StateGraph(MessagesZodState).addNode(\n  \"callModel\",\n  async (state) =&gt; {\n    const response = await model.invoke(state.messages);\n    return { messages: response };\n  }\n);\n// ... other nodes and edges\n// highlight-next-line\nconst subgraph = subgraphBuilder.compile();\n\n// Parent graph\n\nconst builder = new StateGraph(MessagesZodState)\n  // highlight-next-line\n  .addNode(\"subgraphNode\", subgraph)\n  .addEdge(START, \"subgraphNode\");\nconst graph = builder.compile();\n// ...\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi!\" }] });\n</code></pre> <p>:::</p> <ul> <li>parent graph and subgraph have different schemas (no shared state keys in their state schemas). In this case, you have to call the subgraph from inside a node in the parent graph: this is useful when the parent graph and the subgraph have different state schemas and you need to transform state before or after calling the subgraph</li> </ul> <p>:::python</p> <pre><code>from typing_extensions import TypedDict, Annotated\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.graph.message import add_messages\n\nclass SubgraphMessagesState(TypedDict):\n    # highlight-next-line\n    subgraph_messages: Annotated[list[AnyMessage], add_messages]\n\n# Subgraph\n\n# highlight-next-line\ndef call_model(state: SubgraphMessagesState):\n    response = model.invoke(state[\"subgraph_messages\"])\n    return {\"subgraph_messages\": response}\n\nsubgraph_builder = StateGraph(SubgraphMessagesState)\nsubgraph_builder.add_node(\"call_model_from_subgraph\", call_model)\nsubgraph_builder.add_edge(START, \"call_model_from_subgraph\")\n...\n# highlight-next-line\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\ndef call_subgraph(state: MessagesState):\n    response = subgraph.invoke({\"subgraph_messages\": state[\"messages\"]})\n    return {\"messages\": response[\"subgraph_messages\"]}\n\nbuilder = StateGraph(State)\n# highlight-next-line\nbuilder.add_node(\"subgraph_node\", call_subgraph)\nbuilder.add_edge(START, \"subgraph_node\")\ngraph = builder.compile()\n...\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]})\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MessagesZodState, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst SubgraphState = z.object({\n  // highlight-next-line\n  subgraphMessages: MessagesZodState.shape.messages,\n});\n\n// Subgraph\n\nconst subgraphBuilder = new StateGraph(SubgraphState)\n  // highlight-next-line\n  .addNode(\"callModelFromSubgraph\", async (state) =&gt; {\n    const response = await model.invoke(state.subgraphMessages);\n    return { subgraphMessages: response };\n  })\n  .addEdge(START, \"callModelFromSubgraph\");\n// ...\n// highlight-next-line\nconst subgraph = subgraphBuilder.compile();\n\n// Parent graph\n\nconst builder = new StateGraph(MessagesZodState)\n  // highlight-next-line\n  .addNode(\"subgraphNode\", async (state) =&gt; {\n    const response = await subgraph.invoke({\n      subgraphMessages: state.messages,\n    });\n    return { messages: response.subgraphMessages };\n  })\n  .addEdge(START, \"subgraphNode\");\nconst graph = builder.compile();\n// ...\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi!\" }] });\n</code></pre> <p>:::</p>"}, {"location": "concepts/time-travel/", "title": "Time Travel \u23f1\ufe0f", "text": "<p>When working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail:</p> <ol> <li>\ud83e\udd14 Understand reasoning: Analyze the steps that led to a successful result.</li> <li>\ud83d\udc1e Debug mistakes: Identify where and why errors occurred.</li> <li>\ud83d\udd0d Explore alternatives: Test different paths to uncover better solutions.</li> </ol> <p>LangGraph provides time travel functionality to support these use cases. Specifically, you can resume execution from a prior checkpoint \u2014 either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history.</p> <p>Tip</p> <p>For information on how to use time travel, see Use time travel and Time travel using Server API.</p>", "boost": 2}, {"location": "concepts/tools/", "title": "Tools", "text": "<p>Many AI applications interact with users via natural language. However, some use cases require models to interface directly with external systems\u2014such as APIs, databases, or file systems\u2014using structured input. In these scenarios, tool calling enables models to generate requests that conform to a specified input schema.</p> <p>:::python Tools encapsulate a callable function and its input schema. These can be passed to compatible chat models, allowing the model to decide whether to invoke a tool and with what arguments. :::</p> <p>:::js Tools encapsulate a callable function and its input schema. These can be passed to compatible chat models, allowing the model to decide whether to invoke a tool and with what arguments. :::</p>"}, {"location": "concepts/tools/#tool-calling", "title": "Tool calling", "text": "<p>Tool calling is typically conditional. Based on the user input and available tools, the model may choose to issue a tool call request. This request is returned in an <code>AIMessage</code> object, which includes a <code>tool_calls</code> field that specifies the tool name and input arguments:</p> <p>:::python</p> <pre><code>llm_with_tools.invoke(\"What is 2 multiplied by 3?\")\n# -&gt; AIMessage(tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, ...}])\n</code></pre> <pre><code>AIMessage(\n  tool_calls=[\n    ToolCall(name=\"multiply\", args={\"a\": 2, \"b\": 3}),\n    ...\n  ]\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>await llmWithTools.invoke(\"What is 2 multiplied by 3?\");\n</code></pre> <pre><code>AIMessage {\n  tool_calls: [\n    ToolCall {\n      name: \"multiply\",\n      args: { a: 2, b: 3 },\n      ...\n    },\n    ...\n  ]\n}\n</code></pre> <p>:::</p> <p>If the input is unrelated to any tool, the model returns only a natural language message:</p> <p>:::python</p> <pre><code>llm_with_tools.invoke(\"Hello world!\")  # -&gt; AIMessage(content=\"Hello!\")\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>await llmWithTools.invoke(\"Hello world!\"); // { content: \"Hello!\" }\n</code></pre> <p>:::</p> <p>Importantly, the model does not execute the tool\u2014it only generates a request. A separate executor (such as a runtime or agent) is responsible for handling the tool call and returning the result.</p> <p>See the tool calling guide for more details.</p>"}, {"location": "concepts/tools/#prebuilt-tools", "title": "Prebuilt tools", "text": "<p>LangChain provides prebuilt tool integrations for common external systems including APIs, databases, file systems, and web data.</p> <p>:::python Browse the integrations directory for available tools. :::</p> <p>:::js Browse the integrations directory for available tools. :::</p> <p>Common categories:</p> <ul> <li>Search: Bing, SerpAPI, Tavily</li> <li>Code execution: Python REPL, Node.js REPL</li> <li>Databases: SQL, MongoDB, Redis</li> <li>Web data: Scraping and browsing</li> <li>APIs: OpenWeatherMap, NewsAPI, etc.</li> </ul>"}, {"location": "concepts/tools/#custom-tools", "title": "Custom tools", "text": "<p>:::python You can define custom tools using the <code>@tool</code> decorator or plain Python functions. For example:</p> <pre><code>from langchain_core.tools import tool\n\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n</code></pre> <p>:::</p> <p>:::js You can define custom tools using the <code>tool</code> function. For example:</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst multiply = tool(\n  (input) =&gt; {\n    return input.a * input.b;\n  },\n  {\n    name: \"multiply\",\n    description: \"Multiply two numbers.\",\n    schema: z.object({\n      a: z.number(),\n      b: z.number(),\n    }),\n  }\n);\n</code></pre> <p>:::</p> <p>See the tool calling guide for more details.</p>"}, {"location": "concepts/tools/#tool-execution", "title": "Tool execution", "text": "<p>While the model determines when to call a tool, execution of the tool call must be handled by a runtime component.</p> <p>LangGraph provides prebuilt components for this:</p> <p>:::python</p> <ul> <li>@[<code>ToolNode</code>][ToolNode]: A prebuilt node that executes tools.</li> <li>@[<code>create_react_agent</code>][create_react_agent]: Constructs a full agent that manages tool calling automatically. :::</li> </ul> <p>:::js</p> <ul> <li>@[ToolNode]: A prebuilt node that executes tools.</li> <li>@[<code>createReactAgent</code>][create_react_agent]: Constructs a full agent that manages tool calling automatically. :::</li> </ul>"}, {"location": "concepts/tracing/", "title": "Tracing", "text": "<p>Traces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use LangSmith to visualize these execution steps. To use it, enable tracing for your application. This enables you to do the following:</p> <ul> <li>Debug a locally running application.</li> <li>Evaluate the application performance.</li> <li>Monitor the application.</li> </ul> <p>To get started, sign up for a free account at LangSmith.</p>"}, {"location": "concepts/tracing/#learn-more", "title": "Learn more", "text": "<ul> <li>Graph runs in LangSmith</li> <li>LangSmith Observability quickstart</li> <li>Trace with LangGraph</li> <li>Tracing conceptual guide</li> </ul>"}, {"location": "examples/", "title": "Examples", "text": "<p>The pages in this section provide end-to-end examples for the following topics:</p>"}, {"location": "examples/#general", "title": "General", "text": "<ul> <li>Template Applications: Create a LangGraph application from a template.</li> <li>Agentic RAG: Build a retrieval agent that can decide when to use a retriever tool.</li> <li>Agent Supervisor: Build a supervisor agent that can manage a team of agents.</li> <li>SQL agent: Build a SQL agent that can execute SQL queries and return the results.</li> <li>Prebuilt chat UI: Use a prebuilt chat UI to interact with any LangGraph agent.</li> <li>Graph runs in LangSmith: Use LangSmith to track and analyze graph runs.</li> </ul>"}, {"location": "guides/", "title": "Guides", "text": "<p>The pages in this section provide a conceptual overview and how-tos for the following topics:</p>"}, {"location": "guides/#agent-development", "title": "Agent development", "text": "<ul> <li>Overview: Use prebuilt components to build an agent.</li> <li>Run an agent: Run an agent by providing input, interpreting output, enabling streaming, and controlling execution limits.</li> </ul>"}, {"location": "guides/#langgraph-apis", "title": "LangGraph APIs", "text": "<ul> <li>Graph API: Use the Graph API to define workflows using a graph paradigm.</li> <li>Functional API: Use Functional API to build workflows using a functional paradigm without thinking about the graph structure.</li> <li>Runtime: Pregel implements LangGraph's runtime, managing the execution of LangGraph applications.</li> </ul>"}, {"location": "guides/#core-capabilities", "title": "Core capabilities", "text": "<p>These capabilities are available in both LangGraph OSS and the LangGraph Platform.</p> <ul> <li>Streaming: Stream outputs from a LangGraph graph.</li> <li>Persistence: Persist the state of a LangGraph graph.</li> <li>Durable execution: Save progress at key points in the graph execution.</li> <li>Memory: Remember information about previous interactions.</li> <li>Context: Pass outside data to a LangGraph graph to provide context for the graph execution.</li> <li>Models: Integrate various LLMs into your LangGraph application.</li> <li>Tools: Interface directly with external systems.</li> <li>Human-in-the-loop: Pause a graph and wait for human input at any point in a workflow.</li> <li>Time travel: Travel back in time to a specific point in the execution of a LangGraph graph.</li> <li>Subgraphs: Build modular graphs.</li> <li>Multi-agent: Break down a complex workflow into multiple agents.</li> <li>MCP: Use MCP servers in a LangGraph graph.</li> <li>Evaluation: Use LangSmith to evaluate your graph's performance.</li> </ul>"}, {"location": "how-tos/enable-tracing/", "title": "Enable tracing for your application", "text": "<p>To enable tracing for your application, set the following environment variables:</p> <pre><code>export LANGSMITH_TRACING=true\nexport LANGSMITH_API_KEY=&lt;your-api-key&gt;\n</code></pre> <p>For more information, see Trace with LangGraph.</p>"}, {"location": "how-tos/enable-tracing/#learn-more", "title": "Learn more", "text": "<ul> <li>Graph runs in LangSmith</li> <li>LangSmith Observability quickstart</li> <li>Tracing conceptual guide</li> </ul>"}, {"location": "how-tos/graph-api/", "title": "Graph API \uc0ac\uc6a9\ubc95", "text": "<p>\uc774 \uac00\uc774\ub4dc\ub294 LangGraph\uc758 Graph API \uae30\ubcf8 \uc0ac\ud56d\uc744 \uc124\uba85\ud569\ub2c8\ub2e4. \uc0c1\ud0dc\ubd80\ud130 \uc2dc\uc791\ud558\uc5ec \uc2dc\ud000\uc2a4, \ube0c\ub79c\uce58, \ub8e8\ud504\uc640 \uac19\uc740 \uc77c\ubc18\uc801\uc778 \uadf8\ub798\ud504 \uad6c\uc870\ub97c \uad6c\uc131\ud558\ub294 \ubc29\ubc95\uc744 \ub2e4\ub8f9\ub2c8\ub2e4. \ub610\ud55c \ub9f5-\ub9ac\ub4c0\uc2a4 \uc6cc\ud06c\ud50c\ub85c\ub97c \uc704\ud55c Send API\uc640 \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8\ub97c \ub178\ub4dc \uac04 \"\uc774\ub3d9\"\uacfc \uacb0\ud569\ud558\ub294 Command API \uac19\uc740 LangGraph\uc758 \uc81c\uc5b4 \uae30\ub2a5\ub3c4 \ub2e4\ub8f9\ub2c8\ub2e4.</p>"}, {"location": "how-tos/graph-api/#_1", "title": "\uc124\uc815", "text": "<p><code>langgraph</code>\ub97c \uc124\uce58\ud558\uc138\uc694:</p> <pre><code>pip install -U langgraph\n</code></pre> <p>\ub354 \ub098\uc740 \ub514\ubc84\uae45\uc744 \uc704\ud574 LangSmith\ub97c \uc124\uc815\ud558\uc138\uc694</p> <p>LangSmith\uc5d0 \uac00\uc785\ud558\uc5ec LangGraph \ud504\ub85c\uc81d\ud2b8\uc758 \ubb38\uc81c\ub97c \ube60\ub974\uac8c \ud30c\uc545\ud558\uace0 \uc131\ub2a5\uc744 \uac1c\uc120\ud558\uc138\uc694. LangSmith\ub97c \uc0ac\uc6a9\ud558\uba74 \ucd94\uc801 \ub370\uc774\ud130\ub97c \ud1b5\ud574 LangGraph\ub85c \uad6c\ucd95\ud55c LLM \uc571\uc744 \ub514\ubc84\uae45, \ud14c\uc2a4\ud2b8 \ubc0f \ubaa8\ub2c8\ud130\ub9c1\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc2dc\uc791\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \ubb38\uc11c\ub97c \ucc38\uc870\ud558\uc138\uc694.</p>"}, {"location": "how-tos/graph-api/#_2", "title": "\uc0c1\ud0dc \uc815\uc758 \ubc0f \uc5c5\ub370\uc774\ud2b8", "text": "<p>\uc5ec\uae30\uc11c\ub294 LangGraph\uc5d0\uc11c \uc0c1\ud0dc\ub97c \uc815\uc758\ud558\uace0 \uc5c5\ub370\uc774\ud2b8\ud558\ub294 \ubc29\ubc95\uc744 \uc124\uba85\ud569\ub2c8\ub2e4. \ub2e4\uc74c\uc744 \ubcf4\uc5ec\ub4dc\ub9ac\uaca0\uc2b5\ub2c8\ub2e4:</p> <ol> <li>\uadf8\ub798\ud504\uc758 \uc2a4\ud0a4\ub9c8\ub97c \uc815\uc758\ud558\ub294 \ub370 \uc0c1\ud0dc\ub97c \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95</li> <li>\uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8\uac00 \ucc98\ub9ac\ub418\ub294 \ubc29\uc2dd\uc744 \uc81c\uc5b4\ud558\ub294 \ub9ac\ub4c0\uc11c\ub97c \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95</li> </ol>"}, {"location": "how-tos/graph-api/#_3", "title": "\uc0c1\ud0dc \uc815\uc758", "text": "<p>LangGraph\uc758 \uc0c1\ud0dc\ub294 <code>TypedDict</code>, <code>Pydantic</code> \ubaa8\ub378 \ub610\ub294 \ub370\uc774\ud130\ud074\ub798\uc2a4\uac00 \ub420 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc544\ub798\uc5d0\uc11c\ub294 <code>TypedDict</code>\ub97c \uc0ac\uc6a9\ud558\uaca0\uc2b5\ub2c8\ub2e4. Pydantic \uc0ac\uc6a9\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uc774 \uc139\uc158\uc744 \ucc38\uc870\ud558\uc138\uc694.</p> <p>\uae30\ubcf8\uc801\uc73c\ub85c \uadf8\ub798\ud504\ub294 \uc785\ub825\uacfc \ucd9c\ub825 \uc2a4\ud0a4\ub9c8\uac00 \ub3d9\uc77c\ud558\uba70, \uc0c1\ud0dc\uac00 \ud574\ub2f9 \uc2a4\ud0a4\ub9c8\ub97c \uacb0\uc815\ud569\ub2c8\ub2e4. \ubcc4\ub3c4\uc758 \uc785\ub825 \ubc0f \ucd9c\ub825 \uc2a4\ud0a4\ub9c8\ub97c \uc815\uc758\ud558\ub294 \ubc29\ubc95\uc740 \uc774 \uc139\uc158\uc744 \ucc38\uc870\ud558\uc138\uc694.</p> <p>\ub9ce\uc740 LLM \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0\uc11c \uc720\uc6a9\ud55c \uc0c1\ud0dc\uc758 \ub2e4\uc6a9\ub3c4 \ud615\uc2dd\uc744 \ub098\ud0c0\ub0b4\ub294 \uba54\uc2dc\uc9c0\ub97c \uc0ac\uc6a9\ud55c \uac04\ub2e8\ud55c \uc608\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uac1c\ub150 \ud398\uc774\uc9c0\ub97c \ucc38\uc870\ud558\uc138\uc694.</p> <p><sup>API \ucc38\uc870: AnyMessage</sup></p> <pre><code>from langchain_core.messages import AnyMessage\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    messages: list[AnyMessage]\n    extra_field: int\n</code></pre> <p>\uc774 \uc0c1\ud0dc\ub294 \uba54\uc2dc\uc9c0 \uac1d\uccb4 \ubaa9\ub85d\uacfc \ucd94\uac00 \uc815\uc218 \ud544\ub4dc\ub97c \ucd94\uc801\ud569\ub2c8\ub2e4.</p>"}, {"location": "how-tos/graph-api/#_4", "title": "\uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8", "text": "<p>\ub2e8\uc77c \ub178\ub4dc\ub85c \uad6c\uc131\ub41c \uc608\uc81c \uadf8\ub798\ud504\ub97c \ub9cc\ub4e4\uc5b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \ub178\ub4dc\ub294 \uadf8\ub798\ud504\uc758 \uc0c1\ud0dc\ub97c \uc77d\uace0 \uc5c5\ub370\uc774\ud2b8\ud558\ub294 Python \ud568\uc218\uc785\ub2c8\ub2e4. \uc774 \ud568\uc218\uc758 \uccab \ubc88\uc9f8 \uc778\uc790\ub294 \ud56d\uc0c1 \uc0c1\ud0dc\uc785\ub2c8\ub2e4:</p> <p><sup>API \ucc38\uc870: AIMessage</sup></p> <pre><code>from langchain_core.messages import AIMessage\n\ndef node(state: State):\n    messages = state[\"messages\"]\n    new_message = AIMessage(\"Hello!\")\n    return {\"messages\": messages + [new_message], \"extra_field\": 10}\n</code></pre> <p>\uc774 \ub178\ub4dc\ub294 \ub2e8\uc21c\ud788 \uba54\uc2dc\uc9c0 \ubaa9\ub85d\uc5d0 \uba54\uc2dc\uc9c0\ub97c \ucd94\uac00\ud558\uace0 \ucd94\uac00 \ud544\ub4dc\ub97c \ucc44\uc6c1\ub2c8\ub2e4.</p> <p>\uc911\uc694</p> <p>\ub178\ub4dc\ub294 \uc0c1\ud0dc\ub97c \uc9c1\uc811 \ubcc0\uacbd\ud558\uc9c0 \ub9d0\uace0 \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8\ub97c \ubc18\ud658\ud574\uc57c \ud569\ub2c8\ub2e4.</p> <p>\ub2e4\uc74c\uc73c\ub85c \uc774 \ub178\ub4dc\ub97c \ud3ec\ud568\ud558\ub294 \uac04\ub2e8\ud55c \uadf8\ub798\ud504\ub97c \uc815\uc758\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774 \uc0c1\ud0dc\uc5d0\uc11c \uc791\ub3d9\ud558\ub294 \uadf8\ub798\ud504\ub97c \uc815\uc758\ud558\ub824\uba74 StateGraph\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c add_node\ub97c \uc0ac\uc6a9\ud558\uc5ec \uadf8\ub798\ud504\ub97c \ucc44\uc6c1\ub2c8\ub2e4.</p> <p><sup>API \ucc38\uc870: StateGraph</sup></p> <pre><code>from langgraph.graph import StateGraph\n\nbuilder = StateGraph(State)\nbuilder.add_node(node)\nbuilder.set_entry_point(\"node\")\ngraph = builder.compile()\n</code></pre> <p>\uadf8\ub798\ud504\ub97c \uc2dc\uac01\ud654\ud558\ub294 \uae30\ubcf8 \uc81c\uacf5 \uc720\ud2f8\ub9ac\ud2f0\uac00 LangGraph\uc5d0 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub798\ud504\ub97c \uac80\uc0ac\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc2dc\uac01\ud654\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uc774 \uc139\uc158\uc744 \ucc38\uc870\ud558\uc138\uc694.</p> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p></p> <p>\uc774 \uacbd\uc6b0 \uadf8\ub798\ud504\ub294 \ub2e8\uc77c \ub178\ub4dc\ub9cc \uc2e4\ud589\ud569\ub2c8\ub2e4. \uac04\ub2e8\ud55c \ud638\ucd9c\uc744 \uc9c4\ud589\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4:</p> <p><sup>API \ucc38\uc870: HumanMessage</sup></p> <pre><code>from langchain_core.messages import HumanMessage\n\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\nresult\n</code></pre> <pre><code>{'messages': [HumanMessage(content='Hi'), AIMessage(content='Hello!')], 'extra_field': 10}\n</code></pre> <p>\ub2e4\uc74c \uc0ac\ud56d\uc5d0 \uc720\uc758\ud558\uc138\uc694:</p> <ul> <li>\ud638\ucd9c\uc744 \uc2dc\uc791\ud560 \ub54c \uc0c1\ud0dc\uc758 \ub2e8\uc77c \ud0a4\ub97c \uc5c5\ub370\uc774\ud2b8\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\ud638\ucd9c \uacb0\uacfc\uc5d0\uc11c \uc804\uccb4 \uc0c1\ud0dc\ub97c \ubc1b\uc2b5\ub2c8\ub2e4.</li> </ul> <p>\ud3b8\uc758\ub97c \uc704\ud574 \uba54\uc2dc\uc9c0 \uac1d\uccb4\uc758 \ub0b4\uc6a9\uc744 \uc608\uc058\uac8c \ucd9c\ub825\uc73c\ub85c \uc790\uc8fc \uac80\uc0ac\ud569\ub2c8\ub2e4:</p> <pre><code>for message in result[\"messages\"]:\n    message.pretty_print()\n</code></pre>"}, {"location": "how-tos/graph-api/#_5", "title": "\ub9ac\ub4c0\uc11c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8 \ucc98\ub9ac", "text": "<p>\uc0c1\ud0dc\uc758 \uac01 \ud0a4\ub294 \uc5c5\ub370\uc774\ud2b8\uac00 \ub178\ub4dc\uc5d0\uc11c \uc5b4\ub5bb\uac8c \uc801\uc6a9\ub418\ub294\uc9c0 \uc81c\uc5b4\ud558\ub294 \ub3c5\ub9bd\uc801\uc778 \ub9ac\ub4c0\uc11c \ud568\uc218\ub97c \uac00\uc9c8 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub9ac\ub4c0\uc11c \ud568\uc218\uac00 \uba85\uc2dc\uc801\uc73c\ub85c \uc9c0\uc815\ub418\uc9c0 \uc54a\uc73c\uba74 \ud0a4\uc5d0 \ub300\ud55c \ubaa8\ub4e0 \uc5c5\ub370\uc774\ud2b8\uac00 \uc774\ub97c \ub36e\uc5b4\uc4f0\ub294 \uac83\uc73c\ub85c \uac00\uc815\ud569\ub2c8\ub2e4.</p> <p><code>TypedDict</code> \uc0c1\ud0dc \uc2a4\ud0a4\ub9c8\uc758 \uacbd\uc6b0 \ud574\ub2f9 \uc0c1\ud0dc \ud544\ub4dc\uc5d0 \ub9ac\ub4c0\uc11c \ud568\uc218\ub97c \uc8fc\uc11d\uc73c\ub85c \ucd94\uac00\ud558\uc5ec \ub9ac\ub4c0\uc11c\ub97c \uc815\uc758\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc55e\uc120 \uc608\uc81c\uc5d0\uc11c \ub178\ub4dc\ub294 \uba54\uc2dc\uc9c0\ub97c \ucd94\uac00\ud558\uc5ec <code>\"messages\"</code> \ud0a4\ub97c \uc5c5\ub370\uc774\ud2b8\ud588\uc2b5\ub2c8\ub2e4. \uc544\ub798\uc5d0\uc11c\ub294 \uc5c5\ub370\uc774\ud2b8\uac00 \uc790\ub3d9\uc73c\ub85c \ucd94\uac00\ub418\ub3c4\ub85d \uc774 \ud0a4\uc5d0 \ub9ac\ub4c0\uc11c\ub97c \ucd94\uac00\ud558\uaca0\uc2b5\ub2c8\ub2e4:</p> <pre><code>from typing_extensions import Annotated\n\ndef add(left, right):\n    \"\"\"\ube4c\ud2b8\uc778 \\`operator\\`\uc5d0\uc11c \\`add\\`\ub97c \uac00\uc838\uc62c \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.\"\"\"\n    return left + right\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add]\n    extra_field: int\n</code></pre> <p>\uc774\uc81c \ub178\ub4dc\ub97c \ub2e8\uc21c\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>def node(state: State):\n    new_message = AIMessage(\"Hello!\")\n    return {\"messages\": [new_message], \"extra_field\": 10}\n</code></pre> <p><sup>API \ucc38\uc870: START</sup></p> <pre><code>from langgraph.graph import START\n\ngraph = StateGraph(State).add_node(node).add_edge(START, \"node\").compile()\n\nresult = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]})\n\nfor message in result[\"messages\"]:\n    message.pretty_print()\n</code></pre>"}, {"location": "how-tos/graph-api/#messagesstate", "title": "MessagesState", "text": "<p>\uba54\uc2dc\uc9c0 \ubaa9\ub85d\uc744 \uc5c5\ub370\uc774\ud2b8\ud560 \ub54c \ucd94\uac00\uc801\uc778 \uace0\ub824\uc0ac\ud56d\uc774 \uc788\uc2b5\ub2c8\ub2e4:</p> <ul> <li>\uc0c1\ud0dc\uc758 \uae30\uc874 \uba54\uc2dc\uc9c0\ub97c \uc5c5\ub370\uc774\ud2b8\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>\uba54\uc2dc\uc9c0 \ud615\uc2dd\uc5d0 \ub300\ud55c \uc57d\uce6d(\uc608: OpenAI \ud615\uc2dd)\uc744 \ud5c8\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> </ul> <p>LangGraph\uc5d0\ub294 \uc774\ub7ec\ud55c \uace0\ub824\uc0ac\ud56d\uc744 \ucc98\ub9ac\ud558\ub294 \uae30\ubcf8 \uc81c\uacf5 \ub9ac\ub4c0\uc11c <code>add_messages</code>\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4:</p> <p><sup>API \ucc38\uc870: add_messages</sup></p> <pre><code>from langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n    extra_field: int\n\ndef node(state: State):\n    new_message = AIMessage(\"Hello!\")\n    return {\"messages\": [new_message], \"extra_field\": 10}\n\ngraph = StateGraph(State).add_node(node).set_entry_point(\"node\").compile()\n</code></pre> <pre><code>input_message = {\"role\": \"user\", \"content\": \"Hi\"}\n\nresult = graph.invoke({\"messages\": [input_message]})\n\nfor message in result[\"messages\"]:\n    message.pretty_print()\n</code></pre> <p>\uc774\ub294 \ucc44\ud305 \ubaa8\ub378\uacfc \uad00\ub828\ub41c \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0\uc11c \uc0c1\ud0dc\ub97c \ud45c\ud604\ud558\ub294 \ub2e4\uc6a9\ub3c4 \ubc29\uc2dd\uc785\ub2c8\ub2e4. \ud3b8\uc758\ub97c \uc704\ud574 LangGraph\uc5d0\ub294 \uc0ac\uc804 \uad6c\ucd95\ub41c <code>MessagesState</code>\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc5b4 \ub2e4\uc74c\uacfc \uac19\uc774 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>from langgraph.graph import MessagesState\n\nclass State(MessagesState):\n    extra_field: int\n</code></pre>"}, {"location": "how-tos/graph-api/#_6", "title": "\uc785\ub825 \ubc0f \ucd9c\ub825 \uc2a4\ud0a4\ub9c8 \uc815\uc758", "text": "<p>\uae30\ubcf8\uc801\uc73c\ub85c <code>StateGraph</code>\ub294 \ub2e8\uc77c \uc2a4\ud0a4\ub9c8\ub85c \uc791\ub3d9\ud558\uba70 \ubaa8\ub4e0 \ub178\ub4dc\uac00 \ud574\ub2f9 \uc2a4\ud0a4\ub9c8\ub97c \uc0ac\uc6a9\ud558\uc5ec \ud1b5\uc2e0\ud560 \uac83\uc73c\ub85c \uc608\uc0c1\ub429\ub2c8\ub2e4. \uadf8\ub7ec\ub098 \uadf8\ub798\ud504\uc5d0 \ub300\ud574 \ubcc4\uac1c\uc758 \uc785\ub825 \ubc0f \ucd9c\ub825 \uc2a4\ud0a4\ub9c8\ub97c \uc815\uc758\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\ubcc4\uac1c\uc758 \uc2a4\ud0a4\ub9c8\uac00 \uc9c0\uc815\ub418\uba74 \ub178\ub4dc \uac04 \ud1b5\uc2e0\uc5d0\ub294 \uc5ec\uc804\ud788 \ub0b4\ubd80 \uc2a4\ud0a4\ub9c8\uac00 \uc0ac\uc6a9\ub429\ub2c8\ub2e4. \uc785\ub825 \uc2a4\ud0a4\ub9c8\ub294 \uc81c\uacf5\ub41c \uc785\ub825\uc774 \uc608\uc0c1 \uad6c\uc870\uc640 \uc77c\uce58\ud558\ub294\uc9c0 \ud655\uc778\ud558\ub294 \ubc18\uba74, \ucd9c\ub825 \uc2a4\ud0a4\ub9c8\ub294 \uc815\uc758\ub41c \ucd9c\ub825 \uc2a4\ud0a4\ub9c8\uc5d0 \ub530\ub77c \uad00\ub828 \uc815\ubcf4\ub9cc \ubc18\ud658\ud558\ub3c4\ub85d \ub0b4\ubd80 \ub370\uc774\ud130\ub97c \ud544\ud130\ub9c1\ud569\ub2c8\ub2e4.</p> <p>\uc544\ub798\uc5d0\uc11c \ubcc4\uac1c\uc758 \uc785\ub825 \ubc0f \ucd9c\ub825 \uc2a4\ud0a4\ub9c8\ub97c \uc815\uc758\ud558\ub294 \ubc29\ubc95\uc744 \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p><sup>API \ucc38\uc870: StateGraph | START | END</sup></p> <pre><code>from langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\n# \uc785\ub825 \uc2a4\ud0a4\ub9c8 \uc815\uc758\nclass InputState(TypedDict):\n    question: str\n\n# \ucd9c\ub825 \uc2a4\ud0a4\ub9c8 \uc815\uc758\nclass OutputState(TypedDict):\n    answer: str\n\n# \uc785\ub825\uacfc \ucd9c\ub825\uc744 \ubaa8\ub450 \uacb0\ud569\ud55c \uc804\uccb4 \uc2a4\ud0a4\ub9c8 \uc815\uc758\nclass OverallState(InputState, OutputState):\n    pass\n\n# \uc785\ub825\uc744 \ucc98\ub9ac\ud558\uace0 \ub2f5\ubcc0\uc744 \uc0dd\uc131\ud558\ub294 \ub178\ub4dc \uc815\uc758\ndef answer_node(state: InputState):\n    # \uc608\uc2dc \ub2f5\ubcc0\uacfc \ucd94\uac00 \ud0a4\n    return {\"answer\": \"bye\", \"question\": state[\"question\"]}\n\n# \uc785\ub825 \ubc0f \ucd9c\ub825 \uc2a4\ud0a4\ub9c8\uac00 \uc9c0\uc815\ub41c \uadf8\ub798\ud504 \uad6c\ucd95\nbuilder = StateGraph(OverallState, input_schema=InputState, output_schema=OutputState)\nbuilder.add_node(answer_node)  # \ub2f5\ubcc0 \ub178\ub4dc \ucd94\uac00\nbuilder.add_edge(START, \"answer_node\")  # \uc2dc\uc791 \uc5e3\uc9c0 \uc815\uc758\nbuilder.add_edge(\"answer_node\", END)  # \uc885\ub8cc \uc5e3\uc9c0 \uc815\uc758\ngraph = builder.compile()  # \uadf8\ub798\ud504 \ucef4\ud30c\uc77c\n\n# \uc785\ub825\uacfc \ud568\uaed8 \uadf8\ub798\ud504 \ud638\ucd9c\ud558\uace0 \uacb0\uacfc \ucd9c\ub825\nprint(graph.invoke({\"question\": \"hi\"}))\n</code></pre> <pre><code>{'answer': 'bye'}\n</code></pre> <p>\ud638\ucd9c \ucd9c\ub825\uc5d0\ub294 \ucd9c\ub825 \uc2a4\ud0a4\ub9c8\ub9cc \ud3ec\ud568\ub41c\ub2e4\ub294 \uc810\uc5d0 \uc720\uc758\ud558\uc138\uc694.</p>"}, {"location": "how-tos/graph-api/#_7", "title": "\ub7f0\ud0c0\uc784 \uad6c\uc131 \ucd94\uac00", "text": "<p>\uadf8\ub798\ud504\ub97c \ud638\ucd9c\ud560 \ub54c \uad6c\uc131\ud560 \uc218 \uc788\uae30\ub97c \uc6d0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uadf8\ub798\ud504 \uc0c1\ud0dc\uc5d0 \uc774\ub7ec\ud55c \ub9e4\uac1c\ubcc0\uc218\ub97c \uc624\uc5fc\uc2dc\ud0a4\uc9c0 \uc54a\uc73c\uba74\uc11c \uc0ac\uc6a9\ud560 LLM\uc774\ub098 \uc2dc\uc2a4\ud15c \ud504\ub86c\ud504\ud2b8\ub97c \ub7f0\ud0c0\uc784\uc5d0 \uc9c0\uc815\ud560 \uc218 \uc788\uae30\ub97c \uc6d0\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\ub7f0\ud0c0\uc784 \uad6c\uc131\uc744 \ucd94\uac00\ud558\ub824\uba74:</p> <ol> <li>\uad6c\uc131 \uc2a4\ud0a4\ub9c8 \uc9c0\uc815</li> <li>\ub178\ub4dc \ub610\ub294 \uc870\uac74\ubd80 \uc5e3\uc9c0\uc758 \ud568\uc218 \uc2dc\uadf8\ub2c8\ucc98\uc5d0 \uad6c\uc131 \ucd94\uac00</li> <li>\uadf8\ub798\ud504\uc5d0 \uad6c\uc131 \uc804\ub2ec</li> </ol> <p>\uc544\ub798\uc5d0 \uac04\ub2e8\ud55c \uc608\uc81c\uac00 \uc788\uc2b5\ub2c8\ub2e4:</p> <p><sup>API \ucc38\uc870: END | StateGraph | START</sup></p> <pre><code>from langgraph.graph import END, StateGraph, START\nfrom langgraph.runtime import Runtime\nfrom typing_extensions import TypedDict\n\n# 1. \uad6c\uc131 \uc2a4\ud0a4\ub9c8 \uc9c0\uc815\nclass ContextSchema(TypedDict):\n    my_runtime_value: str\n\n# 2. \ub178\ub4dc\uc5d0\uc11c \uad6c\uc131\uc744 \uc561\uc138\uc2a4\ud558\ub294 \uadf8\ub798\ud504 \uc815\uc758\nclass State(TypedDict):\n    my_state_value: str\n\ndef node(state: State, runtime: Runtime[ContextSchema]):\n    if runtime.context[\"my_runtime_value\"] == \"a\":\n        return {\"my_state_value\": 1}\n    elif runtime.context[\"my_runtime_value\"] == \"b\":\n        return {\"my_state_value\": 2}\n    else:\n        raise ValueError(\"\uc54c \uc218 \uc5c6\ub294 \uac12\uc785\ub2c8\ub2e4.\")\n\nbuilder = StateGraph(State, context_schema=ContextSchema)\nbuilder.add_node(node)\nbuilder.add_edge(START, \"node\")\nbuilder.add_edge(\"node\", END)\n\ngraph = builder.compile()\n\n# 3. \ub7f0\ud0c0\uc784\uc5d0 \uad6c\uc131 \uc804\ub2ec:\nprint(graph.invoke({}, context={\"my_runtime_value\": \"a\"}))\nprint(graph.invoke({}, context={\"my_runtime_value\": \"b\"}))\n</code></pre> <pre><code>{'my_state_value': 1}\n{'my_state_value': 2}\n</code></pre>"}, {"location": "how-tos/graph-api/#_8", "title": "\uc7ac\uc2dc\ub3c4 \uc815\ucc45 \ucd94\uac00", "text": "<p>API \ud638\ucd9c, \ub370\uc774\ud130\ubca0\uc774\uc2a4 \ucffc\ub9ac, LLM \ud638\ucd9c \ub4f1\uc5d0\uc11c \ub178\ub4dc\uc5d0 \uc0ac\uc6a9\uc790 \uc815\uc758 \uc7ac\uc2dc\ub3c4 \uc815\ucc45\uc744 \uc801\uc6a9\ud558\uace0 \uc2f6\uc744 \uc218 \uc788\ub294 \ub9ce\uc740 \uc0ac\uc6a9 \uc0ac\ub840\uac00 \uc788\uc2b5\ub2c8\ub2e4. LangGraph\ub97c \uc0ac\uc6a9\ud558\uba74 \ub178\ub4dc\uc5d0 \uc7ac\uc2dc\ub3c4 \uc815\ucc45\uc744 \ucd94\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uc7ac\uc2dc\ub3c4 \uc815\ucc45\uc744 \uad6c\uc131\ud558\ub824\uba74 add_node\uc5d0 <code>retry_policy</code> \ub9e4\uac1c\ubcc0\uc218\ub97c \uc804\ub2ec\ud558\uc138\uc694. <code>retry_policy</code> \ub9e4\uac1c\ubcc0\uc218\ub294 <code>RetryPolicy</code> \uba85\uba85\ub41c \ud29c\ud50c \uac1d\uccb4\ub97c \ubc1b\uc2b5\ub2c8\ub2e4. \uc544\ub798\uc5d0\uc11c\ub294 \uae30\ubcf8 \ub9e4\uac1c\ubcc0\uc218\ub85c <code>RetryPolicy</code> \uac1d\uccb4\ub97c \uc778\uc2a4\ud134\uc2a4\ud654\ud558\uace0 \ub178\ub4dc\uc640 \uc5f0\uacb0\ud569\ub2c8\ub2e4:</p> <p><sup>API \ucc38\uc870: RetryPolicy</sup></p> <p>\uae30\ubcf8\uc801\uc73c\ub85c <code>retry_on</code> \ub9e4\uac1c\ubcc0\uc218\ub294 <code>default_retry_on</code> \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uba70, \ub2e4\uc74c\uc744 \uc81c\uc678\ud55c \ubaa8\ub4e0 \uc608\uc678\uc5d0 \ub300\ud574 \uc7ac\uc2dc\ub3c4\ud569\ub2c8\ub2e4:</p> <ul> <li><code>ValueError</code></li> <li><code>TypeError</code></li> <li><code>ArithmeticError</code></li> <li><code>ImportError</code></li> <li><code>LookupError</code></li> <li><code>NameError</code></li> <li><code>SyntaxError</code></li> <li><code>RuntimeError</code></li> <li><code>ReferenceError</code></li> <li><code>StopIteration</code></li> <li><code>StopAsyncIteration</code></li> <li><code>OSError</code></li> </ul> <p>\ub610\ud55c <code>requests</code> \ubc0f <code>httpx</code>\uc640 \uac19\uc740 \uc778\uae30 \uc788\ub294 HTTP \uc694\uccad \ub77c\uc774\ube0c\ub7ec\ub9ac\uc758 \uc608\uc678\uc758 \uacbd\uc6b0 5xx \uc0c1\ud0dc \ucf54\ub4dc\uc5d0\uc11c\ub9cc \uc7ac\uc2dc\ub3c4\ud569\ub2c8\ub2e4.</p> <p>\ud655\uc7a5 \uc608\uc81c: \uc7ac\uc2dc\ub3c4 \uc815\ucc45 \uc0ac\uc6a9\uc790 \uc815\uc758</p> <p>SQL \ub370\uc774\ud130\ubca0\uc774\uc2a4\uc5d0\uc11c \uc77d\uae30\ub97c \uc218\ud589\ud558\ub294 \uc608\uc81c\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc544\ub798\uc5d0\uc11c\ub294 \ub450 \ub178\ub4dc\uc5d0 \uc11c\ub85c \ub2e4\ub978 \uc7ac\uc2dc\ub3c4 \uc815\ucc45\uc744 \uc804\ub2ec\ud569\ub2c8\ub2e4:</p> <pre><code>import sqlite3\nfrom typing_extensions import TypedDict\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import END, MessagesState, StateGraph, START\nfrom langgraph.types import RetryPolicy\nfrom langchain_community.utilities import SQLDatabase\nfrom langchain_core.messages import AIMessage\n\ndb = SQLDatabase.from_uri(\"sqlite:///:memory:\")\nmodel = init_chat_model(\"anthropic:claude-3-5-haiku-latest\")\n\ndef query_database(state: MessagesState):\n    query_result = db.run(\"SELECT * FROM Artist LIMIT 10;\")\n    return {\"messages\": [AIMessage(content=query_result)]}\n\ndef call_model(state: MessagesState):\n    response = model.invoke(state[\"messages\"])\n    return {\"messages\": [response]}\n\n# \uc0c8 \uadf8\ub798\ud504 \uc815\uc758\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(\n    \"query_database\",\n    query_database,\n    retry_policy=RetryPolicy(retry_on=sqlite3.OperationalError),\n)\nbuilder.add_node(\"model\", call_model, retry_policy=RetryPolicy(max_attempts=5))\nbuilder.add_edge(START, \"model\")\nbuilder.add_edge(\"model\", \"query_database\")\nbuilder.add_edge(\"query_database\", END)\ngraph = builder.compile()\n</code></pre>"}, {"location": "how-tos/graph-api/#_9", "title": "\ub178\ub4dc \uce90\uc2f1 \ucd94\uac00", "text": "<p>\ub178\ub4dc \uce90\uc2f1\uc740 \uc2dc\uac04\uc774\ub098 \ube44\uc6a9 \uce21\uba74\uc5d0\uc11c \ube44\uc2fc \uc791\uc5c5\uc744 \ubc18\ubcf5\ud558\uc9c0 \uc54a\uc73c\ub824\ub294 \uacbd\uc6b0\uc5d0 \uc720\uc6a9\ud569\ub2c8\ub2e4. LangGraph\ub97c \uc0ac\uc6a9\ud558\uba74 \uadf8\ub798\ud504\uc758 \ub178\ub4dc\uc5d0 \uac1c\ubcc4\ud654\ub41c \uce90\uc2f1 \uc815\ucc45\uc744 \ucd94\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p>\uce90\uc2dc \uc815\ucc45\uc744 \uad6c\uc131\ud558\ub824\uba74 add_node \ud568\uc218\uc5d0 <code>cache_policy</code> \ub9e4\uac1c\ubcc0\uc218\ub97c \uc804\ub2ec\ud558\uc138\uc694. \ub2e4\uc74c \uc608\uc81c\uc5d0\uc11c\ub294 120\ucd08\uc758 TTL\uacfc \uae30\ubcf8 <code>key_func</code> \uc0dd\uc131\uae30\ub85c <code>CachePolicy</code> \uac1d\uccb4\ub97c \uc778\uc2a4\ud134\uc2a4\ud654\ud55c \ub2e4\uc74c \ub178\ub4dc\uc640 \uc5f0\uacb0\ud569\ub2c8\ub2e4:</p> <p>\uadf8\ub7f0 \ub2e4\uc74c \uadf8\ub798\ud504\ub97c \ucef4\ud30c\uc77c\ud560 \ub54c <code>cache</code> \uc778\uc790\ub97c \uc124\uc815\ud558\uc5ec \uadf8\ub798\ud504\uc758 \ub178\ub4dc \uc218\uc900 \uce90\uc2f1\uc744 \ud65c\uc131\ud654\ud558\uc138\uc694. \uc544\ub798 \uc608\uc81c\uc5d0\uc11c\ub294 \uba54\ubaa8\ub9ac \ub0b4 \uce90\uc2dc\ub97c \uc124\uc815\ud558\uae30 \uc704\ud574 <code>InMemoryCache</code>\ub97c \uc0ac\uc6a9\ud558\uc9c0\ub9cc <code>SqliteCache</code>\ub3c4 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <pre><code>from langgraph.cache.memory import InMemoryCache\n\ngraph = builder.compile(cache=InMemoryCache())\n</code></pre>"}, {"location": "how-tos/graph-api/#_10", "title": "\ub2e8\uacc4 \uc2dc\ud000\uc2a4 \uc0dd\uc131", "text": "<p>\uc804\uc81c \uc870\uac74</p> <p>\uc774 \uac00\uc774\ub4dc\ub294 \uc704\uc758 \uc0c1\ud0dc \uc139\uc158\uc5d0 \uc775\uc219\ud558\ub2e4\uace0 \uac00\uc815\ud569\ub2c8\ub2e4.</p> <p>\uc5ec\uae30\uc11c\ub294 \uac04\ub2e8\ud55c \ub2e8\uacc4 \uc2dc\ud000\uc2a4\ub97c \uad6c\uc131\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\ub4dc\ub9ac\uaca0\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c\uc744 \uc124\uba85\ud558\uaca0\uc2b5\ub2c8\ub2e4:</p> <ol> <li>\uc2dc\ud000\uc15c \uadf8\ub798\ud504\ub97c \uad6c\ucd95\ud558\ub294 \ubc29\ubc95</li> <li>\uc720\uc0ac\ud55c \uadf8\ub798\ud504\ub97c \uad6c\uc131\ud558\ub294 \uae30\ubcf8 \uc81c\uacf5 \ub2e8\ucd95\ud0a4</li> </ol> <p>\ub178\ub4dc \uc2dc\ud000\uc2a4\ub97c \ucd94\uac00\ud558\ub824\uba74 \uadf8\ub798\ud504\uc758 <code>.add_node</code> \ubc0f <code>.add_edge</code> \uba54\uc11c\ub4dc\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4:</p> <p><sup>API \ucc38\uc870: START | StateGraph</sup></p> <p>\uae30\ubcf8 \uc81c\uacf5 \ub2e8\ucd95\ud0a4 <code>.add_sequence</code>\ub3c4 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>builder = StateGraph(State).add_sequence([step_1, step_2, step_3])\nbuilder.add_edge(START, \"step_1\")\n</code></pre> <p>LangGraph\ub85c \uc560\ud50c\ub9ac\ucf00\uc774\uc158 \ub2e8\uacc4\ub97c \uc2dc\ud000\uc2a4\ub85c \ubd84\ud560\ud558\ub294 \uc774\uc720\ub294 \ubb34\uc5c7\uc77c\uae4c\uc694?</p> <p>LangGraph\ub97c \uc0ac\uc6a9\ud558\uba74 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \uae30\ubcf8 \uc9c0\uc18d\uc131 \uacc4\uce35\uc744 \uc27d\uac8c \ucd94\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \ub178\ub4dc \uc2e4\ud589 \uc0ac\uc774\uc5d0 \uc0c1\ud0dc\ub97c \uccb4\ud06c\ud3ec\uc778\ud2b8\ud560 \uc218 \uc788\uc73c\ubbc0\ub85c LangGraph \ub178\ub4dc\uac00 \ub2e4\uc74c\uc744 \uad00\ub9ac\ud569\ub2c8\ub2e4:</p> <ul> <li>\uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8\uac00 \uccb4\ud06c\ud3ec\uc778\ud2b8\ub418\ub294 \ubc29\uc2dd</li> <li>\uc778\uac04-\ub8e8\ud504 \uc6cc\ud06c\ud50c\ub85c\uc5d0\uc11c \uc911\ub2e8\uc774 \uc7ac\uac1c\ub418\ub294 \ubc29\uc2dd</li> <li>LangGraph\uc758 \uc2dc\uac04 \uc5ec\ud589 \uae30\ub2a5\uc744 \uc0ac\uc6a9\ud558\uc5ec \uc2e4\ud589\uc744 \"\ub418\uac10\uace0\" \ubd84\uae30\ud558\ub294 \ubc29\uc2dd</li> </ul> <p>\ub610\ud55c \uc2e4\ud589 \ub2e8\uacc4\uac00 \uc2a4\ud2b8\ub9ac\ubc0d\ub418\ub294 \ubc29\uc2dd\uacfc LangGraph Studio\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc774 \uc2dc\uac01\ud654\ub418\uace0 \ub514\ubc84\uae45\ub418\ub294 \ubc29\uc2dd\uc744 \uacb0\uc815\ud569\ub2c8\ub2e4.</p> <p>\uc885\ub2e8\uac04 \uc608\uc81c\ub97c \ubcf4\uc5ec\ub4dc\ub9ac\uaca0\uc2b5\ub2c8\ub2e4. \uc138 \ub2e8\uacc4\uc758 \uc2dc\ud000\uc2a4\ub97c \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4:</p> <ol> <li>\uc0c1\ud0dc \ud0a4\uc758 \uac12\uc744 \ucc44\uc6b0\uae30</li> <li>\ub3d9\uc77c\ud55c \uac12 \uc5c5\ub370\uc774\ud2b8</li> <li>\ub2e4\ub978 \uac12 \ucc44\uc6b0\uae30</li> </ol> <p>\uba3c\uc800 \uc0c1\ud0dc\ub97c \uc815\uc758\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uadf8\ub798\ud504\uc758 \uc2a4\ud0a4\ub9c8\ub97c \uad00\ub9ac\ud558\uba70 \uc5c5\ub370\uc774\ud2b8\uac00 \uc5b4\ub5bb\uac8c \uc801\uc6a9\ub418\ub294\uc9c0 \uc9c0\uc815\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uc774 \uc139\uc158\uc744 \ucc38\uc870\ud558\uc138\uc694.</p> <p>\uc774 \uacbd\uc6b0 \ub450 \uac12\ub9cc \ucd94\uc801\ud558\uaca0\uc2b5\ub2c8\ub2e4:</p> <pre><code>from typing_extensions import TypedDict\n\nclass State(TypedDict):\n    value_1: str\n    value_2: int\n</code></pre> <p>\ub178\ub4dc\ub294 \uadf8\ub798\ud504\uc758 \uc0c1\ud0dc\ub97c \uc77d\uace0 \uc5c5\ub370\uc774\ud2b8\ud558\ub294 Python \ud568\uc218\uc785\ub2c8\ub2e4. \uc774 \ud568\uc218\uc758 \uccab \ubc88\uc9f8 \uc778\uc790\ub294 \ud56d\uc0c1 \uc0c1\ud0dc\uc785\ub2c8\ub2e4:</p> <pre><code>def step_1(state: State):\n    return {\"value_1\": \"a\"}\n\ndef step_2(state: State):\n    current_value_1 = state[\"value_1\"]\n    return {\"value_1\": f\"{current_value_1} b\"}\n\ndef step_3(state: State):\n    return {\"value_2\": 10}\n</code></pre> <p>\ucc38\uace0</p> <p>\uc0c1\ud0dc\ub97c \uc5c5\ub370\uc774\ud2b8\ud560 \ub54c \uac01 \ub178\ub4dc\ub294 \uc5c5\ub370\uc774\ud2b8\ud558\ub824\ub294 \ud0a4\uc758 \uac12\ub9cc \uc9c0\uc815\ud558\uba74 \ub429\ub2c8\ub2e4.</p> <p>\uae30\ubcf8\uc801\uc73c\ub85c \uc774\ub294 \ud574\ub2f9 \ud0a4\uc758 \uac12\uc744 \ub36e\uc5b4\uc501\ub2c8\ub2e4. \uc5c5\ub370\uc774\ud2b8\uac00 \ucc98\ub9ac\ub418\ub294 \ubc29\uc2dd\uc744 \uc81c\uc5b4\ud558\ub824\uba74 \ub9ac\ub4c0\uc11c\ub97c \uc0ac\uc6a9\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4 \ud0a4\uc5d0 \ub300\ud55c \uc5f0\uc18d \uc5c5\ub370\uc774\ud2b8\ub97c \ucd94\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uc774 \uc139\uc158\uc744 \ucc38\uc870\ud558\uc138\uc694.</p> <p>\ub9c8\uc9c0\ub9c9\uc73c\ub85c \uadf8\ub798\ud504\ub97c \uc815\uc758\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774 \uc0c1\ud0dc\uc5d0\uc11c \uc791\ub3d9\ud558\ub294 \uadf8\ub798\ud504\ub97c \uc815\uc758\ud558\ub824\uba74 StateGraph\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4.</p> <p>\uadf8\ub7f0 \ub2e4\uc74c add_node\uc640 add_edge\ub97c \uc0ac\uc6a9\ud558\uc5ec \uadf8\ub798\ud504\ub97c \ucc44\uc6b0\uace0 \uc81c\uc5b4 \ud750\ub984\uc744 \uc815\uc758\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <p><sup>API \ucc38\uc870: START | StateGraph</sup></p> <p>\uc0ac\uc6a9\uc790 \uc815\uc758 \uc774\ub984 \uc9c0\uc815</p> <p><code>.add_node</code>\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub178\ub4dc\uc5d0 \uc0ac\uc6a9\uc790 \uc815\uc758 \uc774\ub984\uc744 \uc9c0\uc815\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>builder.add_node(\"my_node\", step_1)\n</code></pre> <p>\ub2e4\uc74c \uc0ac\ud56d\uc5d0 \uc720\uc758\ud558\uc138\uc694:</p> <ul> <li><code>.add_edge</code>\ub294 \ud568\uc218\uc758 \uacbd\uc6b0 \uae30\ubcf8\uc801\uc73c\ub85c <code>node.__name__</code>\uc778 \ub178\ub4dc \uc774\ub984\uc744 \ubc1b\uc2b5\ub2c8\ub2e4.</li> <li>\uadf8\ub798\ud504\uc758 \uc9c4\uc785\uc810\uc744 \uc9c0\uc815\ud574\uc57c \ud569\ub2c8\ub2e4. \uc774\ub97c \uc704\ud574 START \ub178\ub4dc\uc640 \uc5e3\uc9c0\ub97c \ucd94\uac00\ud569\ub2c8\ub2e4.</li> <li>\ub354 \uc774\uc0c1 \uc2e4\ud589\ud560 \ub178\ub4dc\uac00 \uc5c6\uc73c\uba74 \uadf8\ub798\ud504\uac00 \uc911\ub2e8\ub429\ub2c8\ub2e4.</li> </ul> <p>\ub2e4\uc74c\uc73c\ub85c \uadf8\ub798\ud504\ub97c \ucef4\ud30c\uc77c\ud558\uaca0\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uadf8\ub798\ud504 \uad6c\uc870\uc5d0 \ub300\ud55c \uba87 \uac00\uc9c0 \uae30\ubcf8 \uac80\uc0ac\ub97c \uc81c\uacf5\ud569\ub2c8\ub2e4(\uc608: \uace0\uc544 \ub178\ub4dc \uc2dd\ubcc4). \uccb4\ud06c\ud3ec\uc778\ud130\ub97c \ud1b5\ud574 \uc560\ud50c\ub9ac\ucf00\uc774\uc158\uc5d0 \uc9c0\uc18d\uc131\uc744 \ucd94\uac00\ud558\ub294 \uacbd\uc6b0 \uc5ec\uae30\uc5d0\ub3c4 \uc804\ub2ec\ub429\ub2c8\ub2e4.</p> <pre><code>graph = builder.compile()\n</code></pre> <p>\uadf8\ub798\ud504\ub97c \uc2dc\uac01\ud654\ud558\ub294 \uae30\ubcf8 \uc81c\uacf5 \uc720\ud2f8\ub9ac\ud2f0\uac00 LangGraph\uc5d0 \uc788\uc2b5\ub2c8\ub2e4. \uc2dc\ud000\uc2a4\ub97c \uac80\uc0ac\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \uc2dc\uac01\ud654\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uc774 \uac00\uc774\ub4dc\ub97c \ucc38\uc870\ud558\uc138\uc694.</p> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p></p> <p>\uac04\ub2e8\ud55c \ud638\ucd9c\uc744 \uc9c4\ud589\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4:</p> <pre><code>graph.invoke({\"value_1\": \"c\"})\n</code></pre> <pre><code>{'value_1': 'a b', 'value_2': 10}\n</code></pre> <p>\ub2e4\uc74c \uc0ac\ud56d\uc5d0 \uc720\uc758\ud558\uc138\uc694:</p> <ul> <li>\ub2e8\uc77c \uc0c1\ud0dc \ud0a4\uc5d0 \ub300\ud55c \uac12\ub9cc \uc81c\uacf5\ud558\uc5ec \ud638\ucd9c\uc744 \uc2dc\uc791\ud588\uc2b5\ub2c8\ub2e4. \ucd5c\uc18c \ud558\ub098\uc758 \ud0a4\uc5d0 \ub300\ud55c \uac12\uc744 \ud56d\uc0c1 \uc81c\uacf5\ud574\uc57c \ud569\ub2c8\ub2e4.</li> <li>\uc804\ub2ec\ud55c \uac12\uc740 \uccab \ubc88\uc9f8 \ub178\ub4dc\uc5d0 \uc758\ud574 \ub36e\uc5b4\uc4f0\uc5ec\uc84c\uc2b5\ub2c8\ub2e4.</li> <li>\ub450 \ubc88\uc9f8 \ub178\ub4dc\uac00 \uac12\uc744 \uc5c5\ub370\uc774\ud2b8\ud588\uc2b5\ub2c8\ub2e4.</li> <li>\uc138 \ubc88\uc9f8 \ub178\ub4dc\uac00 \ub2e4\ub978 \uac12\uc744 \ucc44\uc6e0\uc2b5\ub2c8\ub2e4.</li> </ul> <p>\uae30\ubcf8 \uc81c\uacf5 \ub2e8\ucd95\ud0a4</p> <p><code>langgraph&gt;=0.2.46</code>\uc5d0\ub294 \ub178\ub4dc \uc2dc\ud000\uc2a4\ub97c \ucd94\uac00\ud558\uae30 \uc704\ud55c \uae30\ubcf8 \uc81c\uacf5 \ub2e8\ucd95\ud0a4 <code>add_sequence</code>\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c\uacfc \uac19\uc774 \ub3d9\uc77c\ud55c \uadf8\ub798\ud504\ub97c \ucef4\ud30c\uc77c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>builder = StateGraph(State).add_sequence([step_1, step_2, step_3])\n\ngraph = builder.compile()\n\ngraph.invoke({\"value_1\": \"c\"})\n</code></pre>"}, {"location": "how-tos/graph-api/#_11", "title": "\ube0c\ub79c\uce58 \uc0dd\uc131", "text": "<p>\ub178\ub4dc\ub97c \ubcd1\ub82c\ub85c \uc2e4\ud589\ud558\ub294 \uac83\uc740 \uc804\uccb4 \uadf8\ub798\ud504 \uc791\uc5c5 \uc18d\ub3c4\ub97c \ub192\uc774\ub294 \ub370 \ud544\uc218\uc801\uc785\ub2c8\ub2e4. LangGraph\ub294 \ud45c\uc900 \uc5e3\uc9c0\uc640 \uc870\uac74\ubd80 \uc5e3\uc9c0\ub97c \ubaa8\ub450 \ud65c\uc6a9\ud55c \ud32c\uc544\uc6c3 \ubc0f \ud32c\uc778 \uba54\ucee4\ub2c8\uc998\uc744 \ud1b5\ud574 \ub178\ub4dc\uc758 \ubcd1\ub82c \uc2e4\ud589\uc744 \uae30\ubcf8\uc801\uc73c\ub85c \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \uc774\ub97c \ud1b5\ud574 \uadf8\ub798\ud504 \uae30\ubc18 \uc6cc\ud06c\ud50c\ub85c\uc758 \uc131\ub2a5\uc744 \ud06c\uac8c \ud5a5\uc0c1\uc2dc\ud0ac \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ub2e4\uc74c\uc740 \uc0ac\uc6a9\uc790\uc5d0\uac8c \uc801\ud569\ud55c \ubd84\uae30 \ub370\uc774\ud130\ud50c\ub85c\ub97c \uc0dd\uc131\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud55c \uba87 \uac00\uc9c0 \uc608\uc81c\uc785\ub2c8\ub2e4.</p>"}, {"location": "how-tos/graph-api/#_12", "title": "\uadf8\ub798\ud504 \ub178\ub4dc\ub97c \ubcd1\ub82c\ub85c \uc2e4\ud589", "text": "<p>\uc774 \uc608\uc81c\uc5d0\uc11c\ub294 <code>Node A</code>\uc5d0\uc11c <code>B\uc640 C</code>\ub85c \ud32c\uc544\uc6c3\ud55c \ub2e4\uc74c <code>D</code>\ub85c \ud32c\uc778\ud569\ub2c8\ub2e4. \uc0c1\ud0dc\uc640 \ud568\uaed8 \ub9ac\ub4c0\uc11c add \uc791\uc5c5\uc744 \uc9c0\uc815\ud569\ub2c8\ub2e4. \uc774\ub294 \ub2e8\uc21c\ud788 \uae30\uc874 \uac12\uc744 \ub36e\uc5b4\uc4f0\uc9c0 \uc54a\uace0 \ud2b9\uc815 \ud0a4\uc758 \uac12\uc744 \uacb0\ud569\ud558\uac70\ub098 \ub204\uc801\ud569\ub2c8\ub2e4. \ubaa9\ub85d\uc758 \uacbd\uc6b0 \uc0c8 \ubaa9\ub85d\uc744 \uae30\uc874 \ubaa9\ub85d\uacfc \uc5f0\uacb0\ud569\ub2c8\ub2e4. \ub9ac\ub4c0\uc11c\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc0c1\ud0dc\ub97c \uc5c5\ub370\uc774\ud2b8\ud558\ub294 \ubc29\ubc95\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uc704\uc758 \uc0c1\ud0dc \ub9ac\ub4c0\uc11c \uc139\uc158\uc744 \ucc38\uc870\ud558\uc138\uc694.</p> <p><sup>API \ucc38\uc870: StateGraph | START | END</sup></p> <pre><code>import operator\nfrom typing import Annotated, Any\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # operator.add \ub9ac\ub4c0\uc11c \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uba74 \ucd94\uac00 \uc804\uc6a9\uc774 \ub429\ub2c8\ub2e4\n    aggregate: Annotated[list, operator.add]\n\ndef a(state: State):\n    print(f'\uc9d1\uacc4\uc5d0 \"A\" \ucd94\uac00 \uc911 {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"A\"]}\n\ndef b(state: State):\n    print(f'\uc9d1\uacc4\uc5d0 \"B\" \ucd94\uac00 \uc911 {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"B\"]}\n\ndef c(state: State):\n    print(f'\uc9d1\uacc4\uc5d0 \"C\" \ucd94\uac00 \uc911 {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"C\"]}\n\ndef d(state: State):\n    print(f'\uc9d1\uacc4\uc5d0 \"D\" \ucd94\uac00 \uc911 {state[\"aggregate\"]}')\n    return {\"aggregate\": [\"D\"]}\n\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\nbuilder.add_node(c)\nbuilder.add_node(d)\nbuilder.add_edge(START, \"a\")\nbuilder.add_edge(\"a\", \"b\")\nbuilder.add_edge(\"a\", \"c\")\nbuilder.add_edge(\"b\", \"d\")\nbuilder.add_edge(\"c\", \"d\")\nbuilder.add_edge(\"d\", END)\ngraph = builder.compile()\n</code></pre> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p></p> <p>\ub9ac\ub4c0\uc11c\ub97c \uc0ac\uc6a9\ud558\uba74 \uac01 \ub178\ub4dc\uc5d0\uc11c \ucd94\uac00\ub41c \uac12\uc774 \ub204\uc801\ub418\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <pre><code>graph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread_id\": \"foo\"}})\n</code></pre> <pre><code>\uc9d1\uacc4\uc5d0 \"A\" \ucd94\uac00 \uc911 []\n\uc9d1\uacc4\uc5d0 \"B\" \ucd94\uac00 \uc911 ['A']\n\uc9d1\uacc4\uc5d0 \"C\" \ucd94\uac00 \uc911 ['A']\n\uc9d1\uacc4\uc5d0 \"D\" \ucd94\uac00 \uc911 ['A', 'B', 'C']\n</code></pre> <p>\ucc38\uace0</p> <p>\uc704 \uc608\uc81c\uc5d0\uc11c \ub178\ub4dc <code>\"b\"</code>\uc640 <code>\"c\"</code>\ub294 \ub3d9\uc77c\ud55c \uc288\ud37c\uc2a4\ud15d\uc5d0\uc11c \ub3d9\uc2dc\uc5d0 \uc2e4\ud589\ub429\ub2c8\ub2e4. \ub3d9\uc77c\ud55c \ub2e8\uacc4\uc5d0 \uc788\uae30 \ub54c\ubb38\uc5d0 \ub178\ub4dc <code>\"d\"</code>\ub294 <code>\"b\"</code>\uc640 <code>\"c\"</code>\uac00 \ubaa8\ub450 \uc644\ub8cc\ub41c \ud6c4\uc5d0 \uc2e4\ud589\ub429\ub2c8\ub2e4.</p> <p>\uc911\uc694\ud558\uac8c\ub3c4 \ubcd1\ub82c \uc288\ud37c\uc2a4\ud15d\uc758 \uc5c5\ub370\uc774\ud2b8\ub294 \uc77c\uad00\ub418\uac8c \uc815\ub82c\ub418\uc9c0 \uc54a\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \ubcd1\ub82c \uc288\ud37c\uc2a4\ud15d\uc5d0\uc11c \uc5c5\ub370\uc774\ud2b8\uc758 \uc77c\uad00\ub418\uace0 \ubbf8\ub9ac \uacb0\uc815\ub41c \uc21c\uc11c\uac00 \ud544\uc694\ud55c \uacbd\uc6b0 \ubcc4\ub3c4\uc758 \ud544\ub4dc\uc5d0 \ucd9c\ub825\uacfc \ud568\uaed8 \uc21c\uc11c\ub97c \uc9c0\uc815\ud558\ub294 \uac12\uc744 \uc791\uc131\ud574\uc57c \ud569\ub2c8\ub2e4.</p> <p>\uc608\uc678 \ucc98\ub9ac?</p> <p>LangGraph\ub294 \uc288\ud37c\uc2a4\ud15d \ub0b4\uc5d0\uc11c \ub178\ub4dc\ub97c \uc2e4\ud589\ud569\ub2c8\ub2e4. \uc989, \ubcd1\ub82c \ube0c\ub79c\uce58\uac00 \ubcd1\ub82c\ub85c \uc2e4\ud589\ub418\uc9c0\ub9cc \uc804\uccb4 \uc288\ud37c\uc2a4\ud15d\uc740 **\ud2b8\ub79c\uc7ad\uc158**\uc785\ub2c8\ub2e4. \uc774\ub7ec\ud55c \ube0c\ub79c\uce58 \uc911 \ud558\ub098\uac00 \uc608\uc678\ub97c \ubc1c\uc0dd\uc2dc\ud0a4\uba74 \uc5c5\ub370\uc774\ud2b8\uac00 \uc0c1\ud0dc\uc5d0 \uc801\uc6a9\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4(\uc804\uccb4 \uc288\ud37c\uc2a4\ud15d\uc774 \uc624\ub958 \ubc1c\uc0dd).</p> <p>\uc911\uc694\ud558\uac8c\ub294 \uccb4\ud06c\ud3ec\uc778\ud130\ub97c \uc0ac\uc6a9\ud560 \ub54c \uc288\ud37c\uc2a4\ud15d \ub0b4 \uc131\uacf5\ud55c \ub178\ub4dc\uc758 \uacb0\uacfc\uac00 \uc800\uc7a5\ub418\uba70 \uc7ac\uac1c \uc2dc \ubc18\ubcf5\ub418\uc9c0 \uc54a\uc2b5\ub2c8\ub2e4.</p> <p>\uc624\ub958\uac00 \ubc1c\uc0dd\ud558\uae30 \uc26c\uc6b4 \uacbd\uc6b0(\uc608: \ubd88\uc548\uc815\ud55c API \ud638\ucd9c\uc744 \ucc98\ub9ac\ud558\ub824\ub294 \uacbd\uc6b0) LangGraph\ub294 \uc774\ub97c \ud574\uacb0\ud558\ub294 \ub450 \uac00\uc9c0 \ubc29\ubc95\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4:</p> <ol> <li>\ub178\ub4dc \ub0b4\uc5d0\uc11c \uc77c\ubc18 Python \ucf54\ub4dc\ub97c \uc791\uc131\ud558\uc5ec \uc608\uc678\ub97c \ud3ec\ucc29\ud558\uace0 \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</li> <li>**\uc7ac\uc2dc\ub3c4 \uc815\ucc45**\uc744 \uc124\uc815\ud558\uc5ec \uadf8\ub798\ud504\uac00 \ud2b9\uc815 \uc720\ud615\uc758 \uc608\uc678\ub97c \ubc1c\uc0dd\uc2dc\ud0a4\ub294 \ub178\ub4dc\ub97c \uc7ac\uc2dc\ub3c4\ud558\ub3c4\ub85d \uc9c0\uc2dc\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc2e4\ud328\ud55c \ube0c\ub79c\uce58\ub9cc \uc7ac\uc2dc\ub3c4\ub418\ubbc0\ub85c \uc911\ubcf5 \uc791\uc5c5\uc744 \uc218\ud589\ud560 \ud544\uc694\uac00 \uc5c6\uc2b5\ub2c8\ub2e4.</li> </ol> <p>\uc774\ub97c \ud1b5\ud574 \ubcd1\ub82c \uc2e4\ud589\uc744 \uc218\ud589\ud558\uace0 \uc608\uc678 \ucc98\ub9ac\ub97c \uc644\uc804\ud788 \uc81c\uc5b4\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p>"}, {"location": "how-tos/graph-api/#-send-api", "title": "\ub9f5-\ub9ac\ub4c0\uc2a4 \ubc0f Send API", "text": "<p>LangGraph\ub294 Send API\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub9f5-\ub9ac\ub4c0\uc2a4 \ubc0f \uae30\ud0c0 \uace0\uae09 \ube0c\ub79c\uce58 \ud328\ud134\uc744 \uc9c0\uc6d0\ud569\ub2c8\ub2e4. \uc0ac\uc6a9\ud558\ub294 \ubc29\ubc95\uc758 \uc608\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4:</p> <p><sup>API \ucc38\uc870: StateGraph | START | END | Send</sup></p> <pre><code>from langgraph.graph import StateGraph, START, END\nfrom langgraph.types import Send\nfrom typing_extensions import TypedDict, Annotated\nimport operator\n\nclass OverallState(TypedDict):\n    topic: str\n    subjects: list[str]\n    jokes: Annotated[list[str], operator.add]\n    best_selected_joke: str\n\ndef generate_topics(state: OverallState):\n    return {\"subjects\": [\"lions\", \"elephants\", \"penguins\"]}\n\ndef generate_joke(state: OverallState):\n    joke_map = {\n        \"lions\": \"Why don't lions like fast food? Because they can't catch it!\",\n        \"elephants\": \"Why don't elephants use computers? They're afraid of the mouse!\",\n        \"penguins\": \"Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice.\"\n    }\n    return {\"jokes\": [joke_map[state[\"subject\"]]]}\n\ndef continue_to_jokes(state: OverallState):\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n\ndef best_joke(state: OverallState):\n    return {\"best_selected_joke\": \"penguins\"}\n\nbuilder = StateGraph(OverallState)\nbuilder.add_node(\"generate_topics\", generate_topics)\nbuilder.add_node(\"generate_joke\", generate_joke)\nbuilder.add_node(\"best_joke\", best_joke)\nbuilder.add_edge(START, \"generate_topics\")\nbuilder.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\nbuilder.add_edge(\"generate_joke\", \"best_joke\")\nbuilder.add_edge(\"best_joke\", END)\ngraph = builder.compile()\n</code></pre> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p></p> <pre><code># \uadf8\ub798\ud504 \ud638\ucd9c: \uc5ec\uae30\uc11c\ub294 \ub18d\ub2f4 \ubaa9\ub85d\uc744 \uc0dd\uc131\ud558\ub3c4\ub85d \ud638\ucd9c\ud569\ub2c8\ub2e4\nfor step in graph.stream({\"topic\": \"animals\"}):\n    print(step)\n</code></pre> <pre><code>{'generate_topics': {'subjects': ['lions', 'elephants', 'penguins']}}\n{'generate_joke': {'jokes': [\"Why don't lions like fast food? Because they can't catch it!\"]}}\n{'generate_joke': {'jokes': [\"Why don't elephants use computers? They're afraid of the mouse!\"]}}\n{'generate_joke': {'jokes': ['Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice.']}}\n{'best_joke': {'best_selected_joke': 'penguins'}}\n</code></pre>"}, {"location": "how-tos/graph-api/#_13", "title": "\ub8e8\ud504 \uc0dd\uc131 \ubc0f \uc81c\uc5b4", "text": "<p>\ub8e8\ud504\uac00 \uc788\ub294 \uadf8\ub798\ud504\ub97c \uc0dd\uc131\ud560 \ub54c \uc2e4\ud589\uc744 \uc885\ub8cc\ud558\ub294 \uba54\ucee4\ub2c8\uc998\uc774 \ud544\uc694\ud569\ub2c8\ub2e4. \uc774\ub294 \uc77c\ubc18\uc801\uc73c\ub85c \uc885\ub8cc \uc870\uac74\uc5d0 \ub3c4\ub2ec\ud558\uba74 END \ub178\ub4dc\ub85c \ub77c\uc6b0\ud305\ud558\ub294 \uc870\uac74\ubd80 \uc5e3\uc9c0\ub97c \ucd94\uac00\ud558\uc5ec \uc218\ud589\ub429\ub2c8\ub2e4.</p> <p>\uadf8\ub798\ud504 \ud638\ucd9c \ub610\ub294 \uc2a4\ud2b8\ub9ac\ubc0d \uc2dc \uadf8\ub798\ud504 \uc7ac\uadc0 \uc81c\ud55c\uc744 \uc124\uc815\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4. \uc7ac\uadc0 \uc81c\ud55c\uc740 \uc624\ub958\ub97c \ubc1c\uc0dd\uc2dc\ud0a4\uae30 \uc804\uc5d0 \uadf8\ub798\ud504\uac00 \uc2e4\ud589\ud560 \uc218 \uc788\ub294 \uc288\ud37c\uc2a4\ud15d \uc218\ub97c \uc124\uc815\ud569\ub2c8\ub2e4. \uc7ac\uadc0 \uc81c\ud55c \uac1c\ub150\uc5d0 \ub300\ud55c \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 \uc5ec\uae30\ub97c \ucc38\uc870\ud558\uc138\uc694.</p> <p>\uac04\ub2e8\ud55c \ub8e8\ud504\uac00 \uc788\ub294 \uadf8\ub798\ud504\ub97c \ud1b5\ud574 \uc774\ub7ec\ud55c \uba54\ucee4\ub2c8\uc998\uc774 \uc5b4\ub5bb\uac8c \uc791\ub3d9\ud558\ub294\uc9c0 \ub354 \uc798 \uc774\ud574\ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\ud301</p> <p>\uc7ac\uadc0 \uc81c\ud55c \uc624\ub958 \ub300\uc2e0 \uc0c1\ud0dc\uc758 \ub9c8\uc9c0\ub9c9 \uac12\uc744 \ubc18\ud658\ud558\ub824\uba74 \ub2e4\uc74c \uc139\uc158\uc744 \ucc38\uc870\ud558\uc138\uc694.</p> <p>\ub8e8\ud504\ub97c \uc0dd\uc131\ud560 \ub54c \uc885\ub8cc \uc870\uac74\uc744 \uc9c0\uc815\ud558\ub294 \uc870\uac74\ubd80 \uc5e3\uc9c0\ub97c \ud3ec\ud568\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>builder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\n\ndef route(state: State) -&gt; Literal[\"b\", END]:\n    if termination_condition(state):\n        return END\n    else:\n        return \"b\"\n\nbuilder.add_edge(START, \"a\")\nbuilder.add_conditional_edges(\"a\", route)\nbuilder.add_edge(\"b\", \"a\")\ngraph = builder.compile()\n</code></pre> <p>\uc7ac\uadc0 \uc81c\ud55c\uc744 \uc81c\uc5b4\ud558\ub824\uba74 \uad6c\uc131\uc5d0\uc11c <code>\"recursionLimit\"</code>\uc744 \uc9c0\uc815\ud558\uc138\uc694. \uc774\ub294 \uc9c0\uc815\ub41c \uc218\uc758 \uc288\ud37c\uc2a4\ud15d \ud6c4\uc5d0 <code>GraphRecursionError</code>\ub97c \ubc1c\uc0dd\uc2dc\ud0b5\ub2c8\ub2e4. \uadf8\ub7f0 \ub2e4\uc74c \uc774\ub97c \ud3ec\ucc29\ud558\uace0 \ucc98\ub9ac\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <pre><code>from langgraph.errors import GraphRecursionError\n\ntry:\n    graph.invoke(inputs, {\"recursion_limit\": 3})\nexcept GraphRecursionError:\n    print(\"\uc7ac\uadc0 \uc624\ub958\")\n</code></pre> <p>\uc885\ub8cc \uc870\uac74\uc744 \uc9c0\uc815\ud558\ub294 \uc870\uac74\ubd80 \uc5e3\uc9c0\ub97c \uc0ac\uc6a9\ud558\uc5ec \uac04\ub2e8\ud55c \ub8e8\ud504\uac00 \uc788\ub294 \uadf8\ub798\ud504\ub97c \uc815\uc758\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <p><sup>API \ucc38\uc870: StateGraph | START | END</sup></p> <pre><code>import operator\nfrom typing import Annotated, Literal\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    # operator.add \ub9ac\ub4c0\uc11c \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uba74 \ucd94\uac00 \uc804\uc6a9\uc774 \ub429\ub2c8\ub2e4\n    aggregate: Annotated[list, operator.add]\n\ndef a(state: State):\n    print(f'\ub178\ub4dc A\uac00 {state[\"aggregate\"]}\ub97c \ud655\uc778\ud568')\n    return {\"aggregate\": [\"A\"]}\n\ndef b(state: State):\n    print(f'\ub178\ub4dc B\uac00 {state[\"aggregate\"]}\ub97c \ud655\uc778\ud568')\n    return {\"aggregate\": [\"B\"]}\n\n# \ub178\ub4dc \uc815\uc758\nbuilder = StateGraph(State)\nbuilder.add_node(a)\nbuilder.add_node(b)\n\n# \uc5e3\uc9c0 \uc815\uc758\ndef route(state: State) -&gt; Literal[\"b\", END]:\n    if len(state[\"aggregate\"]) &lt; 7:\n        return \"b\"\n    else:\n        return END\n\nbuilder.add_edge(START, \"a\")\nbuilder.add_conditional_edges(\"a\", route)\nbuilder.add_edge(\"b\", \"a\")\ngraph = builder.compile()\n</code></pre> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p></p> <p>\uc774 \uc544\ud0a4\ud14d\ucc98\ub294 \ub178\ub4dc <code>\"a\"</code>\uac00 \ub3c4\uad6c \ud638\ucd9c \ubaa8\ub378\uc774\uace0 \ub178\ub4dc <code>\"b\"</code>\uac00 \ub3c4\uad6c\ub97c \ub098\ud0c0\ub0b4\ub294 React \uc5d0\uc774\uc804\ud2b8\uc640 \uc720\uc0ac\ud569\ub2c8\ub2e4.</p> <p><code>route</code> \uc870\uac74\ubd80 \uc5e3\uc9c0\uc5d0\uc11c \uc0c1\ud0dc\uc758 <code>\"aggregate\"</code> \ubaa9\ub85d\uc774 \uc784\uacc4\uac12 \uae38\uc774\ub97c \ucd08\uacfc\ud558\uba74 \uc885\ub8cc\ud558\ub3c4\ub85d \uc9c0\uc815\ud569\ub2c8\ub2e4.</p> <p>\uadf8\ub798\ud504\ub97c \ud638\ucd9c\ud558\uba74 \uc885\ub8cc \uc870\uac74\uc5d0 \ub3c4\ub2ec\ud560 \ub54c\uae4c\uc9c0 \ub178\ub4dc <code>\"a\"</code>\uc640 <code>\"b\"</code>\ub97c \ubc88\uac08\uc544 \uc2e4\ud589\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <pre><code>graph.invoke({\"aggregate\": []})\n</code></pre> <pre><code>\ub178\ub4dc A\uac00 []\ub97c \ud655\uc778\ud568\n\ub178\ub4dc B\uac00 ['A']\ub97c \ud655\uc778\ud568\n\ub178\ub4dc A\uac00 ['A', 'B']\ub97c \ud655\uc778\ud568\n\ub178\ub4dc B\uac00 ['A', 'B', 'A']\ub97c \ud655\uc778\ud568\n\ub178\ub4dc A\uac00 ['A', 'B', 'A', 'B']\ub97c \ud655\uc778\ud568\n\ub178\ub4dc B\uac00 ['A', 'B', 'A', 'B', 'A']\ub97c \ud655\uc778\ud568\n\ub178\ub4dc A\uac00 ['A', 'B', 'A', 'B', 'A', 'B']\ub97c \ud655\uc778\ud568\n</code></pre>"}, {"location": "how-tos/graph-api/#_14", "title": "\ube44\ub3d9\uae30", "text": "<p>\ube44\ub3d9\uae30 \ud504\ub85c\uadf8\ub798\ubc0d \ud328\ub7ec\ub2e4\uc784\uc744 \uc0ac\uc6a9\ud558\uba74 IO \ubc14\uc6b4\ub4dc \ucf54\ub4dc(\uc608: \ucc44\ud305 \ubaa8\ub378 \uc81c\uacf5\uc5c5\uccb4\uc5d0 \ub300\ud55c \ub3d9\uc2dc API \uc694\uccad)\ub97c \ub3d9\uc2dc\uc5d0 \uc2e4\ud589\ud560 \ub54c \uc0c1\ub2f9\ud55c \uc131\ub2a5 \ud5a5\uc0c1\uc744 \uc5bb\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <p><code>sync</code> \uad6c\ud604\uc744 <code>async</code> \uad6c\ud604\uc73c\ub85c \ubcc0\ud658\ud558\ub824\uba74:</p> <ol> <li>\ub178\ub4dc\uc5d0\uc11c <code>async def</code> \ub300\uc2e0 <code>def</code>\ub97c \uc0ac\uc6a9\ud558\ub3c4\ub85d \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4.</li> <li>\uc801\uc808\ud788 <code>await</code>\ub97c \uc0ac\uc6a9\ud558\ub3c4\ub85d \ub0b4\ubd80 \ucf54\ub4dc \uc5c5\ub370\uc774\ud2b8\ud569\ub2c8\ub2e4.</li> <li><code>.ainvoke</code> \ub610\ub294 <code>.astream</code>\uc73c\ub85c \uadf8\ub798\ud504\ub97c \ud638\ucd9c\ud569\ub2c8\ub2e4.</li> </ol> <p>\ub9ce\uc740 LangChain \uac1d\uccb4\uac00 \ubaa8\ub4e0 <code>sync</code> \uba54\uc11c\ub4dc\uc758 <code>async</code> \ubcc0\ud615\uc744 \ud3ec\ud568\ud558\ub294 Runnable Protocol\uc744 \uad6c\ud604\ud558\ubbc0\ub85c \uc77c\ubc18\uc801\uc73c\ub85c <code>sync</code> \uadf8\ub798\ud504\ub97c <code>async</code> \uadf8\ub798\ud504\ub85c \uc5c5\uadf8\ub808\uc774\ub4dc\ud558\ub294 \ub370 \uc0c1\ub2f9\ud788 \ube60\ub985\ub2c8\ub2e4.</p> <p>\uc544\ub798 \uc608\uc81c\ub97c \ucc38\uc870\ud558\uc138\uc694. \uae30\ubcf8 LLM\uc758 \ube44\ub3d9\uae30 \ud638\ucd9c\uc744 \uc2dc\uc5f0\ud558\uae30 \uc704\ud574 \ucc44\ud305 \ubaa8\ub378\uc744 \ud3ec\ud568\ud558\uaca0\uc2b5\ub2c8\ub2e4:</p> <p><sup>API \ucc38\uc870: init_chat_model | StateGraph</sup></p> <pre><code>from langchain.chat_models import init_chat_model\nfrom langgraph.graph import MessagesState, StateGraph\n\nasync def node(state: MessagesState): \n    new_message = await llm.ainvoke(state[\"messages\"]) \n    return {\"messages\": [new_message]}\n\nbuilder = StateGraph(MessagesState).add_node(node).set_entry_point(\"node\")\ngraph = builder.compile()\n\ninput_message = {\"role\": \"user\", \"content\": \"Hello\"}\nresult = await graph.ainvoke({\"messages\": [input_message]})\n</code></pre> <p>\ube44\ub3d9\uae30 \uc2a4\ud2b8\ub9ac\ubc0d</p> <p>\ube44\ub3d9\uae30\ub85c \uc2a4\ud2b8\ub9ac\ubc0d\ud558\ub294 \uc608\uc81c\ub294 \uc2a4\ud2b8\ub9ac\ubc0d \uac00\uc774\ub4dc\ub97c \ucc38\uc870\ud558\uc138\uc694.</p>"}, {"location": "how-tos/graph-api/#command", "title": "Command\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc81c\uc5b4 \ud750\ub984\uacfc \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8 \uacb0\ud569", "text": "<p>\uc81c\uc5b4 \ud750\ub984(\uc5e3\uc9c0)\uacfc \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8(\ub178\ub4dc)\ub97c \uacb0\ud569\ud558\ub294 \uac83\uc774 \uc720\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8\ub97c \uc218\ud589\ud558\uace0 \ub3d9\uc2dc\uc5d0 \ub2e4\uc74c\uc5d0 \uc774\ub3d9\ud560 \ub178\ub4dc\ub97c \uacb0\uc815\ud558\ub294 \ub178\ub4dc\uc5d0\uc11c \ubaa8\ub450 \uc218\ud589\ud558\uace0 \uc2f6\uc744 \uc218 \uc788\uc2b5\ub2c8\ub2e4. LangGraph\ub294 \ub178\ub4dc \ud568\uc218\uc5d0\uc11c Command \uac1d\uccb4\ub97c \ubc18\ud658\ud558\uc5ec \uc774\ub97c \uc218\ud589\ud558\ub294 \ubc29\ubc95\uc744 \uc81c\uacf5\ud569\ub2c8\ub2e4:</p> <pre><code>def my_node(state: State) -&gt; Command[Literal[\"my_other_node\"]]:\n    return Command(\n        # \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8\n        update={\"foo\": \"bar\"},\n        # \uc81c\uc5b4 \ud750\ub984\n        goto=\"my_other_node\"\n    )\n</code></pre> <p>\uc544\ub798\uc5d0\uc11c \uc885\ub2e8\uac04 \uc608\uc81c\ub97c \ubcf4\uc5ec\ub4dc\ub9ac\uaca0\uc2b5\ub2c8\ub2e4. 3\uac1c\uc758 \ub178\ub4dc A, B, C\uac00 \uc788\ub294 \uac04\ub2e8\ud55c \uadf8\ub798\ud504\ub97c \ub9cc\ub4e4\uaca0\uc2b5\ub2c8\ub2e4. \uba3c\uc800 \ub178\ub4dc A\ub97c \uc2e4\ud589\ud55c \ub2e4\uc74c \ub178\ub4dc A\uc758 \ucd9c\ub825\uc5d0 \ub530\ub77c \ub2e4\uc74c\uc5d0 \ub178\ub4dc B \ub610\ub294 \ub178\ub4dc C\ub85c \uc774\ub3d9\ud560\uc9c0 \uacb0\uc815\ud558\uaca0\uc2b5\ub2c8\ub2e4.</p> <p><sup>API \ucc38\uc870: StateGraph | START | Command</sup></p> <pre><code>import random\nfrom typing_extensions import TypedDict, Literal\nfrom langgraph.graph import StateGraph, START\nfrom langgraph.types import Command\n\n# \uadf8\ub798\ud504 \uc0c1\ud0dc \uc815\uc758\nclass State(TypedDict):\n    foo: str\n\n# \ub178\ub4dc \uc815\uc758\n\ndef node_a(state: State) -&gt; Command[Literal[\"node_b\", \"node_c\"]]:\n    print(\"A \ud638\ucd9c\ub428\")\n    value = random.choice([\"b\", \"c\"])\n    # \uc774\ub294 \uc870\uac74\ubd80 \uc5e3\uc9c0 \ud568\uc218\uc758 \ub300\uccb4\uc785\ub2c8\ub2e4\n    if value == \"b\":\n        goto = \"node_b\"\n    else:\n        goto = \"node_c\"\n\n    # Command\ub97c \uc0ac\uc6a9\ud558\uba74 \uadf8\ub798\ud504 \uc0c1\ud0dc\ub97c \uc5c5\ub370\uc774\ud2b8\ud558\uace0 \ub2e4\uc74c \ub178\ub4dc\ub85c \ub77c\uc6b0\ud305\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4\n    return Command(\n        # \uc774\ub294 \uc0c1\ud0dc \uc5c5\ub370\uc774\ud2b8\uc785\ub2c8\ub2e4\n        update={\"foo\": value},\n        # \uc774\ub294 \uc5e3\uc9c0\uc758 \ub300\uccb4\uc785\ub2c8\ub2e4\n        goto=goto,\n    )\n\ndef node_b(state: State):\n    print(\"B \ud638\ucd9c\ub428\")\n    return {\"foo\": state[\"foo\"] + \"b\"}\n\ndef node_c(state: State):\n    print(\"C \ud638\ucd9c\ub428\")\n    return {\"foo\": state[\"foo\"] + \"c\"}\n</code></pre> <p>\uc774\uc81c \uc704 \ub178\ub4dc\ub85c <code>StateGraph</code>\ub97c \uc0dd\uc131\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub798\ud504\uc5d0 \uc870\uac74\ubd80 \uc5e3\uc9c0\uac00 \uc5c6\ub2e4\ub294 \uc810\uc5d0 \uc720\uc758\ud558\uc138\uc694! \uc774\ub294 \uc81c\uc5b4 \ud750\ub984\uc774 <code>node_a</code> \ub0b4\ubd80\uc758 <code>Command</code>\ub85c \uc815\uc758\ub418\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4.</p> <pre><code>builder = StateGraph(State)\nbuilder.add_edge(START, \"node_a\")\nbuilder.add_node(node_a)\nbuilder.add_node(node_b)\nbuilder.add_node(node_c)\n# \ucc38\uace0: \ub178\ub4dc A, B, C \uac04\uc5d0\ub294 \uc5e3\uc9c0\uac00 \uc5c6\uc2b5\ub2c8\ub2e4!\n\ngraph = builder.compile()\n</code></pre> <p>\uc911\uc694</p> <p><code>Command[Literal[\"node_b\", \"node_c\"]]</code>\ucc98\ub7fc <code>Command</code>\ub97c \ubc18\ud658 \ud0c0\uc785 \uc8fc\uc11d\uc73c\ub85c \uc0ac\uc6a9\ud588\ub2e4\ub294 \uc810\uc5d0 \uc720\uc758\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ub294 \uadf8\ub798\ud504 \ub80c\ub354\ub9c1\uc5d0 \ud544\uc694\ud558\uba70 <code>node_a</code>\uac00 <code>node_b</code>\uc640 <code>node_c</code>\ub85c \uc774\ub3d9\ud560 \uc218 \uc788\uc74c\uc744 LangGraph\uc5d0 \uc54c\ub824\uc90d\ub2c8\ub2e4.</p> <pre><code>from IPython.display import display, Image\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p></p> <p>\uadf8\ub798\ud504\ub97c \uc5ec\ub7ec \ubc88 \uc2e4\ud589\ud558\uba74 \ub178\ub4dc A\uc758 \ubb34\uc791\uc704 \uc120\ud0dd\uc5d0 \ub530\ub77c \ub2e4\ub978 \uacbd\ub85c(A -&gt; B \ub610\ub294 A -&gt; C)\ub97c \ucde8\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.</p> <pre><code>graph.invoke({\"foo\": \"\"})\n</code></pre> <pre><code>A \ud638\ucd9c\ub428\nC \ud638\ucd9c\ub428\n</code></pre>"}, {"location": "how-tos/graph-api/#_15", "title": "\uadf8\ub798\ud504 \uc2dc\uac01\ud654", "text": "<p>\uc0dd\uc131\ud55c \uadf8\ub798\ud504\ub97c \uc2dc\uac01\ud654\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\ub4dc\ub9ac\uaca0\uc2b5\ub2c8\ub2e4.</p> <p>\uc784\uc758\uc758 Graph\ub97c \uc2dc\uac01\ud654\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4( StateGraph \ud3ec\ud568).</p> <p>\ud504\ub799\ud0c8\uc744 \uadf8\ub824\uc11c \uc7ac\ubbf8\uc788\uac8c \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4 :).</p> <p><sup>API \ucc38\uc870: StateGraph | START | END | add_messages</sup></p> <pre><code>import random\nfrom typing import Annotated, Literal\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\nclass MyNode:\n    def __init__(self, name: str):\n        self.name = name\n    def __call__(self, state: State):\n        return {\"messages\": [(\"assistant\", f\"\ub178\ub4dc {self.name} \ud638\ucd9c\ub428\")]}\n\ndef route(state) -&gt; Literal[\"entry_node\", \"__end__\"]:\n    if len(state[\"messages\"]) &gt; 10:\n        return \"__end__\"\n    return \"entry_node\"\n\ndef add_fractal_nodes(builder, current_node, level, max_level):\n    if level &gt; max_level:\n        return\n    # \uc774 \uc218\uc900\uc5d0\uc11c \uc0dd\uc131\ud560 \ub178\ub4dc \uc218\n    num_nodes = random.randint(1, 3)  # \ud544\uc694\uc5d0 \ub530\ub77c \ubb34\uc791\uc704\uc131 \uc870\uc815\n    for i in range(num_nodes):\n        nm = [\"A\", \"B\", \"C\"][i]\n        node_name = f\"node_{current_node}_{nm}\"\n        builder.add_node(node_name, MyNode(node_name))\n        builder.add_edge(current_node, node_name)\n        # \ub354 \ub9ce\uc740 \ub178\ub4dc\ub97c \uc7ac\uadc0\uc801\uc73c\ub85c \ucd94\uac00\n        r = random.random()\n        if r &gt; 0.2 and level + 1 &lt; max_level:\n            add_fractal_nodes(builder, node_name, level + 1, max_level)\n        elif r &gt; 0.05:\n            builder.add_conditional_edges(node_name, route, node_name)\n        else:\n            # \uc885\ub8cc\n            builder.add_edge(node_name, \"__end__\")\n\ndef build_fractal_graph(max_level: int):\n    builder = StateGraph(State)\n    entry_point = \"entry_node\"\n    builder.add_node(entry_point, MyNode(entry_point))\n    builder.add_edge(START, entry_point)\n    add_fractal_nodes(builder, entry_point, 1, max_level)\n    # \ud544\uc694\uc2dc \uc644\ub8cc \uc9c0\uc810 \uc124\uc815\n    builder.add_edge(entry_point, END)  # \ub610\ub294 \ud2b9\uc815 \ub178\ub4dc\n    return builder.compile()\n\napp = build_fractal_graph(3)\n</code></pre>"}, {"location": "how-tos/graph-api/#mermaid", "title": "Mermaid", "text": "<p>\uadf8\ub798\ud504 \ud074\ub798\uc2a4\ub97c Mermaid \uad6c\ubb38\uc73c\ub85c \ubcc0\ud658\ud560 \uc218\ub3c4 \uc788\uc2b5\ub2c8\ub2e4.</p> <pre><code>print(app.get_graph().draw_mermaid())\n</code></pre>"}, {"location": "how-tos/graph-api/#png", "title": "PNG", "text": "<p>\uc6d0\ud558\ub294 \uacbd\uc6b0 \uadf8\ub798\ud504\ub97c <code>.png</code>\ub85c \ub80c\ub354\ub9c1\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc5ec\uae30\uc11c\ub294 \uc138 \uac00\uc9c0 \uc635\uc158\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4:</p> <ul> <li>Mermaid.ink API \uc0ac\uc6a9(\ucd94\uac00 \ud328\ud0a4\uc9c0 \ubd88\ud544\uc694)</li> <li>Mermaid + Pyppeteer \uc0ac\uc6a9(<code>pip install pyppeteer</code> \ud544\uc694)</li> <li>graphviz \uc0ac\uc6a9(<code>pip install graphviz</code> \ud544\uc694)</li> </ul> <p>Mermaid.Ink \uc0ac\uc6a9</p> <p>\uae30\ubcf8\uc801\uc73c\ub85c <code>draw_mermaid_png()</code>\ub294 Mermaid.Ink\uc758 API\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc774\uc5b4\uadf8\ub7a8\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.</p> <p><sup>API \ucc38\uc870: CurveStyle | MermaidDrawMethod | NodeStyles</sup></p> <pre><code>from IPython.display import Image, display\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n\ndisplay(Image(app.get_graph().draw_mermaid_png()))\n</code></pre> <p></p> <p>Mermaid + Pyppeteer \uc0ac\uc6a9</p> <p>Graphviz \uc0ac\uc6a9</p> <p>```python try:     display(Image(app.get_graph().draw_png())) except ImportError:     print(         \"pygraphviz\uc758 \uc885\uc18d\uc131\uc744 \uc124\uce58\ud574\uc57c \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc790\uc138\ud55c \ub0b4\uc6a9\uc740 https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\ub97c \ucc38\uc870\ud558\uc138\uc694\"     )</p>"}, {"location": "how-tos/multi_agent/", "title": "Build multi-agent systems", "text": "<p>A single agent might struggle if it needs to specialize in multiple domains or manage many tools. To tackle this, you can break your agent into smaller, independent agents and composing them into a multi-agent system.</p> <p>In multi-agent systems, agents need to communicate between each other. They do so via handoffs \u2014 a primitive that describes which agent to hand control to and the payload to send to that agent.</p> <p>This guide covers the following:</p> <ul> <li>implementing handoffs between agents</li> <li>using handoffs and the prebuilt agent to build a custom multi-agent system</li> </ul> <p>To get started with building multi-agent systems, check out LangGraph prebuilt implementations of two of the most popular multi-agent architectures \u2014 supervisor and swarm.</p>"}, {"location": "how-tos/multi_agent/#handoffs", "title": "Handoffs", "text": "<p>To set up communication between the agents in a multi-agent system you can use handoffs \u2014 a pattern where one agent hands off control to another. Handoffs allow you to specify:</p> <ul> <li>destination: target agent to navigate to (e.g., name of the LangGraph node to go to)</li> <li>payload: information to pass to that agent (e.g., state update)</li> </ul>"}, {"location": "how-tos/multi_agent/#create-handoffs", "title": "Create handoffs", "text": "<p>To implement handoffs, you can return <code>Command</code> objects from your agent nodes or tools:</p> <p>:::python <pre><code>from typing import Annotated\nfrom langchain_core.tools import tool, InjectedToolCallId\nfrom langgraph.prebuilt import create_react_agent, InjectedState\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.types import Command\n\ndef create_handoff_tool(*, agent_name: str, description: str | None = None):\n    name = f\"transfer_to_{agent_name}\"\n    description = description or f\"Transfer to {agent_name}\"\n\n    @tool(name, description=description)\n    def handoff_tool(\n        # highlight-next-line\n        state: Annotated[MessagesState, InjectedState], # (1)!\n        # highlight-next-line\n        tool_call_id: Annotated[str, InjectedToolCallId],\n    ) -&gt; Command:\n        tool_message = {\n            \"role\": \"tool\",\n            \"content\": f\"Successfully transferred to {agent_name}\",\n            \"name\": name,\n            \"tool_call_id\": tool_call_id,\n        }\n        return Command(  # (2)!\n            # highlight-next-line\n            goto=agent_name,  # (3)!\n            # highlight-next-line\n            update={\"messages\": state[\"messages\"] + [tool_message]},  # (4)!\n            # highlight-next-line\n            graph=Command.PARENT,  # (5)!\n        )\n    return handoff_tool\n</code></pre></p> <ol> <li>Access the state of the agent that is calling the handoff tool using the @[InjectedState] annotation. </li> <li>The <code>Command</code> primitive allows specifying a state update and a node transition as a single operation, making it useful for implementing handoffs.</li> <li>Name of the agent or node to hand off to.</li> <li>Take the agent's messages and add them to the parent's state as part of the handoff. The next agent will see the parent state.</li> <li>Indicate to LangGraph that we need to navigate to agent node in a parent multi-agent graph.</li> </ol> <p>Tip</p> <p>If you want to use tools that return <code>Command</code>, you can either use prebuilt @[<code>create_react_agent</code>][create_react_agent] / @[<code>ToolNode</code>][ToolNode] components, or implement your own tool-executing node that collects <code>Command</code> objects returned by the tools and returns a list of them, e.g.:</p> <pre><code>def call_tools(state):\n    ...\n    commands = [tools_by_name[tool_call[\"name\"]].invoke(tool_call) for tool_call in tool_calls]\n    return commands\n</code></pre> <p>:::</p> <p>:::js <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { Command, MessagesZodState } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nfunction createHandoffTool({\n  agentName,\n  description,\n}: {\n  agentName: string;\n  description?: string;\n}) {\n  const name = `transfer_to_${agentName}`;\n  const toolDescription = description || `Transfer to ${agentName}`;\n\n  return tool(\n    async (_, config) =&gt; {\n      // (1)!\n      const state = config.state;\n      const toolCallId = config.toolCall.id;\n\n      const toolMessage = {\n        role: \"tool\" as const,\n        content: `Successfully transferred to ${agentName}`,\n        name: name,\n        tool_call_id: toolCallId,\n      };\n\n      return new Command({\n        // (3)!\n        goto: agentName,\n        // (4)!\n        update: { messages: [...state.messages, toolMessage] },\n        // (5)!\n        graph: Command.PARENT,\n      });\n    },\n    {\n      name,\n      description: toolDescription,\n      schema: z.object({}),\n    }\n  );\n}\n</code></pre></p> <ol> <li>Access the state of the agent that is calling the handoff tool through the <code>config</code> parameter.</li> <li>The <code>Command</code> primitive allows specifying a state update and a node transition as a single operation, making it useful for implementing handoffs.</li> <li>Name of the agent or node to hand off to.</li> <li>Take the agent's messages and add them to the parent's state as part of the handoff. The next agent will see the parent state.</li> <li>Indicate to LangGraph that we need to navigate to agent node in a parent multi-agent graph.</li> </ol> <p>Tip</p> <p>If you want to use tools that return <code>Command</code>, you can either use prebuilt @[<code>create_react_agent</code>][create_react_agent] / @[<code>ToolNode</code>][ToolNode] components, or implement your own tool-executing node that collects <code>Command</code> objects returned by the tools and returns a list of them, e.g.:</p> <pre><code>const callTools = async (state) =&gt; {\n  // ...\n  const commands = await Promise.all(\n    toolCalls.map(toolCall =&gt; toolsByName[toolCall.name].invoke(toolCall))\n  );\n  return commands;\n};\n</code></pre> <p>:::</p> <p>Important</p> <p>This handoff implementation assumes that:</p> <ul> <li>each agent receives overall message history (across all agents) in the multi-agent system as its input. If you want more control over agent inputs, see this section</li> <li>each agent outputs its internal messages history to the overall message history of the multi-agent system. If you want more control over how agent outputs are added, wrap the agent in a separate node function:</li> </ul> <p>:::python   <pre><code>def call_hotel_assistant(state):\n    # return agent's final response,\n    # excluding inner monologue\n    response = hotel_assistant.invoke(state)\n    # highlight-next-line\n    return {\"messages\": response[\"messages\"][-1]}\n</code></pre>   :::</p> <p>:::js   <pre><code>const callHotelAssistant = async (state) =&gt; {\n  // return agent's final response,\n  // excluding inner monologue\n  const response = await hotelAssistant.invoke(state);\n  // highlight-next-line\n  return { messages: [response.messages.at(-1)] };\n};\n</code></pre>   :::</p>"}, {"location": "how-tos/multi_agent/#control-agent-inputs", "title": "Control agent inputs", "text": "<p>:::python You can use the @[<code>Send()</code>][Send] primitive to directly send data to the worker agents during the handoff. For example, you can request that the calling agent populate a task description for the next agent:</p> <p><pre><code>from typing import Annotated\nfrom langchain_core.tools import tool, InjectedToolCallId\nfrom langgraph.prebuilt import InjectedState\nfrom langgraph.graph import StateGraph, START, MessagesState\n# highlight-next-line\nfrom langgraph.types import Command, Send\n\ndef create_task_description_handoff_tool(\n    *, agent_name: str, description: str | None = None\n):\n    name = f\"transfer_to_{agent_name}\"\n    description = description or f\"Ask {agent_name} for help.\"\n\n    @tool(name, description=description)\n    def handoff_tool(\n        # this is populated by the calling agent\n        task_description: Annotated[\n            str,\n            \"Description of what the next agent should do, including all of the relevant context.\",\n        ],\n        # these parameters are ignored by the LLM\n        state: Annotated[MessagesState, InjectedState],\n    ) -&gt; Command:\n        task_description_message = {\"role\": \"user\", \"content\": task_description}\n        agent_input = {**state, \"messages\": [task_description_message]}\n        return Command(\n            # highlight-next-line\n            goto=[Send(agent_name, agent_input)],\n            graph=Command.PARENT,\n        )\n\n    return handoff_tool\n</code></pre> :::</p> <p>:::js You can use the @[<code>Send()</code>][Send] primitive to directly send data to the worker agents during the handoff. For example, you can request that the calling agent populate a task description for the next agent:</p> <p><pre><code>import { tool } from \"@langchain/core/tools\";\nimport { Command, Send, MessagesZodState } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nfunction createTaskDescriptionHandoffTool({\n  agentName,\n  description,\n}: {\n  agentName: string;\n  description?: string;\n}) {\n  const name = `transfer_to_${agentName}`;\n  const toolDescription = description || `Ask ${agentName} for help.`;\n\n  return tool(\n    async (\n      { taskDescription },\n      config\n    ) =&gt; {\n      const state = config.state;\n\n      const taskDescriptionMessage = {\n        role: \"user\" as const,\n        content: taskDescription,\n      };\n      const agentInput = {\n        ...state,\n        messages: [taskDescriptionMessage],\n      };\n\n      return new Command({\n        // highlight-next-line\n        goto: [new Send(agentName, agentInput)],\n        graph: Command.PARENT,\n      });\n    },\n    {\n      name,\n      description: toolDescription,\n      schema: z.object({\n        taskDescription: z\n          .string()\n          .describe(\n            \"Description of what the next agent should do, including all of the relevant context.\"\n          ),\n      }),\n    }\n  );\n}\n</code></pre> :::</p> <p>See the multi-agent supervisor example for a full example of using @[<code>Send()</code>][Send] in handoffs.</p>"}, {"location": "how-tos/multi_agent/#build-a-multi-agent-system", "title": "Build a multi-agent system", "text": "<p>You can use handoffs in any agents built with LangGraph. We recommend using the prebuilt agent or <code>ToolNode</code>, as they natively support handoffs tools returning <code>Command</code>. Below is an example of how you can implement a multi-agent system for booking travel using handoffs:</p> <p>:::python <pre><code>from langgraph.prebuilt import create_react_agent\nfrom langgraph.graph import StateGraph, START, MessagesState\n\ndef create_handoff_tool(*, agent_name: str, description: str | None = None):\n    # same implementation as above\n    ...\n    return Command(...)\n\n# Handoffs\ntransfer_to_hotel_assistant = create_handoff_tool(agent_name=\"hotel_assistant\")\ntransfer_to_flight_assistant = create_handoff_tool(agent_name=\"flight_assistant\")\n\n# Define agents\nflight_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    # highlight-next-line\n    tools=[..., transfer_to_hotel_assistant],\n    # highlight-next-line\n    name=\"flight_assistant\"\n)\nhotel_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    # highlight-next-line\n    tools=[..., transfer_to_flight_assistant],\n    # highlight-next-line\n    name=\"hotel_assistant\"\n)\n\n# Define multi-agent graph\nmulti_agent_graph = (\n    StateGraph(MessagesState)\n    # highlight-next-line\n    .add_node(flight_assistant)\n    # highlight-next-line\n    .add_node(hotel_assistant)\n    .add_edge(START, \"flight_assistant\")\n    .compile()\n)\n</code></pre> :::</p> <p>:::js <pre><code>import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { StateGraph, START, MessagesZodState } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nfunction createHandoffTool({\n  agentName,\n  description,\n}: {\n  agentName: string;\n  description?: string;\n}) {\n  // same implementation as above\n  // ...\n  return new Command(/* ... */);\n}\n\n// Handoffs\nconst transferToHotelAssistant = createHandoffTool({\n  agentName: \"hotel_assistant\",\n});\nconst transferToFlightAssistant = createHandoffTool({\n  agentName: \"flight_assistant\",\n});\n\n// Define agents\nconst flightAssistant = createReactAgent({\n  llm: model,\n  // highlight-next-line\n  tools: [/* ... */, transferToHotelAssistant],\n  // highlight-next-line\n  name: \"flight_assistant\",\n});\n\nconst hotelAssistant = createReactAgent({\n  llm: model,\n  // highlight-next-line\n  tools: [/* ... */, transferToFlightAssistant],\n  // highlight-next-line\n  name: \"hotel_assistant\",\n});\n\n// Define multi-agent graph\nconst multiAgentGraph = new StateGraph(MessagesZodState)\n  // highlight-next-line\n  .addNode(\"flight_assistant\", flightAssistant)\n  // highlight-next-line\n  .addNode(\"hotel_assistant\", hotelAssistant)\n  .addEdge(START, \"flight_assistant\")\n  .compile();\n</code></pre> :::</p> Full example: Multi-agent system for booking travel <p>:::python <pre><code>from typing import Annotated\nfrom langchain_core.messages import convert_to_messages\nfrom langchain_core.tools import tool, InjectedToolCallId\nfrom langgraph.prebuilt import create_react_agent, InjectedState\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.types import Command\n\n# We'll use `pretty_print_messages` helper to render the streamed agent outputs nicely\n\ndef pretty_print_message(message, indent=False):\n    pretty_message = message.pretty_repr(html=True)\n    if not indent:\n        print(pretty_message)\n        return\n\n    indented = \"\\n\".join(\"\\t\" + c for c in pretty_message.split(\"\\n\"))\n    print(indented)\n\n\ndef pretty_print_messages(update, last_message=False):\n    is_subgraph = False\n    if isinstance(update, tuple):\n        ns, update = update\n        # skip parent graph updates in the printouts\n        if len(ns) == 0:\n            return\n\n        graph_id = ns[-1].split(\":\")[0]\n        print(f\"Update from subgraph {graph_id}:\")\n        print(\"\\n\")\n        is_subgraph = True\n\n    for node_name, node_update in update.items():\n        update_label = f\"Update from node {node_name}:\"\n        if is_subgraph:\n            update_label = \"\\t\" + update_label\n\n        print(update_label)\n        print(\"\\n\")\n\n        messages = convert_to_messages(node_update[\"messages\"])\n        if last_message:\n            messages = messages[-1:]\n\n        for m in messages:\n            pretty_print_message(m, indent=is_subgraph)\n        print(\"\\n\")\n\n\ndef create_handoff_tool(*, agent_name: str, description: str | None = None):\n    name = f\"transfer_to_{agent_name}\"\n    description = description or f\"Transfer to {agent_name}\"\n\n    @tool(name, description=description)\n    def handoff_tool(\n        # highlight-next-line\n        state: Annotated[MessagesState, InjectedState], # (1)!\n        # highlight-next-line\n        tool_call_id: Annotated[str, InjectedToolCallId],\n    ) -&gt; Command:\n        tool_message = {\n            \"role\": \"tool\",\n            \"content\": f\"Successfully transferred to {agent_name}\",\n            \"name\": name,\n            \"tool_call_id\": tool_call_id,\n        }\n        return Command(  # (2)!\n            # highlight-next-line\n            goto=agent_name,  # (3)!\n            # highlight-next-line\n            update={\"messages\": state[\"messages\"] + [tool_message]},  # (4)!\n            # highlight-next-line\n            graph=Command.PARENT,  # (5)!\n        )\n    return handoff_tool\n\n# Handoffs\ntransfer_to_hotel_assistant = create_handoff_tool(\n    agent_name=\"hotel_assistant\",\n    description=\"Transfer user to the hotel-booking assistant.\",\n)\ntransfer_to_flight_assistant = create_handoff_tool(\n    agent_name=\"flight_assistant\",\n    description=\"Transfer user to the flight-booking assistant.\",\n)\n\n# Simple agent tools\ndef book_hotel(hotel_name: str):\n    \"\"\"Book a hotel\"\"\"\n    return f\"Successfully booked a stay at {hotel_name}.\"\n\ndef book_flight(from_airport: str, to_airport: str):\n    \"\"\"Book a flight\"\"\"\n    return f\"Successfully booked a flight from {from_airport} to {to_airport}.\"\n\n# Define agents\nflight_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    # highlight-next-line\n    tools=[book_flight, transfer_to_hotel_assistant],\n    prompt=\"You are a flight booking assistant\",\n    # highlight-next-line\n    name=\"flight_assistant\"\n)\nhotel_assistant = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    # highlight-next-line\n    tools=[book_hotel, transfer_to_flight_assistant],\n    prompt=\"You are a hotel booking assistant\",\n    # highlight-next-line\n    name=\"hotel_assistant\"\n)\n\n# Define multi-agent graph\nmulti_agent_graph = (\n    StateGraph(MessagesState)\n    .add_node(flight_assistant)\n    .add_node(hotel_assistant)\n    .add_edge(START, \"flight_assistant\")\n    .compile()\n)\n\n# Run the multi-agent graph\nfor chunk in multi_agent_graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"book a flight from BOS to JFK and a stay at McKittrick Hotel\"\n            }\n        ]\n    },\n    # highlight-next-line\n    subgraphs=True\n):\n    pretty_print_messages(chunk)\n</code></pre></p> <ol> <li>Access agent's state</li> <li>The <code>Command</code> primitive allows specifying a state update and a node transition as a single operation, making it useful for implementing handoffs.</li> <li>Name of the agent or node to hand off to.</li> <li>Take the agent's messages and add them to the parent's state as part of the handoff. The next agent will see the parent state.</li> <li>Indicate to LangGraph that we need to navigate to agent node in a parent multi-agent graph. :::</li> </ol> <p>:::js <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { StateGraph, START, MessagesZodState, Command } from \"@langchain/langgraph\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { isBaseMessage } from \"@langchain/core/messages\";\nimport { z } from \"zod\";\n\n// We'll use a helper to render the streamed agent outputs nicely\nconst prettyPrintMessages = (update: Record&lt;string, any&gt;) =&gt; {\n  // Handle tuple case with namespace\n  if (Array.isArray(update)) {\n    const [ns, updateData] = update;\n    // Skip parent graph updates in the printouts\n    if (ns.length === 0) {\n      return;\n    }\n\n    const graphId = ns[ns.length - 1].split(\":\")[0];\n    console.log(`Update from subgraph ${graphId}:\\n`);\n    update = updateData;\n  }\n\n  for (const [nodeName, updateValue] of Object.entries(update)) {\n    console.log(`Update from node ${nodeName}:\\n`);\n\n    const messages = updateValue.messages || [];\n    for (const message of messages) {\n      if (isBaseMessage(message)) {\n        const textContent =\n          typeof message.content === \"string\"\n            ? message.content\n            : JSON.stringify(message.content);\n        console.log(`${message.getType()}: ${textContent}`);\n      }\n    }\n    console.log(\"\\n\");\n  }\n};\n\nfunction createHandoffTool({\n  agentName,\n  description,\n}: {\n  agentName: string;\n  description?: string;\n}) {\n  const name = `transfer_to_${agentName}`;\n  const toolDescription = description || `Transfer to ${agentName}`;\n\n  return tool(\n    async (_, config) =&gt; {\n      // highlight-next-line\n      const state = config.state; // (1)!\n      const toolCallId = config.toolCall.id;\n\n      const toolMessage = {\n        role: \"tool\" as const,\n        content: `Successfully transferred to ${agentName}`,\n        name: name,\n        tool_call_id: toolCallId,\n      };\n\n      return new Command({\n        // highlight-next-line\n        goto: agentName, // (3)!\n        // highlight-next-line\n        update: { messages: [...state.messages, toolMessage] }, // (4)!\n        // highlight-next-line\n        graph: Command.PARENT, // (5)!\n      });\n    },\n    {\n      name,\n      description: toolDescription,\n      schema: z.object({}),\n    }\n  );\n}\n\n// Handoffs\nconst transferToHotelAssistant = createHandoffTool({\n  agentName: \"hotel_assistant\",\n  description: \"Transfer user to the hotel-booking assistant.\",\n});\n\nconst transferToFlightAssistant = createHandoffTool({\n  agentName: \"flight_assistant\",\n  description: \"Transfer user to the flight-booking assistant.\",\n});\n\n// Simple agent tools\nconst bookHotel = tool(\n  async ({ hotelName }) =&gt; {\n    return `Successfully booked a stay at ${hotelName}.`;\n  },\n  {\n    name: \"book_hotel\",\n    description: \"Book a hotel\",\n    schema: z.object({\n      hotelName: z.string(),\n    }),\n  }\n);\n\nconst bookFlight = tool(\n  async ({ fromAirport, toAirport }) =&gt; {\n    return `Successfully booked a flight from ${fromAirport} to ${toAirport}.`;\n  },\n  {\n    name: \"book_flight\",\n    description: \"Book a flight\",\n    schema: z.object({\n      fromAirport: z.string(),\n      toAirport: z.string(),\n    }),\n  }\n);\n\nconst model = new ChatAnthropic({\n  model: \"claude-3-5-sonnet-latest\",\n});\n\n// Define agents\nconst flightAssistant = createReactAgent({\n  llm: model,\n  // highlight-next-line\n  tools: [bookFlight, transferToHotelAssistant],\n  prompt: \"You are a flight booking assistant\",\n  // highlight-next-line\n  name: \"flight_assistant\",\n});\n\nconst hotelAssistant = createReactAgent({\n  llm: model,\n  // highlight-next-line\n  tools: [bookHotel, transferToFlightAssistant],\n  prompt: \"You are a hotel booking assistant\",\n  // highlight-next-line\n  name: \"hotel_assistant\",\n});\n\n// Define multi-agent graph\nconst multiAgentGraph = new StateGraph(MessagesZodState)\n  .addNode(\"flight_assistant\", flightAssistant)\n  .addNode(\"hotel_assistant\", hotelAssistant)\n  .addEdge(START, \"flight_assistant\")\n  .compile();\n\n// Run the multi-agent graph\nconst stream = await multiAgentGraph.stream(\n  {\n    messages: [\n      {\n        role: \"user\",\n        content: \"book a flight from BOS to JFK and a stay at McKittrick Hotel\",\n      },\n    ],\n  },\n  // highlight-next-line\n  { subgraphs: true }\n);\n\nfor await (const chunk of stream) {\n  prettyPrintMessages(chunk);\n}\n</code></pre></p> <ol> <li>Access agent's state</li> <li>The <code>Command</code> primitive allows specifying a state update and a node transition as a single operation, making it useful for implementing handoffs.</li> <li>Name of the agent or node to hand off to.</li> <li>Take the agent's messages and add them to the parent's state as part of the handoff. The next agent will see the parent state.</li> <li>Indicate to LangGraph that we need to navigate to agent node in a parent multi-agent graph. :::</li> </ol>"}, {"location": "how-tos/multi_agent/#multi-turn-conversation", "title": "Multi-turn conversation", "text": "<p>Users might want to engage in a multi-turn conversation with one or more agents. To build a system that can handle this, you can create a node that uses an @[<code>interrupt</code>][interrupt] to collect user input and routes back to the active agent.</p> <p>The agents can then be implemented as nodes in a graph that executes agent steps and determines the next action:</p> <ol> <li>Wait for user input to continue the conversation, or  </li> <li>Route to another agent (or back to itself, such as in a loop) via a handoff</li> </ol> <p>:::python <pre><code>def human(state) -&gt; Command[Literal[\"agent\", \"another_agent\"]]:\n    \"\"\"A node for collecting user input.\"\"\"\n    user_input = interrupt(value=\"Ready for user input.\")\n\n    # Determine the active agent.\n    active_agent = ...\n\n    ...\n    return Command(\n        update={\n            \"messages\": [{\n                \"role\": \"human\",\n                \"content\": user_input,\n            }]\n        },\n        goto=active_agent\n    )\n\ndef agent(state) -&gt; Command[Literal[\"agent\", \"another_agent\", \"human\"]]:\n    # The condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\n    goto = get_next_agent(...)  # 'agent' / 'another_agent'\n    if goto:\n        return Command(goto=goto, update={\"my_state_key\": \"my_state_value\"})\n    else:\n        return Command(goto=\"human\") # Go to human node\n</code></pre> :::</p> <p>:::js <pre><code>import { interrupt, Command } from \"@langchain/langgraph\";\n\nfunction human(state: MessagesState): Command {\n  const userInput: string = interrupt(\"Ready for user input.\");\n\n  // Determine the active agent\n  const activeAgent = /* ... */;\n\n  return new Command({\n    update: {\n      messages: [{\n        role: \"human\",\n        content: userInput,\n      }]\n    },\n    goto: activeAgent,\n  });\n}\n\nfunction agent(state: MessagesState): Command {\n  // The condition for routing/halting can be anything, e.g. LLM tool call / structured output, etc.\n  const goto = getNextAgent(/* ... */); // 'agent' / 'anotherAgent'\n\n  if (goto) {\n    return new Command({\n      goto,\n      update: { myStateKey: \"myStateValue\" }\n    });\n  }\n\n  return new Command({ goto: \"human\" });\n}\n</code></pre> :::</p> Full example: multi-agent system for travel recommendations <p>In this example, we will build a team of travel assistant agents that can communicate with each other via handoffs.</p> <p>We will create 2 agents:</p> <ul> <li>travel_advisor: can help with travel destination recommendations. Can ask hotel_advisor for help.</li> <li>hotel_advisor: can help with hotel recommendations. Can ask travel_advisor for help.</li> </ul> <p>:::python <pre><code>from langchain_anthropic import ChatAnthropic\nfrom langgraph.graph import MessagesState, StateGraph, START\nfrom langgraph.prebuilt import create_react_agent, InjectedState\nfrom langgraph.types import Command, interrupt\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n\nclass MultiAgentState(MessagesState):\n    last_active_agent: str\n\n\n# Define travel advisor tools and ReAct agent\ntravel_advisor_tools = [\n    get_travel_recommendations,\n    make_handoff_tool(agent_name=\"hotel_advisor\"),\n]\ntravel_advisor = create_react_agent(\n    model,\n    travel_advisor_tools,\n    prompt=(\n        \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). \"\n        \"If you need hotel recommendations, ask 'hotel_advisor' for help. \"\n        \"You MUST include human-readable response before transferring to another agent.\"\n    ),\n)\n\n\ndef call_travel_advisor(\n    state: MultiAgentState,\n) -&gt; Command[Literal[\"hotel_advisor\", \"human\"]]:\n    # You can also add additional logic like changing the input to the agent / output from the agent, etc.\n    # NOTE: we're invoking the ReAct agent with the full history of messages in the state\n    response = travel_advisor.invoke(state)\n    update = {**response, \"last_active_agent\": \"travel_advisor\"}\n    return Command(update=update, goto=\"human\")\n\n\n# Define hotel advisor tools and ReAct agent\nhotel_advisor_tools = [\n    get_hotel_recommendations,\n    make_handoff_tool(agent_name=\"travel_advisor\"),\n]\nhotel_advisor = create_react_agent(\n    model,\n    hotel_advisor_tools,\n    prompt=(\n        \"You are a hotel expert that can provide hotel recommendations for a given destination. \"\n        \"If you need help picking travel destinations, ask 'travel_advisor' for help.\"\n        \"You MUST include human-readable response before transferring to another agent.\"\n    ),\n)\n\n\ndef call_hotel_advisor(\n    state: MultiAgentState,\n) -&gt; Command[Literal[\"travel_advisor\", \"human\"]]:\n    response = hotel_advisor.invoke(state)\n    update = {**response, \"last_active_agent\": \"hotel_advisor\"}\n    return Command(update=update, goto=\"human\")\n\n\ndef human_node(\n    state: MultiAgentState, config\n) -&gt; Command[Literal[\"hotel_advisor\", \"travel_advisor\", \"human\"]]:\n    \"\"\"A node for collecting user input.\"\"\"\n\n    user_input = interrupt(value=\"Ready for user input.\")\n    active_agent = state[\"last_active_agent\"]\n\n    return Command(\n        update={\n            \"messages\": [\n                {\n                    \"role\": \"human\",\n                    \"content\": user_input,\n                }\n            ]\n        },\n        goto=active_agent,\n    )\n\n\nbuilder = StateGraph(MultiAgentState)\nbuilder.add_node(\"travel_advisor\", call_travel_advisor)\nbuilder.add_node(\"hotel_advisor\", call_hotel_advisor)\n\n# This adds a node to collect human input, which will route\n# back to the active agent.\nbuilder.add_node(\"human\", human_node)\n\n# We'll always start with a general travel advisor.\nbuilder.add_edge(START, \"travel_advisor\")\n\n\ncheckpointer = InMemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n</code></pre></p> <p>Let's test a multi turn conversation with this application.</p> <pre><code>import uuid\n\nthread_config = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n\ninputs = [\n    # 1st round of conversation,\n    {\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"i wanna go somewhere warm in the caribbean\"}\n        ]\n    },\n    # Since we're using `interrupt`, we'll need to resume using the Command primitive.\n    # 2nd round of conversation,\n    Command(\n        resume=\"could you recommend a nice hotel in one of the areas and tell me which area it is.\"\n    ),\n    # 3rd round of conversation,\n    Command(\n        resume=\"i like the first one. could you recommend something to do near the hotel?\"\n    ),\n]\n\nfor idx, user_input in enumerate(inputs):\n    print()\n    print(f\"--- Conversation Turn {idx + 1} ---\")\n    print()\n    print(f\"User: {user_input}\")\n    print()\n    for update in graph.stream(\n        user_input,\n        config=thread_config,\n        stream_mode=\"updates\",\n    ):\n        for node_id, value in update.items():\n            if isinstance(value, dict) and value.get(\"messages\", []):\n                last_message = value[\"messages\"][-1]\n                if isinstance(last_message, dict) or last_message.type != \"ai\":\n                    continue\n                print(f\"{node_id}: {last_message.content}\")\n</code></pre> <p><pre><code>--- Conversation Turn 1 ---\n\nUser: {'messages': [{'role': 'user', 'content': 'i wanna go somewhere warm in the caribbean'}]}\n\ntravel_advisor: Based on the recommendations, Aruba would be an excellent choice for your Caribbean getaway! Aruba is known as \"One Happy Island\" and offers:\n- Year-round warm weather with consistent temperatures around 82\u00b0F (28\u00b0C)\n- Beautiful white sand beaches like Eagle Beach and Palm Beach\n- Clear turquoise waters perfect for swimming and snorkeling\n- Minimal rainfall and location outside the hurricane belt\n- A blend of Caribbean and Dutch culture\n- Great dining options and nightlife\n- Various water sports and activities\n\nWould you like me to get some specific hotel recommendations in Aruba for your stay? I can transfer you to our hotel advisor who can help with accommodations.\n\n--- Conversation Turn 2 ---\n\nUser: Command(resume='could you recommend a nice hotel in one of the areas and tell me which area it is.')\n\nhotel_advisor: Based on the recommendations, I can suggest two excellent options:\n\n1. The Ritz-Carlton, Aruba - Located in Palm Beach\n- This luxury resort is situated in the vibrant Palm Beach area\n- Known for its exceptional service and amenities\n- Perfect if you want to be close to dining, shopping, and entertainment\n- Features multiple restaurants, a casino, and a world-class spa\n- Located on a pristine stretch of Palm Beach\n\n2. Bucuti &amp; Tara Beach Resort - Located in Eagle Beach\n- An adults-only boutique resort on Eagle Beach\n- Known for being more intimate and peaceful\n- Award-winning for its sustainability practices\n- Perfect for a romantic getaway or peaceful vacation\n- Located on one of the most beautiful beaches in the Caribbean\n\nWould you like more specific information about either of these properties or their locations?\n\n--- Conversation Turn 3 ---\n\nUser: Command(resume='i like the first one. could you recommend something to do near the hotel?')\n\ntravel_advisor: Near the Ritz-Carlton in Palm Beach, here are some highly recommended activities:\n\n1. Visit the Palm Beach Plaza Mall - Just a short walk from the hotel, featuring shopping, dining, and entertainment\n2. Try your luck at the Stellaris Casino - It's right in the Ritz-Carlton\n3. Take a sunset sailing cruise - Many depart from the nearby pier\n4. Visit the California Lighthouse - A scenic landmark just north of Palm Beach\n5. Enjoy water sports at Palm Beach:\n   - Jet skiing\n   - Parasailing\n   - Snorkeling\n   - Stand-up paddleboarding\n\nWould you like more specific information about any of these activities or would you like to know about other options in the area?\n</code></pre> :::</p> <p>:::js <pre><code>import { ChatAnthropic } from \"@langchain/anthropic\";\nimport { StateGraph, START, MessagesZodState, Command, interrupt, MemorySaver } from \"@langchain/langgraph\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst model = new ChatAnthropic({ model: \"claude-3-5-sonnet-latest\" });\n\nconst MultiAgentState = MessagesZodState.extend({\n  lastActiveAgent: z.string().optional(),\n});\n\n// Define travel advisor tools\nconst getTravelRecommendations = tool(\n  async () =&gt; {\n    // Placeholder implementation\n    return \"Based on current trends, I recommend visiting Japan, Portugal, or New Zealand.\";\n  },\n  {\n    name: \"get_travel_recommendations\",\n    description: \"Get current travel destination recommendations\",\n    schema: z.object({}),\n  }\n);\n\nconst makeHandoffTool = (agentName: string) =&gt; {\n  return tool(\n    async (_, config) =&gt; {\n      const state = config.state;\n      const toolCallId = config.toolCall.id;\n\n      const toolMessage = {\n        role: \"tool\" as const,\n        content: `Successfully transferred to ${agentName}`,\n        name: `transfer_to_${agentName}`,\n        tool_call_id: toolCallId,\n      };\n\n      return new Command({\n        goto: agentName,\n        update: { messages: [...state.messages, toolMessage] },\n        graph: Command.PARENT,\n      });\n    },\n    {\n      name: `transfer_to_${agentName}`,\n      description: `Transfer to ${agentName}`,\n      schema: z.object({}),\n    }\n  );\n};\n\nconst travelAdvisorTools = [\n  getTravelRecommendations,\n  makeHandoffTool(\"hotel_advisor\"),\n];\n\nconst travelAdvisor = createReactAgent({\n  llm: model,\n  tools: travelAdvisorTools,\n  prompt: [\n    \"You are a general travel expert that can recommend travel destinations (e.g. countries, cities, etc). \",\n    \"If you need hotel recommendations, ask 'hotel_advisor' for help. \",\n    \"You MUST include human-readable response before transferring to another agent.\"\n  ].join(\"\")\n});\n\nconst callTravelAdvisor = async (\n  state: z.infer&lt;typeof MultiAgentState&gt;\n): Promise&lt;Command&gt; =&gt; {\n  const response = await travelAdvisor.invoke(state);\n  const update = { ...response, lastActiveAgent: \"travel_advisor\" };\n  return new Command({ update, goto: \"human\" });\n};\n\n// Define hotel advisor tools\nconst getHotelRecommendations = tool(\n  async () =&gt; {\n    // Placeholder implementation\n    return \"I recommend the Ritz-Carlton for luxury stays or boutique hotels for unique experiences.\";\n  },\n  {\n    name: \"get_hotel_recommendations\",\n    description: \"Get hotel recommendations for destinations\",\n    schema: z.object({}),\n  }\n);\n\nconst hotelAdvisorTools = [\n  getHotelRecommendations,\n  makeHandoffTool(\"travel_advisor\"),\n];\n\nconst hotelAdvisor = createReactAgent({\n  llm: model,\n  tools: hotelAdvisorTools,\n  prompt: [\n    \"You are a hotel expert that can provide hotel recommendations for a given destination. \",\n    \"If you need help picking travel destinations, ask 'travel_advisor' for help.\",\n    \"You MUST include human-readable response before transferring to another agent.\"\n  ].join(\"\")\n});\n\nconst callHotelAdvisor = async (\n  state: z.infer&lt;typeof MultiAgentState&gt;\n): Promise&lt;Command&gt; =&gt; {\n  const response = await hotelAdvisor.invoke(state);\n  const update = { ...response, lastActiveAgent: \"hotel_advisor\" };\n  return new Command({ update, goto: \"human\" });\n};\n\nconst humanNode = async (\n  state: z.infer&lt;typeof MultiAgentState&gt;\n): Promise&lt;Command&gt; =&gt; {\n  const userInput: string = interrupt(\"Ready for user input.\");\n  const activeAgent = state.lastActiveAgent || \"travel_advisor\";\n\n  return new Command({\n    update: {\n      messages: [\n        {\n          role: \"human\",\n          content: userInput,\n        }\n      ]\n    },\n    goto: activeAgent,\n  });\n};\n\nconst builder = new StateGraph(MultiAgentState)\n  .addNode(\"travel_advisor\", callTravelAdvisor)\n  .addNode(\"hotel_advisor\", callHotelAdvisor)\n  .addNode(\"human\", humanNode)\n  .addEdge(START, \"travel_advisor\");\n\nconst checkpointer = new MemorySaver();\nconst graph = builder.compile({ checkpointer });\n</code></pre></p> <p>Let's test a multi turn conversation with this application.</p> <pre><code>import { v4 as uuidv4 } from \"uuid\";\nimport { Command } from \"@langchain/langgraph\";\n\nconst threadConfig = { configurable: { thread_id: uuidv4() } };\n\nconst inputs = [\n  // 1st round of conversation\n  {\n    messages: [\n      { role: \"user\", content: \"i wanna go somewhere warm in the caribbean\" }\n    ]\n  },\n  // Since we're using `interrupt`, we'll need to resume using the Command primitive.\n  // 2nd round of conversation\n  new Command({\n    resume: \"could you recommend a nice hotel in one of the areas and tell me which area it is.\"\n  }),\n  // 3rd round of conversation\n  new Command({\n    resume: \"i like the first one. could you recommend something to do near the hotel?\"\n  }),\n];\n\nfor (const [idx, userInput] of inputs.entries()) {\n  console.log();\n  console.log(`--- Conversation Turn ${idx + 1} ---`);\n  console.log();\n  console.log(`User: ${JSON.stringify(userInput)}`);\n  console.log();\n\n  for await (const update of await graph.stream(\n    userInput,\n    { ...threadConfig, streamMode: \"updates\" }\n  )) {\n    for (const [nodeId, value] of Object.entries(update)) {\n      if (value?.messages?.length) {\n        const lastMessage = value.messages.at(-1);\n        if (lastMessage?.getType?.() === \"ai\") {\n          console.log(`${nodeId}: ${lastMessage.content}`);\n        }\n      }\n    }\n  }\n}\n</code></pre> <p><pre><code>--- Conversation Turn 1 ---\n\nUser: {\"messages\":[{\"role\":\"user\",\"content\":\"i wanna go somewhere warm in the caribbean\"}]}\n\ntravel_advisor: Based on the recommendations, Aruba would be an excellent choice for your Caribbean getaway! Aruba is known as \"One Happy Island\" and offers:\n- Year-round warm weather with consistent temperatures around 82\u00b0F (28\u00b0C)\n- Beautiful white sand beaches like Eagle Beach and Palm Beach\n- Clear turquoise waters perfect for swimming and snorkeling\n- Minimal rainfall and location outside the hurricane belt\n- A blend of Caribbean and Dutch culture\n- Great dining options and nightlife\n- Various water sports and activities\n\nWould you like me to get some specific hotel recommendations in Aruba for your stay? I can transfer you to our hotel advisor who can help with accommodations.\n\n--- Conversation Turn 2 ---\n\nUser: Command { resume: 'could you recommend a nice hotel in one of the areas and tell me which area it is.' }\n\nhotel_advisor: Based on the recommendations, I can suggest two excellent options:\n\n1. The Ritz-Carlton, Aruba - Located in Palm Beach\n- This luxury resort is situated in the vibrant Palm Beach area\n- Known for its exceptional service and amenities\n- Perfect if you want to be close to dining, shopping, and entertainment\n- Features multiple restaurants, a casino, and a world-class spa\n- Located on a pristine stretch of Palm Beach\n\n2. Bucuti &amp; Tara Beach Resort - Located in Eagle Beach\n- An adults-only boutique resort on Eagle Beach\n- Known for being more intimate and peaceful\n- Award-winning for its sustainability practices\n- Perfect for a romantic getaway or peaceful vacation\n- Located on one of the most beautiful beaches in the Caribbean\n\nWould you like more specific information about either of these properties or their locations?\n\n--- Conversation Turn 3 ---\n\nUser: Command { resume: 'i like the first one. could you recommend something to do near the hotel?' }\n\ntravel_advisor: Near the Ritz-Carlton in Palm Beach, here are some highly recommended activities:\n\n1. Visit the Palm Beach Plaza Mall - Just a short walk from the hotel, featuring shopping, dining, and entertainment\n2. Try your luck at the Stellaris Casino - It's right in the Ritz-Carlton\n3. Take a sunset sailing cruise - Many depart from the nearby pier\n4. Visit the California Lighthouse - A scenic landmark just north of Palm Beach\n5. Enjoy water sports at Palm Beach:\n   - Jet skiing\n   - Parasailing\n   - Snorkeling\n   - Stand-up paddleboarding\n\nWould you like more specific information about any of these activities or would you like to know about other options in the area?\n</code></pre> :::</p>"}, {"location": "how-tos/multi_agent/#prebuilt-implementations", "title": "Prebuilt implementations", "text": "<p>LangGraph comes with prebuilt implementations of two of the most popular multi-agent architectures:</p> <p>:::python - supervisor \u2014 individual agents are coordinated by a central supervisor agent. The supervisor controls all communication flow and task delegation, making decisions about which agent to invoke based on the current context and task requirements. You can use <code>langgraph-supervisor</code> library to create a supervisor multi-agent systems. - swarm \u2014 agents dynamically hand off control to one another based on their specializations. The system remembers which agent was last active, ensuring that on subsequent interactions, the conversation resumes with that agent. You can use <code>langgraph-swarm</code> library to create a swarm multi-agent systems. :::</p> <p>:::js - supervisor \u2014 individual agents are coordinated by a central supervisor agent. The supervisor controls all communication flow and task delegation, making decisions about which agent to invoke based on the current context and task requirements. You can use <code>langgraph-supervisor</code> library to create a supervisor multi-agent systems. - swarm \u2014 agents dynamically hand off control to one another based on their specializations. The system remembers which agent was last active, ensuring that on subsequent interactions, the conversation resumes with that agent. You can use <code>langgraph-swarm</code> library to create a swarm multi-agent systems. :::</p>"}, {"location": "how-tos/run-id-langsmith/", "title": "How to pass custom run ID or set tags and metadata for graph runs in LangSmith", "text": "<p>Prerequisites</p> <p>This guide assumes familiarity with the following:</p> <ul> <li>LangSmith Documentation</li> <li>LangSmith Platform</li> <li>RunnableConfig</li> <li>Add metadata and tags to traces</li> <li>Customize run name</li> </ul> <p>Debugging graph runs can sometimes be difficult to do in an IDE or terminal. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read the LangSmith documentation for more information on how to get started.</p> <p>To make it easier to identify and analyzed traces generated during graph invocation, you can set additional configuration at run time (see RunnableConfig):</p> Field Type Description run_name <code>str</code> Name for the tracer run for this call. Defaults to the name of the class. run_id <code>UUID</code> Unique identifier for the tracer run for this call. If not provided, a new UUID will be generated. tags <code>List[str]</code> Tags for this call and any sub-calls (e.g., a Chain calling an LLM). You can use these to filter calls. metadata <code>Dict[str, Any]</code> Metadata for this call and any sub-calls (e.g., a Chain calling an LLM). Keys should be strings, values should be JSON-serializable. <p>LangGraph graphs implement the LangChain Runnable Interface and accept a second argument (<code>RunnableConfig</code>) in methods like <code>invoke</code>, <code>ainvoke</code>, <code>stream</code> etc.</p> <p>The LangSmith platform will allow you to search and filter traces based on <code>run_name</code>, <code>run_id</code>, <code>tags</code> and <code>metadata</code>.</p>"}, {"location": "how-tos/run-id-langsmith/#tldr", "title": "TLDR", "text": "<pre><code>import uuid\n# Generate a random UUID -- it must be a UUID\nconfig = {\"run_id\": uuid.uuid4()}, \"tags\": [\"my_tag1\"], \"metadata\": {\"a\": 5}}\n# Works with all standard Runnable methods \n# like invoke, batch, ainvoke, astream_events etc\ngraph.stream(inputs, config, stream_mode=\"values\")\n</code></pre> <p>The rest of the how to guide will show a full agent.</p>"}, {"location": "how-tos/run-id-langsmith/#setup", "title": "Setup", "text": "<p>First, let's install the required packages and set our API keys</p> <pre><code>%%capture --no-stderr\n%pip install --quiet -U langgraph langchain_openai\n</code></pre> <pre><code>import getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"OPENAI_API_KEY\")\n_set_env(\"LANGSMITH_API_KEY\")\n</code></pre> <p>Tip</p> <p>Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here.</p>"}, {"location": "how-tos/run-id-langsmith/#define-the-graph", "title": "Define the graph", "text": "<p>For this example we will use the prebuilt ReAct agent.</p> <pre><code>from langchain_openai import ChatOpenAI\nfrom typing import Literal\nfrom langgraph.prebuilt import create_react_agent\nfrom langchain_core.tools import tool\n\n# First we initialize the model we want to use.\nmodel = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n\n\n# For this tutorial we will use custom tool that returns pre-defined values for weather in two cities (NYC &amp; SF)\n@tool\ndef get_weather(city: Literal[\"nyc\", \"sf\"]):\n    \"\"\"Use this to get weather information.\"\"\"\n    if city == \"nyc\":\n        return \"It might be cloudy in nyc\"\n    elif city == \"sf\":\n        return \"It's always sunny in sf\"\n    else:\n        raise AssertionError(\"Unknown city\")\n\n\ntools = [get_weather]\n\n\n# Define the graph\ngraph = create_react_agent(model, tools=tools)\n</code></pre>"}, {"location": "how-tos/run-id-langsmith/#run-your-graph", "title": "Run your graph", "text": "<p>Now that we've defined our graph let's run it once and view the trace in LangSmith. In order for our trace to be easily accessible in LangSmith, we will pass in a custom <code>run_id</code> in the config.</p> <p>This assumes that you have set your <code>LANGSMITH_API_KEY</code> environment variable.</p> <p>Note that you can also configure what project to trace to by setting the <code>LANGCHAIN_PROJECT</code> environment variable, by default runs will be traced to the <code>default</code> project.</p> <pre><code>import uuid\n\n\ndef print_stream(stream):\n    for s in stream:\n        message = s[\"messages\"][-1]\n        if isinstance(message, tuple):\n            print(message)\n        else:\n            message.pretty_print()\n\n\ninputs = {\"messages\": [(\"user\", \"what is the weather in sf\")]}\n\nconfig = {\"run_name\": \"agent_007\", \"tags\": [\"cats are awesome\"]}\n\nprint_stream(graph.stream(inputs, config, stream_mode=\"values\"))\n</code></pre> <p>Output: <pre><code>================================ Human Message ==================================\n\nwhat is the weather in sf\n================================== Ai Message ===================================\nTool Calls:\n  get_weather (call_9ZudXyMAdlUjptq9oMGtQo8o)\n Call ID: call_9ZudXyMAdlUjptq9oMGtQo8o\n  Args:\n    city: sf\n================================= Tool Message ==================================\nName: get_weather\n\nIt's always sunny in sf\n================================== Ai Message ===================================\n\nThe weather in San Francisco is currently sunny.\n</code></pre></p>"}, {"location": "how-tos/run-id-langsmith/#view-the-trace-in-langsmith", "title": "View the trace in LangSmith", "text": "<p>Now that we've ran our graph, let's head over to LangSmith and view our trace. First click into the project that you traced to (in our case the default project). You should see a run with the custom run name \"agent_007\".</p> <p></p> <p>In addition, you will be able to filter traces after the fact using the tags or metadata provided. For example,</p> <p> </p>"}, {"location": "how-tos/streaming/", "title": "Stream outputs", "text": "<p>You can stream outputs from a LangGraph agent or workflow.</p>"}, {"location": "how-tos/streaming/#supported-stream-modes", "title": "Supported stream modes", "text": "<p>:::python Pass one or more of the following stream modes as a list to the @[<code>stream()</code>][CompiledStateGraph.stream] or @[<code>astream()</code>][CompiledStateGraph.astream] methods: :::</p> <p>:::js Pass one or more of the following stream modes as a list to the @[<code>stream()</code>][CompiledStateGraph.stream] method: :::</p> Mode Description <code>values</code> Streams the full value of the state after each step of the graph. <code>updates</code> Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. <code>custom</code> Streams custom data from inside your graph nodes. <code>messages</code> Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked. <code>debug</code> Streams as much information as possible throughout the execution of the graph."}, {"location": "how-tos/streaming/#stream-from-an-agent", "title": "Stream from an agent", "text": ""}, {"location": "how-tos/streaming/#agent-progress", "title": "Agent progress", "text": "<p>:::python To stream agent progress, use the @[<code>stream()</code>][CompiledStateGraph.stream] or @[<code>astream()</code>][CompiledStateGraph.astream] methods with <code>stream_mode=\"updates\"</code>. This emits an event after every agent step. :::</p> <p>:::js To stream agent progress, use the @[<code>stream()</code>][CompiledStateGraph.stream] method with <code>streamMode: \"updates\"</code>. This emits an event after every agent step. :::</p> <p>For example, if you have an agent that calls a tool once, you should see the following updates:</p> <ul> <li>LLM node: AI message with tool call requests</li> <li>Tool node: Tool message with execution result</li> <li>LLM node: Final AI response</li> </ul> <p>:::python</p> SyncAsync <pre><code>agent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n)\n# highlight-next-line\nfor chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    # highlight-next-line\n    stream_mode=\"updates\"\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <pre><code>agent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n)\n# highlight-next-line\nasync for chunk in agent.astream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    # highlight-next-line\n    stream_mode=\"updates\"\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const agent = createReactAgent({\n  llm: model,\n  tools: [getWeather],\n});\n\nfor await (const chunk of await agent.stream(\n  { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n  { streamMode: \"updates\" }\n)) {\n  console.log(chunk);\n  console.log(\"\\n\");\n}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/streaming/#llm-tokens", "title": "LLM tokens", "text": "<p>:::python To stream tokens as they are produced by the LLM, use <code>stream_mode=\"messages\"</code>:</p> SyncAsync <pre><code>agent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n)\n# highlight-next-line\nfor token, metadata in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    # highlight-next-line\n    stream_mode=\"messages\"\n):\n    print(\"Token\", token)\n    print(\"Metadata\", metadata)\n    print(\"\\n\")\n</code></pre> <pre><code>agent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n)\n# highlight-next-line\nasync for token, metadata in agent.astream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    # highlight-next-line\n    stream_mode=\"messages\"\n):\n    print(\"Token\", token)\n    print(\"Metadata\", metadata)\n    print(\"\\n\")\n</code></pre> <p>:::</p> <p>:::js To stream tokens as they are produced by the LLM, use <code>streamMode: \"messages\"</code>:</p> <pre><code>const agent = createReactAgent({\n  llm: model,\n  tools: [getWeather],\n});\n\nfor await (const [token, metadata] of await agent.stream(\n  { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n  { streamMode: \"messages\" }\n)) {\n  console.log(\"Token\", token);\n  console.log(\"Metadata\", metadata);\n  console.log(\"\\n\");\n}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/streaming/#tool-updates", "title": "Tool updates", "text": "<p>:::python To stream updates from tools as they are executed, you can use @[get_stream_writer][get_stream_writer].</p> SyncAsync <pre><code># highlight-next-line\nfrom langgraph.config import get_stream_writer\n\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get weather for a given city.\"\"\"\n    # highlight-next-line\n    writer = get_stream_writer()\n    # stream any arbitrary data\n    # highlight-next-line\n    writer(f\"Looking up data for city: {city}\")\n    return f\"It's always sunny in {city}!\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n)\n\nfor chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    # highlight-next-line\n    stream_mode=\"custom\"\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <pre><code># highlight-next-line\nfrom langgraph.config import get_stream_writer\n\ndef get_weather(city: str) -&gt; str:\n    \"\"\"Get weather for a given city.\"\"\"\n    # highlight-next-line\n    writer = get_stream_writer()\n    # stream any arbitrary data\n    # highlight-next-line\n    writer(f\"Looking up data for city: {city}\")\n    return f\"It's always sunny in {city}!\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n)\n\nasync for chunk in agent.astream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    # highlight-next-line\n    stream_mode=\"custom\"\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <p>Note</p> <p>If you add <code>get_stream_writer</code> inside your tool, you won't be able to invoke the tool outside of a LangGraph execution context.</p> <p>:::</p> <p>:::js To stream updates from tools as they are executed, you can use the <code>writer</code> parameter from the configuration.</p> <pre><code>import { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n\nconst getWeather = tool(\n  async (input, config: LangGraphRunnableConfig) =&gt; {\n    // Stream any arbitrary data\n    config.writer?.(\"Looking up data for city: \" + input.city);\n    return `It's always sunny in ${input.city}!`;\n  },\n  {\n    name: \"get_weather\",\n    description: \"Get weather for a given city.\",\n    schema: z.object({\n      city: z.string().describe(\"The city to get weather for.\"),\n    }),\n  }\n);\n\nconst agent = createReactAgent({\n  llm: model,\n  tools: [getWeather],\n});\n\nfor await (const chunk of await agent.stream(\n  { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n  { streamMode: \"custom\" }\n)) {\n  console.log(chunk);\n  console.log(\"\\n\");\n}\n</code></pre> <p>Note</p> <p>If you add the <code>writer</code> parameter to your tool, you won't be able to invoke the tool outside of a LangGraph execution context without providing a writer function.</p> <p>:::</p>"}, {"location": "how-tos/streaming/#stream-multiple-modes", "title": "Stream multiple modes", "text": "<p>:::python You can specify multiple streaming modes by passing stream mode as a list: <code>stream_mode=[\"updates\", \"messages\", \"custom\"]</code>:</p> SyncAsync <pre><code>agent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n)\n\nfor stream_mode, chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    # highlight-next-line\n    stream_mode=[\"updates\", \"messages\", \"custom\"]\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <pre><code>agent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_weather],\n)\n\nasync for stream_mode, chunk in agent.astream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n    # highlight-next-line\n    stream_mode=[\"updates\", \"messages\", \"custom\"]\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <p>:::</p> <p>:::js You can specify multiple streaming modes by passing streamMode as an array: <code>streamMode: [\"updates\", \"messages\", \"custom\"]</code>:</p> <pre><code>const agent = createReactAgent({\n  llm: model,\n  tools: [getWeather],\n});\n\nfor await (const chunk of await agent.stream(\n  { messages: [{ role: \"user\", content: \"what is the weather in sf\" }] },\n  { streamMode: [\"updates\", \"messages\", \"custom\"] }\n)) {\n  console.log(chunk);\n  console.log(\"\\n\");\n}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/streaming/#disable-streaming", "title": "Disable streaming", "text": "<p>In some applications you might need to disable streaming of individual tokens for a given model. This is useful in multi-agent systems to control which agents stream their output.</p> <p>See the Models guide to learn how to disable streaming.</p>"}, {"location": "how-tos/streaming/#stream-from-a-workflow", "title": "Stream from a workflow", "text": ""}, {"location": "how-tos/streaming/#basic-usage-example", "title": "Basic usage example", "text": "<p>:::python LangGraph graphs expose the @[<code>.stream()</code>][Pregel.stream] (sync) and @[<code>.astream()</code>][Pregel.astream] (async) methods to yield streamed outputs as iterators.</p> SyncAsync <pre><code>for chunk in graph.stream(inputs, stream_mode=\"updates\"):\n    print(chunk)\n</code></pre> <pre><code>async for chunk in graph.astream(inputs, stream_mode=\"updates\"):\n    print(chunk)\n</code></pre> <p>:::</p> <p>:::js LangGraph graphs expose the @[<code>.stream()</code>][Pregel.stream] method to yield streamed outputs as iterators.</p> <pre><code>for await (const chunk of await graph.stream(inputs, {\n  streamMode: \"updates\",\n})) {\n  console.log(chunk);\n}\n</code></pre> <p>:::</p> Extended example: streaming updates <p>:::python   <pre><code>from typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\nclass State(TypedDict):\n    topic: str\n    joke: str\n\ndef refine_topic(state: State):\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\n\ndef generate_joke(state: State):\n    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n    StateGraph(State)\n    .add_node(refine_topic)\n    .add_node(generate_joke)\n    .add_edge(START, \"refine_topic\")\n    .add_edge(\"refine_topic\", \"generate_joke\")\n    .add_edge(\"generate_joke\", END)\n    .compile()\n)\n\n# highlight-next-line\nfor chunk in graph.stream( # (1)!\n    {\"topic\": \"ice cream\"},\n    # highlight-next-line\n    stream_mode=\"updates\", # (2)!\n):\n    print(chunk)\n</code></pre></p> <ol> <li>The <code>stream()</code> method returns an iterator that yields streamed outputs.</li> <li>Set <code>stream_mode=\"updates\"</code> to stream only the updates to the graph state after each node. Other stream modes are also available. See supported stream modes for details.   :::</li> </ol> <p>:::js   <pre><code>import { StateGraph, START, END } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({\n  topic: z.string(),\n  joke: z.string(),\n});\n\nconst graph = new StateGraph(State)\n  .addNode(\"refineTopic\", (state) =&gt; {\n    return { topic: state.topic + \" and cats\" };\n  })\n  .addNode(\"generateJoke\", (state) =&gt; {\n    return { joke: `This is a joke about ${state.topic}` };\n  })\n  .addEdge(START, \"refineTopic\")\n  .addEdge(\"refineTopic\", \"generateJoke\")\n  .addEdge(\"generateJoke\", END)\n  .compile();\n\nfor await (const chunk of await graph.stream(\n  { topic: \"ice cream\" },\n  { streamMode: \"updates\" } // (1)!\n)) {\n  console.log(chunk);\n}\n</code></pre></p> <ol> <li>Set <code>streamMode: \"updates\"</code> to stream only the updates to the graph state after each node. Other stream modes are also available. See supported stream modes for details.   :::</li> </ol> <p><code>output   {'refineTopic': {'topic': 'ice cream and cats'}}   {'generateJoke': {'joke': 'This is a joke about ice cream and cats'}}</code>                                                                                                   |</p>"}, {"location": "how-tos/streaming/#stream-multiple-modes_1", "title": "Stream multiple modes", "text": "<p>:::python You can pass a list as the <code>stream_mode</code> parameter to stream multiple modes at once.</p> <p>The streamed outputs will be tuples of <code>(mode, chunk)</code> where <code>mode</code> is the name of the stream mode and <code>chunk</code> is the data streamed by that mode.</p> SyncAsync <pre><code>for mode, chunk in graph.stream(inputs, stream_mode=[\"updates\", \"custom\"]):\n    print(chunk)\n</code></pre> <pre><code>async for mode, chunk in graph.astream(inputs, stream_mode=[\"updates\", \"custom\"]):\n    print(chunk)\n</code></pre> <p>:::</p> <p>:::js You can pass an array as the <code>streamMode</code> parameter to stream multiple modes at once.</p> <p>The streamed outputs will be tuples of <code>[mode, chunk]</code> where <code>mode</code> is the name of the stream mode and <code>chunk</code> is the data streamed by that mode.</p> <pre><code>for await (const [mode, chunk] of await graph.stream(inputs, {\n  streamMode: [\"updates\", \"custom\"],\n})) {\n  console.log(chunk);\n}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/streaming/#stream-graph-state", "title": "Stream graph state", "text": "<p>Use the stream modes <code>updates</code> and <code>values</code> to stream the state of the graph as it executes.</p> <ul> <li><code>updates</code> streams the updates to the state after each step of the graph.</li> <li><code>values</code> streams the full value of the state after each step of the graph.</li> </ul> <p>:::python</p> <pre><code>from typing import TypedDict\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n  topic: str\n  joke: str\n\n\ndef refine_topic(state: State):\n    return {\"topic\": state[\"topic\"] + \" and cats\"}\n\n\ndef generate_joke(state: State):\n    return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n  StateGraph(State)\n  .add_node(refine_topic)\n  .add_node(generate_joke)\n  .add_edge(START, \"refine_topic\")\n  .add_edge(\"refine_topic\", \"generate_joke\")\n  .add_edge(\"generate_joke\", END)\n  .compile()\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, START, END } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({\n  topic: z.string(),\n  joke: z.string(),\n});\n\nconst graph = new StateGraph(State)\n  .addNode(\"refineTopic\", (state) =&gt; {\n    return { topic: state.topic + \" and cats\" };\n  })\n  .addNode(\"generateJoke\", (state) =&gt; {\n    return { joke: `This is a joke about ${state.topic}` };\n  })\n  .addEdge(START, \"refineTopic\")\n  .addEdge(\"refineTopic\", \"generateJoke\")\n  .addEdge(\"generateJoke\", END)\n  .compile();\n</code></pre> <p>:::</p> updatesvalues <p>Use this to stream only the state updates returned by the nodes after each step. The streamed outputs include the name of the node as well as the update.</p> <p>:::python <pre><code>for chunk in graph.stream(\n    {\"topic\": \"ice cream\"},\n    # highlight-next-line\n    stream_mode=\"updates\",\n):\n    print(chunk)\n</code></pre> :::</p> <p>:::js <pre><code>for await (const chunk of await graph.stream(\n  { topic: \"ice cream\" },\n  { streamMode: \"updates\" }\n)) {\n  console.log(chunk);\n}\n</code></pre> :::</p> <p>Use this to stream the full state of the graph after each step.</p> <p>:::python <pre><code>for chunk in graph.stream(\n    {\"topic\": \"ice cream\"},\n    # highlight-next-line\n    stream_mode=\"values\",\n):\n    print(chunk)\n</code></pre> :::</p> <p>:::js <pre><code>for await (const chunk of await graph.stream(\n  { topic: \"ice cream\" },\n  { streamMode: \"values\" }\n)) {\n  console.log(chunk);\n}\n</code></pre> :::</p>"}, {"location": "how-tos/streaming/#stream-subgraph-outputs", "title": "Stream subgraph outputs", "text": "<p>:::python To include outputs from subgraphs in the streamed outputs, you can set <code>subgraphs=True</code> in the <code>.stream()</code> method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.</p> <p>The outputs will be streamed as tuples <code>(namespace, data)</code>, where <code>namespace</code> is a tuple with the path to the node where a subgraph is invoked, e.g. <code>(\"parent_node:&lt;task_id&gt;\", \"child_node:&lt;task_id&gt;\")</code>.</p> <pre><code>for chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    # highlight-next-line\n    subgraphs=True, # (1)!\n    stream_mode=\"updates\",\n):\n    print(chunk)\n</code></pre> <ol> <li>Set <code>subgraphs=True</code> to stream outputs from subgraphs.    :::</li> </ol> <p>:::js To include outputs from subgraphs in the streamed outputs, you can set <code>subgraphs: true</code> in the <code>.stream()</code> method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.</p> <p>The outputs will be streamed as tuples <code>[namespace, data]</code>, where <code>namespace</code> is a tuple with the path to the node where a subgraph is invoked, e.g. <code>[\"parent_node:&lt;task_id&gt;\", \"child_node:&lt;task_id&gt;\"]</code>.</p> <pre><code>for await (const chunk of await graph.stream(\n  { foo: \"foo\" },\n  {\n    subgraphs: true, // (1)!\n    streamMode: \"updates\",\n  }\n)) {\n  console.log(chunk);\n}\n</code></pre> <ol> <li>Set <code>subgraphs: true</code> to stream outputs from subgraphs.    :::</li> </ol> Extended example: streaming from subgraphs <p>:::python   <pre><code>from langgraph.graph import START, StateGraph\nfrom typing import TypedDict\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    foo: str  # note that this key is shared with the parent graph state\n    bar: str\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    stream_mode=\"updates\",\n    # highlight-next-line\n    subgraphs=True, # (1)!\n):\n    print(chunk)\n</code></pre></p> <ol> <li>Set <code>subgraphs=True</code> to stream outputs from subgraphs.   :::</li> </ol> <p>:::js   <pre><code>import { StateGraph, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\n// Define subgraph\nconst SubgraphState = z.object({\n  foo: z.string(), // note that this key is shared with the parent graph state\n  bar: z.string(),\n});\n\nconst subgraphBuilder = new StateGraph(SubgraphState)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    return { bar: \"bar\" };\n  })\n  .addNode(\"subgraphNode2\", (state) =&gt; {\n    return { foo: state.foo + state.bar };\n  })\n  .addEdge(START, \"subgraphNode1\")\n  .addEdge(\"subgraphNode1\", \"subgraphNode2\");\nconst subgraph = subgraphBuilder.compile();\n\n// Define parent graph\nconst ParentState = z.object({\n  foo: z.string(),\n});\n\nconst builder = new StateGraph(ParentState)\n  .addNode(\"node1\", (state) =&gt; {\n    return { foo: \"hi! \" + state.foo };\n  })\n  .addNode(\"node2\", subgraph)\n  .addEdge(START, \"node1\")\n  .addEdge(\"node1\", \"node2\");\nconst graph = builder.compile();\n\nfor await (const chunk of await graph.stream(\n  { foo: \"foo\" },\n  {\n    streamMode: \"updates\",\n    subgraphs: true, // (1)!\n  }\n)) {\n  console.log(chunk);\n}\n</code></pre></p> <ol> <li>Set <code>subgraphs: true</code> to stream outputs from subgraphs.   :::</li> </ol> <p>:::python   <pre><code>((), {'node_1': {'foo': 'hi! foo'}})\n(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_1': {'bar': 'bar'}})\n(('node_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n((), {'node_2': {'foo': 'hi! foobar'}})\n</code></pre>   :::</p> <p>:::js   <pre><code>[[], {'node1': {'foo': 'hi! foo'}}]\n[['node2:dfddc4ba-c3c5-6887-5012-a243b5b377c2'], {'subgraphNode1': {'bar': 'bar'}}]\n[['node2:dfddc4ba-c3c5-6887-5012-a243b5b377c2'], {'subgraphNode2': {'foo': 'hi! foobar'}}]\n[[], {'node2': {'foo': 'hi! foobar'}}]\n</code></pre>   :::</p> <p>Note that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.</p>"}, {"location": "how-tos/streaming/#debug", "title": "Debugging", "text": "<p>Use the <code>debug</code> streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state.</p> <p>:::python</p> <pre><code>for chunk in graph.stream(\n    {\"topic\": \"ice cream\"},\n    # highlight-next-line\n    stream_mode=\"debug\",\n):\n    print(chunk)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>for await (const chunk of await graph.stream(\n  { topic: \"ice cream\" },\n  { streamMode: \"debug\" }\n)) {\n  console.log(chunk);\n}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/streaming/#messages", "title": "LLM tokens", "text": "<p>Use the <code>messages</code> streaming mode to stream Large Language Model (LLM) outputs token by token from any part of your graph, including nodes, tools, subgraphs, or tasks.</p> <p>:::python The streamed output from <code>messages</code> mode is a tuple <code>(message_chunk, metadata)</code> where:</p> <ul> <li><code>message_chunk</code>: the token or message segment from the LLM.</li> <li><code>metadata</code>: a dictionary containing details about the graph node and LLM invocation.</li> </ul> <p>If your LLM is not available as a LangChain integration, you can stream its outputs using <code>custom</code> mode instead. See use with any LLM for details.</p> <p>Manual config required for async in Python &lt; 3.11</p> <p>When using Python &lt; 3.11 with async code, you must explicitly pass <code>RunnableConfig</code> to <code>ainvoke()</code> to enable proper streaming. See Async with Python &lt; 3.11 for details or upgrade to Python 3.11+.</p> <pre><code>from dataclasses import dataclass\n\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, START\n\n\n@dataclass\nclass MyState:\n    topic: str\n    joke: str = \"\"\n\n\nllm = init_chat_model(model=\"openai:gpt-4o-mini\")\n\ndef call_model(state: MyState):\n    \"\"\"Call the LLM to generate a joke about a topic\"\"\"\n    # highlight-next-line\n    llm_response = llm.invoke( # (1)!\n        [\n            {\"role\": \"user\", \"content\": f\"Generate a joke about {state.topic}\"}\n        ]\n    )\n    return {\"joke\": llm_response.content}\n\ngraph = (\n    StateGraph(MyState)\n    .add_node(call_model)\n    .add_edge(START, \"call_model\")\n    .compile()\n)\n\nfor message_chunk, metadata in graph.stream( # (2)!\n    {\"topic\": \"ice cream\"},\n    # highlight-next-line\n    stream_mode=\"messages\",\n):\n    if message_chunk.content:\n        print(message_chunk.content, end=\"|\", flush=True)\n</code></pre> <ol> <li>Note that the message events are emitted even when the LLM is run using <code>.invoke</code> rather than <code>.stream</code>.</li> <li>The \"messages\" stream mode returns an iterator of tuples <code>(message_chunk, metadata)</code> where <code>message_chunk</code> is the token streamed by the LLM and <code>metadata</code> is a dictionary with information about the graph node where the LLM was called and other information.    :::</li> </ol> <p>:::js The streamed output from <code>messages</code> mode is a tuple <code>[message_chunk, metadata]</code> where:</p> <ul> <li><code>message_chunk</code>: the token or message segment from the LLM.</li> <li><code>metadata</code>: a dictionary containing details about the graph node and LLM invocation.</li> </ul> <p>If your LLM is not available as a LangChain integration, you can stream its outputs using <code>custom</code> mode instead. See use with any LLM for details.</p> <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\nimport { StateGraph, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst MyState = z.object({\n  topic: z.string(),\n  joke: z.string().default(\"\"),\n});\n\nconst llm = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n\nconst callModel = async (state: z.infer&lt;typeof MyState&gt;) =&gt; {\n  // Call the LLM to generate a joke about a topic\n  const llmResponse = await llm.invoke([\n    { role: \"user\", content: `Generate a joke about ${state.topic}` },\n  ]); // (1)!\n  return { joke: llmResponse.content };\n};\n\nconst graph = new StateGraph(MyState)\n  .addNode(\"callModel\", callModel)\n  .addEdge(START, \"callModel\")\n  .compile();\n\nfor await (const [messageChunk, metadata] of await graph.stream(\n  // (2)!\n  { topic: \"ice cream\" },\n  { streamMode: \"messages\" }\n)) {\n  if (messageChunk.content) {\n    console.log(messageChunk.content + \"|\");\n  }\n}\n</code></pre> <ol> <li>Note that the message events are emitted even when the LLM is run using <code>.invoke</code> rather than <code>.stream</code>.</li> <li>The \"messages\" stream mode returns an iterator of tuples <code>[messageChunk, metadata]</code> where <code>messageChunk</code> is the token streamed by the LLM and <code>metadata</code> is a dictionary with information about the graph node where the LLM was called and other information.    :::</li> </ol>"}, {"location": "how-tos/streaming/#filter-by-llm-invocation", "title": "Filter by LLM invocation", "text": "<p>You can associate <code>tags</code> with LLM invocations to filter the streamed tokens by LLM invocation.</p> <p>:::python</p> <pre><code>from langchain.chat_models import init_chat_model\n\nllm_1 = init_chat_model(model=\"openai:gpt-4o-mini\", tags=['joke']) # (1)!\nllm_2 = init_chat_model(model=\"openai:gpt-4o-mini\", tags=['poem']) # (2)!\n\ngraph = ... # define a graph that uses these LLMs\n\nasync for msg, metadata in graph.astream(  # (3)!\n    {\"topic\": \"cats\"},\n    # highlight-next-line\n    stream_mode=\"messages\",\n):\n    if metadata[\"tags\"] == [\"joke\"]: # (4)!\n        print(msg.content, end=\"|\", flush=True)\n</code></pre> <ol> <li>llm_1 is tagged with \"joke\".</li> <li>llm_2 is tagged with \"poem\".</li> <li>The <code>stream_mode</code> is set to \"messages\" to stream LLM tokens. The <code>metadata</code> contains information about the LLM invocation, including the tags.</li> <li>Filter the streamed tokens by the <code>tags</code> field in the metadata to only include the tokens from the LLM invocation with the \"joke\" tag.    :::</li> </ol> <p>:::js</p> <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm1 = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  tags: ['joke'] // (1)!\n});\nconst llm2 = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  tags: ['poem'] // (2)!\n});\n\nconst graph = // ... define a graph that uses these LLMs\n\nfor await (const [msg, metadata] of await graph.stream( // (3)!\n  { topic: \"cats\" },\n  { streamMode: \"messages\" }\n)) {\n  if (metadata.tags?.includes(\"joke\")) { // (4)!\n    console.log(msg.content + \"|\");\n  }\n}\n</code></pre> <ol> <li>llm1 is tagged with \"joke\".</li> <li>llm2 is tagged with \"poem\".</li> <li>The <code>streamMode</code> is set to \"messages\" to stream LLM tokens. The <code>metadata</code> contains information about the LLM invocation, including the tags.</li> <li>Filter the streamed tokens by the <code>tags</code> field in the metadata to only include the tokens from the LLM invocation with the \"joke\" tag.    :::</li> </ol> Extended example: filtering by tags <p>:::python   <pre><code>from typing import TypedDict\n\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import START, StateGraph\n\njoke_model = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\"joke\"]) # (1)!\npoem_model = init_chat_model(model=\"openai:gpt-4o-mini\", tags=[\"poem\"]) # (2)!\n\n\nclass State(TypedDict):\n      topic: str\n      joke: str\n      poem: str\n\n\nasync def call_model(state, config):\n      topic = state[\"topic\"]\n      print(\"Writing joke...\")\n      # Note: Passing the config through explicitly is required for python &lt; 3.11\n      # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks\n      joke_response = await joke_model.ainvoke(\n            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n            config, # (3)!\n      )\n      print(\"\\n\\nWriting poem...\")\n      poem_response = await poem_model.ainvoke(\n            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}],\n            config, # (3)!\n      )\n      return {\"joke\": joke_response.content, \"poem\": poem_response.content}\n\n\ngraph = (\n      StateGraph(State)\n      .add_node(call_model)\n      .add_edge(START, \"call_model\")\n      .compile()\n)\n\nasync for msg, metadata in graph.astream(\n      {\"topic\": \"cats\"},\n      # highlight-next-line\n      stream_mode=\"messages\", # (4)!\n):\n    if metadata[\"tags\"] == [\"joke\"]: # (4)!\n        print(msg.content, end=\"|\", flush=True)\n</code></pre></p> <ol> <li>The <code>joke_model</code> is tagged with \"joke\".</li> <li>The <code>poem_model</code> is tagged with \"poem\".</li> <li>The <code>config</code> is passed through explicitly to ensure the context vars are propagated correctly. This is required for Python &lt; 3.11 when using async code. Please see the async section for more details.</li> <li>The <code>stream_mode</code> is set to \"messages\" to stream LLM tokens. The <code>metadata</code> contains information about the LLM invocation, including the tags.   :::</li> </ol> <p>:::js   <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\nimport { StateGraph, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst jokeModel = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  tags: [\"joke\"] // (1)!\n});\nconst poemModel = new ChatOpenAI({\n  model: \"gpt-4o-mini\",\n  tags: [\"poem\"] // (2)!\n});\n\nconst State = z.object({\n  topic: z.string(),\n  joke: z.string(),\n  poem: z.string(),\n});\n\nconst graph = new StateGraph(State)\n  .addNode(\"callModel\", (state) =&gt; {\n    const topic = state.topic;\n    console.log(\"Writing joke...\");\n\n    const jokeResponse = await jokeModel.invoke([\n      { role: \"user\", content: `Write a joke about ${topic}` }\n    ]);\n\n    console.log(\"\\n\\nWriting poem...\");\n    const poemResponse = await poemModel.invoke([\n      { role: \"user\", content: `Write a short poem about ${topic}` }\n    ]);\n\n    return {\n      joke: jokeResponse.content,\n      poem: poemResponse.content\n    };\n  })\n  .addEdge(START, \"callModel\")\n  .compile();\n\nfor await (const [msg, metadata] of await graph.stream(\n  { topic: \"cats\" },\n  { streamMode: \"messages\" } // (3)!\n)) {\n  if (metadata.tags?.includes(\"joke\")) { // (4)!\n    console.log(msg.content + \"|\");\n  }\n}\n</code></pre></p> <ol> <li>The <code>jokeModel</code> is tagged with \"joke\".</li> <li>The <code>poemModel</code> is tagged with \"poem\".</li> <li>The <code>streamMode</code> is set to \"messages\" to stream LLM tokens. The <code>metadata</code> contains information about the LLM invocation, including the tags.</li> <li>Filter the streamed tokens by the <code>tags</code> field in the metadata to only include the tokens from the LLM invocation with the \"joke\" tag.   :::</li> </ol>"}, {"location": "how-tos/streaming/#filter-by-node", "title": "Filter by node", "text": "<p>To stream tokens only from specific nodes, use <code>stream_mode=\"messages\"</code> and filter the outputs by the <code>langgraph_node</code> field in the streamed metadata:</p> <p>:::python</p> <pre><code>for msg, metadata in graph.stream( # (1)!\n    inputs,\n    # highlight-next-line\n    stream_mode=\"messages\",\n):\n    # highlight-next-line\n    if msg.content and metadata[\"langgraph_node\"] == \"some_node_name\": # (2)!\n        ...\n</code></pre> <ol> <li>The \"messages\" stream mode returns a tuple of <code>(message_chunk, metadata)</code> where <code>message_chunk</code> is the token streamed by the LLM and <code>metadata</code> is a dictionary with information about the graph node where the LLM was called and other information.</li> <li>Filter the streamed tokens by the <code>langgraph_node</code> field in the metadata to only include the tokens from the <code>write_poem</code> node.    :::</li> </ol> <p>:::js</p> <pre><code>for await (const [msg, metadata] of await graph.stream(\n  // (1)!\n  inputs,\n  { streamMode: \"messages\" }\n)) {\n  if (msg.content &amp;&amp; metadata.langgraph_node === \"some_node_name\") {\n    // (2)!\n    // ...\n  }\n}\n</code></pre> <ol> <li>The \"messages\" stream mode returns a tuple of <code>[messageChunk, metadata]</code> where <code>messageChunk</code> is the token streamed by the LLM and <code>metadata</code> is a dictionary with information about the graph node where the LLM was called and other information.</li> <li>Filter the streamed tokens by the <code>langgraph_node</code> field in the metadata to only include the tokens from the <code>writePoem</code> node.    :::</li> </ol> Extended example: streaming LLM tokens from specific nodes <p>:::python   <pre><code>from typing import TypedDict\nfrom langgraph.graph import START, StateGraph\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\n\nclass State(TypedDict):\n      topic: str\n      joke: str\n      poem: str\n\n\ndef write_joke(state: State):\n      topic = state[\"topic\"]\n      joke_response = model.invoke(\n            [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}]\n      )\n      return {\"joke\": joke_response.content}\n\n\ndef write_poem(state: State):\n      topic = state[\"topic\"]\n      poem_response = model.invoke(\n            [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}]\n      )\n      return {\"poem\": poem_response.content}\n\n\ngraph = (\n      StateGraph(State)\n      .add_node(write_joke)\n      .add_node(write_poem)\n      # write both the joke and the poem concurrently\n      .add_edge(START, \"write_joke\")\n      .add_edge(START, \"write_poem\")\n      .compile()\n)\n\n# highlight-next-line\nfor msg, metadata in graph.stream( # (1)!\n    {\"topic\": \"cats\"},\n    stream_mode=\"messages\",\n):\n    # highlight-next-line\n    if msg.content and metadata[\"langgraph_node\"] == \"write_poem\": # (2)!\n        print(msg.content, end=\"|\", flush=True)\n</code></pre></p> <ol> <li>The \"messages\" stream mode returns a tuple of <code>(message_chunk, metadata)</code> where <code>message_chunk</code> is the token streamed by the LLM and <code>metadata</code> is a dictionary with information about the graph node where the LLM was called and other information.</li> <li>Filter the streamed tokens by the <code>langgraph_node</code> field in the metadata to only include the tokens from the <code>write_poem</code> node.   :::</li> </ol> <p>:::js   <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\nimport { StateGraph, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst model = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n\nconst State = z.object({\n  topic: z.string(),\n  joke: z.string(),\n  poem: z.string(),\n});\n\nconst graph = new StateGraph(State)\n  .addNode(\"writeJoke\", async (state) =&gt; {\n    const topic = state.topic;\n    const jokeResponse = await model.invoke([\n      { role: \"user\", content: `Write a joke about ${topic}` }\n    ]);\n    return { joke: jokeResponse.content };\n  })\n  .addNode(\"writePoem\", async (state) =&gt; {\n    const topic = state.topic;\n    const poemResponse = await model.invoke([\n      { role: \"user\", content: `Write a short poem about ${topic}` }\n    ]);\n    return { poem: poemResponse.content };\n  })\n  // write both the joke and the poem concurrently\n  .addEdge(START, \"writeJoke\")\n  .addEdge(START, \"writePoem\")\n  .compile();\n\nfor await (const [msg, metadata] of await graph.stream( // (1)!\n  { topic: \"cats\" },\n  { streamMode: \"messages\" }\n)) {\n  if (msg.content &amp;&amp; metadata.langgraph_node === \"writePoem\") { // (2)!\n    console.log(msg.content + \"|\");\n  }\n}\n</code></pre></p> <ol> <li>The \"messages\" stream mode returns a tuple of <code>[messageChunk, metadata]</code> where <code>messageChunk</code> is the token streamed by the LLM and <code>metadata</code> is a dictionary with information about the graph node where the LLM was called and other information.</li> <li>Filter the streamed tokens by the <code>langgraph_node</code> field in the metadata to only include the tokens from the <code>writePoem</code> node.   :::</li> </ol>"}, {"location": "how-tos/streaming/#stream-custom-data", "title": "Stream custom data", "text": "<p>:::python To send custom user-defined data from inside a LangGraph node or tool, follow these steps:</p> <ol> <li>Use <code>get_stream_writer()</code> to access the stream writer and emit custom data.</li> <li>Set <code>stream_mode=\"custom\"</code> when calling <code>.stream()</code> or <code>.astream()</code> to get the custom data in the stream. You can combine multiple modes (e.g., <code>[\"updates\", \"custom\"]</code>), but at least one must be <code>\"custom\"</code>.</li> </ol> <p>No <code>get_stream_writer()</code> in async for Python &lt; 3.11</p> <p>In async code running on Python &lt; 3.11, <code>get_stream_writer()</code> will not work. Instead, add a <code>writer</code> parameter to your node or tool and pass it manually. See Async with Python &lt; 3.11 for usage examples.</p> nodetool <pre><code>from typing import TypedDict\nfrom langgraph.config import get_stream_writer\nfrom langgraph.graph import StateGraph, START\n\nclass State(TypedDict):\n    query: str\n    answer: str\n\ndef node(state: State):\n    writer = get_stream_writer()  # (1)!\n    writer({\"custom_key\": \"Generating custom data inside node\"}) # (2)!\n    return {\"answer\": \"some data\"}\n\ngraph = (\n    StateGraph(State)\n    .add_node(node)\n    .add_edge(START, \"node\")\n    .compile()\n)\n\ninputs = {\"query\": \"example\"}\n\n# Usage\nfor chunk in graph.stream(inputs, stream_mode=\"custom\"):  # (3)!\n    print(chunk)\n</code></pre> <ol> <li>Get the stream writer to send custom data.</li> <li>Emit a custom key-value pair (e.g., progress update).</li> <li>Set <code>stream_mode=\"custom\"</code> to receive the custom data in the stream.</li> </ol> <pre><code>from langchain_core.tools import tool\nfrom langgraph.config import get_stream_writer\n\n@tool\ndef query_database(query: str) -&gt; str:\n    \"\"\"Query the database.\"\"\"\n    writer = get_stream_writer() # (1)!\n    # highlight-next-line\n    writer({\"data\": \"Retrieved 0/100 records\", \"type\": \"progress\"}) # (2)!\n    # perform query\n    # highlight-next-line\n    writer({\"data\": \"Retrieved 100/100 records\", \"type\": \"progress\"}) # (3)!\n    return \"some-answer\"\n\n\ngraph = ... # define a graph that uses this tool\n\nfor chunk in graph.stream(inputs, stream_mode=\"custom\"): # (4)!\n    print(chunk)\n</code></pre> <ol> <li>Access the stream writer to send custom data.</li> <li>Emit a custom key-value pair (e.g., progress update).</li> <li>Emit another custom key-value pair.</li> <li>Set <code>stream_mode=\"custom\"</code> to receive the custom data in the stream.</li> </ol> <p>:::</p> <p>:::js To send custom user-defined data from inside a LangGraph node or tool, follow these steps:</p> <ol> <li>Use the <code>writer</code> parameter from the <code>LangGraphRunnableConfig</code> to emit custom data.</li> <li>Set <code>streamMode: \"custom\"</code> when calling <code>.stream()</code> to get the custom data in the stream. You can combine multiple modes (e.g., <code>[\"updates\", \"custom\"]</code>), but at least one must be <code>\"custom\"</code>.</li> </ol> nodetool <pre><code>import { StateGraph, START, LangGraphRunnableConfig } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({\n  query: z.string(),\n  answer: z.string(),\n});\n\nconst graph = new StateGraph(State)\n  .addNode(\"node\", async (state, config) =&gt; {\n    config.writer({ custom_key: \"Generating custom data inside node\" }); // (1)!\n    return { answer: \"some data\" };\n  })\n  .addEdge(START, \"node\")\n  .compile();\n\nconst inputs = { query: \"example\" };\n\n// Usage\nfor await (const chunk of await graph.stream(inputs, { streamMode: \"custom\" })) { // (2)!\n  console.log(chunk);\n}\n</code></pre> <ol> <li>Use the writer to emit a custom key-value pair (e.g., progress update).</li> <li>Set <code>streamMode: \"custom\"</code> to receive the custom data in the stream.</li> </ol> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { LangGraphRunnableConfig } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst queryDatabase = tool(\n  async (input, config: LangGraphRunnableConfig) =&gt; {\n    config.writer({ data: \"Retrieved 0/100 records\", type: \"progress\" }); // (1)!\n    // perform query\n    config.writer({ data: \"Retrieved 100/100 records\", type: \"progress\" }); // (2)!\n    return \"some-answer\";\n  },\n  {\n    name: \"query_database\",\n    description: \"Query the database.\",\n    schema: z.object({\n      query: z.string().describe(\"The query to execute.\"),\n    }),\n  }\n);\n\nconst graph = // ... define a graph that uses this tool\n\nfor await (const chunk of await graph.stream(inputs, { streamMode: \"custom\" })) { // (3)!\n  console.log(chunk);\n}\n</code></pre> <ol> <li>Use the writer to emit a custom key-value pair (e.g., progress update).</li> <li>Emit another custom key-value pair.</li> <li>Set <code>streamMode: \"custom\"</code> to receive the custom data in the stream.</li> </ol> <p>:::</p>"}, {"location": "how-tos/streaming/#use-with-any-llm", "title": "Use with any LLM", "text": "<p>:::python You can use <code>stream_mode=\"custom\"</code> to stream data from any LLM API \u2014 even if that API does not implement the LangChain chat model interface.</p> <p>This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.</p> <pre><code>from langgraph.config import get_stream_writer\n\ndef call_arbitrary_model(state):\n    \"\"\"Example node that calls an arbitrary model and streams the output\"\"\"\n    # highlight-next-line\n    writer = get_stream_writer() # (1)!\n    # Assume you have a streaming client that yields chunks\n    for chunk in your_custom_streaming_client(state[\"topic\"]): # (2)!\n        # highlight-next-line\n        writer({\"custom_llm_chunk\": chunk}) # (3)!\n    return {\"result\": \"completed\"}\n\ngraph = (\n    StateGraph(State)\n    .add_node(call_arbitrary_model)\n    # Add other nodes and edges as needed\n    .compile()\n)\n\nfor chunk in graph.stream(\n    {\"topic\": \"cats\"},\n    # highlight-next-line\n    stream_mode=\"custom\", # (4)!\n):\n    # The chunk will contain the custom data streamed from the llm\n    print(chunk)\n</code></pre> <ol> <li>Get the stream writer to send custom data.</li> <li>Generate LLM tokens using your custom streaming client.</li> <li>Use the writer to send custom data to the stream.</li> <li>Set <code>stream_mode=\"custom\"</code> to receive the custom data in the stream.    :::</li> </ol> <p>:::js You can use <code>streamMode: \"custom\"</code> to stream data from any LLM API \u2014 even if that API does not implement the LangChain chat model interface.</p> <p>This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups.</p> <pre><code>import { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n\nconst callArbitraryModel = async (\n  state: any,\n  config: LangGraphRunnableConfig\n) =&gt; {\n  // Example node that calls an arbitrary model and streams the output\n  // Assume you have a streaming client that yields chunks\n  for await (const chunk of yourCustomStreamingClient(state.topic)) {\n    // (1)!\n    config.writer({ custom_llm_chunk: chunk }); // (2)!\n  }\n  return { result: \"completed\" };\n};\n\nconst graph = new StateGraph(State)\n  .addNode(\"callArbitraryModel\", callArbitraryModel)\n  // Add other nodes and edges as needed\n  .compile();\n\nfor await (const chunk of await graph.stream(\n  { topic: \"cats\" },\n  { streamMode: \"custom\" } // (3)!\n)) {\n  // The chunk will contain the custom data streamed from the llm\n  console.log(chunk);\n}\n</code></pre> <ol> <li>Generate LLM tokens using your custom streaming client.</li> <li>Use the writer to send custom data to the stream.</li> <li>Set <code>streamMode: \"custom\"</code> to receive the custom data in the stream.    :::</li> </ol> Extended example: streaming arbitrary chat model <p>:::python   <pre><code>import operator\nimport json\n\nfrom typing import TypedDict\nfrom typing_extensions import Annotated\nfrom langgraph.graph import StateGraph, START\n\nfrom openai import AsyncOpenAI\n\nopenai_client = AsyncOpenAI()\nmodel_name = \"gpt-4o-mini\"\n\n\nasync def stream_tokens(model_name: str, messages: list[dict]):\n    response = await openai_client.chat.completions.create(\n        messages=messages, model=model_name, stream=True\n    )\n    role = None\n    async for chunk in response:\n        delta = chunk.choices[0].delta\n\n        if delta.role is not None:\n            role = delta.role\n\n        if delta.content:\n            yield {\"role\": role, \"content\": delta.content}\n\n\n# this is our tool\nasync def get_items(place: str) -&gt; str:\n    \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\"\n    writer = get_stream_writer()\n    response = \"\"\n    async for msg_chunk in stream_tokens(\n        model_name,\n        [\n            {\n                \"role\": \"user\",\n                \"content\": (\n                    \"Can you tell me what kind of items \"\n                    f\"i might find in the following place: '{place}'. \"\n                    \"List at least 3 such items separating them by a comma. \"\n                    \"And include a brief description of each item.\"\n                ),\n            }\n        ],\n    ):\n        response += msg_chunk[\"content\"]\n        writer(msg_chunk)\n\n    return response\n\n\nclass State(TypedDict):\n    messages: Annotated[list[dict], operator.add]\n\n\n# this is the tool-calling graph node\nasync def call_tool(state: State):\n    ai_message = state[\"messages\"][-1]\n    tool_call = ai_message[\"tool_calls\"][-1]\n\n    function_name = tool_call[\"function\"][\"name\"]\n    if function_name != \"get_items\":\n        raise ValueError(f\"Tool {function_name} not supported\")\n\n    function_arguments = tool_call[\"function\"][\"arguments\"]\n    arguments = json.loads(function_arguments)\n\n    function_response = await get_items(**arguments)\n    tool_message = {\n        \"tool_call_id\": tool_call[\"id\"],\n        \"role\": \"tool\",\n        \"name\": function_name,\n        \"content\": function_response,\n    }\n    return {\"messages\": [tool_message]}\n\n\ngraph = (\n    StateGraph(State)\n    .add_node(call_tool)\n    .add_edge(START, \"call_tool\")\n    .compile()\n)\n</code></pre></p> <p>Let's invoke the graph with an AI message that includes a tool call:</p> <p><pre><code>inputs = {\n    \"messages\": [\n        {\n            \"content\": None,\n            \"role\": \"assistant\",\n            \"tool_calls\": [\n                {\n                    \"id\": \"1\",\n                    \"function\": {\n                        \"arguments\": '{\"place\":\"bedroom\"}',\n                        \"name\": \"get_items\",\n                    },\n                    \"type\": \"function\",\n                }\n            ],\n        }\n    ]\n}\n\nasync for chunk in graph.astream(\n    inputs,\n    stream_mode=\"custom\",\n):\n    print(chunk[\"content\"], end=\"|\", flush=True)\n</code></pre>   :::</p> <p>:::js   <pre><code>import { StateGraph, START, LangGraphRunnableConfig } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\nimport OpenAI from \"openai\";\n\nconst openaiClient = new OpenAI();\nconst modelName = \"gpt-4o-mini\";\n\nasync function* streamTokens(modelName: string, messages: any[]) {\n  const response = await openaiClient.chat.completions.create({\n    messages,\n    model: modelName,\n    stream: true,\n  });\n\n  let role: string | null = null;\n  for await (const chunk of response) {\n    const delta = chunk.choices[0]?.delta;\n\n    if (delta?.role) {\n      role = delta.role;\n    }\n\n    if (delta?.content) {\n      yield { role, content: delta.content };\n    }\n  }\n}\n\n// this is our tool\nconst getItems = tool(\n  async (input, config: LangGraphRunnableConfig) =&gt; {\n    let response = \"\";\n    for await (const msgChunk of streamTokens(\n      modelName,\n      [\n        {\n          role: \"user\",\n          content: `Can you tell me what kind of items i might find in the following place: '${input.place}'. List at least 3 such items separating them by a comma. And include a brief description of each item.`,\n        },\n      ]\n    )) {\n      response += msgChunk.content;\n      config.writer?.(msgChunk);\n    }\n    return response;\n  },\n  {\n    name: \"get_items\",\n    description: \"Use this tool to list items one might find in a place you're asked about.\",\n    schema: z.object({\n      place: z.string().describe(\"The place to look up items for.\"),\n    }),\n  }\n);\n\nconst State = z.object({\n  messages: z.array(z.any()),\n});\n\nconst graph = new StateGraph(State)\n  // this is the tool-calling graph node\n  .addNode(\"callTool\", async (state) =&gt; {\n    const aiMessage = state.messages.at(-1);\n    const toolCall = aiMessage.tool_calls?.at(-1);\n\n    const functionName = toolCall?.function?.name;\n    if (functionName !== \"get_items\") {\n      throw new Error(`Tool ${functionName} not supported`);\n    }\n\n    const functionArguments = toolCall?.function?.arguments;\n    const args = JSON.parse(functionArguments);\n\n    const functionResponse = await getItems.invoke(args);\n    const toolMessage = {\n      tool_call_id: toolCall.id,\n      role: \"tool\",\n      name: functionName,\n      content: functionResponse,\n    };\n    return { messages: [toolMessage] };\n  })\n  .addEdge(START, \"callTool\")\n  .compile();\n</code></pre></p> <p>Let's invoke the graph with an AI message that includes a tool call:</p> <p><pre><code>const inputs = {\n  messages: [\n    {\n      content: null,\n      role: \"assistant\",\n      tool_calls: [\n        {\n          id: \"1\",\n          function: {\n            arguments: '{\"place\":\"bedroom\"}',\n            name: \"get_items\",\n          },\n          type: \"function\",\n        }\n      ],\n    }\n  ]\n};\n\nfor await (const chunk of await graph.stream(\n  inputs,\n  { streamMode: \"custom\" }\n)) {\n  console.log(chunk.content + \"|\");\n}\n</code></pre>   :::</p>"}, {"location": "how-tos/streaming/#disable-streaming-for-specific-chat-models", "title": "Disable streaming for specific chat models", "text": "<p>If your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for models that do not support it.</p> <p>:::python Set <code>disable_streaming=True</code> when initializing the model.</p> init_chat_modelchat model interface <pre><code>from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(\n    \"anthropic:claude-3-7-sonnet-latest\",\n    # highlight-next-line\n    disable_streaming=True # (1)!\n)\n</code></pre> <ol> <li>Set <code>disable_streaming=True</code> to disable streaming for the chat model.</li> </ol> <pre><code>from langchain_openai import ChatOpenAI\n\nllm = ChatOpenAI(model=\"o1-preview\", disable_streaming=True) # (1)!\n</code></pre> <ol> <li>Set <code>disable_streaming=True</code> to disable streaming for the chat model.</li> </ol> <p>:::</p> <p>:::js Set <code>streaming: false</code> when initializing the model.</p> <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({\n  model: \"o1-preview\",\n  streaming: false, // (1)!\n});\n</code></pre> <p>:::</p> <p>:::python</p>"}, {"location": "how-tos/streaming/#async", "title": "Async with Python &lt; 3.11", "text": "<p>In Python versions &lt; 3.11, asyncio tasks do not support the <code>context</code> parameter. This limits LangGraph ability to automatically propagate context, and affects LangGraph's streaming mechanisms in two key ways:</p> <ol> <li>You must explicitly pass <code>RunnableConfig</code> into async LLM calls (e.g., <code>ainvoke()</code>), as callbacks are not automatically propagated.</li> <li>You cannot use <code>get_stream_writer()</code> in async nodes or tools \u2014 you must pass a <code>writer</code> argument directly.</li> </ol> Extended example: async LLM call with manual config <pre><code>from typing import TypedDict\nfrom langgraph.graph import START, StateGraph\nfrom langchain.chat_models import init_chat_model\n\nllm = init_chat_model(model=\"openai:gpt-4o-mini\")\n\nclass State(TypedDict):\n    topic: str\n    joke: str\n\nasync def call_model(state, config): # (1)!\n    topic = state[\"topic\"]\n    print(\"Generating joke...\")\n    joke_response = await llm.ainvoke(\n        [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}],\n        # highlight-next-line\n        config, # (2)!\n    )\n    return {\"joke\": joke_response.content}\n\ngraph = (\n    StateGraph(State)\n    .add_node(call_model)\n    .add_edge(START, \"call_model\")\n    .compile()\n)\n\nasync for chunk, metadata in graph.astream(\n    {\"topic\": \"ice cream\"},\n    # highlight-next-line\n    stream_mode=\"messages\", # (3)!\n):\n    if chunk.content:\n        print(chunk.content, end=\"|\", flush=True)\n</code></pre> <ol> <li>Accept <code>config</code> as an argument in the async node function.</li> <li>Pass <code>config</code> to <code>llm.ainvoke()</code> to ensure proper context propagation.</li> <li>Set <code>stream_mode=\"messages\"</code> to stream LLM tokens.</li> </ol> Extended example: async custom streaming with stream writer <pre><code>from typing import TypedDict\nfrom langgraph.types import StreamWriter\n\nclass State(TypedDict):\n      topic: str\n      joke: str\n\n# highlight-next-line\nasync def generate_joke(state: State, writer: StreamWriter): # (1)!\n      writer({\"custom_key\": \"Streaming custom data while generating a joke\"})\n      return {\"joke\": f\"This is a joke about {state['topic']}\"}\n\ngraph = (\n      StateGraph(State)\n      .add_node(generate_joke)\n      .add_edge(START, \"generate_joke\")\n      .compile()\n)\n\nasync for chunk in graph.astream(\n      {\"topic\": \"ice cream\"},\n      # highlight-next-line\n      stream_mode=\"custom\", # (2)!\n):\n      print(chunk)\n</code></pre> <ol> <li>Add <code>writer</code> as an argument in the function signature of the async node or tool. LangGraph will automatically pass the stream writer to the function.</li> <li>Set <code>stream_mode=\"custom\"</code> to receive the custom data in the stream.</li> </ol> <p>:::</p>"}, {"location": "how-tos/subgraph/", "title": "Use subgraphs", "text": "<p>This guide explains the mechanics of using subgraphs. A common application of subgraphs is to build multi-agent systems.</p> <p>When adding subgraphs, you need to define how the parent graph and the subgraph communicate:</p> <ul> <li>Shared state schemas \u2014 parent and subgraph have shared state keys in their state schemas</li> <li>Different state schemas \u2014 no shared state keys in parent and subgraph schemas</li> </ul>"}, {"location": "how-tos/subgraph/#setup", "title": "Setup", "text": "<p>:::python <pre><code>pip install -U langgraph\n</code></pre> :::</p> <p>:::js <pre><code>npm install @langchain/langgraph\n</code></pre> :::</p> <p>Set up LangSmith for LangGraph development</p> <p>Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here.</p>"}, {"location": "how-tos/subgraph/#shared-state-schemas", "title": "Shared state schemas", "text": "<p>A common case is for the parent graph and subgraph to communicate over a shared state key (channel) in the schema. For example, in multi-agent systems, the agents often communicate over a shared messages key.</p> <p>If your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph:</p> <p>:::python 1. Define the subgraph workflow (<code>subgraph_builder</code> in the example below) and compile it 2. Pass compiled subgraph to the <code>.add_node</code> method when defining the parent graph workflow</p> <p><pre><code>from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\ngraph = builder.compile()\n</code></pre> :::</p> <p>:::js 1. Define the subgraph workflow (<code>subgraphBuilder</code> in the example below) and compile it 2. Pass compiled subgraph to the <code>.addNode</code> method when defining the parent graph workflow</p> <p><pre><code>import { StateGraph, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({\n  foo: z.string(),\n});\n\n// Subgraph\nconst subgraphBuilder = new StateGraph(State)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    return { foo: \"hi! \" + state.foo };\n  })\n  .addEdge(START, \"subgraphNode1\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Parent graph\nconst builder = new StateGraph(State)\n  .addNode(\"node1\", subgraph)\n  .addEdge(START, \"node1\");\n\nconst graph = builder.compile();\n</code></pre> :::</p> Full example: shared state schemas <p>:::python <pre><code>from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    foo: str  # (1)! \n    bar: str  # (2)!\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    # note that this node is using a state key ('bar') that is only available in the subgraph\n    # and is sending update on the shared state key ('foo')\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream({\"foo\": \"foo\"}):\n    print(chunk)\n</code></pre></p> <ol> <li>This key is shared with the parent graph state</li> <li>This key is private to the <code>SubgraphState</code> and is not visible to the parent graph</li> </ol> <p><pre><code>{'node_1': {'foo': 'hi! foo'}}\n{'node_2': {'foo': 'hi! foobar'}}\n</code></pre> :::</p> <p>:::js <pre><code>import { StateGraph, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\n// Define subgraph\nconst SubgraphState = z.object({\n  foo: z.string(),  // (1)! \n  bar: z.string(),  // (2)!\n});\n\nconst subgraphBuilder = new StateGraph(SubgraphState)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    return { bar: \"bar\" };\n  })\n  .addNode(\"subgraphNode2\", (state) =&gt; {\n    // note that this node is using a state key ('bar') that is only available in the subgraph\n    // and is sending update on the shared state key ('foo')\n    return { foo: state.foo + state.bar };\n  })\n  .addEdge(START, \"subgraphNode1\")\n  .addEdge(\"subgraphNode1\", \"subgraphNode2\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Define parent graph\nconst ParentState = z.object({\n  foo: z.string(),\n});\n\nconst builder = new StateGraph(ParentState)\n  .addNode(\"node1\", (state) =&gt; {\n    return { foo: \"hi! \" + state.foo };\n  })\n  .addNode(\"node2\", subgraph)\n  .addEdge(START, \"node1\")\n  .addEdge(\"node1\", \"node2\");\n\nconst graph = builder.compile();\n\nfor await (const chunk of await graph.stream({ foo: \"foo\" })) {\n  console.log(chunk);\n}\n</code></pre></p> <ol> <li>This key is shared with the parent graph state</li> <li>This key is private to the <code>SubgraphState</code> and is not visible to the parent graph</li> </ol> <p><pre><code>{ node1: { foo: 'hi! foo' } }\n{ node2: { foo: 'hi! foobar' } }\n</code></pre> :::</p>"}, {"location": "how-tos/subgraph/#different-state-schemas", "title": "Different state schemas", "text": "<p>For more complex systems you might want to define subgraphs that have a completely different schema from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a multi-agent system.</p> <p>If that's the case for your application, you need to define a node function that invokes the subgraph. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node.</p> <p>:::python <pre><code>from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\nclass SubgraphState(TypedDict):\n    bar: str\n\n# Subgraph\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"hi! \" + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nclass State(TypedDict):\n    foo: str\n\ndef call_subgraph(state: State):\n    subgraph_output = subgraph.invoke({\"bar\": state[\"foo\"]})  # (1)!\n    return {\"foo\": subgraph_output[\"bar\"]}  # (2)!\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", call_subgraph)\nbuilder.add_edge(START, \"node_1\")\ngraph = builder.compile()\n</code></pre></p> <ol> <li>Transform the state to the subgraph state</li> <li>Transform response back to the parent state :::</li> </ol> <p>:::js <pre><code>import { StateGraph, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst SubgraphState = z.object({\n  bar: z.string(),\n});\n\n// Subgraph\nconst subgraphBuilder = new StateGraph(SubgraphState)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    return { bar: \"hi! \" + state.bar };\n  })\n  .addEdge(START, \"subgraphNode1\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Parent graph\nconst State = z.object({\n  foo: z.string(),\n});\n\nconst builder = new StateGraph(State)\n  .addNode(\"node1\", async (state) =&gt; {\n    const subgraphOutput = await subgraph.invoke({ bar: state.foo }); // (1)!\n    return { foo: subgraphOutput.bar }; // (2)!\n  })\n  .addEdge(START, \"node1\");\n\nconst graph = builder.compile();\n</code></pre></p> <ol> <li>Transform the state to the subgraph state</li> <li>Transform response back to the parent state :::</li> </ol> Full example: different state schemas <p>:::python <pre><code>from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    # note that none of these keys are shared with the parent graph state\n    bar: str\n    baz: str\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"baz\": \"baz\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    return {\"bar\": state[\"bar\"] + state[\"baz\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\ndef node_2(state: ParentState):\n    response = subgraph.invoke({\"bar\": state[\"foo\"]})  # (1)!\n    return {\"foo\": response[\"bar\"]}  # (2)!\n\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream({\"foo\": \"foo\"}, subgraphs=True):\n    print(chunk)\n</code></pre></p> <ol> <li>Transform the state to the subgraph state</li> <li>Transform response back to the parent state</li> </ol> <p><pre><code>((), {'node_1': {'foo': 'hi! foo'}})\n(('node_2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7',), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})\n(('node_2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7',), {'grandchild_2': {'bar': 'hi! foobaz'}})\n((), {'node_2': {'foo': 'hi! foobaz'}})\n</code></pre> :::</p> <p>:::js <pre><code>import { StateGraph, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\n// Define subgraph\nconst SubgraphState = z.object({\n  // note that none of these keys are shared with the parent graph state\n  bar: z.string(),\n  baz: z.string(),\n});\n\nconst subgraphBuilder = new StateGraph(SubgraphState)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    return { baz: \"baz\" };\n  })\n  .addNode(\"subgraphNode2\", (state) =&gt; {\n    return { bar: state.bar + state.baz };\n  })\n  .addEdge(START, \"subgraphNode1\")\n  .addEdge(\"subgraphNode1\", \"subgraphNode2\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Define parent graph\nconst ParentState = z.object({\n  foo: z.string(),\n});\n\nconst builder = new StateGraph(ParentState)\n  .addNode(\"node1\", (state) =&gt; {\n    return { foo: \"hi! \" + state.foo };\n  })\n  .addNode(\"node2\", async (state) =&gt; {\n    const response = await subgraph.invoke({ bar: state.foo }); // (1)!\n    return { foo: response.bar }; // (2)!\n  })\n  .addEdge(START, \"node1\")\n  .addEdge(\"node1\", \"node2\");\n\nconst graph = builder.compile();\n\nfor await (const chunk of await graph.stream(\n  { foo: \"foo\" }, \n  { subgraphs: true }\n)) {\n  console.log(chunk);\n}\n</code></pre></p> <ol> <li>Transform the state to the subgraph state</li> <li>Transform response back to the parent state</li> </ol> <p><pre><code>[[], { node1: { foo: 'hi! foo' } }]\n[['node2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7'], { subgraphNode1: { baz: 'baz' } }]\n[['node2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7'], { subgraphNode2: { bar: 'hi! foobaz' } }]\n[[], { node2: { foo: 'hi! foobaz' } }]\n</code></pre> :::</p> Full example: different state schemas (two levels of subgraphs) <p>This is an example with two levels of subgraphs: parent -&gt; child -&gt; grandchild.</p> <p>:::python <pre><code># Grandchild graph\nfrom typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START, END\n\nclass GrandChildState(TypedDict):\n    my_grandchild_key: str\n\ndef grandchild_1(state: GrandChildState) -&gt; GrandChildState:\n    # NOTE: child or parent keys will not be accessible here\n    return {\"my_grandchild_key\": state[\"my_grandchild_key\"] + \", how are you\"}\n\n\ngrandchild = StateGraph(GrandChildState)\ngrandchild.add_node(\"grandchild_1\", grandchild_1)\n\ngrandchild.add_edge(START, \"grandchild_1\")\ngrandchild.add_edge(\"grandchild_1\", END)\n\ngrandchild_graph = grandchild.compile()\n\n# Child graph\nclass ChildState(TypedDict):\n    my_child_key: str\n\ndef call_grandchild_graph(state: ChildState) -&gt; ChildState:\n    # NOTE: parent or grandchild keys won't be accessible here\n    grandchild_graph_input = {\"my_grandchild_key\": state[\"my_child_key\"]}  # (1)!\n    grandchild_graph_output = grandchild_graph.invoke(grandchild_graph_input)\n    return {\"my_child_key\": grandchild_graph_output[\"my_grandchild_key\"] + \" today?\"}  # (2)!\n\nchild = StateGraph(ChildState)\nchild.add_node(\"child_1\", call_grandchild_graph)  # (3)!\nchild.add_edge(START, \"child_1\")\nchild.add_edge(\"child_1\", END)\nchild_graph = child.compile()\n\n# Parent graph\nclass ParentState(TypedDict):\n    my_key: str\n\ndef parent_1(state: ParentState) -&gt; ParentState:\n    # NOTE: child or grandchild keys won't be accessible here\n    return {\"my_key\": \"hi \" + state[\"my_key\"]}\n\ndef parent_2(state: ParentState) -&gt; ParentState:\n    return {\"my_key\": state[\"my_key\"] + \" bye!\"}\n\ndef call_child_graph(state: ParentState) -&gt; ParentState:\n    child_graph_input = {\"my_child_key\": state[\"my_key\"]}  # (4)!\n    child_graph_output = child_graph.invoke(child_graph_input)\n    return {\"my_key\": child_graph_output[\"my_child_key\"]}  # (5)!\n\nparent = StateGraph(ParentState)\nparent.add_node(\"parent_1\", parent_1)\nparent.add_node(\"child\", call_child_graph)  # (6)!\nparent.add_node(\"parent_2\", parent_2)\n\nparent.add_edge(START, \"parent_1\")\nparent.add_edge(\"parent_1\", \"child\")\nparent.add_edge(\"child\", \"parent_2\")\nparent.add_edge(\"parent_2\", END)\n\nparent_graph = parent.compile()\n\nfor chunk in parent_graph.stream({\"my_key\": \"Bob\"}, subgraphs=True):\n    print(chunk)\n</code></pre></p> <ol> <li>We're transforming the state from the child state channels (<code>my_child_key</code>) to the child state channels (<code>my_grandchild_key</code>)</li> <li>We're transforming the state from the grandchild state channels (<code>my_grandchild_key</code>) back to the child state channels (<code>my_child_key</code>)</li> <li>We're passing a function here instead of just compiled graph (<code>grandchild_graph</code>)</li> <li>We're transforming the state from the parent state channels (<code>my_key</code>) to the child state channels (<code>my_child_key</code>)</li> <li>We're transforming the state from the child state channels (<code>my_child_key</code>) back to the parent state channels (<code>my_key</code>)</li> <li>We're passing a function here instead of just a compiled graph (<code>child_graph</code>)</li> </ol> <p><pre><code>((), {'parent_1': {'my_key': 'hi Bob'}})\n(('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b', 'child_1:781bb3b1-3971-84ce-810b-acf819a03f9c'), {'grandchild_1': {'my_grandchild_key': 'hi Bob, how are you'}})\n(('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b',), {'child_1': {'my_child_key': 'hi Bob, how are you today?'}})\n((), {'child': {'my_key': 'hi Bob, how are you today?'}})\n((), {'parent_2': {'my_key': 'hi Bob, how are you today? bye!'}})\n</code></pre> :::</p> <p>:::js <pre><code>import { StateGraph, START, END } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\n// Grandchild graph\nconst GrandChildState = z.object({\n  myGrandchildKey: z.string(),\n});\n\nconst grandchild = new StateGraph(GrandChildState)\n  .addNode(\"grandchild1\", (state) =&gt; {\n    // NOTE: child or parent keys will not be accessible here\n    return { myGrandchildKey: state.myGrandchildKey + \", how are you\" };\n  })\n  .addEdge(START, \"grandchild1\")\n  .addEdge(\"grandchild1\", END);\n\nconst grandchildGraph = grandchild.compile();\n\n// Child graph\nconst ChildState = z.object({\n  myChildKey: z.string(),\n});\n\nconst child = new StateGraph(ChildState)\n  .addNode(\"child1\", async (state) =&gt; {\n    // NOTE: parent or grandchild keys won't be accessible here\n    const grandchildGraphInput = { myGrandchildKey: state.myChildKey }; // (1)!\n    const grandchildGraphOutput = await grandchildGraph.invoke(grandchildGraphInput);\n    return { myChildKey: grandchildGraphOutput.myGrandchildKey + \" today?\" }; // (2)!\n  }) // (3)!\n  .addEdge(START, \"child1\")\n  .addEdge(\"child1\", END);\n\nconst childGraph = child.compile();\n\n// Parent graph\nconst ParentState = z.object({\n  myKey: z.string(),\n});\n\nconst parent = new StateGraph(ParentState)\n  .addNode(\"parent1\", (state) =&gt; {\n    // NOTE: child or grandchild keys won't be accessible here\n    return { myKey: \"hi \" + state.myKey };\n  })\n  .addNode(\"child\", async (state) =&gt; {\n    const childGraphInput = { myChildKey: state.myKey }; // (4)!\n    const childGraphOutput = await childGraph.invoke(childGraphInput);\n    return { myKey: childGraphOutput.myChildKey }; // (5)!\n  }) // (6)!\n  .addNode(\"parent2\", (state) =&gt; {\n    return { myKey: state.myKey + \" bye!\" };\n  })\n  .addEdge(START, \"parent1\")\n  .addEdge(\"parent1\", \"child\")\n  .addEdge(\"child\", \"parent2\")\n  .addEdge(\"parent2\", END);\n\nconst parentGraph = parent.compile();\n\nfor await (const chunk of await parentGraph.stream(\n  { myKey: \"Bob\" }, \n  { subgraphs: true }\n)) {\n  console.log(chunk);\n}\n</code></pre></p> <ol> <li>We're transforming the state from the child state channels (<code>myChildKey</code>) to the grandchild state channels (<code>myGrandchildKey</code>)</li> <li>We're transforming the state from the grandchild state channels (<code>myGrandchildKey</code>) back to the child state channels (<code>myChildKey</code>)</li> <li>We're passing a function here instead of just compiled graph (<code>grandchildGraph</code>)</li> <li>We're transforming the state from the parent state channels (<code>myKey</code>) to the child state channels (<code>myChildKey</code>)</li> <li>We're transforming the state from the child state channels (<code>myChildKey</code>) back to the parent state channels (<code>myKey</code>)</li> <li>We're passing a function here instead of just a compiled graph (<code>childGraph</code>)</li> </ol> <p><pre><code>[[], { parent1: { myKey: 'hi Bob' } }]\n[['child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b', 'child1:781bb3b1-3971-84ce-810b-acf819a03f9c'], { grandchild1: { myGrandchildKey: 'hi Bob, how are you' } }]\n[['child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b'], { child1: { myChildKey: 'hi Bob, how are you today?' } }]\n[[], { child: { myKey: 'hi Bob, how are you today?' } }]\n[[], { parent2: { myKey: 'hi Bob, how are you today? bye!' } }]\n</code></pre> :::</p>"}, {"location": "how-tos/subgraph/#add-persistence", "title": "Add persistence", "text": "<p>You only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.</p> <p>:::python <pre><code>from langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n</code></pre> :::</p> <p>:::js <pre><code>import { StateGraph, START, MemorySaver } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({\n  foo: z.string(),\n});\n\n// Subgraph\nconst subgraphBuilder = new StateGraph(State)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    return { foo: state.foo + \"bar\" };\n  })\n  .addEdge(START, \"subgraphNode1\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Parent graph\nconst builder = new StateGraph(State)\n  .addNode(\"node1\", subgraph)\n  .addEdge(START, \"node1\");\n\nconst checkpointer = new MemorySaver();\nconst graph = builder.compile({ checkpointer });\n</code></pre> :::</p> <p>If you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories:</p> <p>:::python <pre><code>subgraph_builder = StateGraph(...)\nsubgraph = subgraph_builder.compile(checkpointer=True)\n</code></pre> :::</p> <p>:::js <pre><code>const subgraphBuilder = new StateGraph(...)\nconst subgraph = subgraphBuilder.compile({ checkpointer: true });\n</code></pre> :::</p>"}, {"location": "how-tos/subgraph/#view-subgraph-state", "title": "View subgraph state", "text": "<p>When you enable persistence, you can inspect the graph state (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option.</p> <p>:::python You can inspect the graph state via <code>graph.get_state(config)</code>. To view the subgraph state, you can use <code>graph.get_state(config, subgraphs=True)</code>. :::</p> <p>:::js You can inspect the graph state via <code>graph.getState(config)</code>. To view the subgraph state, you can use <code>graph.getState(config, { subgraphs: true })</code>. :::</p> <p>Available only when interrupted</p> <p>Subgraph state can only be viewed when the subgraph is interrupted. Once you resume the graph, you won't be able to access the subgraph state.</p> View interrupted subgraph state <p>:::python <pre><code>from langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import MemorySaver\nfrom langgraph.types import interrupt, Command\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    value = interrupt(\"Provide value:\")\n    return {\"foo\": state[\"foo\"] + value}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\n\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = MemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\ngraph.invoke({\"foo\": \"\"}, config)\nparent_state = graph.get_state(config)\nsubgraph_state = graph.get_state(config, subgraphs=True).tasks[0].state  # (1)!\n\n# resume the subgraph\ngraph.invoke(Command(resume=\"bar\"), config)\n</code></pre></p> <ol> <li>This will be available only when the subgraph is interrupted. Once you resume the graph, you won't be able to access the subgraph state. :::</li> </ol> <p>:::js <pre><code>import { StateGraph, START, MemorySaver, interrupt, Command } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({\n  foo: z.string(),\n});\n\n// Subgraph\nconst subgraphBuilder = new StateGraph(State)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    const value = interrupt(\"Provide value:\");\n    return { foo: state.foo + value };\n  })\n  .addEdge(START, \"subgraphNode1\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Parent graph\nconst builder = new StateGraph(State)\n  .addNode(\"node1\", subgraph)\n  .addEdge(START, \"node1\");\n\nconst checkpointer = new MemorySaver();\nconst graph = builder.compile({ checkpointer });\n\nconst config = { configurable: { thread_id: \"1\" } };\n\nawait graph.invoke({ foo: \"\" }, config);\nconst parentState = await graph.getState(config);\nconst subgraphState = (await graph.getState(config, { subgraphs: true })).tasks[0].state; // (1)!\n\n// resume the subgraph\nawait graph.invoke(new Command({ resume: \"bar\" }), config);\n</code></pre></p> <ol> <li>This will be available only when the subgraph is interrupted. Once you resume the graph, you won't be able to access the subgraph state. :::</li> </ol>"}, {"location": "how-tos/subgraph/#stream-subgraph-outputs", "title": "Stream subgraph outputs", "text": "<p>To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs.</p> <p>:::python <pre><code>for chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    subgraphs=True, # (1)!\n    stream_mode=\"updates\",\n):\n    print(chunk)\n</code></pre></p> <ol> <li>Set <code>subgraphs=True</code> to stream outputs from subgraphs. :::</li> </ol> <p>:::js <pre><code>for await (const chunk of await graph.stream(\n  { foo: \"foo\" },\n  {\n    subgraphs: true, // (1)!\n    streamMode: \"updates\",\n  }\n)) {\n  console.log(chunk);\n}\n</code></pre></p> <ol> <li>Set <code>subgraphs: true</code> to stream outputs from subgraphs. :::</li> </ol> Stream from subgraphs <p>:::python <pre><code>from typing_extensions import TypedDict\nfrom langgraph.graph.state import StateGraph, START\n\n# Define subgraph\nclass SubgraphState(TypedDict):\n    foo: str\n    bar: str\n\ndef subgraph_node_1(state: SubgraphState):\n    return {\"bar\": \"bar\"}\n\ndef subgraph_node_2(state: SubgraphState):\n    # note that this node is using a state key ('bar') that is only available in the subgraph\n    # and is sending update on the shared state key ('foo')\n    return {\"foo\": state[\"foo\"] + state[\"bar\"]}\n\nsubgraph_builder = StateGraph(SubgraphState)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_node(subgraph_node_2)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\nsubgraph_builder.add_edge(\"subgraph_node_1\", \"subgraph_node_2\")\nsubgraph = subgraph_builder.compile()\n\n# Define parent graph\nclass ParentState(TypedDict):\n    foo: str\n\ndef node_1(state: ParentState):\n    return {\"foo\": \"hi! \" + state[\"foo\"]}\n\nbuilder = StateGraph(ParentState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", subgraph)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\ngraph = builder.compile()\n\nfor chunk in graph.stream(\n    {\"foo\": \"foo\"},\n    stream_mode=\"updates\",\n    subgraphs=True, # (1)!\n):\n    print(chunk)\n</code></pre></p> <ol> <li>Set <code>subgraphs=True</code> to stream outputs from subgraphs.</li> </ol> <p><pre><code>((), {'node_1': {'foo': 'hi! foo'}})\n(('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_1': {'bar': 'bar'}})\n(('node_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph_node_2': {'foo': 'hi! foobar'}})\n((), {'node_2': {'foo': 'hi! foobar'}})\n</code></pre> :::</p> <p>:::js <pre><code>import { StateGraph, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\n// Define subgraph\nconst SubgraphState = z.object({\n  foo: z.string(),\n  bar: z.string(),\n});\n\nconst subgraphBuilder = new StateGraph(SubgraphState)\n  .addNode(\"subgraphNode1\", (state) =&gt; {\n    return { bar: \"bar\" };\n  })\n  .addNode(\"subgraphNode2\", (state) =&gt; {\n    // note that this node is using a state key ('bar') that is only available in the subgraph\n    // and is sending update on the shared state key ('foo')\n    return { foo: state.foo + state.bar };\n  })\n  .addEdge(START, \"subgraphNode1\")\n  .addEdge(\"subgraphNode1\", \"subgraphNode2\");\n\nconst subgraph = subgraphBuilder.compile();\n\n// Define parent graph\nconst ParentState = z.object({\n  foo: z.string(),\n});\n\nconst builder = new StateGraph(ParentState)\n  .addNode(\"node1\", (state) =&gt; {\n    return { foo: \"hi! \" + state.foo };\n  })\n  .addNode(\"node2\", subgraph)\n  .addEdge(START, \"node1\")\n  .addEdge(\"node1\", \"node2\");\n\nconst graph = builder.compile();\n\nfor await (const chunk of await graph.stream(\n  { foo: \"foo\" },\n  {\n    streamMode: \"updates\",\n    subgraphs: true, // (1)!\n  }\n)) {\n  console.log(chunk);\n}\n</code></pre></p> <ol> <li>Set <code>subgraphs: true</code> to stream outputs from subgraphs.</li> </ol> <p><pre><code>[[], { node1: { foo: 'hi! foo' } }]\n[['node2:e58e5673-a661-ebb0-70d4-e298a7fc28b7'], { subgraphNode1: { bar: 'bar' } }]\n[['node2:e58e5673-a661-ebb0-70d4-e298a7fc28b7'], { subgraphNode2: { foo: 'hi! foobar' } }]\n[[], { node2: { foo: 'hi! foobar' } }]\n</code></pre> :::</p>"}, {"location": "how-tos/tool-calling/", "title": "Call tools", "text": "<p>Tools encapsulate a callable function and its input schema. These can be passed to compatible chat models, allowing the model to decide whether to invoke a tool and determine the appropriate arguments.</p> <p>You can define your own tools or use prebuilt tools</p>"}, {"location": "how-tos/tool-calling/#define-a-tool", "title": "Define a tool", "text": "<p>:::python Define a basic tool with the @tool decorator:</p> <pre><code>from langchain_core.tools import tool\n\n# highlight-next-line\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n</code></pre> <p>:::</p> <p>:::js Define a basic tool with the tool function:</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\n// highlight-next-line\nconst multiply = tool(\n  (input) =&gt; {\n    return input.a * input.b;\n  },\n  {\n    name: \"multiply\",\n    description: \"Multiply two numbers.\",\n    schema: z.object({\n      a: z.number().describe(\"First operand\"),\n      b: z.number().describe(\"Second operand\"),\n    }),\n  }\n);\n</code></pre> <p>:::</p>"}, {"location": "how-tos/tool-calling/#run-a-tool", "title": "Run a tool", "text": "<p>Tools conform to the Runnable interface, which means you can run a tool using the <code>invoke</code> method:</p> <p>:::python</p> <pre><code>multiply.invoke({\"a\": 6, \"b\": 7})  # returns 42\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>await multiply.invoke({ a: 6, b: 7 }); // returns 42\n</code></pre> <p>:::</p> <p>If the tool is invoked with <code>type=\"tool_call\"</code>, it will return a ToolMessage:</p> <p>:::python</p> <pre><code>tool_call = {\n    \"type\": \"tool_call\",\n    \"id\": \"1\",\n    \"args\": {\"a\": 42, \"b\": 7}\n}\nmultiply.invoke(tool_call) # returns a ToolMessage object\n</code></pre> <p>Output:</p> <pre><code>ToolMessage(content='294', name='multiply', tool_call_id='1')\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const toolCall = {\n  type: \"tool_call\",\n  id: \"1\",\n  name: \"multiply\",\n  args: { a: 42, b: 7 },\n};\nawait multiply.invoke(toolCall); // returns a ToolMessage object\n</code></pre> <p>Output:</p> <pre><code>ToolMessage {\n  content: \"294\",\n  name: \"multiply\",\n  tool_call_id: \"1\"\n}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/tool-calling/#use-in-an-agent", "title": "Use in an agent", "text": "<p>:::python To create a tool-calling agent, you can use the prebuilt @[create_react_agent][create_react_agent]:</p> <pre><code>from langchain_core.tools import tool\n# highlight-next-line\nfrom langgraph.prebuilt import create_react_agent\n\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\n# highlight-next-line\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet\",\n    tools=[multiply]\n)\nagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]})\n</code></pre> <p>:::</p> <p>:::js To create a tool-calling agent, you can use the prebuilt createReactAgent:</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n// highlight-next-line\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst multiply = tool(\n  (input) =&gt; {\n    return input.a * input.b;\n  },\n  {\n    name: \"multiply\",\n    description: \"Multiply two numbers.\",\n    schema: z.object({\n      a: z.number().describe(\"First operand\"),\n      b: z.number().describe(\"Second operand\"),\n    }),\n  }\n);\n\n// highlight-next-line\nconst agent = createReactAgent({\n  llm: new ChatAnthropic({ model: \"claude-3-5-sonnet-20240620\" }),\n  tools: [multiply],\n});\n\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"what's 42 x 7?\" }],\n});\n</code></pre> <p>:::</p> <p>:::python</p>"}, {"location": "how-tos/tool-calling/#dynamically-select-tools", "title": "Dynamically select tools", "text": "<p>Configure tool availability at runtime based on context:</p> <pre><code>from dataclasses import dataclass\nfrom typing import Literal\n\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.tools import tool\n\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.runtime import Runtime\n\n\n@dataclass\nclass CustomContext:\n    tools: list[Literal[\"weather\", \"compass\"]]\n\n\n@tool\ndef weather() -&gt; str:\n    \"\"\"Returns the current weather conditions.\"\"\"\n    return \"It's nice and sunny.\"\n\n\n@tool\ndef compass() -&gt; str:\n    \"\"\"Returns the direction the user is facing.\"\"\"\n    return \"North\"\n\nmodel = init_chat_model(\"anthropic:claude-sonnet-4-20250514\")\n\n# highlight-next-line\ndef configure_model(state: AgentState, runtime: Runtime[CustomContext]):\n    \"\"\"Configure the model with tools based on runtime context.\"\"\"\n    selected_tools = [\n        tool\n        for tool in [weather, compass]\n        if tool.name in runtime.context.tools\n    ]\n    return model.bind_tools(selected_tools)\n\n\nagent = create_react_agent(\n    # Dynamically configure the model with tools based on runtime context\n    # highlight-next-line\n    configure_model,\n    # Initialize with all tools available\n    # highlight-next-line\n    tools=[weather, compass]\n)\n\noutput = agent.invoke(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"Who are you and what tools do you have access to?\",\n            }\n        ]\n    },\n    # highlight-next-line\n    context=CustomContext(tools=[\"weather\"]),  # Only enable the weather tool\n)\n\nprint(output[\"messages\"][-1].text())\n</code></pre> <p>New in langgraph&gt;=0.6</p> <p>:::</p>"}, {"location": "how-tos/tool-calling/#use-in-a-workflow", "title": "Use in a workflow", "text": "<p>If you are writing a custom workflow, you will need to:</p> <ol> <li>register the tools with the chat model</li> <li>call the tool if the model decides to use it</li> </ol> <p>:::python Use <code>model.bind_tools()</code> to register the tools with the model.</p> <pre><code>from langchain.chat_models import init_chat_model\n\nmodel = init_chat_model(model=\"claude-3-5-haiku-latest\")\n\n# highlight-next-line\nmodel_with_tools = model.bind_tools([multiply])\n</code></pre> <p>:::</p> <p>:::js Use <code>model.bindTools()</code> to register the tools with the model.</p> <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\n\nconst model = new ChatOpenAI({ model: \"gpt-4o\" });\n\n// highlight-next-line\nconst modelWithTools = model.bindTools([multiply]);\n</code></pre> <p>:::</p> <p>LLMs automatically determine if a tool invocation is necessary and handle calling the tool with the appropriate arguments.</p> Extended example: attach tools to a chat model <p>:::python <pre><code>from langchain_core.tools import tool\nfrom langchain.chat_models import init_chat_model\n\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\nmodel = init_chat_model(model=\"claude-3-5-haiku-latest\")\n# highlight-next-line\nmodel_with_tools = model.bind_tools([multiply])\n\nresponse_message = model_with_tools.invoke(\"what's 42 x 7?\")\ntool_call = response_message.tool_calls[0]\n\nmultiply.invoke(tool_call)\n</code></pre></p> <p><pre><code>ToolMessage(\n    content='294',\n    name='multiply',\n    tool_call_id='toolu_0176DV4YKSD8FndkeuuLj36c'\n)\n</code></pre> :::</p> <p>:::js <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { z } from \"zod\";\n\nconst multiply = tool(\n  (input) =&gt; {\n    return input.a * input.b;\n  },\n  {\n    name: \"multiply\",\n    description: \"Multiply two numbers.\",\n    schema: z.object({\n      a: z.number().describe(\"First operand\"),\n      b: z.number().describe(\"Second operand\"),\n    }),\n  }\n);\n\nconst model = new ChatOpenAI({ model: \"gpt-4o\" });\n// highlight-next-line\nconst modelWithTools = model.bindTools([multiply]);\n\nconst responseMessage = await modelWithTools.invoke(\"what's 42 x 7?\");\nconst toolCall = responseMessage.tool_calls[0];\n\nawait multiply.invoke(toolCall);\n</code></pre></p> <p><pre><code>ToolMessage {\n  content: \"294\",\n  name: \"multiply\",\n  tool_call_id: \"toolu_0176DV4YKSD8FndkeuuLj36c\"\n}\n</code></pre> :::</p>"}, {"location": "how-tos/tool-calling/#toolnode", "title": "ToolNode", "text": "<p>:::python To execute tools in custom workflows, use the prebuilt @[<code>ToolNode</code>][ToolNode] or implement your own custom node.</p> <p><code>ToolNode</code> is a specialized node for executing tools in a workflow. It provides the following features:</p> <ul> <li>Supports both synchronous and asynchronous tools.</li> <li>Executes multiple tools concurrently.</li> <li>Handles errors during tool execution (<code>handle_tool_errors=True</code>, enabled by default). See handling tool errors for more details.</li> </ul> <p><code>ToolNode</code> operates on <code>MessagesState</code>:</p> <ul> <li>Input: <code>MessagesState</code>, where the last message is an <code>AIMessage</code> containing the <code>tool_calls</code> parameter.</li> <li>Output: <code>MessagesState</code> updated with the resulting <code>ToolMessage</code> from executed tools.</li> </ul> <pre><code># highlight-next-line\nfrom langgraph.prebuilt import ToolNode\n\ndef get_weather(location: str):\n    \"\"\"Call to get the current weather.\"\"\"\n    if location.lower() in [\"sf\", \"san francisco\"]:\n        return \"It's 60 degrees and foggy.\"\n    else:\n        return \"It's 90 degrees and sunny.\"\n\ndef get_coolest_cities():\n    \"\"\"Get a list of coolest cities\"\"\"\n    return \"nyc, sf\"\n\n# highlight-next-line\ntool_node = ToolNode([get_weather, get_coolest_cities])\ntool_node.invoke({\"messages\": [...]})\n</code></pre> <p>:::</p> <p>:::js To execute tools in custom workflows, use the prebuilt <code>ToolNode</code> or implement your own custom node.</p> <p><code>ToolNode</code> is a specialized node for executing tools in a workflow. It provides the following features:</p> <ul> <li>Supports both synchronous and asynchronous tools.</li> <li>Executes multiple tools concurrently.</li> <li> <p>Handles errors during tool execution (<code>handleToolErrors: true</code>, enabled by default). See handling tool errors for more details.</p> </li> <li> <p>Input: <code>MessagesZodState</code>, where the last message is an <code>AIMessage</code> containing the <code>tool_calls</code> parameter.</p> </li> <li>Output: <code>MessagesZodState</code> updated with the resulting <code>ToolMessage</code> from executed tools.</li> </ul> <pre><code>// highlight-next-line\nimport { ToolNode } from \"@langchain/langgraph/prebuilt\";\n\nconst getWeather = tool(\n  (input) =&gt; {\n    if ([\"sf\", \"san francisco\"].includes(input.location.toLowerCase())) {\n      return \"It's 60 degrees and foggy.\";\n    } else {\n      return \"It's 90 degrees and sunny.\";\n    }\n  },\n  {\n    name: \"get_weather\",\n    description: \"Call to get the current weather.\",\n    schema: z.object({\n      location: z.string().describe(\"Location to get the weather for.\"),\n    }),\n  }\n);\n\nconst getCoolestCities = tool(\n  () =&gt; {\n    return \"nyc, sf\";\n  },\n  {\n    name: \"get_coolest_cities\",\n    description: \"Get a list of coolest cities\",\n    schema: z.object({\n      noOp: z.string().optional().describe(\"No-op parameter.\"),\n    }),\n  }\n);\n\n// highlight-next-line\nconst toolNode = new ToolNode([getWeather, getCoolestCities]);\nawait toolNode.invoke({ messages: [...] });\n</code></pre> <p>:::</p> Single tool call <p>:::python <pre><code>from langchain_core.messages import AIMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Define tools\n@tool\ndef get_weather(location: str):\n    \"\"\"Call to get the current weather.\"\"\"\n    if location.lower() in [\"sf\", \"san francisco\"]:\n        return \"It's 60 degrees and foggy.\"\n    else:\n        return \"It's 90 degrees and sunny.\"\n\n# highlight-next-line\ntool_node = ToolNode([get_weather])\n\nmessage_with_single_tool_call = AIMessage(\n    content=\"\",\n    tool_calls=[\n        {\n            \"name\": \"get_weather\",\n            \"args\": {\"location\": \"sf\"},\n            \"id\": \"tool_call_id\",\n            \"type\": \"tool_call\",\n        }\n    ],\n)\n\ntool_node.invoke({\"messages\": [message_with_single_tool_call]})\n</code></pre></p> <p><pre><code>{'messages': [ToolMessage(content=\"It's 60 degrees and foggy.\", name='get_weather', tool_call_id='tool_call_id')]}\n</code></pre> :::</p> <p>:::js <pre><code>import { AIMessage } from \"@langchain/core/messages\";\nimport { ToolNode } from \"@langchain/langgraph/prebuilt\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\n// Define tools\nconst getWeather = tool(\n  (input) =&gt; {\n    if ([\"sf\", \"san francisco\"].includes(input.location.toLowerCase())) {\n      return \"It's 60 degrees and foggy.\";\n    } else {\n      return \"It's 90 degrees and sunny.\";\n    }\n  },\n  {\n    name: \"get_weather\",\n    description: \"Call to get the current weather.\",\n    schema: z.object({\n      location: z.string().describe(\"Location to get the weather for.\"),\n    }),\n  }\n);\n\n// highlight-next-line\nconst toolNode = new ToolNode([getWeather]);\n\nconst messageWithSingleToolCall = new AIMessage({\n  content: \"\",\n  tool_calls: [\n    {\n      name: \"get_weather\",\n      args: { location: \"sf\" },\n      id: \"tool_call_id\",\n      type: \"tool_call\",\n    }\n  ],\n});\n\nawait toolNode.invoke({ messages: [messageWithSingleToolCall] });\n</code></pre></p> <p><pre><code>{ messages: [ToolMessage { content: \"It's 60 degrees and foggy.\", name: \"get_weather\", tool_call_id: \"tool_call_id\" }] }\n</code></pre> :::</p> Multiple tool calls <p>:::python <pre><code>from langchain_core.messages import AIMessage\nfrom langgraph.prebuilt import ToolNode\n\n# Define tools\n\ndef get_weather(location: str):\n    \"\"\"Call to get the current weather.\"\"\"\n    if location.lower() in [\"sf\", \"san francisco\"]:\n        return \"It's 60 degrees and foggy.\"\n    else:\n        return \"It's 90 degrees and sunny.\"\n\ndef get_coolest_cities():\n    \"\"\"Get a list of coolest cities\"\"\"\n    return \"nyc, sf\"\n\n# highlight-next-line\ntool_node = ToolNode([get_weather, get_coolest_cities])\n\nmessage_with_multiple_tool_calls = AIMessage(\n    content=\"\",\n    tool_calls=[\n        {\n            \"name\": \"get_coolest_cities\",\n            \"args\": {},\n            \"id\": \"tool_call_id_1\",\n            \"type\": \"tool_call\",\n        },\n        {\n            \"name\": \"get_weather\",\n            \"args\": {\"location\": \"sf\"},\n            \"id\": \"tool_call_id_2\",\n            \"type\": \"tool_call\",\n        },\n    ],\n)\n\n# highlight-next-line\ntool_node.invoke({\"messages\": [message_with_multiple_tool_calls]})  # (1)!\n</code></pre></p> <ol> <li><code>ToolNode</code> will execute both tools in parallel</li> </ol> <p><pre><code>{\n    'messages': [\n        ToolMessage(content='nyc, sf', name='get_coolest_cities', tool_call_id='tool_call_id_1'),\n        ToolMessage(content=\"It's 60 degrees and foggy.\", name='get_weather', tool_call_id='tool_call_id_2')\n    ]\n}\n</code></pre> :::</p> <p>:::js <pre><code>import { AIMessage } from \"@langchain/core/messages\";\nimport { ToolNode } from \"@langchain/langgraph/prebuilt\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\n// Define tools\nconst getWeather = tool(\n  (input) =&gt; {\n    if ([\"sf\", \"san francisco\"].includes(input.location.toLowerCase())) {\n      return \"It's 60 degrees and foggy.\";\n    } else {\n      return \"It's 90 degrees and sunny.\";\n    }\n  },\n  {\n    name: \"get_weather\",\n    description: \"Call to get the current weather.\",\n    schema: z.object({\n      location: z.string().describe(\"Location to get the weather for.\"),\n    }),\n  }\n);\n\nconst getCoolestCities = tool(\n  () =&gt; {\n    return \"nyc, sf\";\n  },\n  {\n    name: \"get_coolest_cities\",\n    description: \"Get a list of coolest cities\",\n    schema: z.object({\n      noOp: z.string().optional().describe(\"No-op parameter.\"),\n    }),\n  }\n);\n\n// highlight-next-line\nconst toolNode = new ToolNode([getWeather, getCoolestCities]);\n\nconst messageWithMultipleToolCalls = new AIMessage({\n  content: \"\",\n  tool_calls: [\n    {\n      name: \"get_coolest_cities\",\n      args: {},\n      id: \"tool_call_id_1\",\n      type: \"tool_call\",\n    },\n    {\n      name: \"get_weather\",\n      args: { location: \"sf\" },\n      id: \"tool_call_id_2\",\n      type: \"tool_call\",\n    },\n  ],\n});\n\n// highlight-next-line\nawait toolNode.invoke({ messages: [messageWithMultipleToolCalls] }); // (1)!\n</code></pre></p> <ol> <li><code>ToolNode</code> will execute both tools in parallel</li> </ol> <p><pre><code>{\n  messages: [\n    ToolMessage { content: \"nyc, sf\", name: \"get_coolest_cities\", tool_call_id: \"tool_call_id_1\" },\n    ToolMessage { content: \"It's 60 degrees and foggy.\", name: \"get_weather\", tool_call_id: \"tool_call_id_2\" }\n  ]\n}\n</code></pre> :::</p> Use with a chat model <p>:::python <pre><code>from langchain.chat_models import init_chat_model\nfrom langgraph.prebuilt import ToolNode\n\ndef get_weather(location: str):\n    \"\"\"Call to get the current weather.\"\"\"\n    if location.lower() in [\"sf\", \"san francisco\"]:\n        return \"It's 60 degrees and foggy.\"\n    else:\n        return \"It's 90 degrees and sunny.\"\n\n# highlight-next-line\ntool_node = ToolNode([get_weather])\n\nmodel = init_chat_model(model=\"claude-3-5-haiku-latest\")\n# highlight-next-line\nmodel_with_tools = model.bind_tools([get_weather])  # (1)!\n\n\n# highlight-next-line\nresponse_message = model_with_tools.invoke(\"what's the weather in sf?\")\ntool_node.invoke({\"messages\": [response_message]})\n</code></pre></p> <ol> <li>Use <code>.bind_tools()</code> to attach the tool schema to the chat model</li> </ol> <p><pre><code>{'messages': [ToolMessage(content=\"It's 60 degrees and foggy.\", name='get_weather', tool_call_id='toolu_01Pnkgw5JeTRxXAU7tyHT4UW')]}\n</code></pre> :::</p> <p>:::js <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\nimport { ToolNode } from \"@langchain/langgraph/prebuilt\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst getWeather = tool(\n  (input) =&gt; {\n    if ([\"sf\", \"san francisco\"].includes(input.location.toLowerCase())) {\n      return \"It's 60 degrees and foggy.\";\n    } else {\n      return \"It's 90 degrees and sunny.\";\n    }\n  },\n  {\n    name: \"get_weather\",\n    description: \"Call to get the current weather.\",\n    schema: z.object({\n      location: z.string().describe(\"Location to get the weather for.\"),\n    }),\n  }\n);\n\n// highlight-next-line\nconst toolNode = new ToolNode([getWeather]);\n\nconst model = new ChatOpenAI({ model: \"gpt-4o\" });\n// highlight-next-line\nconst modelWithTools = model.bindTools([getWeather]); // (1)!\n\n// highlight-next-line\nconst responseMessage = await modelWithTools.invoke(\"what's the weather in sf?\");\nawait toolNode.invoke({ messages: [responseMessage] });\n</code></pre></p> <ol> <li>Use <code>.bindTools()</code> to attach the tool schema to the chat model</li> </ol> <p><pre><code>{ messages: [ToolMessage { content: \"It's 60 degrees and foggy.\", name: \"get_weather\", tool_call_id: \"toolu_01Pnkgw5JeTRxXAU7tyHT4UW\" }] }\n</code></pre> :::</p> Use in a tool-calling agent <p>This is an example of creating a tool-calling agent from scratch using <code>ToolNode</code>. You can also use LangGraph's prebuilt agent.</p> <p>:::python <pre><code>from langchain.chat_models import init_chat_model\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.graph import StateGraph, MessagesState, START, END\n\ndef get_weather(location: str):\n    \"\"\"Call to get the current weather.\"\"\"\n    if location.lower() in [\"sf\", \"san francisco\"]:\n        return \"It's 60 degrees and foggy.\"\n    else:\n        return \"It's 90 degrees and sunny.\"\n\n# highlight-next-line\ntool_node = ToolNode([get_weather])\n\nmodel = init_chat_model(model=\"claude-3-5-haiku-latest\")\n# highlight-next-line\nmodel_with_tools = model.bind_tools([get_weather])\n\ndef should_continue(state: MessagesState):\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if last_message.tool_calls:\n        return \"tools\"\n    return END\n\ndef call_model(state: MessagesState):\n    messages = state[\"messages\"]\n    response = model_with_tools.invoke(messages)\n    return {\"messages\": [response]}\n\nbuilder = StateGraph(MessagesState)\n\n# Define the two nodes we will cycle between\nbuilder.add_node(\"call_model\", call_model)\n# highlight-next-line\nbuilder.add_node(\"tools\", tool_node)\n\nbuilder.add_edge(START, \"call_model\")\nbuilder.add_conditional_edges(\"call_model\", should_continue, [\"tools\", END])\nbuilder.add_edge(\"tools\", \"call_model\")\n\ngraph = builder.compile()\n\ngraph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's the weather in sf?\"}]})\n</code></pre></p> <p><pre><code>{\n    'messages': [\n        HumanMessage(content=\"what's the weather in sf?\"),\n        AIMessage(\n            content=[{'text': \"I'll help you check the weather in San Francisco right now.\", 'type': 'text'}, {'id': 'toolu_01A4vwUEgBKxfFVc5H3v1CNs', 'input': {'location': 'San Francisco'}, 'name': 'get_weather', 'type': 'tool_use'}],\n            tool_calls=[{'name': 'get_weather', 'args': {'location': 'San Francisco'}, 'id': 'toolu_01A4vwUEgBKxfFVc5H3v1CNs', 'type': 'tool_call'}]\n        ),\n        ToolMessage(content=\"It's 60 degrees and foggy.\"),\n        AIMessage(content=\"The current weather in San Francisco is 60 degrees and foggy. Typical San Francisco weather with its famous marine layer!\")\n    ]\n}\n</code></pre> :::</p> <p>:::js <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\nimport { ToolNode } from \"@langchain/langgraph/prebuilt\";\nimport { StateGraph, MessagesZodState, START, END } from \"@langchain/langgraph\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { isAIMessage } from \"@langchain/core/messages\";\n\nconst getWeather = tool(\n  (input) =&gt; {\n    if ([\"sf\", \"san francisco\"].includes(input.location.toLowerCase())) {\n      return \"It's 60 degrees and foggy.\";\n    } else {\n      return \"It's 90 degrees and sunny.\";\n    }\n  },\n  {\n    name: \"get_weather\",\n    description: \"Call to get the current weather.\",\n    schema: z.object({\n      location: z.string().describe(\"Location to get the weather for.\"),\n    }),\n  }\n);\n\n// highlight-next-line\nconst toolNode = new ToolNode([getWeather]);\n\nconst model = new ChatOpenAI({ model: \"gpt-4o\" });\n// highlight-next-line\nconst modelWithTools = model.bindTools([getWeather]);\n\nconst shouldContinue = (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const messages = state.messages;\n  const lastMessage = messages.at(-1);\n  if (lastMessage &amp;&amp; isAIMessage(lastMessage) &amp;&amp; lastMessage.tool_calls?.length) {\n    return \"tools\";\n  }\n  return END;\n};\n\nconst callModel = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const messages = state.messages;\n  const response = await modelWithTools.invoke(messages);\n  return { messages: [response] };\n};\n\nconst builder = new StateGraph(MessagesZodState)\n  // Define the two nodes we will cycle between\n  .addNode(\"agent\", callModel)\n  // highlight-next-line\n  .addNode(\"tools\", toolNode)\n  .addEdge(START, \"agent\")\n  .addConditionalEdges(\"agent\", shouldContinue, [\"tools\", END])\n  .addEdge(\"tools\", \"agent\");\n\nconst graph = builder.compile();\n\nawait graph.invoke({\n  messages: [{ role: \"user\", content: \"what's the weather in sf?\" }]\n});\n</code></pre></p> <p><pre><code>{\n  messages: [\n    HumanMessage { content: \"what's the weather in sf?\" },\n    AIMessage {\n      content: [{ text: \"I'll help you check the weather in San Francisco right now.\", type: \"text\" }, { id: \"toolu_01A4vwUEgBKxfFVc5H3v1CNs\", input: { location: \"San Francisco\" }, name: \"get_weather\", type: \"tool_use\" }],\n      tool_calls: [{ name: \"get_weather\", args: { location: \"San Francisco\" }, id: \"toolu_01A4vwUEgBKxfFVc5H3v1CNs\", type: \"tool_call\" }]\n    },\n    ToolMessage { content: \"It's 60 degrees and foggy.\" },\n    AIMessage { content: \"The current weather in San Francisco is 60 degrees and foggy. Typical San Francisco weather with its famous marine layer!\" }\n  ]\n}\n</code></pre> :::</p>"}, {"location": "how-tos/tool-calling/#tool-customization", "title": "Tool customization", "text": "<p>For more control over tool behavior, use the <code>@tool</code> decorator.</p>"}, {"location": "how-tos/tool-calling/#parameter-descriptions", "title": "Parameter descriptions", "text": "<p>:::python Auto-generate descriptions from docstrings:</p> <pre><code># highlight-next-line\nfrom langchain_core.tools import tool\n\n# highlight-next-line\n@tool(\"multiply_tool\", parse_docstring=True)\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\n\n    Args:\n        a: First operand\n        b: Second operand\n    \"\"\"\n    return a * b\n</code></pre> <p>:::</p> <p>:::js Auto-generate descriptions from schema:</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\n// highlight-next-line\nconst multiply = tool(\n  (input) =&gt; {\n    return input.a * input.b;\n  },\n  {\n    name: \"multiply\",\n    description: \"Multiply two numbers.\",\n    schema: z.object({\n      a: z.number().describe(\"First operand\"),\n      b: z.number().describe(\"Second operand\"),\n    }),\n  }\n);\n</code></pre> <p>:::</p>"}, {"location": "how-tos/tool-calling/#explicit-input-schema", "title": "Explicit input schema", "text": "<p>:::python Define schemas using <code>args_schema</code>:</p> <pre><code>from pydantic import BaseModel, Field\nfrom langchain_core.tools import tool\n\nclass MultiplyInputSchema(BaseModel):\n    \"\"\"Multiply two numbers\"\"\"\n    a: int = Field(description=\"First operand\")\n    b: int = Field(description=\"Second operand\")\n\n# highlight-next-line\n@tool(\"multiply_tool\", args_schema=MultiplyInputSchema)\ndef multiply(a: int, b: int) -&gt; int:\n    return a * b\n</code></pre> <p>:::</p>"}, {"location": "how-tos/tool-calling/#tool-name", "title": "Tool name", "text": "<p>Override the default tool name using the first argument or name property:</p> <p>:::python</p> <pre><code>from langchain_core.tools import tool\n\n# highlight-next-line\n@tool(\"multiply_tool\")\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\n// highlight-next-line\nconst multiply = tool(\n  (input) =&gt; {\n    return input.a * input.b;\n  },\n  {\n    name: \"multiply_tool\", // Custom name\n    description: \"Multiply two numbers.\",\n    schema: z.object({\n      a: z.number().describe(\"First operand\"),\n      b: z.number().describe(\"Second operand\"),\n    }),\n  }\n);\n</code></pre> <p>:::</p>"}, {"location": "how-tos/tool-calling/#context-management", "title": "Context management", "text": "<p>Tools within LangGraph sometimes require context data, such as runtime-only arguments (e.g., user IDs or session details), that should not be controlled by the model. LangGraph provides three methods for managing such context:</p> Type Usage Scenario Mutable Lifetime Configuration Static, immutable runtime data \u274c Single invocation Short-term memory Dynamic, changing data during invocation \u2705 Single invocation Long-term memory Persistent, cross-session data \u2705 Across multiple sessions"}, {"location": "how-tos/tool-calling/#configuration", "title": "Configuration", "text": "<p>:::python Use configuration when you have immutable runtime data that tools require, such as user identifiers. You pass these arguments via <code>RunnableConfig</code> at invocation and access them in the tool:</p> <pre><code>from langchain_core.tools import tool\nfrom langchain_core.runnables import RunnableConfig\n\n@tool\n# highlight-next-line\ndef get_user_info(config: RunnableConfig) -&gt; str:\n    \"\"\"Retrieve user information based on user ID.\"\"\"\n    user_id = config[\"configurable\"].get(\"user_id\")\n    return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n\n# Invocation example with an agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user info\"}]},\n    # highlight-next-line\n    config={\"configurable\": {\"user_id\": \"user_123\"}}\n)\n</code></pre> <p>:::</p> <p>:::js Use configuration when you have immutable runtime data that tools require, such as user identifiers. You pass these arguments via <code>LangGraphRunnableConfig</code> at invocation and access them in the tool:</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport type { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n\nconst getUserInfo = tool(\n  // highlight-next-line\n  async (_, config: LangGraphRunnableConfig) =&gt; {\n    const userId = config?.configurable?.user_id;\n    return userId === \"user_123\" ? \"User is John Smith\" : \"Unknown user\";\n  },\n  {\n    name: \"get_user_info\",\n    description: \"Retrieve user information based on user ID.\",\n    schema: z.object({}),\n  }\n);\n\n// Invocation example with an agent\nawait agent.invoke(\n  { messages: [{ role: \"user\", content: \"look up user info\" }] },\n  // highlight-next-line\n  { configurable: { user_id: \"user_123\" } }\n);\n</code></pre> <p>:::</p> Extended example: Access config in tools <p>:::python <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import create_react_agent\n\ndef get_user_info(\n    # highlight-next-line\n    config: RunnableConfig,\n) -&gt; str:\n    \"\"\"Look up user info.\"\"\"\n    # highlight-next-line\n    user_id = config[\"configurable\"].get(\"user_id\")\n    return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_user_info],\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n    # highlight-next-line\n    config={\"configurable\": {\"user_id\": \"user_123\"}}\n)\n</code></pre> :::</p> <p>:::js <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport type { LangGraphRunnableConfig } from \"@langchain/langgraph\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst getUserInfo = tool(\n  // highlight-next-line\n  async (_, config: LangGraphRunnableConfig) =&gt; {\n    // highlight-next-line\n    const userId = config?.configurable?.user_id;\n    return userId === \"user_123\" ? \"User is John Smith\" : \"Unknown user\";\n  },\n  {\n    name: \"get_user_info\",\n    description: \"Look up user info.\",\n    schema: z.object({}),\n  }\n);\n\nconst agent = createReactAgent({\n  llm: new ChatAnthropic({ model: \"claude-3-5-sonnet-20240620\" }),\n  tools: [getUserInfo],\n});\n\nawait agent.invoke(\n  { messages: [{ role: \"user\", content: \"look up user information\" }] },\n  // highlight-next-line\n  { configurable: { user_id: \"user_123\" } }\n);\n</code></pre> :::</p>"}, {"location": "how-tos/tool-calling/#short-term-memory", "title": "Short-term memory", "text": "<p>Short-term memory maintains dynamic state that changes during a single execution.</p> <p>:::python To access (read) the graph state inside the tools, you can use a special parameter annotation \u2014 @[<code>InjectedState</code>][InjectedState]:</p> <pre><code>from typing import Annotated, NotRequired\nfrom langchain_core.tools import tool\nfrom langgraph.prebuilt import InjectedState, create_react_agent\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\n\nclass CustomState(AgentState):\n    # The user_name field in short-term state\n    user_name: NotRequired[str]\n\n@tool\ndef get_user_name(\n    # highlight-next-line\n    state: Annotated[CustomState, InjectedState]\n) -&gt; str:\n    \"\"\"Retrieve the current user-name from state.\"\"\"\n    # Return stored name or a default if not set\n    return state.get(\"user_name\", \"Unknown user\")\n\n# Example agent setup\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_user_name],\n    state_schema=CustomState,\n)\n\n# Invocation: reads the name from state (initially empty)\nagent.invoke({\"messages\": \"what's my name?\"})\n</code></pre> <p>:::</p> <p>:::js To access (read) the graph state inside the tools, you can use the @[<code>getContextVariable</code>][getContextVariable] function:</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { getContextVariable } from \"@langchain/core/context\";\nimport { MessagesZodState } from \"@langchain/langgraph\";\nimport type { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n\nconst getUserName = tool(\n  // highlight-next-line\n  async (_, config: LangGraphRunnableConfig) =&gt; {\n    // highlight-next-line\n    const currentState = getContextVariable(\"currentState\") as z.infer&lt;\n      typeof MessagesZodState\n    &gt; &amp; { userName?: string };\n    return currentState?.userName || \"Unknown user\";\n  },\n  {\n    name: \"get_user_name\",\n    description: \"Retrieve the current user name from state.\",\n    schema: z.object({}),\n  }\n);\n</code></pre> <p>:::</p> <p>:::python Use a tool that returns a <code>Command</code> to update <code>user_name</code> and append a confirmation message:</p> <pre><code>from typing import Annotated\nfrom langgraph.types import Command\nfrom langchain_core.messages import ToolMessage\nfrom langchain_core.tools import tool, InjectedToolCallId\n\n@tool\ndef update_user_name(\n    new_name: str,\n    tool_call_id: Annotated[str, InjectedToolCallId]\n) -&gt; Command:\n    \"\"\"Update user-name in short-term memory.\"\"\"\n    # highlight-next-line\n    return Command(update={\n        # highlight-next-line\n        \"user_name\": new_name,\n        # highlight-next-line\n        \"messages\": [\n            # highlight-next-line\n            ToolMessage(f\"Updated user name to {new_name}\", tool_call_id=tool_call_id)\n            # highlight-next-line\n        ]\n        # highlight-next-line\n    })\n</code></pre> <p>:::</p> <p>:::js To update short-term memory, you can use tools that return a <code>Command</code> to update state:</p> <pre><code>import { Command } from \"@langchain/langgraph\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst updateUserName = tool(\n  async (input) =&gt; {\n    // highlight-next-line\n    return new Command({\n      // highlight-next-line\n      update: {\n        // highlight-next-line\n        userName: input.newName,\n        // highlight-next-line\n        messages: [\n          // highlight-next-line\n          {\n            // highlight-next-line\n            role: \"assistant\",\n            // highlight-next-line\n            content: `Updated user name to ${input.newName}`,\n            // highlight-next-line\n          },\n          // highlight-next-line\n        ],\n        // highlight-next-line\n      },\n      // highlight-next-line\n    });\n  },\n  {\n    name: \"update_user_name\",\n    description: \"Update user name in short-term memory.\",\n    schema: z.object({\n      newName: z.string().describe(\"The new user name\"),\n    }),\n  }\n);\n</code></pre> <p>:::</p> <p>Important</p> <p>:::python If you want to use tools that return <code>Command</code> and update graph state, you can either use prebuilt @[<code>create_react_agent</code>][create_react_agent] / @[<code>ToolNode</code>][ToolNode] components, or implement your own tool-executing node that collects <code>Command</code> objects returned by the tools and returns a list of them, e.g.:</p> <p><pre><code>def call_tools(state):\n    ...\n    commands = [tools_by_name[tool_call[\"name\"]].invoke(tool_call) for tool_call in tool_calls]\n    return commands\n</code></pre> :::</p> <p>:::js If you want to use tools that return <code>Command</code> and update graph state, you can either use prebuilt @[<code>createReactAgent</code>][create_react_agent] / @[ToolNode] components, or implement your own tool-executing node that collects <code>Command</code> objects returned by the tools and returns a list of them, e.g.:</p> <p><pre><code>const callTools = async (state: State) =&gt; {\n  // ...\n  const commands = await Promise.all(\n    toolCalls.map(toolCall =&gt; toolsByName[toolCall.name].invoke(toolCall))\n  );\n  return commands;\n};\n</code></pre> :::</p>"}, {"location": "how-tos/tool-calling/#long-term-memory", "title": "Long-term memory", "text": "<p>Use long-term memory to store user-specific or application-specific data across conversations. This is useful for applications like chatbots, where you want to remember user preferences or other information.</p> <p>To use long-term memory, you need to:</p> <ol> <li>Configure a store to persist data across invocations.</li> <li>Access the store from within tools.</li> </ol> <p>:::python To access information in the store:</p> <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langgraph.graph import StateGraph\n# highlight-next-line\nfrom langgraph.config import get_store\n\n@tool\ndef get_user_info(config: RunnableConfig) -&gt; str:\n    \"\"\"Look up user info.\"\"\"\n    # Same as that provided to `builder.compile(store=store)`\n    # or `create_react_agent`\n    # highlight-next-line\n    store = get_store()\n    user_id = config[\"configurable\"].get(\"user_id\")\n    # highlight-next-line\n    user_info = store.get((\"users\",), user_id)\n    return str(user_info.value) if user_info else \"Unknown user\"\n\nbuilder = StateGraph(...)\n...\ngraph = builder.compile(store=store)\n</code></pre> <p>:::</p> <p>:::js To access information in the store:</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport type { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n\nconst getUserInfo = tool(\n  async (_, config: LangGraphRunnableConfig) =&gt; {\n    // Same as that provided to `builder.compile({ store })`\n    // or `createReactAgent`\n    // highlight-next-line\n    const store = config.store;\n    if (!store) throw new Error(\"Store not provided\");\n\n    const userId = config?.configurable?.user_id;\n    // highlight-next-line\n    const userInfo = await store.get([\"users\"], userId);\n    return userInfo?.value ? JSON.stringify(userInfo.value) : \"Unknown user\";\n  },\n  {\n    name: \"get_user_info\",\n    description: \"Look up user info.\",\n    schema: z.object({}),\n  }\n);\n</code></pre> <p>:::</p> Access long-term memory <p>:::python <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langgraph.config import get_store\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.store.memory import InMemoryStore\n\n# highlight-next-line\nstore = InMemoryStore() # (1)!\n\n# highlight-next-line\nstore.put(  # (2)!\n    (\"users\",),  # (3)!\n    \"user_123\",  # (4)!\n    {\n        \"name\": \"John Smith\",\n        \"language\": \"English\",\n    } # (5)!\n)\n\n@tool\ndef get_user_info(config: RunnableConfig) -&gt; str:\n    \"\"\"Look up user info.\"\"\"\n    # Same as that provided to `create_react_agent`\n    # highlight-next-line\n    store = get_store() # (6)!\n    user_id = config[\"configurable\"].get(\"user_id\")\n    # highlight-next-line\n    user_info = store.get((\"users\",), user_id) # (7)!\n    return str(user_info.value) if user_info else \"Unknown user\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_user_info],\n    # highlight-next-line\n    store=store # (8)!\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n    # highlight-next-line\n    config={\"configurable\": {\"user_id\": \"user_123\"}}\n)\n</code></pre></p> <ol> <li>The <code>InMemoryStore</code> is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the [store documentation][../reference/store.md) for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you.</li> <li>For this example, we write some sample data to the store using the <code>put</code> method. Please see the @[BaseStore.put] API reference for more details.</li> <li>The first argument is the namespace. This is used to group related data together. In this case, we are using the <code>users</code> namespace to group user data.</li> <li>A key within the namespace. This example uses a user ID for the key.</li> <li>The data that we want to store for the given user.</li> <li>The <code>get_store</code> function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.</li> <li>The <code>get</code> method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a <code>StoreValue</code> object, which contains the value and metadata about the value.</li> <li>The <code>store</code> is passed to the agent. This enables the agent to access the store when running tools. You can also use the <code>get_store</code> function to access the store from anywhere in your code. :::</li> </ol> <p>:::js <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { InMemoryStore } from \"@langchain/langgraph\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport type { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n\n// highlight-next-line\nconst store = new InMemoryStore(); // (1)!\n\n// highlight-next-line\nawait store.put(  // (2)!\n  [\"users\"],  // (3)!\n  \"user_123\",  // (4)!\n  {\n    name: \"John Smith\",\n    language: \"English\",\n  } // (5)!\n);\n\nconst getUserInfo = tool(\n  async (_, config: LangGraphRunnableConfig) =&gt; {\n    // Same as that provided to `createReactAgent`\n    // highlight-next-line\n    const store = config.store; // (6)!\n    if (!store) throw new Error(\"Store not provided\");\n\n    const userId = config?.configurable?.user_id;\n    // highlight-next-line\n    const userInfo = await store.get([\"users\"], userId); // (7)!\n    return userInfo?.value ? JSON.stringify(userInfo.value) : \"Unknown user\";\n  },\n  {\n    name: \"get_user_info\",\n    description: \"Look up user info.\",\n    schema: z.object({}),\n  }\n);\n\nconst agent = createReactAgent({\n  llm: new ChatAnthropic({ model: \"claude-3-5-sonnet-20240620\" }),\n  tools: [getUserInfo],\n  // highlight-next-line\n  store: store // (8)!\n});\n\n// Run the agent\nawait agent.invoke(\n  { messages: [{ role: \"user\", content: \"look up user information\" }] },\n  // highlight-next-line\n  { configurable: { user_id: \"user_123\" } }\n);\n</code></pre></p> <ol> <li>The <code>InMemoryStore</code> is a store that stores data in memory. In production, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you.</li> <li>For this example, we write some sample data to the store using the <code>put</code> method. Please see the BaseStore.put API reference for more details.</li> <li>The first argument is the namespace. This is used to group related data together. In this case, we are using the <code>users</code> namespace to group user data.</li> <li>A key within the namespace. This example uses a user ID for the key.</li> <li>The data that we want to store for the given user.</li> <li>The store is accessible from the config object that is passed to the tool. This enables the tool to access the store when running.</li> <li>The <code>get</code> method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a <code>StoreValue</code> object, which contains the value and metadata about the value.</li> <li>The <code>store</code> is passed to the agent. This enables the agent to access the store when running tools. :::</li> </ol> <p>:::python To update information in the store:</p> <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langchain_core.tools import tool\nfrom langgraph.graph import StateGraph\n# highlight-next-line\nfrom langgraph.config import get_store\n\n@tool\ndef save_user_info(user_info: str, config: RunnableConfig) -&gt; str:\n    \"\"\"Save user info.\"\"\"\n    # Same as that provided to `builder.compile(store=store)`\n    # or `create_react_agent`\n    # highlight-next-line\n    store = get_store()\n    user_id = config[\"configurable\"].get(\"user_id\")\n    # highlight-next-line\n    store.put((\"users\",), user_id, user_info)\n    return \"Successfully saved user info.\"\n\nbuilder = StateGraph(...)\n...\ngraph = builder.compile(store=store)\n</code></pre> <p>:::</p> <p>:::js To update information in the store:</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport type { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n\nconst saveUserInfo = tool(\n  async (input, config: LangGraphRunnableConfig) =&gt; {\n    // Same as that provided to `builder.compile({ store })`\n    // or `createReactAgent`\n    // highlight-next-line\n    const store = config.store;\n    if (!store) throw new Error(\"Store not provided\");\n\n    const userId = config?.configurable?.user_id;\n    // highlight-next-line\n    await store.put([\"users\"], userId, input.userInfo);\n    return \"Successfully saved user info.\";\n  },\n  {\n    name: \"save_user_info\",\n    description: \"Save user info.\",\n    schema: z.object({\n      userInfo: z.string().describe(\"User information to save\"),\n    }),\n  }\n);\n</code></pre> <p>:::</p> Update long-term memory <p>:::python <pre><code>from typing_extensions import TypedDict\n\nfrom langchain_core.tools import tool\nfrom langgraph.config import get_store\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.store.memory import InMemoryStore\n\nstore = InMemoryStore() # (1)!\n\nclass UserInfo(TypedDict): # (2)!\n    name: str\n\n@tool\ndef save_user_info(user_info: UserInfo, config: RunnableConfig) -&gt; str: # (3)!\n    \"\"\"Save user info.\"\"\"\n    # Same as that provided to `create_react_agent`\n    # highlight-next-line\n    store = get_store() # (4)!\n    user_id = config[\"configurable\"].get(\"user_id\")\n    # highlight-next-line\n    store.put((\"users\",), user_id, user_info) # (5)!\n    return \"Successfully saved user info.\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[save_user_info],\n    # highlight-next-line\n    store=store\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]},\n    # highlight-next-line\n    config={\"configurable\": {\"user_id\": \"user_123\"}} # (6)!\n)\n\n# You can access the store directly to get the value\nstore.get((\"users\",), \"user_123\").value\n</code></pre></p> <ol> <li>The <code>InMemoryStore</code> is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you.</li> <li>The <code>UserInfo</code> class is a <code>TypedDict</code> that defines the structure of the user information. The LLM will use this to format the response according to the schema.</li> <li>The <code>save_user_info</code> function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information.</li> <li>The <code>get_store</code> function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.</li> <li>The <code>put</code> method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store.</li> <li>The <code>user_id</code> is passed in the config. This is used to identify the user whose information is being updated. :::</li> </ol> <p>:::js <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { InMemoryStore } from \"@langchain/langgraph\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport type { LangGraphRunnableConfig } from \"@langchain/langgraph\";\n\nconst store = new InMemoryStore(); // (1)!\n\nconst UserInfoSchema = z.object({ // (2)!\n  name: z.string(),\n});\n\nconst saveUserInfo = tool(\n  async (input, config: LangGraphRunnableConfig) =&gt; { // (3)!\n    // Same as that provided to `createReactAgent`\n    // highlight-next-line\n    const store = config.store; // (4)!\n    if (!store) throw new Error(\"Store not provided\");\n\n    const userId = config?.configurable?.user_id;\n    // highlight-next-line\n    await store.put([\"users\"], userId, input); // (5)!\n    return \"Successfully saved user info.\";\n  },\n  {\n    name: \"save_user_info\",\n    description: \"Save user info.\",\n    schema: UserInfoSchema,\n  }\n);\n\nconst agent = createReactAgent({\n  llm: new ChatAnthropic({ model: \"claude-3-5-sonnet-20240620\" }),\n  tools: [saveUserInfo],\n  // highlight-next-line\n  store: store\n});\n\n// Run the agent\nawait agent.invoke(\n  { messages: [{ role: \"user\", content: \"My name is John Smith\" }] },\n  // highlight-next-line\n  { configurable: { user_id: \"user_123\" } } // (6)!\n);\n\n// You can access the store directly to get the value\nconst userInfo = await store.get([\"users\"], \"user_123\");\nconsole.log(userInfo?.value);\n</code></pre></p> <ol> <li>The <code>InMemoryStore</code> is a store that stores data in memory. In production, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you.</li> <li>The <code>UserInfoSchema</code> is a Zod schema that defines the structure of the user information. The LLM will use this to format the response according to the schema.</li> <li>The <code>saveUserInfo</code> function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information.</li> <li>The store is accessible from the config object that is passed to the tool. This enables the tool to access the store when running.</li> <li>The <code>put</code> method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store.</li> <li>The <code>user_id</code> is passed in the config. This is used to identify the user whose information is being updated. :::</li> </ol>"}, {"location": "how-tos/tool-calling/#advanced-tool-features", "title": "Advanced tool features", "text": ""}, {"location": "how-tos/tool-calling/#immediate-return", "title": "Immediate return", "text": "<p>:::python Use <code>return_direct=True</code> to immediately return a tool's result without executing additional logic.</p> <p>This is useful for tools that should not trigger further processing or tool calls, allowing you to return results directly to the user.</p> <pre><code># highlight-next-line\n@tool(return_direct=True)\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n</code></pre> <p>:::</p> <p>:::js Use <code>returnDirect: true</code> to immediately return a tool's result without executing additional logic.</p> <p>This is useful for tools that should not trigger further processing or tool calls, allowing you to return results directly to the user.</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\n// highlight-next-line\nconst add = tool(\n  (input) =&gt; {\n    return input.a + input.b;\n  },\n  {\n    name: \"add\",\n    description: \"Add two numbers\",\n    schema: z.object({\n      a: z.number(),\n      b: z.number(),\n    }),\n    // highlight-next-line\n    returnDirect: true,\n  }\n);\n</code></pre> <p>:::</p> Extended example: Using return_direct in a prebuilt agent <p>:::python <pre><code>from langchain_core.tools import tool\nfrom langgraph.prebuilt import create_react_agent\n\n# highlight-next-line\n@tool(return_direct=True)\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[add]\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's 3 + 5?\"}]}\n)\n</code></pre> :::</p> <p>:::js <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\n// highlight-next-line\nconst add = tool(\n  (input) =&gt; {\n    return input.a + input.b;\n  },\n  {\n    name: \"add\",\n    description: \"Add two numbers\",\n    schema: z.object({\n      a: z.number(),\n      b: z.number(),\n    }),\n    // highlight-next-line\n    returnDirect: true,\n  }\n);\n\nconst agent = createReactAgent({\n  llm: new ChatAnthropic({ model: \"claude-3-5-sonnet-20240620\" }),\n  tools: [add]\n});\n\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"what's 3 + 5?\" }]\n});\n</code></pre> :::</p> <p>Using without prebuilt components</p> <p>:::python If you are building a custom workflow and are not relying on <code>create_react_agent</code> or <code>ToolNode</code>, you will also need to implement the control flow to handle <code>return_direct=True</code>. :::</p> <p>:::js If you are building a custom workflow and are not relying on <code>createReactAgent</code> or <code>ToolNode</code>, you will also need to implement the control flow to handle <code>returnDirect: true</code>. :::</p>"}, {"location": "how-tos/tool-calling/#force-tool-use", "title": "Force tool use", "text": "<p>If you need to force a specific tool to be used, you will need to configure this at the model level using the <code>tool_choice</code> parameter in the bind_tools method.</p> <p>Force specific tool usage via tool_choice:</p> <p>:::python</p> <pre><code>@tool(return_direct=True)\ndef greet(user_name: str) -&gt; int:\n    \"\"\"Greet user.\"\"\"\n    return f\"Hello {user_name}!\"\n\ntools = [greet]\n\nconfigured_model = model.bind_tools(\n    tools,\n    # Force the use of the 'greet' tool\n    # highlight-next-line\n    tool_choice={\"type\": \"tool\", \"name\": \"greet\"}\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const greet = tool(\n  (input) =&gt; {\n    return `Hello ${input.userName}!`;\n  },\n  {\n    name: \"greet\",\n    description: \"Greet user.\",\n    schema: z.object({\n      userName: z.string(),\n    }),\n    returnDirect: true,\n  }\n);\n\nconst tools = [greet];\n\nconst configuredModel = model.bindTools(\n  tools,\n  // Force the use of the 'greet' tool\n  // highlight-next-line\n  { tool_choice: { type: \"tool\", name: \"greet\" } }\n);\n</code></pre> <p>:::</p> Extended example: Force tool usage in an agent <p>:::python To force the agent to use specific tools, you can set the <code>tool_choice</code> option in <code>model.bind_tools()</code>:</p> <p><pre><code>from langchain_core.tools import tool\n\n# highlight-next-line\n@tool(return_direct=True)\ndef greet(user_name: str) -&gt; int:\n    \"\"\"Greet user.\"\"\"\n    return f\"Hello {user_name}!\"\n\ntools = [greet]\n\nagent = create_react_agent(\n    # highlight-next-line\n    model=model.bind_tools(tools, tool_choice={\"type\": \"tool\", \"name\": \"greet\"}),\n    tools=tools\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"Hi, I am Bob\"}]}\n)\n</code></pre> :::</p> <p>:::js To force the agent to use specific tools, you can set the <code>tool_choice</code> option in <code>model.bindTools()</code>:</p> <p><pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\n// highlight-next-line\nconst greet = tool(\n  (input) =&gt; {\n    return `Hello ${input.userName}!`;\n  },\n  {\n    name: \"greet\",\n    description: \"Greet user.\",\n    schema: z.object({\n      userName: z.string(),\n    }),\n    // highlight-next-line\n    returnDirect: true,\n  }\n);\n\nconst tools = [greet];\nconst model = new ChatOpenAI({ model: \"gpt-4o\" });\n\nconst agent = createReactAgent({\n  // highlight-next-line\n  llm: model.bindTools(tools, { tool_choice: { type: \"tool\", name: \"greet\" } }),\n  tools: tools\n});\n\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"Hi, I am Bob\" }]\n});\n</code></pre> :::</p> <p>Avoid infinite loops</p> <p>:::python Forcing tool usage without stopping conditions can create infinite loops. Use one of the following safeguards:</p> <ul> <li>Mark the tool with <code>return_direct=True</code> to end the loop after execution.</li> <li>Set <code>recursion_limit</code> to restrict the number of execution steps. :::</li> </ul> <p>:::js Forcing tool usage without stopping conditions can create infinite loops. Use one of the following safeguards:</p> <ul> <li>Mark the tool with <code>returnDirect: true</code> to end the loop after execution.</li> <li>Set <code>recursionLimit</code> to restrict the number of execution steps. :::</li> </ul> <p>Tool choice configuration</p> <p>The <code>tool_choice</code> parameter is used to configure which tool should be used by the model when it decides to call a tool. This is useful when you want to ensure that a specific tool is always called for a particular task or when you want to override the model's default behavior of choosing a tool based on its internal logic.</p> <p>Note that not all models support this feature, and the exact configuration may vary depending on the model you are using.</p>"}, {"location": "how-tos/tool-calling/#disable-parallel-calls", "title": "Disable parallel calls", "text": "<p>:::python For supported providers, you can disable parallel tool calling by setting <code>parallel_tool_calls=False</code> via the <code>model.bind_tools()</code> method:</p> <pre><code>model.bind_tools(\n    tools,\n    # highlight-next-line\n    parallel_tool_calls=False\n)\n</code></pre> <p>:::</p> <p>:::js For supported providers, you can disable parallel tool calling by setting <code>parallel_tool_calls: false</code> via the <code>model.bindTools()</code> method:</p> <pre><code>model.bindTools(\n  tools,\n  // highlight-next-line\n  { parallel_tool_calls: false }\n);\n</code></pre> <p>:::</p> Extended example: disable parallel tool calls in a prebuilt agent <p>:::python <pre><code>from langchain.chat_models import init_chat_model\n\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers\"\"\"\n    return a + b\n\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\nmodel = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\", temperature=0)\ntools = [add, multiply]\nagent = create_react_agent(\n    # disable parallel tool calls\n    # highlight-next-line\n    model=model.bind_tools(tools, parallel_tool_calls=False),\n    tools=tools\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's 3 + 5 and 4 * 7?\"}]}\n)\n</code></pre> :::</p> <p>:::js <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst add = tool(\n  (input) =&gt; {\n    return input.a + input.b;\n  },\n  {\n    name: \"add\",\n    description: \"Add two numbers\",\n    schema: z.object({\n      a: z.number(),\n      b: z.number(),\n    }),\n  }\n);\n\nconst multiply = tool(\n  (input) =&gt; {\n    return input.a * input.b;\n  },\n  {\n    name: \"multiply\",\n    description: \"Multiply two numbers.\",\n    schema: z.object({\n      a: z.number(),\n      b: z.number(),\n    }),\n  }\n);\n\nconst model = new ChatOpenAI({ model: \"gpt-4o\", temperature: 0 });\nconst tools = [add, multiply];\n\nconst agent = createReactAgent({\n  // disable parallel tool calls\n  // highlight-next-line\n  llm: model.bindTools(tools, { parallel_tool_calls: false }),\n  tools: tools\n});\n\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"what's 3 + 5 and 4 * 7?\" }]\n});\n</code></pre> :::</p>"}, {"location": "how-tos/tool-calling/#handle-errors", "title": "Handle errors", "text": "<p>:::python LangGraph provides built-in error handling for tool execution through the prebuilt @[ToolNode][ToolNode] component, used both independently and in prebuilt agents.</p> <p>By default, <code>ToolNode</code> catches exceptions raised during tool execution and returns them as <code>ToolMessage</code> objects with a status indicating an error.</p> <pre><code>from langchain_core.messages import AIMessage\nfrom langgraph.prebuilt import ToolNode\n\ndef multiply(a: int, b: int) -&gt; int:\n    if a == 42:\n        raise ValueError(\"The ultimate error\")\n    return a * b\n\n# Default error handling (enabled by default)\ntool_node = ToolNode([multiply])\n\nmessage = AIMessage(\n    content=\"\",\n    tool_calls=[{\n        \"name\": \"multiply\",\n        \"args\": {\"a\": 42, \"b\": 7},\n        \"id\": \"tool_call_id\",\n        \"type\": \"tool_call\"\n    }]\n)\n\nresult = tool_node.invoke({\"messages\": [message]})\n</code></pre> <p>Output:</p> <pre><code>{'messages': [\n    ToolMessage(\n        content=\"Error: ValueError('The ultimate error')\\n Please fix your mistakes.\",\n        name='multiply',\n        tool_call_id='tool_call_id',\n        status='error'\n    )\n]}\n</code></pre> <p>:::</p> <p>:::js LangGraph provides built-in error handling for tool execution through the prebuilt ToolNode component, used both independently and in prebuilt agents.</p> <p>By default, <code>ToolNode</code> catches exceptions raised during tool execution and returns them as <code>ToolMessage</code> objects with a status indicating an error.</p> <pre><code>import { AIMessage } from \"@langchain/core/messages\";\nimport { ToolNode } from \"@langchain/langgraph/prebuilt\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst multiply = tool(\n  (input) =&gt; {\n    if (input.a === 42) {\n      throw new Error(\"The ultimate error\");\n    }\n    return input.a * input.b;\n  },\n  {\n    name: \"multiply\",\n    description: \"Multiply two numbers\",\n    schema: z.object({\n      a: z.number(),\n      b: z.number(),\n    }),\n  }\n);\n\n// Default error handling (enabled by default)\nconst toolNode = new ToolNode([multiply]);\n\nconst message = new AIMessage({\n  content: \"\",\n  tool_calls: [\n    {\n      name: \"multiply\",\n      args: { a: 42, b: 7 },\n      id: \"tool_call_id\",\n      type: \"tool_call\",\n    },\n  ],\n});\n\nconst result = await toolNode.invoke({ messages: [message] });\n</code></pre> <p>Output:</p> <pre><code>{ messages: [\n  ToolMessage {\n    content: \"Error: The ultimate error\\n Please fix your mistakes.\",\n    name: \"multiply\",\n    tool_call_id: \"tool_call_id\",\n    status: \"error\"\n  }\n]}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/tool-calling/#disable-error-handling", "title": "Disable error handling", "text": "<p>To propagate exceptions directly, disable error handling:</p> <p>:::python</p> <pre><code>tool_node = ToolNode([multiply], handle_tool_errors=False)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const toolNode = new ToolNode([multiply], { handleToolErrors: false });\n</code></pre> <p>:::</p> <p>With error handling disabled, exceptions raised by tools will propagate up, requiring explicit management.</p>"}, {"location": "how-tos/tool-calling/#custom-error-messages", "title": "Custom error messages", "text": "<p>Provide a custom error message by setting the error handling parameter to a string:</p> <p>:::python</p> <pre><code>tool_node = ToolNode(\n    [multiply],\n    handle_tool_errors=\"Can't use 42 as the first operand, please switch operands!\"\n)\n</code></pre> <p>Example output:</p> <pre><code>{'messages': [\n    ToolMessage(\n        content=\"Can't use 42 as the first operand, please switch operands!\",\n        name='multiply',\n        tool_call_id='tool_call_id',\n        status='error'\n    )\n]}\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const toolNode = new ToolNode([multiply], {\n  handleToolErrors:\n    \"Can't use 42 as the first operand, please switch operands!\",\n});\n</code></pre> <p>Example output:</p> <pre><code>{ messages: [\n  ToolMessage {\n    content: \"Can't use 42 as the first operand, please switch operands!\",\n    name: \"multiply\",\n    tool_call_id: \"tool_call_id\",\n    status: \"error\"\n  }\n]}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/tool-calling/#error-handling-in-agents", "title": "Error handling in agents", "text": "<p>:::python Error handling in prebuilt agents (<code>create_react_agent</code>) leverages <code>ToolNode</code>:</p> <pre><code>from langgraph.prebuilt import create_react_agent\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[multiply]\n)\n\n# Default error handling\nagent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]})\n</code></pre> <p>To disable or customize error handling in prebuilt agents, explicitly pass a configured <code>ToolNode</code>:</p> <pre><code>custom_tool_node = ToolNode(\n    [multiply],\n    handle_tool_errors=\"Cannot use 42 as a first operand!\"\n)\n\nagent_custom = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=custom_tool_node\n)\n\nagent_custom.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"what's 42 x 7?\"}]})\n</code></pre> <p>:::</p> <p>:::js Error handling in prebuilt agents (<code>createReactAgent</code>) leverages <code>ToolNode</code>:</p> <pre><code>import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst agent = createReactAgent({\n  llm: new ChatAnthropic({ model: \"claude-3-5-sonnet-20240620\" }),\n  tools: [multiply],\n});\n\n// Default error handling\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"what's 42 x 7?\" }],\n});\n</code></pre> <p>To disable or customize error handling in prebuilt agents, explicitly pass a configured <code>ToolNode</code>:</p> <pre><code>const customToolNode = new ToolNode([multiply], {\n  handleToolErrors: \"Cannot use 42 as a first operand!\",\n});\n\nconst agentCustom = createReactAgent({\n  llm: new ChatAnthropic({ model: \"claude-3-5-sonnet-20240620\" }),\n  tools: customToolNode,\n});\n\nawait agentCustom.invoke({\n  messages: [{ role: \"user\", content: \"what's 42 x 7?\" }],\n});\n</code></pre> <p>:::</p>"}, {"location": "how-tos/tool-calling/#handle-large-numbers-of-tools", "title": "Handle large numbers of tools", "text": "<p>As the number of available tools grows, you may want to limit the scope of the LLM's selection, to decrease token consumption and to help manage sources of error in LLM reasoning.</p> <p>To address this, you can dynamically adjust the tools available to a model by retrieving relevant tools at runtime using semantic search.</p> <p>See <code>langgraph-bigtool</code> prebuilt library for a ready-to-use implementation.</p>"}, {"location": "how-tos/tool-calling/#prebuilt-tools", "title": "Prebuilt tools", "text": ""}, {"location": "how-tos/tool-calling/#llm-provider-tools", "title": "LLM provider tools", "text": "<p>:::python You can use prebuilt tools from model providers by passing a dictionary with tool specs to the <code>tools</code> parameter of <code>create_react_agent</code>. For example, to use the <code>web_search_preview</code> tool from OpenAI:</p> <pre><code>from langgraph.prebuilt import create_react_agent\n\nagent = create_react_agent(\n    model=\"openai:gpt-4o-mini\",\n    tools=[{\"type\": \"web_search_preview\"}]\n)\nresponse = agent.invoke(\n    {\"messages\": [\"What was a positive news story from today?\"]}\n)\n</code></pre> <p>Please consult the documentation for the specific model you are using to see which tools are available and how to use them. :::</p> <p>:::js You can use prebuilt tools from model providers by passing a dictionary with tool specs to the <code>tools</code> parameter of <code>createReactAgent</code>. For example, to use the <code>web_search_preview</code> tool from OpenAI:</p> <pre><code>import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst agent = createReactAgent({\n  llm: new ChatOpenAI({ model: \"gpt-4o-mini\" }),\n  tools: [{ type: \"web_search_preview\" }],\n});\n\nconst response = await agent.invoke({\n  messages: [\n    { role: \"user\", content: \"What was a positive news story from today?\" },\n  ],\n});\n</code></pre> <p>Please consult the documentation for the specific model you are using to see which tools are available and how to use them. :::</p>"}, {"location": "how-tos/tool-calling/#langchain-tools", "title": "LangChain tools", "text": "<p>Additionally, LangChain supports a wide range of prebuilt tool integrations for interacting with APIs, databases, file systems, web data, and more. These tools extend the functionality of agents and enable rapid development.</p> <p>:::python You can browse the full list of available integrations in the LangChain integrations directory.</p> <p>Some commonly used tool categories include:</p> <ul> <li>Search: Bing, SerpAPI, Tavily</li> <li>Code interpreters: Python REPL, Node.js REPL</li> <li>Databases: SQL, MongoDB, Redis</li> <li>Web data: Web scraping and browsing</li> <li>APIs: OpenWeatherMap, NewsAPI, and others</li> </ul> <p>These integrations can be configured and added to your agents using the same <code>tools</code> parameter shown in the examples above. :::</p> <p>:::js You can browse the full list of available integrations in the LangChain integrations directory.</p> <p>Some commonly used tool categories include:</p> <ul> <li>Search: Tavily, SerpAPI</li> <li>Code interpreters: Web browsers, calculators</li> <li>Databases: SQL, vector databases</li> <li>Web data: Web scraping and browsing</li> <li>APIs: Various API integrations</li> </ul> <p>These integrations can be configured and added to your agents using the same <code>tools</code> parameter shown in the examples above. :::</p>"}, {"location": "how-tos/use-functional-api/", "title": "Use the functional API", "text": "<p>The Functional API allows you to add LangGraph's key features \u2014 persistence, memory, human-in-the-loop, and streaming \u2014 to your applications with minimal changes to your existing code.</p> <p>Tip</p> <p>For conceptual information on the functional API, see Functional API.</p>"}, {"location": "how-tos/use-functional-api/#creating-a-simple-workflow", "title": "Creating a simple workflow", "text": "<p>When defining an <code>entrypoint</code>, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary.</p> <p>:::python</p> <pre><code>@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -&gt; int:\n    value = inputs[\"value\"]\n    another_value = inputs[\"another_value\"]\n    ...\n\nmy_workflow.invoke({\"value\": 1, \"another_value\": 2})\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const checkpointer = new MemorySaver();\n\nconst myWorkflow = entrypoint(\n  { checkpointer, name: \"myWorkflow\" },\n  async (inputs: { value: number; anotherValue: number }) =&gt; {\n    const value = inputs.value;\n    const anotherValue = inputs.anotherValue;\n    // ...\n  }\n);\n\nawait myWorkflow.invoke({ value: 1, anotherValue: 2 });\n</code></pre> <p>:::</p> Extended example: simple workflow <p>:::python <pre><code>import uuid\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Task that checks if a number is even\n@task\ndef is_even(number: int) -&gt; bool:\n    return number % 2 == 0\n\n# Task that formats a message\n@task\ndef format_message(is_even: bool) -&gt; str:\n    return \"The number is even.\" if is_even else \"The number is odd.\"\n\n# Create a checkpointer for persistence\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(inputs: dict) -&gt; str:\n    \"\"\"Simple workflow to classify a number.\"\"\"\n    even = is_even(inputs[\"number\"]).result()\n    return format_message(even).result()\n\n# Run the workflow with a unique thread ID\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\nresult = workflow.invoke({\"number\": 7}, config=config)\nprint(result)\n</code></pre> :::</p> <p>:::js <pre><code>import { v4 as uuidv4 } from \"uuid\";\nimport { entrypoint, task, MemorySaver } from \"@langchain/langgraph\";\n\n// Task that checks if a number is even\nconst isEven = task(\"isEven\", async (number: number) =&gt; {\n  return number % 2 === 0;\n});\n\n// Task that formats a message\nconst formatMessage = task(\"formatMessage\", async (isEven: boolean) =&gt; {\n  return isEven ? \"The number is even.\" : \"The number is odd.\";\n});\n\n// Create a checkpointer for persistence\nconst checkpointer = new MemorySaver();\n\nconst workflow = entrypoint(\n  { checkpointer, name: \"workflow\" },\n  async (inputs: { number: number }) =&gt; {\n    // Simple workflow to classify a number\n    const even = await isEven(inputs.number);\n    return await formatMessage(even);\n  }\n);\n\n// Run the workflow with a unique thread ID\nconst config = { configurable: { thread_id: uuidv4() } };\nconst result = await workflow.invoke({ number: 7 }, config);\nconsole.log(result);\n</code></pre> :::</p> Extended example: Compose an essay with an LLM <p>This example demonstrates how to use the <code>@task</code> and <code>@entrypoint</code> decorators syntactically. Given that a checkpointer is provided, the workflow results will be persisted in the checkpointer.</p> <p>:::python <pre><code>import uuid\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\n\nllm = init_chat_model('openai:gpt-3.5-turbo')\n\n# Task: generate essay using an LLM\n@task\ndef compose_essay(topic: str) -&gt; str:\n    \"\"\"Generate an essay about the given topic.\"\"\"\n    return llm.invoke([\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes essays.\"},\n        {\"role\": \"user\", \"content\": f\"Write an essay about {topic}.\"}\n    ]).content\n\n# Create a checkpointer for persistence\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(topic: str) -&gt; str:\n    \"\"\"Simple workflow that generates an essay with an LLM.\"\"\"\n    return compose_essay(topic).result()\n\n# Execute the workflow\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\nresult = workflow.invoke(\"the history of flight\", config=config)\nprint(result)\n</code></pre> :::</p> <p>:::js <pre><code>import { v4 as uuidv4 } from \"uuid\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { entrypoint, task, MemorySaver } from \"@langchain/langgraph\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-3.5-turbo\" });\n\n// Task: generate essay using an LLM\nconst composeEssay = task(\"composeEssay\", async (topic: string) =&gt; {\n  // Generate an essay about the given topic\n  const response = await llm.invoke([\n    { role: \"system\", content: \"You are a helpful assistant that writes essays.\" },\n    { role: \"user\", content: `Write an essay about ${topic}.` }\n  ]);\n  return response.content as string;\n});\n\n// Create a checkpointer for persistence\nconst checkpointer = new MemorySaver();\n\nconst workflow = entrypoint(\n  { checkpointer, name: \"workflow\" },\n  async (topic: string) =&gt; {\n    // Simple workflow that generates an essay with an LLM\n    return await composeEssay(topic);\n  }\n);\n\n// Execute the workflow\nconst config = { configurable: { thread_id: uuidv4() } };\nconst result = await workflow.invoke(\"the history of flight\", config);\nconsole.log(result);\n</code></pre> :::</p>"}, {"location": "how-tos/use-functional-api/#parallel-execution", "title": "Parallel execution", "text": "<p>Tasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs).</p> <p>:::python</p> <pre><code>@task\ndef add_one(number: int) -&gt; int:\n    return number + 1\n\n@entrypoint(checkpointer=checkpointer)\ndef graph(numbers: list[int]) -&gt; list[str]:\n    futures = [add_one(i) for i in numbers]\n    return [f.result() for f in futures]\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const addOne = task(\"addOne\", async (number: number) =&gt; {\n  return number + 1;\n});\n\nconst graph = entrypoint(\n  { checkpointer, name: \"graph\" },\n  async (numbers: number[]) =&gt; {\n    return await Promise.all(numbers.map(addOne));\n  }\n);\n</code></pre> <p>:::</p> Extended example: parallel LLM calls <p>This example demonstrates how to run multiple LLM calls in parallel using <code>@task</code>. Each call generates a paragraph on a different topic, and results are joined into a single text output.</p> <p>:::python <pre><code>import uuid\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Initialize the LLM model\nllm = init_chat_model(\"openai:gpt-3.5-turbo\")\n\n# Task that generates a paragraph about a given topic\n@task\ndef generate_paragraph(topic: str) -&gt; str:\n    response = llm.invoke([\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes educational paragraphs.\"},\n        {\"role\": \"user\", \"content\": f\"Write a paragraph about {topic}.\"}\n    ])\n    return response.content\n\n# Create a checkpointer for persistence\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(topics: list[str]) -&gt; str:\n    \"\"\"Generates multiple paragraphs in parallel and combines them.\"\"\"\n    futures = [generate_paragraph(topic) for topic in topics]\n    paragraphs = [f.result() for f in futures]\n    return \"\\n\\n\".join(paragraphs)\n\n# Run the workflow\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\nresult = workflow.invoke([\"quantum computing\", \"climate change\", \"history of aviation\"], config=config)\nprint(result)\n</code></pre> :::</p> <p>:::js <pre><code>import { v4 as uuidv4 } from \"uuid\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { entrypoint, task, MemorySaver } from \"@langchain/langgraph\";\n\n// Initialize the LLM model\nconst llm = new ChatOpenAI({ model: \"gpt-3.5-turbo\" });\n\n// Task that generates a paragraph about a given topic\nconst generateParagraph = task(\"generateParagraph\", async (topic: string) =&gt; {\n  const response = await llm.invoke([\n    { role: \"system\", content: \"You are a helpful assistant that writes educational paragraphs.\" },\n    { role: \"user\", content: `Write a paragraph about ${topic}.` }\n  ]);\n  return response.content as string;\n});\n\n// Create a checkpointer for persistence\nconst checkpointer = new MemorySaver();\n\nconst workflow = entrypoint(\n  { checkpointer, name: \"workflow\" },\n  async (topics: string[]) =&gt; {\n    // Generates multiple paragraphs in parallel and combines them\n    const paragraphs = await Promise.all(topics.map(generateParagraph));\n    return paragraphs.join(\"\\n\\n\");\n  }\n);\n\n// Run the workflow\nconst config = { configurable: { thread_id: uuidv4() } };\nconst result = await workflow.invoke([\"quantum computing\", \"climate change\", \"history of aviation\"], config);\nconsole.log(result);\n</code></pre> :::</p> <p>This example uses LangGraph's concurrency model to improve execution time, especially when tasks involve I/O like LLM completions.</p>"}, {"location": "how-tos/use-functional-api/#calling-graphs", "title": "Calling graphs", "text": "<p>The Functional API and the Graph API can be used together in the same application as they share the same underlying runtime.</p> <p>:::python</p> <pre><code>from langgraph.func import entrypoint\nfrom langgraph.graph import StateGraph\n\nbuilder = StateGraph()\n...\nsome_graph = builder.compile()\n\n@entrypoint()\ndef some_workflow(some_input: dict) -&gt; int:\n    # Call a graph defined using the graph API\n    result_1 = some_graph.invoke(...)\n    # Call another graph defined using the graph API\n    result_2 = another_graph.invoke(...)\n    return {\n        \"result_1\": result_1,\n        \"result_2\": result_2\n    }\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { entrypoint } from \"@langchain/langgraph\";\nimport { StateGraph } from \"@langchain/langgraph\";\n\nconst builder = new StateGraph(/* ... */);\n// ...\nconst someGraph = builder.compile();\n\nconst someWorkflow = entrypoint(\n  { name: \"someWorkflow\" },\n  async (someInput: Record&lt;string, any&gt;) =&gt; {\n    // Call a graph defined using the graph API\n    const result1 = await someGraph.invoke(/* ... */);\n    // Call another graph defined using the graph API\n    const result2 = await anotherGraph.invoke(/* ... */);\n    return {\n      result1,\n      result2,\n    };\n  }\n);\n</code></pre> <p>:::</p> Extended example: calling a simple graph from the functional API <p>:::python <pre><code>import uuid\nfrom typing import TypedDict\nfrom langgraph.func import entrypoint\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph import StateGraph\n\n# Define the shared state type\nclass State(TypedDict):\n    foo: int\n\n# Define a simple transformation node\ndef double(state: State) -&gt; State:\n    return {\"foo\": state[\"foo\"] * 2}\n\n# Build the graph using the Graph API\nbuilder = StateGraph(State)\nbuilder.add_node(\"double\", double)\nbuilder.set_entry_point(\"double\")\ngraph = builder.compile()\n\n# Define the functional API workflow\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(x: int) -&gt; dict:\n    result = graph.invoke({\"foo\": x})\n    return {\"bar\": result[\"foo\"]}\n\n# Execute the workflow\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\nprint(workflow.invoke(5, config=config))  # Output: {'bar': 10}\n</code></pre> :::</p> <p>:::js <pre><code>import { v4 as uuidv4 } from \"uuid\";\nimport { entrypoint, MemorySaver } from \"@langchain/langgraph\";\nimport { StateGraph } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\n// Define the shared state type\nconst State = z.object({\n  foo: z.number(),\n});\n\n// Build the graph using the Graph API\nconst builder = new StateGraph(State)\n  .addNode(\"double\", (state) =&gt; {\n    return { foo: state.foo * 2 };\n  })\n  .addEdge(\"__start__\", \"double\");\nconst graph = builder.compile();\n\n// Define the functional API workflow\nconst checkpointer = new MemorySaver();\n\nconst workflow = entrypoint(\n  { checkpointer, name: \"workflow\" },\n  async (x: number) =&gt; {\n    const result = await graph.invoke({ foo: x });\n    return { bar: result.foo };\n  }\n);\n\n// Execute the workflow\nconst config = { configurable: { thread_id: uuidv4() } };\nconsole.log(await workflow.invoke(5, config)); // Output: { bar: 10 }\n</code></pre> :::</p>"}, {"location": "how-tos/use-functional-api/#call-other-entrypoints", "title": "Call other entrypoints", "text": "<p>You can call other entrypoints from within an entrypoint or a task.</p> <p>:::python</p> <pre><code>@entrypoint() # Will automatically use the checkpointer from the parent entrypoint\ndef some_other_workflow(inputs: dict) -&gt; int:\n    return inputs[\"value\"]\n\n@entrypoint(checkpointer=checkpointer)\ndef my_workflow(inputs: dict) -&gt; int:\n    value = some_other_workflow.invoke({\"value\": 1})\n    return value\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>// Will automatically use the checkpointer from the parent entrypoint\nconst someOtherWorkflow = entrypoint(\n  { name: \"someOtherWorkflow\" },\n  async (inputs: { value: number }) =&gt; {\n    return inputs.value;\n  }\n);\n\nconst myWorkflow = entrypoint(\n  { checkpointer, name: \"myWorkflow\" },\n  async (inputs: { value: number }) =&gt; {\n    const value = await someOtherWorkflow.invoke({ value: 1 });\n    return value;\n  }\n);\n</code></pre> <p>:::</p> Extended example: calling another entrypoint <p>:::python <pre><code>import uuid\nfrom langgraph.func import entrypoint\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Initialize a checkpointer\ncheckpointer = InMemorySaver()\n\n# A reusable sub-workflow that multiplies a number\n@entrypoint()\ndef multiply(inputs: dict) -&gt; int:\n    return inputs[\"a\"] * inputs[\"b\"]\n\n# Main workflow that invokes the sub-workflow\n@entrypoint(checkpointer=checkpointer)\ndef main(inputs: dict) -&gt; dict:\n    result = multiply.invoke({\"a\": inputs[\"x\"], \"b\": inputs[\"y\"]})\n    return {\"product\": result}\n\n# Execute the main workflow\nconfig = {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\nprint(main.invoke({\"x\": 6, \"y\": 7}, config=config))  # Output: {'product': 42}\n</code></pre> :::</p> <p>:::js <pre><code>import { v4 as uuidv4 } from \"uuid\";\nimport { entrypoint, MemorySaver } from \"@langchain/langgraph\";\n\n// Initialize a checkpointer\nconst checkpointer = new MemorySaver();\n\n// A reusable sub-workflow that multiplies a number\nconst multiply = entrypoint(\n  { name: \"multiply\" },\n  async (inputs: { a: number; b: number }) =&gt; {\n    return inputs.a * inputs.b;\n  }\n);\n\n// Main workflow that invokes the sub-workflow\nconst main = entrypoint(\n  { checkpointer, name: \"main\" },\n  async (inputs: { x: number; y: number }) =&gt; {\n    const result = await multiply.invoke({ a: inputs.x, b: inputs.y });\n    return { product: result };\n  }\n);\n\n// Execute the main workflow\nconst config = { configurable: { thread_id: uuidv4() } };\nconsole.log(await main.invoke({ x: 6, y: 7 }, config)); // Output: { product: 42 }\n</code></pre> :::</p>"}, {"location": "how-tos/use-functional-api/#streaming", "title": "Streaming", "text": "<p>The Functional API uses the same streaming mechanism as the Graph API. Please read the streaming guide section for more details.</p> <p>Example of using the streaming API to stream both updates and custom data.</p> <p>:::python</p> <pre><code>from langgraph.func import entrypoint\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.config import get_stream_writer # (1)!\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef main(inputs: dict) -&gt; int:\n    writer = get_stream_writer() # (2)!\n    writer(\"Started processing\") # (3)!\n    result = inputs[\"x\"] * 2\n    writer(f\"Result is {result}\") # (4)!\n    return result\n\nconfig = {\"configurable\": {\"thread_id\": \"abc\"}}\n\n# highlight-next-line\nfor mode, chunk in main.stream( # (5)!\n    {\"x\": 5},\n    stream_mode=[\"custom\", \"updates\"], # (6)!\n    config=config\n):\n    print(f\"{mode}: {chunk}\")\n</code></pre> <ol> <li>Import <code>get_stream_writer</code> from <code>langgraph.config</code>.</li> <li>Obtain a stream writer instance within the entrypoint.</li> <li>Emit custom data before computation begins.</li> <li>Emit another custom message after computing the result.</li> <li>Use <code>.stream()</code> to process streamed output.</li> <li>Specify which streaming modes to use.</li> </ol> <pre><code>('updates', {'add_one': 2})\n('updates', {'add_two': 3})\n('custom', 'hello')\n('custom', 'world')\n('updates', {'main': 5})\n</code></pre> <p>Async with Python &lt; 3.11</p> <p>If using Python &lt; 3.11 and writing async code, using <code>get_stream_writer()</code> will not work. Instead please use the <code>StreamWriter</code> class directly. See Async with Python &lt; 3.11 for more details.</p> <pre><code>from langgraph.types import StreamWriter\n\n@entrypoint(checkpointer=checkpointer)\n# highlight-next-line\nasync def main(inputs: dict, writer: StreamWriter) -&gt; int:\n    ...\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import {\n  entrypoint,\n  MemorySaver,\n  LangGraphRunnableConfig,\n} from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst main = entrypoint(\n  { checkpointer, name: \"main\" },\n  async (\n    inputs: { x: number },\n    config: LangGraphRunnableConfig\n  ): Promise&lt;number&gt; =&gt; {\n    config.writer?.(\"Started processing\"); // (1)!\n    const result = inputs.x * 2;\n    config.writer?.(`Result is ${result}`); // (2)!\n    return result;\n  }\n);\n\nconst config = { configurable: { thread_id: \"abc\" } };\n\n// (3)!\nfor await (const [mode, chunk] of await main.stream(\n  { x: 5 },\n  { streamMode: [\"custom\", \"updates\"], ...config } // (4)!\n)) {\n  console.log(`${mode}: ${JSON.stringify(chunk)}`);\n}\n</code></pre> <ol> <li>Emit custom data before computation begins.</li> <li>Emit another custom message after computing the result.</li> <li>Use <code>.stream()</code> to process streamed output.</li> <li>Specify which streaming modes to use.</li> </ol> <pre><code>updates: {\"addOne\": 2}\nupdates: {\"addTwo\": 3}\ncustom: \"hello\"\ncustom: \"world\"\nupdates: {\"main\": 5}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/use-functional-api/#retry-policy", "title": "Retry policy", "text": "<p>:::python</p> <pre><code>from langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import RetryPolicy\n\n# This variable is just used for demonstration purposes to simulate a network failure.\n# It's not something you will have in your actual code.\nattempts = 0\n\n# Let's configure the RetryPolicy to retry on ValueError.\n# The default RetryPolicy is optimized for retrying specific network errors.\nretry_policy = RetryPolicy(retry_on=ValueError)\n\n@task(retry_policy=retry_policy)\ndef get_info():\n    global attempts\n    attempts += 1\n\n    if attempts &lt; 2:\n        raise ValueError('Failure')\n    return \"OK\"\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef main(inputs, writer):\n    return get_info().result()\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"\n    }\n}\n\nmain.invoke({'any_input': 'foobar'}, config=config)\n</code></pre> <pre><code>'OK'\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import {\n  MemorySaver,\n  entrypoint,\n  task,\n  RetryPolicy,\n} from \"@langchain/langgraph\";\n\n// This variable is just used for demonstration purposes to simulate a network failure.\n// It's not something you will have in your actual code.\nlet attempts = 0;\n\n// Let's configure the RetryPolicy to retry on ValueError.\n// The default RetryPolicy is optimized for retrying specific network errors.\nconst retryPolicy: RetryPolicy = { retryOn: (error) =&gt; error instanceof Error };\n\nconst getInfo = task(\n  {\n    name: \"getInfo\",\n    retry: retryPolicy,\n  },\n  () =&gt; {\n    attempts += 1;\n\n    if (attempts &lt; 2) {\n      throw new Error(\"Failure\");\n    }\n    return \"OK\";\n  }\n);\n\nconst checkpointer = new MemorySaver();\n\nconst main = entrypoint(\n  { checkpointer, name: \"main\" },\n  async (inputs: Record&lt;string, any&gt;) =&gt; {\n    return await getInfo();\n  }\n);\n\nconst config = {\n  configurable: {\n    thread_id: \"1\",\n  },\n};\n\nawait main.invoke({ any_input: \"foobar\" }, config);\n</code></pre> <pre><code>'OK'\n</code></pre> <p>:::</p>"}, {"location": "how-tos/use-functional-api/#caching-tasks", "title": "Caching Tasks", "text": "<p>:::python</p> <pre><code>import time\nfrom langgraph.cache.memory import InMemoryCache\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import CachePolicy\n\n\n@task(cache_policy=CachePolicy(ttl=120))  # (1)!\ndef slow_add(x: int) -&gt; int:\n    time.sleep(1)\n    return x * 2\n\n\n@entrypoint(cache=InMemoryCache())\ndef main(inputs: dict) -&gt; dict[str, int]:\n    result1 = slow_add(inputs[\"x\"]).result()\n    result2 = slow_add(inputs[\"x\"]).result()\n    return {\"result1\": result1, \"result2\": result2}\n\n\nfor chunk in main.stream({\"x\": 5}, stream_mode=\"updates\"):\n    print(chunk)\n\n#&gt; {'slow_add': 10}\n#&gt; {'slow_add': 10, '__metadata__': {'cached': True}}\n#&gt; {'main': {'result1': 10, 'result2': 10}}\n</code></pre> <ol> <li><code>ttl</code> is specified in seconds. The cache will be invalidated after this time.    :::</li> </ol> <p>:::js</p> <pre><code>import {\n  InMemoryCache,\n  entrypoint,\n  task,\n  CachePolicy,\n} from \"@langchain/langgraph\";\n\nconst slowAdd = task(\n  {\n    name: \"slowAdd\",\n    cache: { ttl: 120 }, // (1)!\n  },\n  async (x: number) =&gt; {\n    await new Promise((resolve) =&gt; setTimeout(resolve, 1000));\n    return x * 2;\n  }\n);\n\nconst main = entrypoint(\n  { cache: new InMemoryCache(), name: \"main\" },\n  async (inputs: { x: number }) =&gt; {\n    const result1 = await slowAdd(inputs.x);\n    const result2 = await slowAdd(inputs.x);\n    return { result1, result2 };\n  }\n);\n\nfor await (const chunk of await main.stream(\n  { x: 5 },\n  { streamMode: \"updates\" }\n)) {\n  console.log(chunk);\n}\n\n//&gt; { slowAdd: 10 }\n//&gt; { slowAdd: 10, '__metadata__': { cached: true } }\n//&gt; { main: { result1: 10, result2: 10 } }\n</code></pre> <ol> <li><code>ttl</code> is specified in seconds. The cache will be invalidated after this time.    :::</li> </ol>"}, {"location": "how-tos/use-functional-api/#resuming-after-an-error", "title": "Resuming after an error", "text": "<p>:::python</p> <pre><code>import time\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.func import entrypoint, task\nfrom langgraph.types import StreamWriter\n\n# This variable is just used for demonstration purposes to simulate a network failure.\n# It's not something you will have in your actual code.\nattempts = 0\n\n@task()\ndef get_info():\n    \"\"\"\n    Simulates a task that fails once before succeeding.\n    Raises an exception on the first attempt, then returns \"OK\" on subsequent tries.\n    \"\"\"\n    global attempts\n    attempts += 1\n\n    if attempts &lt; 2:\n        raise ValueError(\"Failure\")  # Simulate a failure on the first attempt\n    return \"OK\"\n\n# Initialize an in-memory checkpointer for persistence\ncheckpointer = InMemorySaver()\n\n@task\ndef slow_task():\n    \"\"\"\n    Simulates a slow-running task by introducing a 1-second delay.\n    \"\"\"\n    time.sleep(1)\n    return \"Ran slow task.\"\n\n@entrypoint(checkpointer=checkpointer)\ndef main(inputs, writer: StreamWriter):\n    \"\"\"\n    Main workflow function that runs the slow_task and get_info tasks sequentially.\n\n    Parameters:\n    - inputs: Dictionary containing workflow input values.\n    - writer: StreamWriter for streaming custom data.\n\n    The workflow first executes `slow_task` and then attempts to execute `get_info`,\n    which will fail on the first invocation.\n    \"\"\"\n    slow_task_result = slow_task().result()  # Blocking call to slow_task\n    get_info().result()  # Exception will be raised here on the first attempt\n    return slow_task_result\n\n# Workflow execution configuration with a unique thread identifier\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"1\"  # Unique identifier to track workflow execution\n    }\n}\n\n# This invocation will take ~1 second due to the slow_task execution\ntry:\n    # First invocation will raise an exception due to the `get_info` task failing\n    main.invoke({'any_input': 'foobar'}, config=config)\nexcept ValueError:\n    pass  # Handle the failure gracefully\n</code></pre> <p>When we resume execution, we won't need to re-run the <code>slow_task</code> as its result is already saved in the checkpoint.</p> <pre><code>main.invoke(None, config=config)\n</code></pre> <pre><code>'Ran slow task.'\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { entrypoint, task, MemorySaver } from \"@langchain/langgraph\";\n\n// This variable is just used for demonstration purposes to simulate a network failure.\n// It's not something you will have in your actual code.\nlet attempts = 0;\n\nconst getInfo = task(\"getInfo\", async () =&gt; {\n  /**\n   * Simulates a task that fails once before succeeding.\n   * Throws an exception on the first attempt, then returns \"OK\" on subsequent tries.\n   */\n  attempts += 1;\n\n  if (attempts &lt; 2) {\n    throw new Error(\"Failure\"); // Simulate a failure on the first attempt\n  }\n  return \"OK\";\n});\n\n// Initialize an in-memory checkpointer for persistence\nconst checkpointer = new MemorySaver();\n\nconst slowTask = task(\"slowTask\", async () =&gt; {\n  /**\n   * Simulates a slow-running task by introducing a 1-second delay.\n   */\n  await new Promise((resolve) =&gt; setTimeout(resolve, 1000));\n  return \"Ran slow task.\";\n});\n\nconst main = entrypoint(\n  { checkpointer, name: \"main\" },\n  async (inputs: Record&lt;string, any&gt;) =&gt; {\n    /**\n     * Main workflow function that runs the slowTask and getInfo tasks sequentially.\n     *\n     * Parameters:\n     * - inputs: Record&lt;string, any&gt; containing workflow input values.\n     *\n     * The workflow first executes `slowTask` and then attempts to execute `getInfo`,\n     * which will fail on the first invocation.\n     */\n    const slowTaskResult = await slowTask(); // Blocking call to slowTask\n    await getInfo(); // Exception will be raised here on the first attempt\n    return slowTaskResult;\n  }\n);\n\n// Workflow execution configuration with a unique thread identifier\nconst config = {\n  configurable: {\n    thread_id: \"1\", // Unique identifier to track workflow execution\n  },\n};\n\n// This invocation will take ~1 second due to the slowTask execution\ntry {\n  // First invocation will raise an exception due to the `getInfo` task failing\n  await main.invoke({ any_input: \"foobar\" }, config);\n} catch (err) {\n  // Handle the failure gracefully\n}\n</code></pre> <p>When we resume execution, we won't need to re-run the <code>slowTask</code> as its result is already saved in the checkpoint.</p> <pre><code>await main.invoke(null, config);\n</code></pre> <pre><code>'Ran slow task.'\n</code></pre> <p>:::</p>"}, {"location": "how-tos/use-functional-api/#human-in-the-loop", "title": "Human-in-the-loop", "text": "<p>The functional API supports human-in-the-loop workflows using the <code>interrupt</code> function and the <code>Command</code> primitive.</p>"}, {"location": "how-tos/use-functional-api/#basic-human-in-the-loop-workflow", "title": "Basic human-in-the-loop workflow", "text": "<p>We will create three tasks:</p> <ol> <li>Append <code>\"bar\"</code>.</li> <li>Pause for human input. When resuming, append human input.</li> <li>Append <code>\"qux\"</code>.</li> </ol> <p>:::python</p> <pre><code>from langgraph.func import entrypoint, task\nfrom langgraph.types import Command, interrupt\n\n\n@task\ndef step_1(input_query):\n    \"\"\"Append bar.\"\"\"\n    return f\"{input_query} bar\"\n\n\n@task\ndef human_feedback(input_query):\n    \"\"\"Append user input.\"\"\"\n    feedback = interrupt(f\"Please provide feedback: {input_query}\")\n    return f\"{input_query} {feedback}\"\n\n\n@task\ndef step_3(input_query):\n    \"\"\"Append qux.\"\"\"\n    return f\"{input_query} qux\"\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { entrypoint, task, interrupt, Command } from \"@langchain/langgraph\";\n\nconst step1 = task(\"step1\", async (inputQuery: string) =&gt; {\n  // Append bar\n  return `${inputQuery} bar`;\n});\n\nconst humanFeedback = task(\"humanFeedback\", async (inputQuery: string) =&gt; {\n  // Append user input\n  const feedback = interrupt(`Please provide feedback: ${inputQuery}`);\n  return `${inputQuery} ${feedback}`;\n});\n\nconst step3 = task(\"step3\", async (inputQuery: string) =&gt; {\n  // Append qux\n  return `${inputQuery} qux`;\n});\n</code></pre> <p>:::</p> <p>We can now compose these tasks in an entrypoint:</p> <p>:::python</p> <pre><code>from langgraph.checkpoint.memory import InMemorySaver\n\ncheckpointer = InMemorySaver()\n\n\n@entrypoint(checkpointer=checkpointer)\ndef graph(input_query):\n    result_1 = step_1(input_query).result()\n    result_2 = human_feedback(result_1).result()\n    result_3 = step_3(result_2).result()\n\n    return result_3\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { MemorySaver } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst graph = entrypoint(\n  { checkpointer, name: \"graph\" },\n  async (inputQuery: string) =&gt; {\n    const result1 = await step1(inputQuery);\n    const result2 = await humanFeedback(result1);\n    const result3 = await step3(result2);\n\n    return result3;\n  }\n);\n</code></pre> <p>:::</p> <p>interrupt() is called inside a task, enabling a human to review and edit the output of the previous task. The results of prior tasks-- in this case <code>step_1</code>-- are persisted, so that they are not run again following the <code>interrupt</code>.</p> <p>Let's send in a query string:</p> <p>:::python</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"1\"}}\n\nfor event in graph.stream(\"foo\", config):\n    print(event)\n    print(\"\\n\")\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const config = { configurable: { thread_id: \"1\" } };\n\nfor await (const event of await graph.stream(\"foo\", config)) {\n  console.log(event);\n  console.log(\"\\n\");\n}\n</code></pre> <p>:::</p> <p>Note that we've paused with an <code>interrupt</code> after <code>step_1</code>. The interrupt provides instructions to resume the run. To resume, we issue a Command containing the data expected by the <code>human_feedback</code> task.</p> <p>:::python</p> <pre><code># Continue execution\nfor event in graph.stream(Command(resume=\"baz\"), config):\n    print(event)\n    print(\"\\n\")\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>// Continue execution\nfor await (const event of await graph.stream(\n  new Command({ resume: \"baz\" }),\n  config\n)) {\n  console.log(event);\n  console.log(\"\\n\");\n}\n</code></pre> <p>:::</p> <p>After resuming, the run proceeds through the remaining step and terminates as expected.</p>"}, {"location": "how-tos/use-functional-api/#review-tool-calls", "title": "Review tool calls", "text": "<p>To review tool calls before execution, we add a <code>review_tool_call</code> function that calls <code>interrupt</code>. When this function is called, execution will be paused until we issue a command to resume it.</p> <p>Given a tool call, our function will <code>interrupt</code> for human review. At that point we can either:</p> <ul> <li>Accept the tool call</li> <li>Revise the tool call and continue</li> <li>Generate a custom tool message (e.g., instructing the model to re-format its tool call)</li> </ul> <p>:::python</p> <pre><code>from typing import Union\n\ndef review_tool_call(tool_call: ToolCall) -&gt; Union[ToolCall, ToolMessage]:\n    \"\"\"Review a tool call, returning a validated version.\"\"\"\n    human_review = interrupt(\n        {\n            \"question\": \"Is this correct?\",\n            \"tool_call\": tool_call,\n        }\n    )\n    review_action = human_review[\"action\"]\n    review_data = human_review.get(\"data\")\n    if review_action == \"continue\":\n        return tool_call\n    elif review_action == \"update\":\n        updated_tool_call = {**tool_call, **{\"args\": review_data}}\n        return updated_tool_call\n    elif review_action == \"feedback\":\n        return ToolMessage(\n            content=review_data, name=tool_call[\"name\"], tool_call_id=tool_call[\"id\"]\n        )\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { ToolCall } from \"@langchain/core/messages/tool\";\nimport { ToolMessage } from \"@langchain/core/messages\";\n\nfunction reviewToolCall(toolCall: ToolCall): ToolCall | ToolMessage {\n  // Review a tool call, returning a validated version\n  const humanReview = interrupt({\n    question: \"Is this correct?\",\n    tool_call: toolCall,\n  });\n\n  const reviewAction = humanReview.action;\n  const reviewData = humanReview.data;\n\n  if (reviewAction === \"continue\") {\n    return toolCall;\n  } else if (reviewAction === \"update\") {\n    const updatedToolCall = { ...toolCall, args: reviewData };\n    return updatedToolCall;\n  } else if (reviewAction === \"feedback\") {\n    return new ToolMessage({\n      content: reviewData,\n      name: toolCall.name,\n      tool_call_id: toolCall.id,\n    });\n  }\n\n  throw new Error(`Unknown review action: ${reviewAction}`);\n}\n</code></pre> <p>:::</p> <p>We can now update our entrypoint to review the generated tool calls. If a tool call is accepted or revised, we execute in the same way as before. Otherwise, we just append the <code>ToolMessage</code> supplied by the human. The results of prior tasks \u2014 in this case the initial model call \u2014 are persisted, so that they are not run again following the <code>interrupt</code>.</p> <p>:::python</p> <pre><code>from langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph.message import add_messages\nfrom langgraph.types import Command, interrupt\n\n\ncheckpointer = InMemorySaver()\n\n\n@entrypoint(checkpointer=checkpointer)\ndef agent(messages, previous):\n    if previous is not None:\n        messages = add_messages(previous, messages)\n\n    llm_response = call_model(messages).result()\n    while True:\n        if not llm_response.tool_calls:\n            break\n\n        # Review tool calls\n        tool_results = []\n        tool_calls = []\n        for i, tool_call in enumerate(llm_response.tool_calls):\n            review = review_tool_call(tool_call)\n            if isinstance(review, ToolMessage):\n                tool_results.append(review)\n            else:  # is a validated tool call\n                tool_calls.append(review)\n                if review != tool_call:\n                    llm_response.tool_calls[i] = review  # update message\n\n        # Execute remaining tool calls\n        tool_result_futures = [call_tool(tool_call) for tool_call in tool_calls]\n        remaining_tool_results = [fut.result() for fut in tool_result_futures]\n\n        # Append to message list\n        messages = add_messages(\n            messages,\n            [llm_response, *tool_results, *remaining_tool_results],\n        )\n\n        # Call model again\n        llm_response = call_model(messages).result()\n\n    # Generate final response\n    messages = add_messages(messages, llm_response)\n    return entrypoint.final(value=llm_response, save=messages)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import {\n  MemorySaver,\n  entrypoint,\n  interrupt,\n  Command,\n  addMessages,\n} from \"@langchain/langgraph\";\nimport { ToolMessage, AIMessage, BaseMessage } from \"@langchain/core/messages\";\n\nconst checkpointer = new MemorySaver();\n\nconst agent = entrypoint(\n  { checkpointer, name: \"agent\" },\n  async (\n    messages: BaseMessage[],\n    previous?: BaseMessage[]\n  ): Promise&lt;BaseMessage&gt; =&gt; {\n    if (previous !== undefined) {\n      messages = addMessages(previous, messages);\n    }\n\n    let llmResponse = await callModel(messages);\n    while (true) {\n      if (!llmResponse.tool_calls?.length) {\n        break;\n      }\n\n      // Review tool calls\n      const toolResults: ToolMessage[] = [];\n      const toolCalls: ToolCall[] = [];\n\n      for (let i = 0; i &lt; llmResponse.tool_calls.length; i++) {\n        const review = reviewToolCall(llmResponse.tool_calls[i]);\n        if (review instanceof ToolMessage) {\n          toolResults.push(review);\n        } else {\n          // is a validated tool call\n          toolCalls.push(review);\n          if (review !== llmResponse.tool_calls[i]) {\n            llmResponse.tool_calls[i] = review; // update message\n          }\n        }\n      }\n\n      // Execute remaining tool calls\n      const remainingToolResults = await Promise.all(\n        toolCalls.map((toolCall) =&gt; callTool(toolCall))\n      );\n\n      // Append to message list\n      messages = addMessages(messages, [\n        llmResponse,\n        ...toolResults,\n        ...remainingToolResults,\n      ]);\n\n      // Call model again\n      llmResponse = await callModel(messages);\n    }\n\n    // Generate final response\n    messages = addMessages(messages, llmResponse);\n    return entrypoint.final({ value: llmResponse, save: messages });\n  }\n);\n</code></pre> <p>:::</p>"}, {"location": "how-tos/use-functional-api/#short-term-memory", "title": "Short-term memory", "text": "<p>Short-term memory allows storing information across different invocations of the same thread id. See short-term memory for more details.</p>"}, {"location": "how-tos/use-functional-api/#manage-checkpoints", "title": "Manage checkpoints", "text": "<p>You can view and delete the information stored by the checkpointer.</p>"}, {"location": "how-tos/use-functional-api/#view-thread-state-checkpoint", "title": "View thread state (checkpoint)", "text": "<p>:::python</p> <pre><code>config = {\n    \"configurable\": {\n        # highlight-next-line\n        \"thread_id\": \"1\",\n        # optionally provide an ID for a specific checkpoint,\n        # otherwise the latest checkpoint is shown\n        # highlight-next-line\n        # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"\n\n    }\n}\n# highlight-next-line\ngraph.get_state(config)\n</code></pre> <pre><code>StateSnapshot(\n    values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(),\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n    metadata={\n        'source': 'loop',\n        'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n        'step': 4,\n        'parents': {},\n        'thread_id': '1'\n    },\n    created_at='2025-05-05T16:01:24.680462+00:00',\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n    tasks=(),\n    interrupts=()\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const config = {\n  configurable: {\n    // highlight-next-line\n    thread_id: \"1\",\n    // optionally provide an ID for a specific checkpoint,\n    // otherwise the latest checkpoint is shown\n    // highlight-next-line\n    // checkpoint_id: \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"\n  },\n};\n// highlight-next-line\nawait graph.getState(config);\n</code></pre> <pre><code>StateSnapshot {\n  values: {\n    messages: [\n      HumanMessage { content: \"hi! I'm bob\" },\n      AIMessage { content: \"Hi Bob! How are you doing today?\" },\n      HumanMessage { content: \"what's my name?\" },\n      AIMessage { content: \"Your name is Bob.\" }\n    ]\n  },\n  next: [],\n  config: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1f5b-6704-8004-820c16b69a5a' } },\n  metadata: {\n    source: 'loop',\n    writes: { call_model: { messages: AIMessage { content: \"Your name is Bob.\" } } },\n    step: 4,\n    parents: {},\n    thread_id: '1'\n  },\n  createdAt: '2025-05-05T16:01:24.680462+00:00',\n  parentConfig: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1790-6b0a-8003-baf965b6a38f' } },\n  tasks: [],\n  interrupts: []\n}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/use-functional-api/#view-the-history-of-the-thread-checkpoints", "title": "View the history of the thread (checkpoints)", "text": "<p>:::python</p> <pre><code>config = {\n    \"configurable\": {\n        # highlight-next-line\n        \"thread_id\": \"1\"\n    }\n}\n# highlight-next-line\nlist(graph.get_state_history(config))\n</code></pre> <pre><code>[\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n        next=(),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:24.680462+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        tasks=(),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")]},\n        next=('call_model',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.863421+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n        next=('__start__',),\n        config={...},\n        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.863173+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"what's my name?\"}]}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n        next=(),\n        config={...},\n        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.862295+00:00',\n        parent_config={...}\n        tasks=(),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\")]},\n        next=('call_model',),\n        config={...},\n        metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:22.278960+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': []},\n        next=('__start__',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\n        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:22.277497+00:00',\n        parent_config=None,\n        tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}),),\n        interrupts=()\n    )\n]\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const config = {\n  configurable: {\n    // highlight-next-line\n    thread_id: \"1\",\n  },\n};\n// highlight-next-line\nconst history = [];\nfor await (const state of graph.getStateHistory(config)) {\n  history.push(state);\n}\n</code></pre> <pre><code>[\n  StateSnapshot {\n    values: {\n      messages: [\n        HumanMessage { content: \"hi! I'm bob\" },\n        AIMessage { content: \"Hi Bob! How are you doing today? Is there anything I can help you with?\" },\n        HumanMessage { content: \"what's my name?\" },\n        AIMessage { content: \"Your name is Bob.\" }\n      ]\n    },\n    next: [],\n    config: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1f5b-6704-8004-820c16b69a5a' } },\n    metadata: { source: 'loop', writes: { call_model: { messages: AIMessage { content: \"Your name is Bob.\" } } }, step: 4, parents: {}, thread_id: '1' },\n    createdAt: '2025-05-05T16:01:24.680462+00:00',\n    parentConfig: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1790-6b0a-8003-baf965b6a38f' } },\n    tasks: [],\n    interrupts: []\n  },\n  // ... more state snapshots\n]\n</code></pre> <p>:::</p>"}, {"location": "how-tos/use-functional-api/#decouple-return-value-from-saved-value", "title": "Decouple return value from saved value", "text": "<p>Use <code>entrypoint.final</code> to decouple what is returned to the caller from what is persisted in the checkpoint. This is useful when:</p> <ul> <li>You want to return a computed result (e.g., a summary or status), but save a different internal value for use on the next invocation.</li> <li>You need to control what gets passed to the previous parameter on the next run.</li> </ul> <p>:::python</p> <pre><code>from typing import Optional\nfrom langgraph.func import entrypoint\nfrom langgraph.checkpoint.memory import InMemorySaver\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef accumulate(n: int, *, previous: Optional[int]) -&gt; entrypoint.final[int, int]:\n    previous = previous or 0\n    total = previous + n\n    # Return the *previous* value to the caller but save the *new* total to the checkpoint.\n    return entrypoint.final(value=previous, save=total)\n\nconfig = {\"configurable\": {\"thread_id\": \"my-thread\"}}\n\nprint(accumulate.invoke(1, config=config))  # 0\nprint(accumulate.invoke(2, config=config))  # 1\nprint(accumulate.invoke(3, config=config))  # 3\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { entrypoint, MemorySaver } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst accumulate = entrypoint(\n  { checkpointer, name: \"accumulate\" },\n  async (n: number, previous?: number) =&gt; {\n    const prev = previous || 0;\n    const total = prev + n;\n    // Return the *previous* value to the caller but save the *new* total to the checkpoint.\n    return entrypoint.final({ value: prev, save: total });\n  }\n);\n\nconst config = { configurable: { thread_id: \"my-thread\" } };\n\nconsole.log(await accumulate.invoke(1, config)); // 0\nconsole.log(await accumulate.invoke(2, config)); // 1\nconsole.log(await accumulate.invoke(3, config)); // 3\n</code></pre> <p>:::</p>"}, {"location": "how-tos/use-functional-api/#chatbot-example", "title": "Chatbot example", "text": "<p>An example of a simple chatbot using the functional API and the <code>InMemorySaver</code> checkpointer. The bot is able to remember the previous conversation and continue from where it left off.</p> <p>:::python</p> <pre><code>from langchain_core.messages import BaseMessage\nfrom langgraph.graph import add_messages\nfrom langgraph.func import entrypoint, task\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langchain_anthropic import ChatAnthropic\n\nmodel = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n\n@task\ndef call_model(messages: list[BaseMessage]):\n    response = model.invoke(messages)\n    return response\n\ncheckpointer = InMemorySaver()\n\n@entrypoint(checkpointer=checkpointer)\ndef workflow(inputs: list[BaseMessage], *, previous: list[BaseMessage]):\n    if previous:\n        inputs = add_messages(previous, inputs)\n\n    response = call_model(inputs).result()\n    return entrypoint.final(value=response, save=add_messages(inputs, response))\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ninput_message = {\"role\": \"user\", \"content\": \"hi! I'm bob\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n\ninput_message = {\"role\": \"user\", \"content\": \"what's my name?\"}\nfor chunk in workflow.stream([input_message], config, stream_mode=\"values\"):\n    chunk.pretty_print()\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { BaseMessage } from \"@langchain/core/messages\";\nimport {\n  addMessages,\n  entrypoint,\n  task,\n  MemorySaver,\n} from \"@langchain/langgraph\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst model = new ChatAnthropic({ model: \"claude-3-5-sonnet-latest\" });\n\nconst callModel = task(\n  \"callModel\",\n  async (messages: BaseMessage[]): Promise&lt;BaseMessage&gt; =&gt; {\n    const response = await model.invoke(messages);\n    return response;\n  }\n);\n\nconst checkpointer = new MemorySaver();\n\nconst workflow = entrypoint(\n  { checkpointer, name: \"workflow\" },\n  async (\n    inputs: BaseMessage[],\n    previous?: BaseMessage[]\n  ): Promise&lt;BaseMessage&gt; =&gt; {\n    let messages = inputs;\n    if (previous) {\n      messages = addMessages(previous, inputs);\n    }\n\n    const response = await callModel(messages);\n    return entrypoint.final({\n      value: response,\n      save: addMessages(messages, response),\n    });\n  }\n);\n\nconst config = { configurable: { thread_id: \"1\" } };\nconst inputMessage = { role: \"user\", content: \"hi! I'm bob\" };\n\nfor await (const chunk of await workflow.stream([inputMessage], {\n  ...config,\n  streamMode: \"values\",\n})) {\n  console.log(chunk.content);\n}\n\nconst inputMessage2 = { role: \"user\", content: \"what's my name?\" };\nfor await (const chunk of await workflow.stream([inputMessage2], {\n  ...config,\n  streamMode: \"values\",\n})) {\n  console.log(chunk.content);\n}\n</code></pre> <p>:::</p> Extended example: build a simple chatbot <p>How to add thread-level persistence (functional API): Shows how to add thread-level persistence to a functional API workflow and implements a simple chatbot.</p>"}, {"location": "how-tos/use-functional-api/#long-term-memory", "title": "Long-term memory", "text": "<p>long-term memory allows storing information across different thread ids. This could be useful for learning information about a given user in one conversation and using it in another.</p> Extended example: add long-term memory <p>How to add cross-thread persistence (functional API): Shows how to add cross-thread persistence to a functional API workflow and implements a simple chatbot.</p>"}, {"location": "how-tos/use-functional-api/#workflows", "title": "Workflows", "text": "<ul> <li>Workflows and agent guide for more examples of how to build workflows using the Functional API.</li> </ul>"}, {"location": "how-tos/use-functional-api/#agents", "title": "Agents", "text": "<ul> <li>How to create an agent from scratch (Functional API): Shows how to create a simple agent from scratch using the functional API.</li> <li>How to build a multi-agent network: Shows how to build a multi-agent network using the functional API.</li> <li>How to add multi-turn conversation in a multi-agent application (functional API): allow an end-user to engage in a multi-turn conversation with one or more agents.</li> </ul>"}, {"location": "how-tos/use-functional-api/#integrate-with-other-libraries", "title": "Integrate with other libraries", "text": "<ul> <li>Add LangGraph's features to other frameworks using the functional API: Add LangGraph features like persistence, memory and streaming to other agent frameworks that do not provide them out of the box.</li> </ul>"}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/", "title": "Enable human intervention", "text": "<p>To review, edit, and approve tool calls in an agent or workflow, use interrupts to pause a graph and wait for human input. Interrupts use LangGraph's persistence layer, which saves the graph state, to indefinitely pause graph execution until you resume.</p> <p>Info</p> <p>For more information about human-in-the-loop workflows, see the Human-in-the-Loop conceptual guide.</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#pause-using-interrupt", "title": "Pause using <code>interrupt</code>", "text": "<p>:::python Dynamic interrupts (also known as dynamic breakpoints) are triggered based on the current state of the graph. You can set dynamic interrupts by calling @[<code>interrupt</code> function][interrupt] in the appropriate place. The graph will pause, which allows for human intervention, and then resumes the graph with their input. It's useful for tasks like approvals, edits, or gathering additional context.</p> <p>Note</p> <p>As of v1.0, <code>interrupt</code> is the recommended way to pause a graph. <code>NodeInterrupt</code> is deprecated and will be removed in v2.0.</p> <p>:::</p> <p>:::js Dynamic interrupts (also known as dynamic breakpoints) are triggered based on the current state of the graph. You can set dynamic interrupts by calling @[<code>interrupt</code> function][interrupt] in the appropriate place. The graph will pause, which allows for human intervention, and then resumes the graph with their input. It's useful for tasks like approvals, edits, or gathering additional context. :::</p> <p>To use <code>interrupt</code> in your graph, you need to:</p> <ol> <li>Specify a checkpointer to save the graph state after each step.</li> <li>Call <code>interrupt()</code> in the appropriate place. See the Common Patterns section for examples.</li> <li>Run the graph with a thread ID until the <code>interrupt</code> is hit.</li> <li>Resume execution using <code>invoke</code>/<code>stream</code> (see The <code>Command</code> primitive).</li> </ol> <p>:::python</p> <pre><code># highlight-next-line\nfrom langgraph.types import interrupt, Command\n\ndef human_node(state: State):\n    # highlight-next-line\n    value = interrupt( # (1)!\n        {\n            \"text_to_revise\": state[\"some_text\"] # (2)!\n        }\n    )\n    return {\n        \"some_text\": value # (3)!\n    }\n\n\ngraph = graph_builder.compile(checkpointer=checkpointer) # (4)!\n\n# Run the graph until the interrupt is hit.\nconfig = {\"configurable\": {\"thread_id\": \"some_id\"}}\nresult = graph.invoke({\"some_text\": \"original text\"}, config=config) # (5)!\nprint(result['__interrupt__']) # (6)!\n# &gt; [\n# &gt;    Interrupt(\n# &gt;       value={'text_to_revise': 'original text'},\n# &gt;       resumable=True,\n# &gt;       ns=['human_node:6ce9e64f-edef-fe5d-f7dc-511fa9526960']\n# &gt;    )\n# &gt; ]\n\n# highlight-next-line\nprint(graph.invoke(Command(resume=\"Edited text\"), config=config)) # (7)!\n# &gt; {'some_text': 'Edited text'}\n</code></pre> <ol> <li><code>interrupt(...)</code> pauses execution at <code>human_node</code>, surfacing the given payload to a human.</li> <li>Any JSON serializable value can be passed to the <code>interrupt</code> function. Here, a dict containing the text to revise.</li> <li>Once resumed, the return value of <code>interrupt(...)</code> is the human-provided input, which is used to update the state.</li> <li>A checkpointer is required to persist graph state. In production, this should be durable (e.g., backed by a database).</li> <li>The graph is invoked with some initial state.</li> <li>When the graph hits the interrupt, it returns an <code>Interrupt</code> object with the payload and metadata.</li> <li>The graph is resumed with a <code>Command(resume=...)</code>, injecting the human's input and continuing execution.    :::</li> </ol> <p>:::js</p> <pre><code>// highlight-next-line\nimport { interrupt, Command } from \"@langchain/langgraph\";\n\nconst graph = graphBuilder\n  .addNode(\"humanNode\", (state) =&gt; {\n    // highlight-next-line\n    const value = interrupt(\n      // (1)!\n      {\n        textToRevise: state.someText, // (2)!\n      }\n    );\n    return {\n      someText: value, // (3)!\n    };\n  })\n  .addEdge(START, \"humanNode\")\n  .compile({ checkpointer }); // (4)!\n\n// Run the graph until the interrupt is hit.\nconst config = { configurable: { thread_id: \"some_id\" } };\nconst result = await graph.invoke({ someText: \"original text\" }, config); // (5)!\nconsole.log(result.__interrupt__); // (6)!\n// &gt; [\n// &gt;   {\n// &gt;     value: { textToRevise: 'original text' },\n// &gt;     resumable: true,\n// &gt;     ns: ['humanNode:6ce9e64f-edef-fe5d-f7dc-511fa9526960'],\n// &gt;     when: 'during'\n// &gt;   }\n// &gt; ]\n\n// highlight-next-line\nconsole.log(await graph.invoke(new Command({ resume: \"Edited text\" }), config)); // (7)!\n// &gt; { someText: 'Edited text' }\n</code></pre> <ol> <li><code>interrupt(...)</code> pauses execution at <code>humanNode</code>, surfacing the given payload to a human.</li> <li>Any JSON serializable value can be passed to the <code>interrupt</code> function. Here, an object containing the text to revise.</li> <li>Once resumed, the return value of <code>interrupt(...)</code> is the human-provided input, which is used to update the state.</li> <li>A checkpointer is required to persist graph state. In production, this should be durable (e.g., backed by a database).</li> <li>The graph is invoked with some initial state.</li> <li>When the graph hits the interrupt, it returns an object with <code>__interrupt__</code> containing the payload and metadata.</li> <li>The graph is resumed with a <code>Command({ resume: ... })</code>, injecting the human's input and continuing execution.    :::</li> </ol> Extended example: using <code>interrupt</code> <p>:::python <pre><code>from typing import TypedDict\nimport uuid\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.constants import START\nfrom langgraph.graph import StateGraph\n\n# highlight-next-line\nfrom langgraph.types import interrupt, Command\n\n\nclass State(TypedDict):\n    some_text: str\n\n\ndef human_node(state: State):\n    # highlight-next-line\n    value = interrupt(  # (1)!\n        {\n            \"text_to_revise\": state[\"some_text\"]  # (2)!\n        }\n    )\n    return {\n        \"some_text\": value  # (3)!\n    }\n\n\n# Build the graph\ngraph_builder = StateGraph(State)\ngraph_builder.add_node(\"human_node\", human_node)\ngraph_builder.add_edge(START, \"human_node\")\ncheckpointer = InMemorySaver()  # (4)!\ngraph = graph_builder.compile(checkpointer=checkpointer)\n# Pass a thread ID to the graph to run it.\nconfig = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n# Run the graph until the interrupt is hit.\nresult = graph.invoke({\"some_text\": \"original text\"}, config=config)  # (5)!\n\nprint(result['__interrupt__']) # (6)!\n# &gt; [\n# &gt;    Interrupt(\n# &gt;       value={'text_to_revise': 'original text'},\n# &gt;       resumable=True,\n# &gt;       ns=['human_node:6ce9e64f-edef-fe5d-f7dc-511fa9526960']\n# &gt;    )\n# &gt; ]\nprint(result[\"__interrupt__\"])  # (6)!\n# &gt; [Interrupt(value={'text_to_revise': 'original text'}, id='6d7c4048049254c83195429a3659661d')]\n\n# highlight-next-line\nprint(graph.invoke(Command(resume=\"Edited text\"), config=config)) # (7)!\n# &gt; {'some_text': 'Edited text'}\n</code></pre></p> <ol> <li><code>interrupt(...)</code> pauses execution at <code>human_node</code>, surfacing the given payload to a human.</li> <li>Any JSON serializable value can be passed to the <code>interrupt</code> function. Here, a dict containing the text to revise.</li> <li>Once resumed, the return value of <code>interrupt(...)</code> is the human-provided input, which is used to update the state.</li> <li>A checkpointer is required to persist graph state. In production, this should be durable (e.g., backed by a database).</li> <li>The graph is invoked with some initial state.</li> <li>When the graph hits the interrupt, it returns an <code>Interrupt</code> object with the payload and metadata.</li> <li>The graph is resumed with a <code>Command(resume=...)</code>, injecting the human's input and continuing execution. :::</li> </ol> <p>:::js <pre><code>import { z } from \"zod\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport { MemorySaver, StateGraph, START, interrupt, Command } from \"@langchain/langgraph\";\n\nconst StateAnnotation = z.object({\n  someText: z.string(),\n});\n\n// Build the graph\nconst graphBuilder = new StateGraph(StateAnnotation)\n  .addNode(\"humanNode\", (state) =&gt; {\n    // highlight-next-line\n    const value = interrupt( // (1)!\n      {\n        textToRevise: state.someText // (2)!\n      }\n    );\n    return {\n      someText: value // (3)!\n    };\n  })\n  .addEdge(START, \"humanNode\");\n\nconst checkpointer = new MemorySaver(); // (4)!\n\nconst graph = graphBuilder.compile({ checkpointer });\n\n// Pass a thread ID to the graph to run it.\nconst config = { configurable: { thread_id: uuidv4() } };\n\n// Run the graph until the interrupt is hit.\nconst result = await graph.invoke({ someText: \"original text\" }, config); // (5)!\n\nconsole.log(result.__interrupt__); // (6)!\n// &gt; [\n// &gt;   {\n// &gt;     value: { textToRevise: 'original text' },\n// &gt;     resumable: true,\n// &gt;     ns: ['humanNode:6ce9e64f-edef-fe5d-f7dc-511fa9526960'],\n// &gt;     when: 'during'\n// &gt;   }\n// &gt; ]\n\n// highlight-next-line\nconsole.log(await graph.invoke(new Command({ resume: \"Edited text\" }), config)); // (7)!\n// &gt; { someText: 'Edited text' }\n</code></pre></p> <ol> <li><code>interrupt(...)</code> pauses execution at <code>humanNode</code>, surfacing the given payload to a human.</li> <li>Any JSON serializable value can be passed to the <code>interrupt</code> function. Here, an object containing the text to revise.</li> <li>Once resumed, the return value of <code>interrupt(...)</code> is the human-provided input, which is used to update the state.</li> <li>A checkpointer is required to persist graph state. In production, this should be durable (e.g., backed by a database).</li> <li>The graph is invoked with some initial state.</li> <li>When the graph hits the interrupt, it returns an object with <code>__interrupt__</code> containing the payload and metadata.</li> <li>The graph is resumed with a <code>Command({ resume: ... })</code>, injecting the human's input and continuing execution. :::</li> </ol> <p>New in 0.4.0</p> <p>:::python <code>__interrupt__</code> is a special key that will be returned when running the graph if the graph is interrupted. Support for <code>__interrupt__</code> in <code>invoke</code> and <code>ainvoke</code> has been added in version 0.4.0. If you're on an older version, you will only see <code>__interrupt__</code> in the result if you use <code>stream</code> or <code>astream</code>. You can also use <code>graph.get_state(thread_id)</code> to get the interrupt value(s). :::</p> <p>:::js <code>__interrupt__</code> is a special key that will be returned when running the graph if the graph is interrupted. Support for <code>__interrupt__</code> in <code>invoke</code> has been added in version 0.4.0. If you're on an older version, you will only see <code>__interrupt__</code> in the result if you use <code>stream</code>. You can also use <code>graph.getState(config)</code> to get the interrupt value(s). :::</p> <p>Warning</p> <p>:::python Interrupts resemble Python's input() function in terms of developer experience, but they do not automatically resume execution from the interruption point. Instead, they rerun the entire node where the interrupt was used. For this reason, interrupts are typically best placed at the start of a node or in a dedicated node. :::</p> <p>:::js Interrupts are both powerful and ergonomic, but it's important to note that they do not automatically resume execution from the interrupt point. Instead, they rerun the entire where the interrupt was used. For this reason, interrupts are typically best placed at the state of a node or in a dedicated node. :::</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#resume-using-the-command-primitive", "title": "Resume using the <code>Command</code> primitive", "text": "<p>:::python</p> <p>Warning</p> <p>Resuming from an <code>interrupt</code> is different from Python's <code>input()</code> function, where execution resumes from the exact point where the <code>input()</code> function was called.</p> <p>:::</p> <p>When the <code>interrupt</code> function is used within a graph, execution pauses at that point and awaits user input.</p> <p>:::python To resume execution, use the @[<code>Command</code>][Command] primitive, which can be supplied via the <code>invoke</code> or <code>stream</code> methods. The graph resumes execution from the beginning of the node where <code>interrupt(...)</code> was initially called. This time, the <code>interrupt</code> function will return the value provided in <code>Command(resume=value)</code> rather than pausing again. All code from the beginning of the node to the <code>interrupt</code> will be re-executed.</p> <pre><code># Resume graph execution by providing the user's input.\ngraph.invoke(Command(resume={\"age\": \"25\"}), thread_config)\n</code></pre> <p>:::</p> <p>:::js To resume execution, use the @[<code>Command</code>][Command] primitive, which can be supplied via the <code>invoke</code> or <code>stream</code> methods. The graph resumes execution from the beginning of the node where <code>interrupt(...)</code> was initially called. This time, the <code>interrupt</code> function will return the value provided in <code>Command(resume=value)</code> rather than pausing again. All code from the beginning of the node to the <code>interrupt</code> will be re-executed.</p> <pre><code>// Resume graph execution by providing the user's input.\nawait graph.invoke(new Command({ resume: { age: \"25\" } }), threadConfig);\n</code></pre> <p>:::</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#resume-multiple-interrupts-with-one-invocation", "title": "Resume multiple interrupts with one invocation", "text": "<p>When nodes with interrupt conditions are run in parallel, it's possible to have multiple interrupts in the task queue. For example, the following graph has two nodes run in parallel that require human input:</p> <p></p> <p>:::python Once your graph has been interrupted and is stalled, you can resume all the interrupts at once with <code>Command.resume</code>, passing a dictionary mapping of interrupt ids to resume values.</p> <pre><code>from typing import TypedDict\nimport uuid\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.constants import START\nfrom langgraph.graph import StateGraph\nfrom langgraph.types import interrupt, Command\n\n\nclass State(TypedDict):\n    text_1: str\n    text_2: str\n\n\ndef human_node_1(state: State):\n    value = interrupt({\"text_to_revise\": state[\"text_1\"]})\n    return {\"text_1\": value}\n\n\ndef human_node_2(state: State):\n    value = interrupt({\"text_to_revise\": state[\"text_2\"]})\n    return {\"text_2\": value}\n\n\ngraph_builder = StateGraph(State)\ngraph_builder.add_node(\"human_node_1\", human_node_1)\ngraph_builder.add_node(\"human_node_2\", human_node_2)\n\n# Add both nodes in parallel from START\ngraph_builder.add_edge(START, \"human_node_1\")\ngraph_builder.add_edge(START, \"human_node_2\")\n\ncheckpointer = InMemorySaver()\ngraph = graph_builder.compile(checkpointer=checkpointer)\n\nthread_id = str(uuid.uuid4())\nconfig: RunnableConfig = {\"configurable\": {\"thread_id\": thread_id}}\nresult = graph.invoke(\n    {\"text_1\": \"original text 1\", \"text_2\": \"original text 2\"}, config=config\n)\n\n# Resume with mapping of interrupt IDs to values\nresume_map = {\n    i.id: f\"edited text for {i.value['text_to_revise']}\"\n    for i in graph.get_state(config).interrupts\n}\nprint(graph.invoke(Command(resume=resume_map), config=config))\n# &gt; {'text_1': 'edited text for original text 1', 'text_2': 'edited text for original text 2'}\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const state = await parentGraph.getState(threadConfig);\nconst resumeMap = Object.fromEntries(\n  state.interrupts.map((i) =&gt; [\n    i.interruptId,\n    `human input for prompt ${i.value}`,\n  ])\n);\n\nawait parentGraph.invoke(new Command({ resume: resumeMap }), threadConfig);\n</code></pre> <p>:::</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#common-patterns", "title": "Common patterns", "text": "<p>Below we show different design patterns that can be implemented using <code>interrupt</code> and <code>Command</code>.</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#approve-or-reject", "title": "Approve or reject", "text": "Depending on the human's approval or rejection, the graph can proceed with the action or take an alternative path. <p>Pause the graph before a critical step, such as an API call, to review and approve the action. If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action.</p> <p>:::python</p> <pre><code>from typing import Literal\nfrom langgraph.types import interrupt, Command\n\ndef human_approval(state: State) -&gt; Command[Literal[\"some_node\", \"another_node\"]]:\n    is_approved = interrupt(\n        {\n            \"question\": \"Is this correct?\",\n            # Surface the output that should be\n            # reviewed and approved by the human.\n            \"llm_output\": state[\"llm_output\"]\n        }\n    )\n\n    if is_approved:\n        return Command(goto=\"some_node\")\n    else:\n        return Command(goto=\"another_node\")\n\n# Add the node to the graph in an appropriate location\n# and connect it to the relevant nodes.\ngraph_builder.add_node(\"human_approval\", human_approval)\ngraph = graph_builder.compile(checkpointer=checkpointer)\n\n# After running the graph and hitting the interrupt, the graph will pause.\n# Resume it with either an approval or rejection.\nthread_config = {\"configurable\": {\"thread_id\": \"some_id\"}}\ngraph.invoke(Command(resume=True), config=thread_config)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { interrupt, Command } from \"@langchain/langgraph\";\n\n// Add the node to the graph in an appropriate location\n// and connect it to the relevant nodes.\ngraphBuilder.addNode(\"humanApproval\", (state) =&gt; {\n  const isApproved = interrupt({\n    question: \"Is this correct?\",\n    // Surface the output that should be\n    // reviewed and approved by the human.\n    llmOutput: state.llmOutput,\n  });\n\n  if (isApproved) {\n    return new Command({ goto: \"someNode\" });\n  } else {\n    return new Command({ goto: \"anotherNode\" });\n  }\n});\nconst graph = graphBuilder.compile({ checkpointer });\n\n// After running the graph and hitting the interrupt, the graph will pause.\n// Resume it with either an approval or rejection.\nconst threadConfig = { configurable: { thread_id: \"some_id\" } };\nawait graph.invoke(new Command({ resume: true }), threadConfig);\n</code></pre> <p>:::</p> Extended example: approve or reject with interrupt <p>:::python <pre><code>from typing import Literal, TypedDict\nimport uuid\n\nfrom langgraph.constants import START, END\nfrom langgraph.graph import StateGraph\nfrom langgraph.types import interrupt, Command\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Define the shared graph state\nclass State(TypedDict):\n    llm_output: str\n    decision: str\n\n# Simulate an LLM output node\ndef generate_llm_output(state: State) -&gt; State:\n    return {\"llm_output\": \"This is the generated output.\"}\n\n# Human approval node\ndef human_approval(state: State) -&gt; Command[Literal[\"approved_path\", \"rejected_path\"]]:\n    decision = interrupt({\n        \"question\": \"Do you approve the following output?\",\n        \"llm_output\": state[\"llm_output\"]\n    })\n\n    if decision == \"approve\":\n        return Command(goto=\"approved_path\", update={\"decision\": \"approved\"})\n    else:\n        return Command(goto=\"rejected_path\", update={\"decision\": \"rejected\"})\n\n# Next steps after approval\ndef approved_node(state: State) -&gt; State:\n    print(\"\u2705 Approved path taken.\")\n    return state\n\n# Alternative path after rejection\ndef rejected_node(state: State) -&gt; State:\n    print(\"\u274c Rejected path taken.\")\n    return state\n\n# Build the graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"generate_llm_output\", generate_llm_output)\nbuilder.add_node(\"human_approval\", human_approval)\nbuilder.add_node(\"approved_path\", approved_node)\nbuilder.add_node(\"rejected_path\", rejected_node)\n\nbuilder.set_entry_point(\"generate_llm_output\")\nbuilder.add_edge(\"generate_llm_output\", \"human_approval\")\nbuilder.add_edge(\"approved_path\", END)\nbuilder.add_edge(\"rejected_path\", END)\n\ncheckpointer = InMemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Run until interrupt\nconfig = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\nresult = graph.invoke({}, config=config)\nprint(result[\"__interrupt__\"])\n# Output:\n# Interrupt(value={'question': 'Do you approve the following output?', 'llm_output': 'This is the generated output.'}, ...)\n\n# Simulate resuming with human input\n# To test rejection, replace resume=\"approve\" with resume=\"reject\"\nfinal_result = graph.invoke(Command(resume=\"approve\"), config=config)\nprint(final_result)\n</code></pre> :::</p> <p>:::js <pre><code>import { z } from \"zod\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport {\n  StateGraph,\n  START,\n  END,\n  interrupt,\n  Command,\n  MemorySaver\n} from \"@langchain/langgraph\";\n\n// Define the shared graph state\nconst StateAnnotation = z.object({\n  llmOutput: z.string(),\n  decision: z.string(),\n});\n\n// Simulate an LLM output node\nfunction generateLlmOutput(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  return { llmOutput: \"This is the generated output.\" };\n}\n\n// Human approval node\nfunction humanApproval(state: z.infer&lt;typeof StateAnnotation&gt;): Command {\n  const decision = interrupt({\n    question: \"Do you approve the following output?\",\n    llmOutput: state.llmOutput\n  });\n\n  if (decision === \"approve\") {\n    return new Command({\n      goto: \"approvedPath\",\n      update: { decision: \"approved\" }\n    });\n  } else {\n    return new Command({\n      goto: \"rejectedPath\",\n      update: { decision: \"rejected\" }\n    });\n  }\n}\n\n// Next steps after approval\nfunction approvedNode(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  console.log(\"\u2705 Approved path taken.\");\n  return state;\n}\n\n// Alternative path after rejection\nfunction rejectedNode(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  console.log(\"\u274c Rejected path taken.\");\n  return state;\n}\n\n// Build the graph\nconst builder = new StateGraph(StateAnnotation)\n  .addNode(\"generateLlmOutput\", generateLlmOutput)\n  .addNode(\"humanApproval\", humanApproval, {\n    ends: [\"approvedPath\", \"rejectedPath\"]\n  })\n  .addNode(\"approvedPath\", approvedNode)\n  .addNode(\"rejectedPath\", rejectedNode)\n  .addEdge(START, \"generateLlmOutput\")\n  .addEdge(\"generateLlmOutput\", \"humanApproval\")\n  .addEdge(\"approvedPath\", END)\n  .addEdge(\"rejectedPath\", END);\n\nconst checkpointer = new MemorySaver();\nconst graph = builder.compile({ checkpointer });\n\n// Run until interrupt\nconst config = { configurable: { thread_id: uuidv4() } };\nconst result = await graph.invoke({}, config);\nconsole.log(result.__interrupt__);\n// Output:\n// [{\n//   value: {\n//     question: 'Do you approve the following output?',\n//     llmOutput: 'This is the generated output.'\n//   },\n//   ...\n// }]\n\n// Simulate resuming with human input\n// To test rejection, replace resume: \"approve\" with resume: \"reject\"\nconst finalResult = await graph.invoke(\n  new Command({ resume: \"approve\" }),\n  config\n);\nconsole.log(finalResult);\n</code></pre> :::</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#review-and-edit-state", "title": "Review and edit state", "text": "A human can review and edit the state of the graph. This is useful for correcting mistakes or updating the state with additional information.  <p>:::python</p> <pre><code>from langgraph.types import interrupt\n\ndef human_editing(state: State):\n    ...\n    result = interrupt(\n        # Interrupt information to surface to the client.\n        # Can be any JSON serializable value.\n        {\n            \"task\": \"Review the output from the LLM and make any necessary edits.\",\n            \"llm_generated_summary\": state[\"llm_generated_summary\"]\n        }\n    )\n\n    # Update the state with the edited text\n    return {\n        \"llm_generated_summary\": result[\"edited_text\"]\n    }\n\n# Add the node to the graph in an appropriate location\n# and connect it to the relevant nodes.\ngraph_builder.add_node(\"human_editing\", human_editing)\ngraph = graph_builder.compile(checkpointer=checkpointer)\n\n...\n\n# After running the graph and hitting the interrupt, the graph will pause.\n# Resume it with the edited text.\nthread_config = {\"configurable\": {\"thread_id\": \"some_id\"}}\ngraph.invoke(\n    Command(resume={\"edited_text\": \"The edited text\"}),\n    config=thread_config\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { interrupt } from \"@langchain/langgraph\";\n\nfunction humanEditing(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  const result = interrupt({\n    // Interrupt information to surface to the client.\n    // Can be any JSON serializable value.\n    task: \"Review the output from the LLM and make any necessary edits.\",\n    llmGeneratedSummary: state.llmGeneratedSummary,\n  });\n\n  // Update the state with the edited text\n  return {\n    llmGeneratedSummary: result.editedText,\n  };\n}\n\n// Add the node to the graph in an appropriate location\n// and connect it to the relevant nodes.\ngraphBuilder.addNode(\"humanEditing\", humanEditing);\nconst graph = graphBuilder.compile({ checkpointer });\n\n// After running the graph and hitting the interrupt, the graph will pause.\n// Resume it with the edited text.\nconst threadConfig = { configurable: { thread_id: \"some_id\" } };\nawait graph.invoke(\n  new Command({ resume: { editedText: \"The edited text\" } }),\n  threadConfig\n);\n</code></pre> <p>:::</p> Extended example: edit state with interrupt <p>:::python <pre><code>from typing import TypedDict\nimport uuid\n\nfrom langgraph.constants import START, END\nfrom langgraph.graph import StateGraph\nfrom langgraph.types import interrupt, Command\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Define the graph state\nclass State(TypedDict):\n    summary: str\n\n# Simulate an LLM summary generation\ndef generate_summary(state: State) -&gt; State:\n    return {\n        \"summary\": \"The cat sat on the mat and looked at the stars.\"\n    }\n\n# Human editing node\ndef human_review_edit(state: State) -&gt; State:\n    result = interrupt({\n        \"task\": \"Please review and edit the generated summary if necessary.\",\n        \"generated_summary\": state[\"summary\"]\n    })\n    return {\n        \"summary\": result[\"edited_summary\"]\n    }\n\n# Simulate downstream use of the edited summary\ndef downstream_use(state: State) -&gt; State:\n    print(f\"\u2705 Using edited summary: {state['summary']}\")\n    return state\n\n# Build the graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"generate_summary\", generate_summary)\nbuilder.add_node(\"human_review_edit\", human_review_edit)\nbuilder.add_node(\"downstream_use\", downstream_use)\n\nbuilder.set_entry_point(\"generate_summary\")\nbuilder.add_edge(\"generate_summary\", \"human_review_edit\")\nbuilder.add_edge(\"human_review_edit\", \"downstream_use\")\nbuilder.add_edge(\"downstream_use\", END)\n\n# Set up in-memory checkpointing for interrupt support\ncheckpointer = InMemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Invoke the graph until it hits the interrupt\nconfig = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\nresult = graph.invoke({}, config=config)\n\n# Output interrupt payload\nprint(result[\"__interrupt__\"])\n# Example output:\n# &gt; [\n# &gt;     Interrupt(\n# &gt;         value={\n# &gt;             'task': 'Please review and edit the generated summary if necessary.',\n# &gt;             'generated_summary': 'The cat sat on the mat and looked at the stars.'\n# &gt;         },\n# &gt;         id='...'\n# &gt;     )\n# &gt; ]\n\n# Resume the graph with human-edited input\nedited_summary = \"The cat lay on the rug, gazing peacefully at the night sky.\"\nresumed_result = graph.invoke(\n    Command(resume={\"edited_summary\": edited_summary}),\n    config=config\n)\nprint(resumed_result)\n</code></pre> :::</p> <p>:::js <pre><code>import { z } from \"zod\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport {\n  StateGraph,\n  START,\n  END,\n  interrupt,\n  Command,\n  MemorySaver\n} from \"@langchain/langgraph\";\n\n// Define the graph state\nconst StateAnnotation = z.object({\n  summary: z.string(),\n});\n\n// Simulate an LLM summary generation\nfunction generateSummary(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  return {\n    summary: \"The cat sat on the mat and looked at the stars.\"\n  };\n}\n\n// Human editing node\nfunction humanReviewEdit(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  const result = interrupt({\n    task: \"Please review and edit the generated summary if necessary.\",\n    generatedSummary: state.summary\n  });\n  return {\n    summary: result.editedSummary\n  };\n}\n\n// Simulate downstream use of the edited summary\nfunction downstreamUse(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  console.log(`\u2705 Using edited summary: ${state.summary}`);\n  return state;\n}\n\n// Build the graph\nconst builder = new StateGraph(StateAnnotation)\n  .addNode(\"generateSummary\", generateSummary)\n  .addNode(\"humanReviewEdit\", humanReviewEdit)\n  .addNode(\"downstreamUse\", downstreamUse)\n  .addEdge(START, \"generateSummary\")\n  .addEdge(\"generateSummary\", \"humanReviewEdit\")\n  .addEdge(\"humanReviewEdit\", \"downstreamUse\")\n  .addEdge(\"downstreamUse\", END);\n\n// Set up in-memory checkpointing for interrupt support\nconst checkpointer = new MemorySaver();\nconst graph = builder.compile({ checkpointer });\n\n// Invoke the graph until it hits the interrupt\nconst config = { configurable: { thread_id: uuidv4() } };\nconst result = await graph.invoke({}, config);\n\n// Output interrupt payload\nconsole.log(result.__interrupt__);\n// Example output:\n// [{\n//   value: {\n//     task: 'Please review and edit the generated summary if necessary.',\n//     generatedSummary: 'The cat sat on the mat and looked at the stars.'\n//   },\n//   resumable: true,\n//   ...\n// }]\n\n// Resume the graph with human-edited input\nconst editedSummary = \"The cat lay on the rug, gazing peacefully at the night sky.\";\nconst resumedResult = await graph.invoke(\n  new Command({ resume: { editedSummary } }),\n  config\n);\nconsole.log(resumedResult);\n</code></pre> :::</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#review-tool-calls", "title": "Review tool calls", "text": "A human can review and edit the output from the LLM before proceeding. This is particularly critical in applications where the tool calls requested by the LLM may be sensitive or require human oversight.  <p>To add a human approval step to a tool:</p> <ol> <li>Use <code>interrupt()</code> in the tool to pause execution.</li> <li>Resume with a <code>Command</code> to continue based on human input.</li> </ol> <p>:::python</p> <pre><code>from langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.types import interrupt\nfrom langgraph.prebuilt import create_react_agent\n\n# An example of a sensitive tool that requires human review / approval\ndef book_hotel(hotel_name: str):\n    \"\"\"Book a hotel\"\"\"\n    # highlight-next-line\n    response = interrupt(  # (1)!\n        f\"Trying to call `book_hotel` with args {{'hotel_name': {hotel_name}}}. \"\n        \"Please approve or suggest edits.\"\n    )\n    if response[\"type\"] == \"accept\":\n        pass\n    elif response[\"type\"] == \"edit\":\n        hotel_name = response[\"args\"][\"hotel_name\"]\n    else:\n        raise ValueError(f\"Unknown response type: {response['type']}\")\n    return f\"Successfully booked a stay at {hotel_name}.\"\n\n# highlight-next-line\ncheckpointer = InMemorySaver() # (2)!\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[book_hotel],\n    # highlight-next-line\n    checkpointer=checkpointer, # (3)!\n)\n</code></pre> <ol> <li>The @[<code>interrupt</code> function][interrupt] pauses the agent graph at a specific node. In this case, we call <code>interrupt()</code> at the beginning of the tool function, which pauses the graph at the node that executes the tool. The information inside <code>interrupt()</code> (e.g., tool calls) can be presented to a human, and the graph can be resumed with the user input (tool call approval, edit or feedback).</li> <li>The <code>InMemorySaver</code> is used to store the agent state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. In this example, we use <code>InMemorySaver</code> to store the agent state in memory. In a production application, the agent state will be stored in a database.</li> <li>Initialize the agent with the <code>checkpointer</code>.    :::</li> </ol> <p>:::js</p> <pre><code>import { MemorySaver } from \"@langchain/langgraph\";\nimport { interrupt } from \"@langchain/langgraph\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\n// An example of a sensitive tool that requires human review / approval\nconst bookHotel = tool(\n  async ({ hotelName }) =&gt; {\n    // highlight-next-line\n    const response = interrupt(\n      // (1)!\n      `Trying to call \\`bookHotel\\` with args {\"hotelName\": \"${hotelName}\"}. ` +\n        \"Please approve or suggest edits.\"\n    );\n    if (response.type === \"accept\") {\n      // Continue with original args\n    } else if (response.type === \"edit\") {\n      hotelName = response.args.hotelName;\n    } else {\n      throw new Error(`Unknown response type: ${response.type}`);\n    }\n    return `Successfully booked a stay at ${hotelName}.`;\n  },\n  {\n    name: \"bookHotel\",\n    description: \"Book a hotel\",\n    schema: z.object({\n      hotelName: z.string(),\n    }),\n  }\n);\n\n// highlight-next-line\nconst checkpointer = new MemorySaver(); // (2)!\n\nconst agent = createReactAgent({\n  llm: model,\n  tools: [bookHotel],\n  // highlight-next-line\n  checkpointSaver: checkpointer, // (3)!\n});\n</code></pre> <ol> <li>The @[<code>interrupt</code> function][interrupt] pauses the agent graph at a specific node. In this case, we call <code>interrupt()</code> at the beginning of the tool function, which pauses the graph at the node that executes the tool. The information inside <code>interrupt()</code> (e.g., tool calls) can be presented to a human, and the graph can be resumed with the user input (tool call approval, edit or feedback).</li> <li>The <code>MemorySaver</code> is used to store the agent state at every step in the tool calling loop. This enables short-term memory and human-in-the-loop capabilities. In this example, we use <code>MemorySaver</code> to store the agent state in memory. In a production application, the agent state will be stored in a database.</li> <li>Initialize the agent with the <code>checkpointSaver</code>.    :::</li> </ol> <p>Run the agent with the <code>stream()</code> method, passing the <code>config</code> object to specify the thread ID. This allows the agent to resume the same conversation on future invocations.</p> <p>:::python</p> <pre><code>config = {\n   \"configurable\": {\n      # highlight-next-line\n      \"thread_id\": \"1\"\n   }\n}\n\nfor chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]},\n    # highlight-next-line\n    config\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const config = {\n  configurable: {\n    // highlight-next-line\n    thread_id: \"1\",\n  },\n};\n\nconst stream = await agent.stream(\n  { messages: [{ role: \"user\", content: \"book a stay at McKittrick hotel\" }] },\n  // highlight-next-line\n  config\n);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n  console.log(\"\\n\");\n}\n</code></pre> <p>:::</p> <p>You should see that the agent runs until it reaches the <code>interrupt()</code> call, at which point it pauses and waits for human input.</p> <p>Resume the agent with a <code>Command</code> to continue based on human input.</p> <p>:::python</p> <pre><code>from langgraph.types import Command\n\nfor chunk in agent.stream(\n    # highlight-next-line\n    Command(resume={\"type\": \"accept\"}),  # (1)!\n    # Command(resume={\"type\": \"edit\", \"args\": {\"hotel_name\": \"McKittrick Hotel\"}}),\n    config\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <ol> <li>The @[<code>interrupt</code> function][interrupt] is used in conjunction with the @[<code>Command</code>][Command] object to resume the graph with a value provided by the human.    :::</li> </ol> <p>:::js</p> <pre><code>import { Command } from \"@langchain/langgraph\";\n\nconst resumeStream = await agent.stream(\n  // highlight-next-line\n  new Command({ resume: { type: \"accept\" } }), // (1)!\n  // new Command({ resume: { type: \"edit\", args: { hotelName: \"McKittrick Hotel\" } } }),\n  config\n);\n\nfor await (const chunk of resumeStream) {\n  console.log(chunk);\n  console.log(\"\\n\");\n}\n</code></pre> <ol> <li>The @[<code>interrupt</code> function][interrupt] is used in conjunction with the @[<code>Command</code>][Command] object to resume the graph with a value provided by the human.    :::</li> </ol>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#add-interrupts-to-any-tool", "title": "Add interrupts to any tool", "text": "<p>You can create a wrapper to add interrupts to any tool. The example below provides a reference implementation compatible with Agent Inbox UI and Agent Chat UI.</p> <p>:::python</p> Wrapper that adds human-in-the-loop to any tool<pre><code>from typing import Callable\nfrom langchain_core.tools import BaseTool, tool as create_tool\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.types import interrupt\nfrom langgraph.prebuilt.interrupt import HumanInterruptConfig, HumanInterrupt\n\ndef add_human_in_the_loop(\n    tool: Callable | BaseTool,\n    *,\n    interrupt_config: HumanInterruptConfig = None,\n) -&gt; BaseTool:\n    \"\"\"Wrap a tool to support human-in-the-loop review.\"\"\"\n    if not isinstance(tool, BaseTool):\n        tool = create_tool(tool)\n\n    if interrupt_config is None:\n        interrupt_config = {\n            \"allow_accept\": True,\n            \"allow_edit\": True,\n            \"allow_respond\": True,\n        }\n\n    @create_tool(  # (1)!\n        tool.name,\n        description=tool.description,\n        args_schema=tool.args_schema\n    )\n    def call_tool_with_interrupt(config: RunnableConfig, **tool_input):\n        request: HumanInterrupt = {\n            \"action_request\": {\n                \"action\": tool.name,\n                \"args\": tool_input\n            },\n            \"config\": interrupt_config,\n            \"description\": \"Please review the tool call\"\n        }\n        # highlight-next-line\n        response = interrupt([request])[0]  # (2)!\n        # approve the tool call\n        if response[\"type\"] == \"accept\":\n            tool_response = tool.invoke(tool_input, config)\n        # update tool call args\n        elif response[\"type\"] == \"edit\":\n            tool_input = response[\"args\"][\"args\"]\n            tool_response = tool.invoke(tool_input, config)\n        # respond to the LLM with user feedback\n        elif response[\"type\"] == \"response\":\n            user_feedback = response[\"args\"]\n            tool_response = user_feedback\n        else:\n            raise ValueError(f\"Unsupported interrupt response type: {response['type']}\")\n\n        return tool_response\n\n    return call_tool_with_interrupt\n</code></pre> <ol> <li>This wrapper creates a new tool that calls <code>interrupt()</code> before executing the wrapped tool.</li> <li><code>interrupt()</code> is using special input and output format that's expected by Agent Inbox UI: - a list of @[<code>HumanInterrupt</code>][HumanInterrupt] objects is sent to <code>AgentInbox</code> render interrupt information to the end user - resume value is provided by <code>AgentInbox</code> as a list (i.e., <code>Command(resume=[...])</code>)    :::</li> </ol> <p>:::js</p> Wrapper that adds human-in-the-loop to any tool<pre><code>import { StructuredTool, tool } from \"@langchain/core/tools\";\nimport { RunnableConfig } from \"@langchain/core/runnables\";\nimport { interrupt } from \"@langchain/langgraph\";\n\ninterface HumanInterruptConfig {\n  allowAccept?: boolean;\n  allowEdit?: boolean;\n  allowRespond?: boolean;\n}\n\ninterface HumanInterrupt {\n  actionRequest: {\n    action: string;\n    args: Record&lt;string, any&gt;;\n  };\n  config: HumanInterruptConfig;\n  description: string;\n}\n\nfunction addHumanInTheLoop(\n  originalTool: StructuredTool,\n  interruptConfig: HumanInterruptConfig = {\n    allowAccept: true,\n    allowEdit: true,\n    allowRespond: true,\n  }\n): StructuredTool {\n  // Wrap the original tool to support human-in-the-loop review\n  return tool(\n    // (1)!\n    async (toolInput: Record&lt;string, any&gt;, config?: RunnableConfig) =&gt; {\n      const request: HumanInterrupt = {\n        actionRequest: {\n          action: originalTool.name,\n          args: toolInput,\n        },\n        config: interruptConfig,\n        description: \"Please review the tool call\",\n      };\n\n      // highlight-next-line\n      const response = interrupt([request])[0]; // (2)!\n\n      // approve the tool call\n      if (response.type === \"accept\") {\n        return await originalTool.invoke(toolInput, config);\n      }\n      // update tool call args\n      else if (response.type === \"edit\") {\n        const updatedArgs = response.args.args;\n        return await originalTool.invoke(updatedArgs, config);\n      }\n      // respond to the LLM with user feedback\n      else if (response.type === \"response\") {\n        return response.args;\n      } else {\n        throw new Error(\n          `Unsupported interrupt response type: ${response.type}`\n        );\n      }\n    },\n    {\n      name: originalTool.name,\n      description: originalTool.description,\n      schema: originalTool.schema,\n    }\n  );\n}\n</code></pre> <ol> <li>This wrapper creates a new tool that calls <code>interrupt()</code> before executing the wrapped tool.</li> <li><code>interrupt()</code> is using special input and output format that's expected by Agent Inbox UI: - a list of [<code>HumanInterrupt</code>] objects is sent to <code>AgentInbox</code> render interrupt information to the end user - resume value is provided by <code>AgentInbox</code> as a list (i.e., <code>Command({ resume: [...] })</code>)    :::</li> </ol> <p>You can use the wrapper to add <code>interrupt()</code> to any tool without having to add it inside the tool:</p> <p>:::python</p> <pre><code>from langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.prebuilt import create_react_agent\n\n# highlight-next-line\ncheckpointer = InMemorySaver()\n\ndef book_hotel(hotel_name: str):\n   \"\"\"Book a hotel\"\"\"\n   return f\"Successfully booked a stay at {hotel_name}.\"\n\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-5-sonnet-latest\",\n    tools=[\n        # highlight-next-line\n        add_human_in_the_loop(book_hotel), # (1)!\n    ],\n    # highlight-next-line\n    checkpointer=checkpointer,\n)\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Run the agent\nfor chunk in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"book a stay at McKittrick hotel\"}]},\n    # highlight-next-line\n    config\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <ol> <li>The <code>add_human_in_the_loop</code> wrapper is used to add <code>interrupt()</code> to the tool. This allows the agent to pause execution and wait for human input before proceeding with the tool call.    :::</li> </ol> <p>:::js</p> <pre><code>import { MemorySaver } from \"@langchain/langgraph\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\n// highlight-next-line\nconst checkpointer = new MemorySaver();\n\nconst bookHotel = tool(\n  async ({ hotelName }) =&gt; {\n    return `Successfully booked a stay at ${hotelName}.`;\n  },\n  {\n    name: \"bookHotel\",\n    description: \"Book a hotel\",\n    schema: z.object({\n      hotelName: z.string(),\n    }),\n  }\n);\n\nconst agent = createReactAgent({\n  llm: model,\n  tools: [\n    // highlight-next-line\n    addHumanInTheLoop(bookHotel), // (1)!\n  ],\n  // highlight-next-line\n  checkpointSaver: checkpointer,\n});\n\nconst config = { configurable: { thread_id: \"1\" } };\n\n// Run the agent\nconst stream = await agent.stream(\n  { messages: [{ role: \"user\", content: \"book a stay at McKittrick hotel\" }] },\n  // highlight-next-line\n  config\n);\n\nfor await (const chunk of stream) {\n  console.log(chunk);\n  console.log(\"\\n\");\n}\n</code></pre> <ol> <li>The <code>addHumanInTheLoop</code> wrapper is used to add <code>interrupt()</code> to the tool. This allows the agent to pause execution and wait for human input before proceeding with the tool call.    :::</li> </ol> <p>You should see that the agent runs until it reaches the <code>interrupt()</code> call, at which point it pauses and waits for human input.</p> <p>Resume the agent with a <code>Command</code> to continue based on human input.</p> <p>:::python</p> <pre><code>from langgraph.types import Command\n\nfor chunk in agent.stream(\n    # highlight-next-line\n    Command(resume=[{\"type\": \"accept\"}]),\n    # Command(resume=[{\"type\": \"edit\", \"args\": {\"args\": {\"hotel_name\": \"McKittrick Hotel\"}}}]),\n    config\n):\n    print(chunk)\n    print(\"\\n\")\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { Command } from \"@langchain/langgraph\";\n\nconst resumeStream = await agent.stream(\n  // highlight-next-line\n  new Command({ resume: [{ type: \"accept\" }] }),\n  // new Command({ resume: [{ type: \"edit\", args: { args: { hotelName: \"McKittrick Hotel\" } } }] }),\n  config\n);\n\nfor await (const chunk of resumeStream) {\n  console.log(chunk);\n  console.log(\"\\n\");\n}\n</code></pre> <p>:::</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#validate-human-input", "title": "Validate human input", "text": "<p>If you need to validate the input provided by the human within the graph itself (rather than on the client side), you can achieve this by using multiple interrupt calls within a single node.</p> <p>:::python</p> <pre><code>from langgraph.types import interrupt\n\ndef human_node(state: State):\n    \"\"\"Human node with validation.\"\"\"\n    question = \"What is your age?\"\n\n    while True:\n        answer = interrupt(question)\n\n        # Validate answer, if the answer isn't valid ask for input again.\n        if not isinstance(answer, int) or answer &lt; 0:\n            question = f\"'{answer} is not a valid age. What is your age?\"\n            answer = None\n            continue\n        else:\n            # If the answer is valid, we can proceed.\n            break\n\n    print(f\"The human in the loop is {answer} years old.\")\n    return {\n        \"age\": answer\n    }\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { interrupt } from \"@langchain/langgraph\";\n\ngraphBuilder.addNode(\"humanNode\", (state) =&gt; {\n  // Human node with validation.\n  let question = \"What is your age?\";\n\n  while (true) {\n    const answer = interrupt(question);\n\n    // Validate answer, if the answer isn't valid ask for input again.\n    if (typeof answer !== \"number\" || answer &lt; 0) {\n      question = `'${answer}' is not a valid age. What is your age?`;\n      continue;\n    } else {\n      // If the answer is valid, we can proceed.\n      break;\n    }\n  }\n\n  console.log(`The human in the loop is ${answer} years old.`);\n  return {\n    age: answer,\n  };\n});\n</code></pre> <p>:::</p> Extended example: validating user input <p>:::python <pre><code>from typing import TypedDict\nimport uuid\n\nfrom langgraph.constants import START, END\nfrom langgraph.graph import StateGraph\nfrom langgraph.types import interrupt, Command\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Define graph state\nclass State(TypedDict):\n    age: int\n\n# Node that asks for human input and validates it\ndef get_valid_age(state: State) -&gt; State:\n    prompt = \"Please enter your age (must be a non-negative integer).\"\n\n    while True:\n        user_input = interrupt(prompt)\n\n        # Validate the input\n        try:\n            age = int(user_input)\n            if age &lt; 0:\n                raise ValueError(\"Age must be non-negative.\")\n            break  # Valid input received\n        except (ValueError, TypeError):\n            prompt = f\"'{user_input}' is not valid. Please enter a non-negative integer for age.\"\n\n    return {\"age\": age}\n\n# Node that uses the valid input\ndef report_age(state: State) -&gt; State:\n    print(f\"\u2705 Human is {state['age']} years old.\")\n    return state\n\n# Build the graph\nbuilder = StateGraph(State)\nbuilder.add_node(\"get_valid_age\", get_valid_age)\nbuilder.add_node(\"report_age\", report_age)\n\nbuilder.set_entry_point(\"get_valid_age\")\nbuilder.add_edge(\"get_valid_age\", \"report_age\")\nbuilder.add_edge(\"report_age\", END)\n\n# Create the graph with a memory checkpointer\ncheckpointer = InMemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Run the graph until the first interrupt\nconfig = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\nresult = graph.invoke({}, config=config)\nprint(result[\"__interrupt__\"])  # First prompt: \"Please enter your age...\"\n\n# Simulate an invalid input (e.g., string instead of integer)\nresult = graph.invoke(Command(resume=\"not a number\"), config=config)\nprint(result[\"__interrupt__\"])  # Follow-up prompt with validation message\n\n# Simulate a second invalid input (e.g., negative number)\nresult = graph.invoke(Command(resume=\"-10\"), config=config)\nprint(result[\"__interrupt__\"])  # Another retry\n\n# Provide valid input\nfinal_result = graph.invoke(Command(resume=\"25\"), config=config)\nprint(final_result)  # Should include the valid age\n</code></pre> :::</p> <p>:::js <pre><code>import { z } from \"zod\";\nimport { v4 as uuidv4 } from \"uuid\";\nimport {\n  StateGraph,\n  START,\n  END,\n  interrupt,\n  Command,\n  MemorySaver\n} from \"@langchain/langgraph\";\n\n// Define graph state\nconst StateAnnotation = z.object({\n  age: z.number(),\n});\n\n// Node that asks for human input and validates it\nfunction getValidAge(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  let prompt = \"Please enter your age (must be a non-negative integer).\";\n\n  while (true) {\n    const userInput = interrupt(prompt);\n\n    // Validate the input\n    try {\n      const age = parseInt(userInput as string);\n      if (isNaN(age) || age &lt; 0) {\n        throw new Error(\"Age must be non-negative.\");\n      }\n      return { age };\n    } catch (error) {\n      prompt = `'${userInput}' is not valid. Please enter a non-negative integer for age.`;\n    }\n  }\n}\n\n// Node that uses the valid input\nfunction reportAge(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  console.log(`\u2705 Human is ${state.age} years old.`);\n  return state;\n}\n\n// Build the graph\nconst builder = new StateGraph(StateAnnotation)\n  .addNode(\"getValidAge\", getValidAge)\n  .addNode(\"reportAge\", reportAge)\n  .addEdge(START, \"getValidAge\")\n  .addEdge(\"getValidAge\", \"reportAge\")\n  .addEdge(\"reportAge\", END);\n\n// Create the graph with a memory checkpointer\nconst checkpointer = new MemorySaver();\nconst graph = builder.compile({ checkpointer });\n\n// Run the graph until the first interrupt\nconst config = { configurable: { thread_id: uuidv4() } };\nlet result = await graph.invoke({}, config);\nconsole.log(result.__interrupt__);  // First prompt: \"Please enter your age...\"\n\n// Simulate an invalid input (e.g., string instead of integer)\nresult = await graph.invoke(new Command({ resume: \"not a number\" }), config);\nconsole.log(result.__interrupt__);  // Follow-up prompt with validation message\n\n// Simulate a second invalid input (e.g., negative number)\nresult = await graph.invoke(new Command({ resume: \"-10\" }), config);\nconsole.log(result.__interrupt__);  // Another retry\n\n// Provide valid input\nconst finalResult = await graph.invoke(new Command({ resume: \"25\" }), config);\nconsole.log(finalResult);  // Should include the valid age\n</code></pre> :::</p> <p>:::python</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#debug-with-interrupts", "title": "Debug with interrupts", "text": "<p>To debug and test a graph, use static interrupts (also known as static breakpoints) to step through the graph execution one node at a time or to pause the graph execution at specific nodes. Static interrupts are triggered at defined points either before or after a node executes. You can set static interrupts by specifying <code>interrupt_before</code> and <code>interrupt_after</code> at compile time or run time.</p> <p>Warning</p> <p>Static interrupts are not recommended for human-in-the-loop workflows. Use dynamic interrupts instead.</p> Compile timeRun time <pre><code># highlight-next-line\ngraph = graph_builder.compile( # (1)!\n    # highlight-next-line\n    interrupt_before=[\"node_a\"], # (2)!\n    # highlight-next-line\n    interrupt_after=[\"node_b\", \"node_c\"], # (3)!\n    checkpointer=checkpointer, # (4)!\n)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread\"\n    }\n}\n\n# Run the graph until the breakpoint\ngraph.invoke(inputs, config=thread_config) # (5)!\n\n# Resume the graph\ngraph.invoke(None, config=thread_config) # (6)!\n</code></pre> <ol> <li>The breakpoints are set during <code>compile</code> time.</li> <li><code>interrupt_before</code> specifies the nodes where execution should pause before the node is executed.</li> <li><code>interrupt_after</code> specifies the nodes where execution should pause after the node is executed.</li> <li>A checkpointer is required to enable breakpoints.</li> <li>The graph is run until the first breakpoint is hit.</li> <li>The graph is resumed by passing in <code>None</code> for the input. This will run the graph until the next breakpoint is hit.</li> </ol> <pre><code># highlight-next-line\ngraph.invoke( # (1)!\n    inputs,\n    # highlight-next-line\n    interrupt_before=[\"node_a\"], # (2)!\n    # highlight-next-line\n    interrupt_after=[\"node_b\", \"node_c\"] # (3)!\n    config={\n        \"configurable\": {\"thread_id\": \"some_thread\"}\n    },\n)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread\"\n    }\n}\n\n# Run the graph until the breakpoint\ngraph.invoke(inputs, config=config) # (4)!\n\n# Resume the graph\ngraph.invoke(None, config=config) # (5)!\n</code></pre> <ol> <li><code>graph.invoke</code> is called with the <code>interrupt_before</code> and <code>interrupt_after</code> parameters. This is a run-time configuration and can be changed for every invocation.</li> <li><code>interrupt_before</code> specifies the nodes where execution should pause before the node is executed.</li> <li><code>interrupt_after</code> specifies the nodes where execution should pause after the node is executed.</li> <li>The graph is run until the first breakpoint is hit.</li> <li>The graph is resumed by passing in <code>None</code> for the input. This will run the graph until the next breakpoint is hit.</li> </ol> <p>Note</p> <p>You cannot set static breakpoints at runtime for sub-graphs. If you have a sub-graph, you must set the breakpoints at compilation time.</p> Setting static breakpoints <pre><code>from IPython.display import Image, display\nfrom typing_extensions import TypedDict\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n    input: str\n\n\ndef step_1(state):\n    print(\"---Step 1---\")\n    pass\n\n\ndef step_2(state):\n    print(\"---Step 2---\")\n    pass\n\n\ndef step_3(state):\n    print(\"---Step 3---\")\n    pass\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"step_1\", step_1)\nbuilder.add_node(\"step_2\", step_2)\nbuilder.add_node(\"step_3\", step_3)\nbuilder.add_edge(START, \"step_1\")\nbuilder.add_edge(\"step_1\", \"step_2\")\nbuilder.add_edge(\"step_2\", \"step_3\")\nbuilder.add_edge(\"step_3\", END)\n\n# Set up a checkpointer\ncheckpointer = InMemorySaver() # (1)!\n\ngraph = builder.compile(\n    checkpointer=checkpointer, # (2)!\n    interrupt_before=[\"step_3\"] # (3)!\n)\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n\n\n# Input\ninitial_input = {\"input\": \"hello world\"}\n\n# Thread\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Run the graph until the first interruption\nfor event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n    print(event)\n\n# This will run until the breakpoint\n# You can get the state of the graph at this point\nprint(graph.get_state(config))\n\n# You can continue the graph execution by passing in `None` for the input\nfor event in graph.stream(None, thread, stream_mode=\"values\"):\n    print(event)\n</code></pre>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#use-static-interrupts-in-langgraph-studio", "title": "Use static interrupts in LangGraph Studio", "text": "<p>You can use LangGraph Studio to debug your graph. You can set static breakpoints in the UI and then run the graph. You can also use the UI to inspect the graph state at any point in the execution.</p> <p></p> <p>LangGraph Studio is free with locally deployed applications using <code>langgraph dev</code>.</p> <p>:::</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#debug-with-interrupts_1", "title": "Debug with interrupts", "text": "<p>To debug and test a graph, use static interrupts (also known as static breakpoints) to step through the graph execution one node at a time or to pause the graph execution at specific nodes. Static interrupts are triggered at defined points either before or after a node executes. You can set static interrupts by specifying <code>interrupt_before</code> and <code>interrupt_after</code> at compile time or run time.</p> <p>Warning</p> <p>Static interrupts are not recommended for human-in-the-loop workflows. Use dynamic interrupts instead.</p> Compile timeRun time <pre><code># highlight-next-line\ngraph = graph_builder.compile( # (1)!\n    # highlight-next-line\n    interrupt_before=[\"node_a\"], # (2)!\n    # highlight-next-line\n    interrupt_after=[\"node_b\", \"node_c\"], # (3)!\n    checkpointer=checkpointer, # (4)!\n)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread\"\n    }\n}\n\n# Run the graph until the breakpoint\ngraph.invoke(inputs, config=thread_config) # (5)!\n\n# Resume the graph\ngraph.invoke(None, config=thread_config) # (6)!\n</code></pre> <ol> <li>The breakpoints are set during <code>compile</code> time.</li> <li><code>interrupt_before</code> specifies the nodes where execution should pause before the node is executed.</li> <li><code>interrupt_after</code> specifies the nodes where execution should pause after the node is executed.</li> <li>A checkpointer is required to enable breakpoints.</li> <li>The graph is run until the first breakpoint is hit.</li> <li>The graph is resumed by passing in <code>None</code> for the input. This will run the graph until the next breakpoint is hit.</li> </ol> <pre><code># highlight-next-line\ngraph.invoke( # (1)!\n    inputs,\n    # highlight-next-line\n    interrupt_before=[\"node_a\"], # (2)!\n    # highlight-next-line\n    interrupt_after=[\"node_b\", \"node_c\"] # (3)!\n    config={\n        \"configurable\": {\"thread_id\": \"some_thread\"}\n    },\n)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": \"some_thread\"\n    }\n}\n\n# Run the graph until the breakpoint\ngraph.invoke(inputs, config=config) # (4)!\n\n# Resume the graph\ngraph.invoke(None, config=config) # (5)!\n</code></pre> <ol> <li><code>graph.invoke</code> is called with the <code>interrupt_before</code> and <code>interrupt_after</code> parameters. This is a run-time configuration and can be changed for every invocation.</li> <li><code>interrupt_before</code> specifies the nodes where execution should pause before the node is executed.</li> <li><code>interrupt_after</code> specifies the nodes where execution should pause after the node is executed.</li> <li>The graph is run until the first breakpoint is hit.</li> <li>The graph is resumed by passing in <code>None</code> for the input. This will run the graph until the next breakpoint is hit.</li> </ol> <p>Note</p> <p>You cannot set static breakpoints at runtime for sub-graphs. If you have a sub-graph, you must set the breakpoints at compilation time.</p> Setting static breakpoints <pre><code>from IPython.display import Image, display\nfrom typing_extensions import TypedDict\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph import StateGraph, START, END\n\n\nclass State(TypedDict):\n    input: str\n\n\ndef step_1(state):\n    print(\"---Step 1---\")\n    pass\n\n\ndef step_2(state):\n    print(\"---Step 2---\")\n    pass\n\n\ndef step_3(state):\n    print(\"---Step 3---\")\n    pass\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"step_1\", step_1)\nbuilder.add_node(\"step_2\", step_2)\nbuilder.add_node(\"step_3\", step_3)\nbuilder.add_edge(START, \"step_1\")\nbuilder.add_edge(\"step_1\", \"step_2\")\nbuilder.add_edge(\"step_2\", \"step_3\")\nbuilder.add_edge(\"step_3\", END)\n\n# Set up a checkpointer\ncheckpointer = InMemorySaver() # (1)!\n\ngraph = builder.compile(\n    checkpointer=checkpointer, # (2)!\n    interrupt_before=[\"step_3\"] # (3)!\n)\n\n# View\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n\n\n# Input\ninitial_input = {\"input\": \"hello world\"}\n\n# Thread\nthread = {\"configurable\": {\"thread_id\": \"1\"}}\n\n# Run the graph until the first interruption\nfor event in graph.stream(initial_input, thread, stream_mode=\"values\"):\n    print(event)\n\n# This will run until the breakpoint\n# You can get the state of the graph at this point\nprint(graph.get_state(config))\n\n# You can continue the graph execution by passing in `None` for the input\nfor event in graph.stream(None, thread, stream_mode=\"values\"):\n    print(event)\n</code></pre>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#use-static-interrupts-in-langgraph-studio_1", "title": "Use static interrupts in LangGraph Studio", "text": "<p>You can use LangGraph Studio to debug your graph. You can set static breakpoints in the UI and then run the graph. You can also use the UI to inspect the graph state at any point in the execution.</p> <p></p> <p>LangGraph Studio is free with locally deployed applications using <code>langgraph dev</code>.</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#considerations", "title": "Considerations", "text": "<p>When using human-in-the-loop, there are some considerations to keep in mind.</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#using-with-code-with-side-effects", "title": "Using with code with side-effects", "text": "<p>Place code with side effects, such as API calls, after the <code>interrupt</code> or in a separate node to avoid duplication, as these are re-triggered every time the node is resumed.</p> Side effects after interruptSide effects in a separate node <p>:::python <pre><code>from langgraph.types import interrupt\n\ndef human_node(state: State):\n    \"\"\"Human node with validation.\"\"\"\n\n    answer = interrupt(question)\n\n    api_call(answer) # OK as it's after the interrupt\n</code></pre> :::</p> <p>:::js <pre><code>import { interrupt } from \"@langchain/langgraph\";\n\nfunction humanNode(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  // Human node with validation.\n\n  const answer = interrupt(question);\n\n  apiCall(answer); // OK as it's after the interrupt\n}\n</code></pre> :::</p> <p>:::python <pre><code>from langgraph.types import interrupt\n\ndef human_node(state: State):\n    \"\"\"Human node with validation.\"\"\"\n\n    answer = interrupt(question)\n\n    return {\n        \"answer\": answer\n    }\n\ndef api_call_node(state: State):\n    api_call(...) # OK as it's in a separate node\n</code></pre> :::</p> <p>:::js <pre><code>import { interrupt } from \"@langchain/langgraph\";\n\nfunction humanNode(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  // Human node with validation.\n\n  const answer = interrupt(question);\n\n  return {\n    answer\n  };\n}\n\nfunction apiCallNode(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  apiCall(state.answer); // OK as it's in a separate node\n}\n</code></pre> :::</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#using-with-subgraphs-called-as-functions", "title": "Using with subgraphs called as functions", "text": "<p>When invoking a subgraph as a function, the parent graph will resume execution from the beginning of the node where the subgraph was invoked where the <code>interrupt</code> was triggered. Similarly, the subgraph will resume from the beginning of the node where the <code>interrupt()</code> function was called.</p> <p>:::python</p> <pre><code>def node_in_parent_graph(state: State):\n    some_code()  # &lt;-- This will re-execute when the subgraph is resumed.\n    # Invoke a subgraph as a function.\n    # The subgraph contains an `interrupt` call.\n    subgraph_result = subgraph.invoke(some_input)\n    ...\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>async function nodeInParentGraph(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  someCode(); // &lt;-- This will re-execute when the subgraph is resumed.\n  // Invoke a subgraph as a function.\n  // The subgraph contains an `interrupt` call.\n  const subgraphResult = await subgraph.invoke(someInput);\n  // ...\n}\n</code></pre> <p>:::</p> Extended example: parent and subgraph execution flow <p>Say we have a parent graph with 3 nodes:</p> <p>Parent Graph: <code>node_1</code> \u2192 <code>node_2</code> (subgraph call) \u2192 <code>node_3</code></p> <p>And the subgraph has 3 nodes, where the second node contains an <code>interrupt</code>:</p> <p>Subgraph: <code>sub_node_1</code> \u2192 <code>sub_node_2</code> (<code>interrupt</code>) \u2192 <code>sub_node_3</code></p> <p>When resuming the graph, the execution will proceed as follows:</p> <ol> <li>Skip <code>node_1</code> in the parent graph (already executed, graph state was saved in snapshot).</li> <li>Re-execute <code>node_2</code> in the parent graph from the start.</li> <li>Skip <code>sub_node_1</code> in the subgraph (already executed, graph state was saved in snapshot).</li> <li>Re-execute <code>sub_node_2</code> in the subgraph from the beginning.</li> <li>Continue with <code>sub_node_3</code> and subsequent nodes.</li> </ol> <p>Here is abbreviated example code that you can use to understand how subgraphs work with interrupts. It counts the number of times each node is entered and prints the count.</p> <p>:::python <pre><code>import uuid\nfrom typing import TypedDict\n\nfrom langgraph.graph import StateGraph\nfrom langgraph.constants import START\nfrom langgraph.types import interrupt, Command\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass State(TypedDict):\n    \"\"\"The graph state.\"\"\"\n    state_counter: int\n\n\ncounter_node_in_subgraph = 0\n\ndef node_in_subgraph(state: State):\n    \"\"\"A node in the sub-graph.\"\"\"\n    global counter_node_in_subgraph\n    counter_node_in_subgraph += 1  # This code will **NOT** run again!\n    print(f\"Entered `node_in_subgraph` a total of {counter_node_in_subgraph} times\")\n\ncounter_human_node = 0\n\ndef human_node(state: State):\n    global counter_human_node\n    counter_human_node += 1 # This code will run again!\n    print(f\"Entered human_node in sub-graph a total of {counter_human_node} times\")\n    answer = interrupt(\"what is your name?\")\n    print(f\"Got an answer of {answer}\")\n\n\ncheckpointer = InMemorySaver()\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(\"some_node\", node_in_subgraph)\nsubgraph_builder.add_node(\"human_node\", human_node)\nsubgraph_builder.add_edge(START, \"some_node\")\nsubgraph_builder.add_edge(\"some_node\", \"human_node\")\nsubgraph = subgraph_builder.compile(checkpointer=checkpointer)\n\n\ncounter_parent_node = 0\n\ndef parent_node(state: State):\n    \"\"\"This parent node will invoke the subgraph.\"\"\"\n    global counter_parent_node\n\n    counter_parent_node += 1 # This code will run again on resuming!\n    print(f\"Entered `parent_node` a total of {counter_parent_node} times\")\n\n    # Please note that we're intentionally incrementing the state counter\n    # in the graph state as well to demonstrate that the subgraph update\n    # of the same key will not conflict with the parent graph (until\n    subgraph_state = subgraph.invoke(state)\n    return subgraph_state\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"parent_node\", parent_node)\nbuilder.add_edge(START, \"parent_node\")\n\n# A checkpointer must be enabled for interrupts to work!\ncheckpointer = InMemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\n    \"configurable\": {\n      \"thread_id\": uuid.uuid4(),\n    }\n}\n\nfor chunk in graph.stream({\"state_counter\": 1}, config):\n    print(chunk)\n\nprint('--- Resuming ---')\n\nfor chunk in graph.stream(Command(resume=\"35\"), config):\n    print(chunk)\n</code></pre></p> <p>This will print out</p> <p><pre><code>Entered `parent_node` a total of 1 times\nEntered `node_in_subgraph` a total of 1 times\nEntered human_node in sub-graph a total of 1 times\n{'__interrupt__': (Interrupt(value='what is your name?', id='...'),)}\n--- Resuming ---\nEntered `parent_node` a total of 2 times\nEntered human_node in sub-graph a total of 2 times\nGot an answer of 35\n{'parent_node': {'state_counter': 1}}\n</code></pre> :::</p> <p>:::js <pre><code>import { v4 as uuidv4 } from \"uuid\";\nimport {\n  StateGraph,\n  START,\n  interrupt,\n  Command,\n  MemorySaver\n} from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst StateAnnotation = z.object({\n  stateCounter: z.number(),\n});\n\n// Global variable to track the number of attempts\nlet counterNodeInSubgraph = 0;\n\nfunction nodeInSubgraph(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  // A node in the sub-graph.\n  counterNodeInSubgraph += 1; // This code will **NOT** run again!\n  console.log(`Entered 'nodeInSubgraph' a total of ${counterNodeInSubgraph} times`);\n  return {};\n}\n\nlet counterHumanNode = 0;\n\nfunction humanNode(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  counterHumanNode += 1; // This code will run again!\n  console.log(`Entered humanNode in sub-graph a total of ${counterHumanNode} times`);\n  const answer = interrupt(\"what is your name?\");\n  console.log(`Got an answer of ${answer}`);\n  return {};\n}\n\nconst checkpointer = new MemorySaver();\n\nconst subgraphBuilder = new StateGraph(StateAnnotation)\n  .addNode(\"someNode\", nodeInSubgraph)\n  .addNode(\"humanNode\", humanNode)\n  .addEdge(START, \"someNode\")\n  .addEdge(\"someNode\", \"humanNode\");\nconst subgraph = subgraphBuilder.compile({ checkpointer });\n\nlet counterParentNode = 0;\n\nasync function parentNode(state: z.infer&lt;typeof StateAnnotation&gt;) {\n  // This parent node will invoke the subgraph.\n  counterParentNode += 1; // This code will run again on resuming!\n  console.log(`Entered 'parentNode' a total of ${counterParentNode} times`);\n\n  // Please note that we're intentionally incrementing the state counter\n  // in the graph state as well to demonstrate that the subgraph update\n  // of the same key will not conflict with the parent graph (until\n  const subgraphState = await subgraph.invoke(state);\n  return subgraphState;\n}\n\nconst builder = new StateGraph(StateAnnotation)\n  .addNode(\"parentNode\", parentNode)\n  .addEdge(START, \"parentNode\");\n\n// A checkpointer must be enabled for interrupts to work!\nconst graph = builder.compile({ checkpointer });\n\nconst config = {\n  configurable: {\n    thread_id: uuidv4(),\n  }\n};\n\nconst stream = await graph.stream({ stateCounter: 1 }, config);\nfor await (const chunk of stream) {\n  console.log(chunk);\n}\n\nconsole.log('--- Resuming ---');\n\nconst resumeStream = await graph.stream(new Command({ resume: \"35\" }), config);\nfor await (const chunk of resumeStream) {\n  console.log(chunk);\n}\n</code></pre></p> <p>This will print out</p> <p><pre><code>Entered 'parentNode' a total of 1 times\nEntered 'nodeInSubgraph' a total of 1 times\nEntered humanNode in sub-graph a total of 1 times\n{ __interrupt__: [{ value: 'what is your name?', resumable: true, ns: ['parentNode:4c3a0248-21f0-1287-eacf-3002bc304db4', 'humanNode:2fe86d52-6f70-2a3f-6b2f-b1eededd6348'], when: 'during' }] }\n--- Resuming ---\nEntered 'parentNode' a total of 2 times\nEntered humanNode in sub-graph a total of 2 times\nGot an answer of 35\n{ parentNode: null }\n</code></pre> :::</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/add-human-in-the-loop/#using-multiple-interrupts-in-a-single-node", "title": "Using multiple interrupts in a single node", "text": "<p>Using multiple interrupts within a single node can be helpful for patterns like validating human input. However, using multiple interrupts in the same node can lead to unexpected behavior if not handled carefully.</p> <p>When a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task's resume list. Matching is strictly index-based, so the order of interrupt calls within the node is critical.</p> <p>To avoid issues, refrain from dynamically changing the node's structure between executions. This includes adding, removing, or reordering interrupt calls, as such changes can result in mismatched indices. These problems often arise from unconventional patterns, such as mutating state via <code>Command(resume=..., update=SOME_STATE_MUTATION)</code> or relying on global variables to modify the node's structure dynamically.</p> <p>:::python</p> Extended example: incorrect code that introduces non-determinism <pre><code>import uuid\nfrom typing import TypedDict, Optional\n\nfrom langgraph.graph import StateGraph\nfrom langgraph.constants import START\nfrom langgraph.types import interrupt, Command\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass State(TypedDict):\n    \"\"\"The graph state.\"\"\"\n\n    age: Optional[str]\n    name: Optional[str]\n\n\ndef human_node(state: State):\n    if not state.get('name'):\n        name = interrupt(\"what is your name?\")\n    else:\n        name = \"N/A\"\n\n    if not state.get('age'):\n        age = interrupt(\"what is your age?\")\n    else:\n        age = \"N/A\"\n\n    print(f\"Name: {name}. Age: {age}\")\n\n    return {\n        \"age\": age,\n        \"name\": name,\n    }\n\n\nbuilder = StateGraph(State)\nbuilder.add_node(\"human_node\", human_node)\nbuilder.add_edge(START, \"human_node\")\n\n# A checkpointer must be enabled for interrupts to work!\ncheckpointer = InMemorySaver()\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\n    \"configurable\": {\n        \"thread_id\": uuid.uuid4(),\n    }\n}\n\nfor chunk in graph.stream({\"age\": None, \"name\": None}, config):\n    print(chunk)\n\nfor chunk in graph.stream(Command(resume=\"John\", update={\"name\": \"foo\"}), config):\n    print(chunk)\n</code></pre> <pre><code>{'__interrupt__': (Interrupt(value='what is your name?', id='...'),)}\nName: N/A. Age: John\n{'human_node': {'age': 'John', 'name': 'N/A'}}\n</code></pre> <p>:::</p>", "tags": ["human-in-the-loop", "hil", "interrupt"], "boost": 2}, {"location": "how-tos/human_in_the_loop/time-travel/", "title": "Use time-travel", "text": "<p>To use time-travel in LangGraph:</p> <p>:::python</p> <ol> <li>Run the graph with initial inputs using @[<code>invoke</code>][CompiledStateGraph.invoke] or @[<code>stream</code>][CompiledStateGraph.stream] methods.</li> <li>Identify a checkpoint in an existing thread: Use the @[<code>get_state_history()</code>][get_state_history] method to retrieve the execution history for a specific <code>thread_id</code> and locate the desired <code>checkpoint_id</code>.    Alternatively, set an interrupt before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that interrupt.</li> <li>Update the graph state (optional): Use the @[<code>update_state</code>][update_state] method to modify the graph's state at the checkpoint and resume execution from alternative state.</li> <li>Resume execution from the checkpoint: Use the <code>invoke</code> or <code>stream</code> methods with an input of <code>None</code> and a configuration containing the appropriate <code>thread_id</code> and <code>checkpoint_id</code>.    :::</li> </ol> <p>:::js</p> <ol> <li>Run the graph with initial inputs using @[<code>invoke</code>][CompiledStateGraph.invoke] or @[<code>stream</code>][CompiledStateGraph.stream] methods.</li> <li>Identify a checkpoint in an existing thread: Use the @[<code>getStateHistory()</code>][get_state_history] method to retrieve the execution history for a specific <code>thread_id</code> and locate the desired <code>checkpoint_id</code>.    Alternatively, set a breakpoint before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that breakpoint.</li> <li>Update the graph state (optional): Use the @[<code>updateState</code>][update_state] method to modify the graph's state at the checkpoint and resume execution from alternative state.</li> <li>Resume execution from the checkpoint: Use the <code>invoke</code> or <code>stream</code> methods with an input of <code>null</code> and a configuration containing the appropriate <code>thread_id</code> and <code>checkpoint_id</code>.    :::</li> </ol> <p>Tip</p> <p>For a conceptual overview of time-travel, see Time travel.</p>"}, {"location": "how-tos/human_in_the_loop/time-travel/#in-a-workflow", "title": "In a workflow", "text": "<p>This example builds a simple LangGraph workflow that generates a joke topic and writes a joke using an LLM. It demonstrates how to run the graph, retrieve past execution checkpoints, optionally modify the state, and resume execution from a chosen checkpoint to explore alternate outcomes.</p>"}, {"location": "how-tos/human_in_the_loop/time-travel/#setup", "title": "Setup", "text": "<p>First we need to install the packages required</p> <p>:::python</p> <pre><code>%%capture --no-stderr\n%pip install --quiet -U langgraph langchain_anthropic\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>npm install @langchain/langgraph @langchain/anthropic\n</code></pre> <p>:::</p> <p>Next, we need to set API keys for Anthropic (the LLM we will use)</p> <p>:::python</p> <pre><code>import getpass\nimport os\n\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>process.env.ANTHROPIC_API_KEY = \"YOUR_API_KEY\";\n</code></pre> <p>:::</p> <p>Set up LangSmith for LangGraph development</p> <p>         Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started here.      </p> <p>:::python</p> <pre><code>import uuid\n\nfrom typing_extensions import TypedDict, NotRequired\nfrom langgraph.graph import StateGraph, START, END\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n\nclass State(TypedDict):\n    topic: NotRequired[str]\n    joke: NotRequired[str]\n\n\nllm = init_chat_model(\n    \"anthropic:claude-3-7-sonnet-latest\",\n    temperature=0,\n)\n\n\ndef generate_topic(state: State):\n    \"\"\"LLM call to generate a topic for the joke\"\"\"\n    msg = llm.invoke(\"Give me a funny topic for a joke\")\n    return {\"topic\": msg.content}\n\n\ndef write_joke(state: State):\n    \"\"\"LLM call to write a joke based on the topic\"\"\"\n    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\n# Build workflow\nworkflow = StateGraph(State)\n\n# Add nodes\nworkflow.add_node(\"generate_topic\", generate_topic)\nworkflow.add_node(\"write_joke\", write_joke)\n\n# Add edges to connect nodes\nworkflow.add_edge(START, \"generate_topic\")\nworkflow.add_edge(\"generate_topic\", \"write_joke\")\nworkflow.add_edge(\"write_joke\", END)\n\n# Compile\ncheckpointer = InMemorySaver()\ngraph = workflow.compile(checkpointer=checkpointer)\ngraph\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { v4 as uuidv4 } from \"uuid\";\nimport { z } from \"zod\";\nimport { StateGraph, START, END } from \"@langchain/langgraph\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { MemorySaver } from \"@langchain/langgraph\";\n\nconst State = z.object({\n  topic: z.string().optional(),\n  joke: z.string().optional(),\n});\n\nconst llm = new ChatAnthropic({\n  model: \"claude-3-5-sonnet-latest\",\n  temperature: 0,\n});\n\n// Build workflow\nconst workflow = new StateGraph(State)\n  // Add nodes\n  .addNode(\"generateTopic\", async (state) =&gt; {\n    // LLM call to generate a topic for the joke\n    const msg = await llm.invoke(\"Give me a funny topic for a joke\");\n    return { topic: msg.content };\n  })\n  .addNode(\"writeJoke\", async (state) =&gt; {\n    // LLM call to write a joke based on the topic\n    const msg = await llm.invoke(`Write a short joke about ${state.topic}`);\n    return { joke: msg.content };\n  })\n  // Add edges to connect nodes\n  .addEdge(START, \"generateTopic\")\n  .addEdge(\"generateTopic\", \"writeJoke\")\n  .addEdge(\"writeJoke\", END);\n\n// Compile\nconst checkpointer = new MemorySaver();\nconst graph = workflow.compile({ checkpointer });\n</code></pre> <p>:::</p>"}, {"location": "how-tos/human_in_the_loop/time-travel/#1-run-the-graph", "title": "1. Run the graph", "text": "<p>:::python</p> <pre><code>config = {\n    \"configurable\": {\n        \"thread_id\": uuid.uuid4(),\n    }\n}\nstate = graph.invoke({}, config)\n\nprint(state[\"topic\"])\nprint()\nprint(state[\"joke\"])\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const config = {\n  configurable: {\n    thread_id: uuidv4(),\n  },\n};\n\nconst state = await graph.invoke({}, config);\n\nconsole.log(state.topic);\nconsole.log();\nconsole.log(state.joke);\n</code></pre> <p>:::</p> <p>Output:</p> <pre><code>How about \"The Secret Life of Socks in the Dryer\"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don't know about? There's a lot of comedic potential in the everyday mystery that unites us all!\n\n# The Secret Life of Socks in the Dryer\n\nI finally discovered where all my missing socks go after the dryer. Turns out they're not missing at all\u2014they've just eloped with someone else's socks from the laundromat to start new lives together.\n\nMy blue argyle is now living in Bermuda with a red polka dot, posting vacation photos on Sockstagram and sending me lint as alimony.\n</code></pre>"}, {"location": "how-tos/human_in_the_loop/time-travel/#2-identify-a-checkpoint", "title": "2. Identify a checkpoint", "text": "<p>:::python</p> <pre><code># The states are returned in reverse chronological order.\nstates = list(graph.get_state_history(config))\n\nfor state in states:\n    print(state.next)\n    print(state.config[\"configurable\"][\"checkpoint_id\"])\n    print()\n</code></pre> <p>Output:</p> <pre><code>()\n1f02ac4a-ec9f-6524-8002-8f7b0bbeed0e\n\n('write_joke',)\n1f02ac4a-ce2a-6494-8001-cb2e2d651227\n\n('generate_topic',)\n1f02ac4a-a4e0-630d-8000-b73c254ba748\n\n('__start__',)\n1f02ac4a-a4dd-665e-bfff-e6c8c44315d9\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>// The states are returned in reverse chronological order.\nconst states = [];\nfor await (const state of graph.getStateHistory(config)) {\n  states.push(state);\n}\n\nfor (const state of states) {\n  console.log(state.next);\n  console.log(state.config.configurable?.checkpoint_id);\n  console.log();\n}\n</code></pre> <p>Output:</p> <pre><code>[]\n1f02ac4a-ec9f-6524-8002-8f7b0bbeed0e\n\n['writeJoke']\n1f02ac4a-ce2a-6494-8001-cb2e2d651227\n\n['generateTopic']\n1f02ac4a-a4e0-630d-8000-b73c254ba748\n\n['__start__']\n1f02ac4a-a4dd-665e-bfff-e6c8c44315d9\n</code></pre> <p>:::</p> <p>:::python</p> <pre><code># This is the state before last (states are listed in chronological order)\nselected_state = states[1]\nprint(selected_state.next)\nprint(selected_state.values)\n</code></pre> <p>Output:</p> <pre><code>('write_joke',)\n{'topic': 'How about \"The Secret Life of Socks in the Dryer\"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don\\\\'t know about? There\\\\'s a lot of comedic potential in the everyday mystery that unites us all!'}\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>// This is the state before last (states are listed in chronological order)\nconst selectedState = states[1];\nconsole.log(selectedState.next);\nconsole.log(selectedState.values);\n</code></pre> <p>Output:</p> <pre><code>['writeJoke']\n{'topic': 'How about \"The Secret Life of Socks in the Dryer\"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don\\\\'t know about? There\\\\'s a lot of comedic potential in the everyday mystery that unites us all!'}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/human_in_the_loop/time-travel/#3-update-the-state-optional", "title": "3. Update the state (optional)", "text": "<p>:::python <code>update_state</code> will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.</p> <pre><code>new_config = graph.update_state(selected_state.config, values={\"topic\": \"chickens\"})\nprint(new_config)\n</code></pre> <p>Output:</p> <pre><code>{'configurable': {'thread_id': 'c62e2e03-c27b-4cb6-8cea-ea9bfedae006', 'checkpoint_ns': '', 'checkpoint_id': '1f02ac4a-ecee-600b-8002-a1d21df32e4c'}}\n</code></pre> <p>:::</p> <p>:::js <code>updateState</code> will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID.</p> <pre><code>const newConfig = await graph.updateState(selectedState.config, {\n  topic: \"chickens\",\n});\nconsole.log(newConfig);\n</code></pre> <p>Output:</p> <pre><code>{'configurable': {'thread_id': 'c62e2e03-c27b-4cb6-8cea-ea9bfedae006', 'checkpoint_ns': '', 'checkpoint_id': '1f02ac4a-ecee-600b-8002-a1d21df32e4c'}}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/human_in_the_loop/time-travel/#4-resume-execution-from-the-checkpoint", "title": "4. Resume execution from the checkpoint", "text": "<p>:::python</p> <pre><code>graph.invoke(None, new_config)\n</code></pre> <p>Output:</p> <pre><code>{'topic': 'chickens',\n 'joke': 'Why did the chicken join a band?\\n\\nBecause it had excellent drumsticks!'}\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>await graph.invoke(null, newConfig);\n</code></pre> <p>Output:</p> <pre><code>{\n  'topic': 'chickens',\n  'joke': 'Why did the chicken join a band?\\n\\nBecause it had excellent drumsticks!'\n}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/memory/add-memory/", "title": "Add and manage memory", "text": "<p>AI applications need memory to share context across multiple interactions. In LangGraph, you can add two types of memory:</p> <ul> <li>Add short-term memory as a part of your agent's state to enable multi-turn conversations.</li> <li>Add long-term memory to store user-specific or application-level data across sessions.</li> </ul>"}, {"location": "how-tos/memory/add-memory/#add-short-term-memory", "title": "Add short-term memory", "text": "<p>Short-term memory (thread-level persistence) enables agents to track multi-turn conversations. To add short-term memory:</p> <p>:::python</p> <pre><code># highlight-next-line\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph import StateGraph\n\n# highlight-next-line\ncheckpointer = InMemorySaver()\n\nbuilder = StateGraph(...)\n# highlight-next-line\ngraph = builder.compile(checkpointer=checkpointer)\n\ngraph.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! i am Bob\"}]},\n    # highlight-next-line\n    {\"configurable\": {\"thread_id\": \"1\"}},\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { MemorySaver, StateGraph } from \"@langchain/langgraph\";\n\nconst checkpointer = new MemorySaver();\n\nconst builder = new StateGraph(...);\nconst graph = builder.compile({ checkpointer });\n\nawait graph.invoke(\n  { messages: [{ role: \"user\", content: \"hi! i am Bob\" }] },\n  { configurable: { thread_id: \"1\" } }\n);\n</code></pre> <p>:::</p>"}, {"location": "how-tos/memory/add-memory/#use-in-production", "title": "Use in production", "text": "<p>In production, use a checkpointer backed by a database:</p> <p>:::python</p> <pre><code>from langgraph.checkpoint.postgres import PostgresSaver\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n# highlight-next-line\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    builder = StateGraph(...)\n    # highlight-next-line\n    graph = builder.compile(checkpointer=checkpointer)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\n\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\n\nconst builder = new StateGraph(...);\nconst graph = builder.compile({ checkpointer });\n</code></pre> <p>:::</p> Example: using Postgres checkpointer <p>:::python <pre><code>pip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\n</code></pre></p> <p>Setup</p> <p>You need to call <code>checkpointer.setup()</code> the first time you're using Postgres checkpointer</p> SyncAsync <pre><code>from langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\n# highlight-next-line\nfrom langgraph.checkpoint.postgres import PostgresSaver\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n# highlight-next-line\nwith PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    # checkpointer.setup()\n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    # highlight-next-line\n    graph = builder.compile(checkpointer=checkpointer)\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"1\"\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>from langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\n# highlight-next-line\nfrom langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n# highlight-next-line\nasync with AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer:\n    # await checkpointer.setup()\n\n    async def call_model(state: MessagesState):\n        response = await model.ainvoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    # highlight-next-line\n    graph = builder.compile(checkpointer=checkpointer)\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"1\"\n        }\n    }\n\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n</code></pre> <p>:::</p> <p>:::js <pre><code>npm install @langchain/langgraph-checkpoint-postgres\n</code></pre></p> <p>Setup</p> <p>You need to call <code>checkpointer.setup()</code> the first time you're using Postgres checkpointer</p> <p><pre><code>import { ChatAnthropic } from \"@langchain/anthropic\";\nimport { StateGraph, MessagesZodState, START } from \"@langchain/langgraph\";\nimport { PostgresSaver } from \"@langchain/langgraph-checkpoint-postgres\";\n\nconst model = new ChatAnthropic({ model: \"claude-3-5-haiku-20241022\" });\n\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\n// await checkpointer.setup();\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"call_model\", async (state) =&gt; {\n    const response = await model.invoke(state.messages);\n    return { messages: [response] };\n  })\n  .addEdge(START, \"call_model\");\n\nconst graph = builder.compile({ checkpointer });\n\nconst config = {\n  configurable: {\n    thread_id: \"1\"\n  }\n};\n\nfor await (const chunk of await graph.stream(\n  { messages: [{ role: \"user\", content: \"hi! I'm bob\" }] },\n  { ...config, streamMode: \"values\" }\n)) {\n  console.log(chunk.messages.at(-1)?.content);\n}\n\nfor await (const chunk of await graph.stream(\n  { messages: [{ role: \"user\", content: \"what's my name?\" }] },\n  { ...config, streamMode: \"values\" }\n)) {\n  console.log(chunk.messages.at(-1)?.content);\n}\n</code></pre> :::</p> <p>:::python</p> Example: using MongoDB checkpointer <pre><code>pip install -U pymongo langgraph langgraph-checkpoint-mongodb\n</code></pre> <p>Setup</p> <p>To use the MongoDB checkpointer, you will need a MongoDB cluster. Follow this guide to create a cluster if you don't already have one.</p> SyncAsync <pre><code>from langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\n# highlight-next-line\nfrom langgraph.checkpoint.mongodb import MongoDBSaver\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"localhost:27017\"\n# highlight-next-line\nwith MongoDBSaver.from_conn_string(DB_URI) as checkpointer:\n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    # highlight-next-line\n    graph = builder.compile(checkpointer=checkpointer)\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"1\"\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>from langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\n# highlight-next-line\nfrom langgraph.checkpoint.mongodb.aio import AsyncMongoDBSaver\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"localhost:27017\"\n# highlight-next-line\nasync with AsyncMongoDBSaver.from_conn_string(DB_URI) as checkpointer:\n\n    async def call_model(state: MessagesState):\n        response = await model.ainvoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    # highlight-next-line\n    graph = builder.compile(checkpointer=checkpointer)\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"1\"\n        }\n    }\n\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n</code></pre> Example: using Redis checkpointer <pre><code>pip install -U langgraph langgraph-checkpoint-redis\n</code></pre> <p>Setup</p> <p>You need to call <code>checkpointer.setup()</code> the first time you're using Redis checkpointer</p> SyncAsync <pre><code>from langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\n# highlight-next-line\nfrom langgraph.checkpoint.redis import RedisSaver\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"redis://localhost:6379\"\n# highlight-next-line\nwith RedisSaver.from_conn_string(DB_URI) as checkpointer:\n    # checkpointer.setup()\n\n    def call_model(state: MessagesState):\n        response = model.invoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    # highlight-next-line\n    graph = builder.compile(checkpointer=checkpointer)\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"1\"\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>from langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\n# highlight-next-line\nfrom langgraph.checkpoint.redis.aio import AsyncRedisSaver\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"redis://localhost:6379\"\n# highlight-next-line\nasync with AsyncRedisSaver.from_conn_string(DB_URI) as checkpointer:\n    # await checkpointer.asetup()\n\n    async def call_model(state: MessagesState):\n        response = await model.ainvoke(state[\"messages\"])\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    # highlight-next-line\n    graph = builder.compile(checkpointer=checkpointer)\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"1\"\n        }\n    }\n\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\"\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n</code></pre> <p>:::</p>"}, {"location": "how-tos/memory/add-memory/#use-in-subgraphs", "title": "Use in subgraphs", "text": "<p>If your graph contains subgraphs, you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs.</p> <p>:::python</p> <pre><code>from langgraph.graph import START, StateGraph\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import TypedDict\n\nclass State(TypedDict):\n    foo: str\n\n# Subgraph\n\ndef subgraph_node_1(state: State):\n    return {\"foo\": state[\"foo\"] + \"bar\"}\n\nsubgraph_builder = StateGraph(State)\nsubgraph_builder.add_node(subgraph_node_1)\nsubgraph_builder.add_edge(START, \"subgraph_node_1\")\n# highlight-next-line\nsubgraph = subgraph_builder.compile()\n\n# Parent graph\n\nbuilder = StateGraph(State)\n# highlight-next-line\nbuilder.add_node(\"node_1\", subgraph)\nbuilder.add_edge(START, \"node_1\")\n\ncheckpointer = InMemorySaver()\n# highlight-next-line\ngraph = builder.compile(checkpointer=checkpointer)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, START, MemorySaver } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({ foo: z.string() });\n\nconst subgraphBuilder = new StateGraph(State)\n  .addNode(\"subgraph_node_1\", (state) =&gt; {\n    return { foo: state.foo + \"bar\" };\n  })\n  .addEdge(START, \"subgraph_node_1\");\nconst subgraph = subgraphBuilder.compile();\n\nconst builder = new StateGraph(State)\n  .addNode(\"node_1\", subgraph)\n  .addEdge(START, \"node_1\");\n\nconst checkpointer = new MemorySaver();\nconst graph = builder.compile({ checkpointer });\n</code></pre> <p>:::</p> <p>If you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in multi-agent systems, if you want agents to keep track of their internal message histories.</p> <p>:::python</p> <pre><code>subgraph_builder = StateGraph(...)\n# highlight-next-line\nsubgraph = subgraph_builder.compile(checkpointer=True)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const subgraphBuilder = new StateGraph(...);\n// highlight-next-line\nconst subgraph = subgraphBuilder.compile({ checkpointer: true });\n</code></pre> <p>:::</p>"}, {"location": "how-tos/memory/add-memory/#read-short-term", "title": "Read short-term memory in tools", "text": "<p>LangGraph allows agents to access their short-term memory (state) inside the tools.</p> <p>:::python</p> <pre><code>from typing import Annotated\nfrom langgraph.prebuilt import InjectedState, create_react_agent\n\nclass CustomState(AgentState):\n    # highlight-next-line\n    user_id: str\n\ndef get_user_info(\n    # highlight-next-line\n    state: Annotated[CustomState, InjectedState]\n) -&gt; str:\n    \"\"\"Look up user info.\"\"\"\n    # highlight-next-line\n    user_id = state[\"user_id\"]\n    return \"User is John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_user_info],\n    # highlight-next-line\n    state_schema=CustomState,\n)\n\nagent.invoke({\n    \"messages\": \"look up user information\",\n    # highlight-next-line\n    \"user_id\": \"user_123\"\n})\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport {\n  MessagesZodState,\n  LangGraphRunnableConfig,\n} from \"@langchain/langgraph\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst CustomState = z.object({\n  messages: MessagesZodState.shape.messages,\n  userId: z.string(),\n});\n\nconst getUserInfo = tool(\n  async (_, config: LangGraphRunnableConfig) =&gt; {\n    const userId = config.configurable?.userId;\n    return userId === \"user_123\" ? \"User is John Smith\" : \"Unknown user\";\n  },\n  {\n    name: \"get_user_info\",\n    description: \"Look up user info.\",\n    schema: z.object({}),\n  }\n);\n\nconst agent = createReactAgent({\n  llm: model,\n  tools: [getUserInfo],\n  stateSchema: CustomState,\n});\n\nawait agent.invoke({\n  messages: [{ role: \"user\", content: \"look up user information\" }],\n  userId: \"user_123\",\n});\n</code></pre> <p>:::</p> <p>See the Context guide for more information.</p>"}, {"location": "how-tos/memory/add-memory/#write-short-term", "title": "Write short-term memory from tools", "text": "<p>To modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools. This is useful for persisting intermediate results or making information accessible to subsequent tools or prompts.</p> <p>:::python</p> <pre><code>from typing import Annotated\nfrom langchain_core.tools import InjectedToolCallId\nfrom langchain_core.runnables import RunnableConfig\nfrom langchain_core.messages import ToolMessage\nfrom langgraph.prebuilt import InjectedState, create_react_agent\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.types import Command\n\nclass CustomState(AgentState):\n    # highlight-next-line\n    user_name: str\n\ndef update_user_info(\n    tool_call_id: Annotated[str, InjectedToolCallId],\n    config: RunnableConfig\n) -&gt; Command:\n    \"\"\"Look up and update user info.\"\"\"\n    user_id = config[\"configurable\"].get(\"user_id\")\n    name = \"John Smith\" if user_id == \"user_123\" else \"Unknown user\"\n    # highlight-next-line\n    return Command(update={\n        # highlight-next-line\n        \"user_name\": name,\n        # update the message history\n        \"messages\": [\n            ToolMessage(\n                \"Successfully looked up user information\",\n                tool_call_id=tool_call_id\n            )\n        ]\n    })\n\ndef greet(\n    # highlight-next-line\n    state: Annotated[CustomState, InjectedState]\n) -&gt; str:\n    \"\"\"Use this to greet the user once you found their info.\"\"\"\n    user_name = state[\"user_name\"]\n    return f\"Hello {user_name}!\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[update_user_info, greet],\n    # highlight-next-line\n    state_schema=CustomState\n)\n\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]},\n    # highlight-next-line\n    config={\"configurable\": {\"user_id\": \"user_123\"}}\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport {\n  MessagesZodState,\n  LangGraphRunnableConfig,\n  Command,\n} from \"@langchain/langgraph\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst CustomState = z.object({\n  messages: MessagesZodState.shape.messages,\n  userName: z.string().optional(),\n});\n\nconst updateUserInfo = tool(\n  async (_, config: LangGraphRunnableConfig) =&gt; {\n    const userId = config.configurable?.userId;\n    const name = userId === \"user_123\" ? \"John Smith\" : \"Unknown user\";\n    return new Command({\n      update: {\n        userName: name,\n        // update the message history\n        messages: [\n          {\n            role: \"tool\",\n            content: \"Successfully looked up user information\",\n            tool_call_id: config.toolCall?.id,\n          },\n        ],\n      },\n    });\n  },\n  {\n    name: \"update_user_info\",\n    description: \"Look up and update user info.\",\n    schema: z.object({}),\n  }\n);\n\nconst greet = tool(\n  async (_, config: LangGraphRunnableConfig) =&gt; {\n    const userName = config.configurable?.userName;\n    return `Hello ${userName}!`;\n  },\n  {\n    name: \"greet\",\n    description: \"Use this to greet the user once you found their info.\",\n    schema: z.object({}),\n  }\n);\n\nconst agent = createReactAgent({\n  llm: model,\n  tools: [updateUserInfo, greet],\n  stateSchema: CustomState,\n});\n\nawait agent.invoke(\n  { messages: [{ role: \"user\", content: \"greet the user\" }] },\n  { configurable: { userId: \"user_123\" } }\n);\n</code></pre> <p>:::</p>"}, {"location": "how-tos/memory/add-memory/#add-long-term-memory", "title": "Add long-term memory", "text": "<p>Use long-term memory to store user-specific or application-specific data across conversations.</p> <p>:::python</p> <pre><code># highlight-next-line\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.graph import StateGraph\n\n# highlight-next-line\nstore = InMemoryStore()\n\nbuilder = StateGraph(...)\n# highlight-next-line\ngraph = builder.compile(store=store)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { InMemoryStore, StateGraph } from \"@langchain/langgraph\";\n\nconst store = new InMemoryStore();\n\nconst builder = new StateGraph(...);\nconst graph = builder.compile({ store });\n</code></pre> <p>:::</p>"}, {"location": "how-tos/memory/add-memory/#use-in-production_1", "title": "Use in production", "text": "<p>In production, use a store backed by a database:</p> <p>:::python</p> <pre><code>from langgraph.store.postgres import PostgresStore\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n# highlight-next-line\nwith PostgresStore.from_conn_string(DB_URI) as store:\n    builder = StateGraph(...)\n    # highlight-next-line\n    graph = builder.compile(store=store)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { PostgresStore } from \"@langchain/langgraph-checkpoint-postgres\";\n\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\nconst store = PostgresStore.fromConnString(DB_URI);\n\nconst builder = new StateGraph(...);\nconst graph = builder.compile({ store });\n</code></pre> <p>:::</p> Example: using Postgres store <p>:::python <pre><code>pip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres\n</code></pre></p> <p>Setup</p> <p>You need to call <code>store.setup()</code> the first time you're using Postgres store</p> SyncAsync <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.postgres import PostgresSaver\n# highlight-next-line\nfrom langgraph.store.postgres import PostgresStore\nfrom langgraph.store.base import BaseStore\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n\nwith (\n    # highlight-next-line\n    PostgresStore.from_conn_string(DB_URI) as store,\n    PostgresSaver.from_conn_string(DB_URI) as checkpointer,\n):\n    # store.setup()\n    # checkpointer.setup()\n\n    def call_model(\n        state: MessagesState,\n        config: RunnableConfig,\n        *,\n        # highlight-next-line\n        store: BaseStore,\n    ):\n        user_id = config[\"configurable\"][\"user_id\"]\n        namespace = (\"memories\", user_id)\n        # highlight-next-line\n        memories = store.search(namespace, query=str(state[\"messages\"][-1].content))\n        info = \"\\n\".join([d.value[\"data\"] for d in memories])\n        system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n        # Store new memories if the user asks the model to remember\n        last_message = state[\"messages\"][-1]\n        if \"remember\" in last_message.content.lower():\n            memory = \"User name is Bob\"\n            # highlight-next-line\n            store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n\n        response = model.invoke(\n            [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n        )\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(\n        checkpointer=checkpointer,\n        # highlight-next-line\n        store=store,\n    )\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"1\",\n            # highlight-next-line\n            \"user_id\": \"1\",\n        }\n    }\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"2\",\n            \"user_id\": \"1\",\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.postgres.aio import AsyncPostgresSaver\n# highlight-next-line\nfrom langgraph.store.postgres.aio import AsyncPostgresStore\nfrom langgraph.store.base import BaseStore\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\"\n\nasync with (\n    # highlight-next-line\n    AsyncPostgresStore.from_conn_string(DB_URI) as store,\n    AsyncPostgresSaver.from_conn_string(DB_URI) as checkpointer,\n):\n    # await store.setup()\n    # await checkpointer.setup()\n\n    async def call_model(\n        state: MessagesState,\n        config: RunnableConfig,\n        *,\n        # highlight-next-line\n        store: BaseStore,\n    ):\n        user_id = config[\"configurable\"][\"user_id\"]\n        namespace = (\"memories\", user_id)\n        # highlight-next-line\n        memories = await store.asearch(namespace, query=str(state[\"messages\"][-1].content))\n        info = \"\\n\".join([d.value[\"data\"] for d in memories])\n        system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n        # Store new memories if the user asks the model to remember\n        last_message = state[\"messages\"][-1]\n        if \"remember\" in last_message.content.lower():\n            memory = \"User name is Bob\"\n            # highlight-next-line\n            await store.aput(namespace, str(uuid.uuid4()), {\"data\": memory})\n\n        response = await model.ainvoke(\n            [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n        )\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(\n        checkpointer=checkpointer,\n        # highlight-next-line\n        store=store,\n    )\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"1\",\n            # highlight-next-line\n            \"user_id\": \"1\",\n        }\n    }\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"2\",\n            \"user_id\": \"1\",\n        }\n    }\n\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n</code></pre> <p>:::</p> <p>:::js <pre><code>npm install @langchain/langgraph-checkpoint-postgres\n</code></pre></p> <p>Setup</p> <p>You need to call <code>store.setup()</code> the first time you're using Postgres store</p> <p><pre><code>import { ChatAnthropic } from \"@langchain/anthropic\";\nimport { StateGraph, MessagesZodState, START, LangGraphRunnableConfig } from \"@langchain/langgraph\";\nimport { PostgresSaver, PostgresStore } from \"@langchain/langgraph-checkpoint-postgres\";\nimport { z } from \"zod\";\nimport { v4 as uuidv4 } from \"uuid\";\n\nconst model = new ChatAnthropic({ model: \"claude-3-5-haiku-20241022\" });\n\nconst DB_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\";\n\nconst store = PostgresStore.fromConnString(DB_URI);\nconst checkpointer = PostgresSaver.fromConnString(DB_URI);\n// await store.setup();\n// await checkpointer.setup();\n\nconst callModel = async (\n  state: z.infer&lt;typeof MessagesZodState&gt;,\n  config: LangGraphRunnableConfig,\n) =&gt; {\n  const userId = config.configurable?.userId;\n  const namespace = [\"memories\", userId];\n  const memories = await config.store?.search(namespace, { query: state.messages.at(-1)?.content });\n  const info = memories?.map(d =&gt; d.value.data).join(\"\\n\") || \"\";\n  const systemMsg = `You are a helpful assistant talking to the user. User info: ${info}`;\n\n  // Store new memories if the user asks the model to remember\n  const lastMessage = state.messages.at(-1);\n  if (lastMessage?.content?.toLowerCase().includes(\"remember\")) {\n    const memory = \"User name is Bob\";\n    await config.store?.put(namespace, uuidv4(), { data: memory });\n  }\n\n  const response = await model.invoke([\n    { role: \"system\", content: systemMsg },\n    ...state.messages\n  ]);\n  return { messages: [response] };\n};\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"call_model\", callModel)\n  .addEdge(START, \"call_model\");\n\nconst graph = builder.compile({\n  checkpointer,\n  store,\n});\n\nconst config = {\n  configurable: {\n    thread_id: \"1\",\n    userId: \"1\",\n  }\n};\n\nfor await (const chunk of await graph.stream(\n  { messages: [{ role: \"user\", content: \"Hi! Remember: my name is Bob\" }] },\n  { ...config, streamMode: \"values\" }\n)) {\n  console.log(chunk.messages.at(-1)?.content);\n}\n\nconst config2 = {\n  configurable: {\n    thread_id: \"2\",\n    userId: \"1\",\n  }\n};\n\nfor await (const chunk of await graph.stream(\n  { messages: [{ role: \"user\", content: \"what is my name?\" }] },\n  { ...config2, streamMode: \"values\" }\n)) {\n  console.log(chunk.messages.at(-1)?.content);\n}\n</code></pre> :::</p> <p>:::python</p> Example: using Redis store <pre><code>pip install -U langgraph langgraph-checkpoint-redis\n</code></pre> <p>Setup</p> <p>You need to call <code>store.setup()</code> the first time you're using Redis store</p> SyncAsync <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.redis import RedisSaver\n# highlight-next-line\nfrom langgraph.store.redis import RedisStore\nfrom langgraph.store.base import BaseStore\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"redis://localhost:6379\"\n\nwith (\n    # highlight-next-line\n    RedisStore.from_conn_string(DB_URI) as store,\n    RedisSaver.from_conn_string(DB_URI) as checkpointer,\n):\n    store.setup()\n    checkpointer.setup()\n\n    def call_model(\n        state: MessagesState,\n        config: RunnableConfig,\n        *,\n        # highlight-next-line\n        store: BaseStore,\n    ):\n        user_id = config[\"configurable\"][\"user_id\"]\n        namespace = (\"memories\", user_id)\n        # highlight-next-line\n        memories = store.search(namespace, query=str(state[\"messages\"][-1].content))\n        info = \"\\n\".join([d.value[\"data\"] for d in memories])\n        system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n        # Store new memories if the user asks the model to remember\n        last_message = state[\"messages\"][-1]\n        if \"remember\" in last_message.content.lower():\n            memory = \"User name is Bob\"\n            # highlight-next-line\n            store.put(namespace, str(uuid.uuid4()), {\"data\": memory})\n\n        response = model.invoke(\n            [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n        )\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(\n        checkpointer=checkpointer,\n        # highlight-next-line\n        store=store,\n    )\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"1\",\n            # highlight-next-line\n            \"user_id\": \"1\",\n        }\n    }\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"2\",\n            \"user_id\": \"1\",\n        }\n    }\n\n    for chunk in graph.stream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>from langchain_core.runnables import RunnableConfig\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, MessagesState, START\nfrom langgraph.checkpoint.redis.aio import AsyncRedisSaver\n# highlight-next-line\nfrom langgraph.store.redis.aio import AsyncRedisStore\nfrom langgraph.store.base import BaseStore\n\nmodel = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n\nDB_URI = \"redis://localhost:6379\"\n\nasync with (\n    # highlight-next-line\n    AsyncRedisStore.from_conn_string(DB_URI) as store,\n    AsyncRedisSaver.from_conn_string(DB_URI) as checkpointer,\n):\n    # await store.setup()\n    # await checkpointer.asetup()\n\n    async def call_model(\n        state: MessagesState,\n        config: RunnableConfig,\n        *,\n        # highlight-next-line\n        store: BaseStore,\n    ):\n        user_id = config[\"configurable\"][\"user_id\"]\n        namespace = (\"memories\", user_id)\n        # highlight-next-line\n        memories = await store.asearch(namespace, query=str(state[\"messages\"][-1].content))\n        info = \"\\n\".join([d.value[\"data\"] for d in memories])\n        system_msg = f\"You are a helpful assistant talking to the user. User info: {info}\"\n\n        # Store new memories if the user asks the model to remember\n        last_message = state[\"messages\"][-1]\n        if \"remember\" in last_message.content.lower():\n            memory = \"User name is Bob\"\n            # highlight-next-line\n            await store.aput(namespace, str(uuid.uuid4()), {\"data\": memory})\n\n        response = await model.ainvoke(\n            [{\"role\": \"system\", \"content\": system_msg}] + state[\"messages\"]\n        )\n        return {\"messages\": response}\n\n    builder = StateGraph(MessagesState)\n    builder.add_node(call_model)\n    builder.add_edge(START, \"call_model\")\n\n    graph = builder.compile(\n        checkpointer=checkpointer,\n        # highlight-next-line\n        store=store,\n    )\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"1\",\n            # highlight-next-line\n            \"user_id\": \"1\",\n        }\n    }\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n\n    config = {\n        \"configurable\": {\n            # highlight-next-line\n            \"thread_id\": \"2\",\n            \"user_id\": \"1\",\n        }\n    }\n\n    async for chunk in graph.astream(\n        {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]},\n        # highlight-next-line\n        config,\n        stream_mode=\"values\",\n    ):\n        chunk[\"messages\"][-1].pretty_print()\n</code></pre> <p>:::</p>"}, {"location": "how-tos/memory/add-memory/#read-long-term", "title": "Read long-term memory in tools", "text": "<p>:::python</p> A tool the agent can use to look up user information<pre><code>from langchain_core.runnables import RunnableConfig\nfrom langgraph.config import get_store\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.store.memory import InMemoryStore\n\n# highlight-next-line\nstore = InMemoryStore() # (1)!\n\n# highlight-next-line\nstore.put(  # (2)!\n    (\"users\",),  # (3)!\n    \"user_123\",  # (4)!\n    {\n        \"name\": \"John Smith\",\n        \"language\": \"English\",\n    } # (5)!\n)\n\ndef get_user_info(config: RunnableConfig) -&gt; str:\n    \"\"\"Look up user info.\"\"\"\n    # Same as that provided to `create_react_agent`\n    # highlight-next-line\n    store = get_store() # (6)!\n    user_id = config[\"configurable\"].get(\"user_id\")\n    # highlight-next-line\n    user_info = store.get((\"users\",), user_id) # (7)!\n    return str(user_info.value) if user_info else \"Unknown user\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[get_user_info],\n    # highlight-next-line\n    store=store # (8)!\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]},\n    # highlight-next-line\n    config={\"configurable\": {\"user_id\": \"user_123\"}}\n)\n</code></pre> <ol> <li>The <code>InMemoryStore</code> is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you.</li> <li>For this example, we write some sample data to the store using the <code>put</code> method. Please see the @[BaseStore.put] API reference for more details.</li> <li>The first argument is the namespace. This is used to group related data together. In this case, we are using the <code>users</code> namespace to group user data.</li> <li>A key within the namespace. This example uses a user ID for the key.</li> <li>The data that we want to store for the given user.</li> <li>The <code>get_store</code> function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.</li> <li>The <code>get</code> method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a <code>StoreValue</code> object, which contains the value and metadata about the value.</li> <li>The <code>store</code> is passed to the agent. This enables the agent to access the store when running tools. You can also use the <code>get_store</code> function to access the store from anywhere in your code.    :::</li> </ol> <p>:::js</p> A tool the agent can use to look up user information<pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { LangGraphRunnableConfig, InMemoryStore } from \"@langchain/langgraph\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst store = new InMemoryStore(); // (1)!\n\nawait store.put(\n  // (2)!\n  [\"users\"], // (3)!\n  \"user_123\", // (4)!\n  {\n    name: \"John Smith\",\n    language: \"English\",\n  } // (5)!\n);\n\nconst getUserInfo = tool(\n  async (_, config: LangGraphRunnableConfig) =&gt; {\n    /**Look up user info.*/\n    // Same as that provided to `createReactAgent`\n    const store = config.store; // (6)!\n    const userId = config.configurable?.userId;\n    const userInfo = await store?.get([\"users\"], userId); // (7)!\n    return userInfo?.value ? JSON.stringify(userInfo.value) : \"Unknown user\";\n  },\n  {\n    name: \"get_user_info\",\n    description: \"Look up user info.\",\n    schema: z.object({}),\n  }\n);\n\nconst agent = createReactAgent({\n  llm: model,\n  tools: [getUserInfo],\n  store, // (8)!\n});\n\n// Run the agent\nawait agent.invoke(\n  { messages: [{ role: \"user\", content: \"look up user information\" }] },\n  { configurable: { userId: \"user_123\" } }\n);\n</code></pre> <ol> <li>The <code>InMemoryStore</code> is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you.</li> <li>For this example, we write some sample data to the store using the <code>put</code> method. Please see the @[BaseStore.put] API reference for more details.</li> <li>The first argument is the namespace. This is used to group related data together. In this case, we are using the <code>users</code> namespace to group user data.</li> <li>A key within the namespace. This example uses a user ID for the key.</li> <li>The data that we want to store for the given user.</li> <li>The store is accessible through the config. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.</li> <li>The <code>get</code> method is used to retrieve data from the store. The first argument is the namespace, and the second argument is the key. This will return a <code>StoreValue</code> object, which contains the value and metadata about the value.</li> <li>The <code>store</code> is passed to the agent. This enables the agent to access the store when running tools. You can also use the store from the config to access it from anywhere in your code.    :::</li> </ol>"}, {"location": "how-tos/memory/add-memory/#write-long-term", "title": "Write long-term memory from tools", "text": "<p>:::python</p> Example of a tool that updates user information<pre><code>from typing_extensions import TypedDict\n\nfrom langgraph.config import get_store\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.store.memory import InMemoryStore\n\nstore = InMemoryStore() # (1)!\n\nclass UserInfo(TypedDict): # (2)!\n    name: str\n\ndef save_user_info(user_info: UserInfo, config: RunnableConfig) -&gt; str: # (3)!\n    \"\"\"Save user info.\"\"\"\n    # Same as that provided to `create_react_agent`\n    # highlight-next-line\n    store = get_store() # (4)!\n    user_id = config[\"configurable\"].get(\"user_id\")\n    # highlight-next-line\n    store.put((\"users\",), user_id, user_info) # (5)!\n    return \"Successfully saved user info.\"\n\nagent = create_react_agent(\n    model=\"anthropic:claude-3-7-sonnet-latest\",\n    tools=[save_user_info],\n    # highlight-next-line\n    store=store\n)\n\n# Run the agent\nagent.invoke(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]},\n    # highlight-next-line\n    config={\"configurable\": {\"user_id\": \"user_123\"}} # (6)!\n)\n\n# You can access the store directly to get the value\nstore.get((\"users\",), \"user_123\").value\n</code></pre> <ol> <li>The <code>InMemoryStore</code> is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you.</li> <li>The <code>UserInfo</code> class is a <code>TypedDict</code> that defines the structure of the user information. The LLM will use this to format the response according to the schema.</li> <li>The <code>save_user_info</code> function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information.</li> <li>The <code>get_store</code> function is used to access the store. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.</li> <li>The <code>put</code> method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store.</li> <li>The <code>user_id</code> is passed in the config. This is used to identify the user whose information is being updated.    :::</li> </ol> <p>:::js</p> Example of a tool that updates user information<pre><code>import { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\nimport { LangGraphRunnableConfig, InMemoryStore } from \"@langchain/langgraph\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\nconst store = new InMemoryStore(); // (1)!\n\nconst UserInfo = z.object({\n  // (2)!\n  name: z.string(),\n});\n\nconst saveUserInfo = tool(\n  async (\n    userInfo: z.infer&lt;typeof UserInfo&gt;,\n    config: LangGraphRunnableConfig\n  ) =&gt; {\n    // (3)!\n    /**Save user info.*/\n    // Same as that provided to `createReactAgent`\n    const store = config.store; // (4)!\n    const userId = config.configurable?.userId;\n    await store?.put([\"users\"], userId, userInfo); // (5)!\n    return \"Successfully saved user info.\";\n  },\n  {\n    name: \"save_user_info\",\n    description: \"Save user info.\",\n    schema: UserInfo,\n  }\n);\n\nconst agent = createReactAgent({\n  llm: model,\n  tools: [saveUserInfo],\n  store,\n});\n\n// Run the agent\nawait agent.invoke(\n  { messages: [{ role: \"user\", content: \"My name is John Smith\" }] },\n  { configurable: { userId: \"user_123\" } } // (6)!\n);\n\n// You can access the store directly to get the value\nconst result = await store.get([\"users\"], \"user_123\");\nconsole.log(result?.value);\n</code></pre> <ol> <li>The <code>InMemoryStore</code> is a store that stores data in memory. In a production setting, you would typically use a database or other persistent storage. Please review the store documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready store for you.</li> <li>The <code>UserInfo</code> schema defines the structure of the user information. The LLM will use this to format the response according to the schema.</li> <li>The <code>saveUserInfo</code> function is a tool that allows an agent to update user information. This could be useful for a chat application where the user wants to update their profile information.</li> <li>The store is accessible through the config. You can call it from anywhere in your code, including tools and prompts. This function returns the store that was passed to the agent when it was created.</li> <li>The <code>put</code> method is used to store data in the store. The first argument is the namespace, and the second argument is the key. This will store the user information in the store.</li> <li>The <code>userId</code> is passed in the config. This is used to identify the user whose information is being updated.    :::</li> </ol>"}, {"location": "how-tos/memory/add-memory/#use-semantic-search", "title": "Use semantic search", "text": "<p>Enable semantic search in your graph's memory store to let graph agents search for items in the store by semantic similarity.</p> <p>:::python</p> <pre><code>from langchain.embeddings import init_embeddings\nfrom langgraph.store.memory import InMemoryStore\n\n# Create store with semantic search enabled\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)\n\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\nitems = store.search(\n    (\"user_123\", \"memories\"), query=\"I'm hungry\", limit=1\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { OpenAIEmbeddings } from \"@langchain/openai\";\nimport { InMemoryStore } from \"@langchain/langgraph\";\n\n// Create store with semantic search enabled\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" });\nconst store = new InMemoryStore({\n  index: {\n    embeddings,\n    dims: 1536,\n  },\n});\n\nawait store.put([\"user_123\", \"memories\"], \"1\", { text: \"I love pizza\" });\nawait store.put([\"user_123\", \"memories\"], \"2\", { text: \"I am a plumber\" });\n\nconst items = await store.search([\"user_123\", \"memories\"], {\n  query: \"I'm hungry\",\n  limit: 1,\n});\n</code></pre> <p>:::</p> Long-term memory with semantic search <p>:::python <pre><code>from typing import Optional\n\nfrom langchain.embeddings import init_embeddings\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.store.base import BaseStore\nfrom langgraph.store.memory import InMemoryStore\nfrom langgraph.graph import START, MessagesState, StateGraph\n\nllm = init_chat_model(\"openai:gpt-4o-mini\")\n\n# Create store with semantic search enabled\nembeddings = init_embeddings(\"openai:text-embedding-3-small\")\nstore = InMemoryStore(\n    index={\n        \"embed\": embeddings,\n        \"dims\": 1536,\n    }\n)\n\nstore.put((\"user_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"})\nstore.put((\"user_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"})\n\ndef chat(state, *, store: BaseStore):\n    # Search based on user's last message\n    items = store.search(\n        (\"user_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2\n    )\n    memories = \"\\n\".join(item.value[\"text\"] for item in items)\n    memories = f\"## Memories of user\\n{memories}\" if memories else \"\"\n    response = llm.invoke(\n        [\n            {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"},\n            *state[\"messages\"],\n        ]\n    )\n    return {\"messages\": [response]}\n\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(chat)\nbuilder.add_edge(START, \"chat\")\ngraph = builder.compile(store=store)\n\nfor message, metadata in graph.stream(\n    input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]},\n    stream_mode=\"messages\",\n):\n    print(message.content, end=\"\")\n</code></pre> :::</p> <p>:::js <pre><code>import { OpenAIEmbeddings, ChatOpenAI } from \"@langchain/openai\";\nimport { StateGraph, START, MessagesZodState, InMemoryStore } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n\n// Create store with semantic search enabled\nconst embeddings = new OpenAIEmbeddings({ model: \"text-embedding-3-small\" });\nconst store = new InMemoryStore({\n  index: {\n    embeddings,\n    dims: 1536,\n  }\n});\n\nawait store.put([\"user_123\", \"memories\"], \"1\", { text: \"I love pizza\" });\nawait store.put([\"user_123\", \"memories\"], \"2\", { text: \"I am a plumber\" });\n\nconst chat = async (state: z.infer&lt;typeof MessagesZodState&gt;, config) =&gt; {\n  // Search based on user's last message\n  const items = await config.store.search(\n    [\"user_123\", \"memories\"],\n    { query: state.messages.at(-1)?.content, limit: 2 }\n  );\n  const memories = items.map(item =&gt; item.value.text).join(\"\\n\");\n  const memoriesText = memories ? `## Memories of user\\n${memories}` : \"\";\n\n  const response = await llm.invoke([\n    { role: \"system\", content: `You are a helpful assistant.\\n${memoriesText}` },\n    ...state.messages,\n  ]);\n\n  return { messages: [response] };\n};\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"chat\", chat)\n  .addEdge(START, \"chat\");\nconst graph = builder.compile({ store });\n\nfor await (const [message, metadata] of await graph.stream(\n  { messages: [{ role: \"user\", content: \"I'm hungry\" }] },\n  { streamMode: \"messages\" }\n)) {\n  if (message.content) {\n    console.log(message.content);\n  }\n}\n</code></pre> :::</p> <p>See this guide for more information on how to use semantic search with LangGraph memory store.</p>"}, {"location": "how-tos/memory/add-memory/#manage-short-term-memory", "title": "Manage short-term memory", "text": "<p>With short-term memory enabled, long conversations can exceed the LLM's context window. Common solutions are:</p> <ul> <li>Trim messages: Remove first or last N messages (before calling LLM)</li> <li>Delete messages from LangGraph state permanently</li> <li>Summarize messages: Summarize earlier messages in the history and replace them with a summary</li> <li>Manage checkpoints to store and retrieve message history</li> <li>Custom strategies (e.g., message filtering, etc.)</li> </ul> <p>This allows the agent to keep track of the conversation without exceeding the LLM's context window.</p>"}, {"location": "how-tos/memory/add-memory/#trim-messages", "title": "Trim messages", "text": "<p>Most LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the <code>strategy</code> (e.g., keep the last <code>maxTokens</code>) to use for handling the boundary.</p> In an agentIn a workflow <p>:::python To trim message history in an agent, use @[<code>pre_model_hook</code>][create_react_agent] with the <code>trim_messages</code> function:</p> <p><pre><code># highlight-next-line\nfrom langchain_core.messages.utils import (\n    # highlight-next-line\n    trim_messages,\n    # highlight-next-line\n    count_tokens_approximately\n# highlight-next-line\n)\nfrom langgraph.prebuilt import create_react_agent\n\n# This function will be called every time before the node that calls LLM\ndef pre_model_hook(state):\n    trimmed_messages = trim_messages(\n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=count_tokens_approximately,\n        max_tokens=384,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n    )\n    # highlight-next-line\n    return {\"llm_input_messages\": trimmed_messages}\n\ncheckpointer = InMemorySaver()\nagent = create_react_agent(\n    model,\n    tools,\n    # highlight-next-line\n    pre_model_hook=pre_model_hook,\n    checkpointer=checkpointer,\n)\n</code></pre> :::</p> <p>:::js To trim message history in an agent, use <code>stateModifier</code> with the <code>trimMessages</code> function:</p> <p><pre><code>import { trimMessages } from \"@langchain/core/messages\";\nimport { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\n// This function will be called every time before the node that calls LLM\nconst stateModifier = async (state) =&gt; {\n  return trimMessages(state.messages, {\n    strategy: \"last\",\n    maxTokens: 384,\n    startOn: \"human\",\n    endOn: [\"human\", \"tool\"],\n  });\n};\n\nconst checkpointer = new MemorySaver();\nconst agent = createReactAgent({\n  llm: model,\n  tools,\n  stateModifier,\n  checkpointer,\n});\n</code></pre> :::</p> <p>:::python To trim message history, use the <code>trim_messages</code> function:</p> <p><pre><code># highlight-next-line\nfrom langchain_core.messages.utils import (\n    # highlight-next-line\n    trim_messages,\n    # highlight-next-line\n    count_tokens_approximately\n# highlight-next-line\n)\n\ndef call_model(state: MessagesState):\n    # highlight-next-line\n    messages = trim_messages(\n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=count_tokens_approximately,\n        max_tokens=128,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n    )\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(call_model)\n...\n</code></pre> :::</p> <p>:::js To trim message history, use the <code>trimMessages</code> function:</p> <p><pre><code>import { trimMessages } from \"@langchain/core/messages\";\n\nconst callModel = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const messages = trimMessages(state.messages, {\n    strategy: \"last\",\n    maxTokens: 128,\n    startOn: \"human\",\n    endOn: [\"human\", \"tool\"],\n  });\n  const response = await model.invoke(messages);\n  return { messages: [response] };\n};\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"call_model\", callModel);\n// ...\n</code></pre> :::</p> Full example: trim messages <p>:::python <pre><code># highlight-next-line\nfrom langchain_core.messages.utils import (\n    # highlight-next-line\n    trim_messages,\n    # highlight-next-line\n    count_tokens_approximately\n# highlight-next-line\n)\nfrom langchain.chat_models import init_chat_model\nfrom langgraph.graph import StateGraph, START, MessagesState\n\nmodel = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\")\nsummarization_model = model.bind(max_tokens=128)\n\ndef call_model(state: MessagesState):\n    # highlight-next-line\n    messages = trim_messages(\n        state[\"messages\"],\n        strategy=\"last\",\n        token_counter=count_tokens_approximately,\n        max_tokens=128,\n        start_on=\"human\",\n        end_on=(\"human\", \"tool\"),\n    )\n    response = model.invoke(messages)\n    return {\"messages\": [response]}\n\ncheckpointer = InMemorySaver()\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(call_model)\nbuilder.add_edge(START, \"call_model\")\ngraph = builder.compile(checkpointer=checkpointer)\n\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"messages\": \"hi, my name is bob\"}, config)\ngraph.invoke({\"messages\": \"write a short poem about cats\"}, config)\ngraph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\n</code></pre></p> <p><pre><code>================================== Ai Message ==================================\n\nYour name is Bob, as you mentioned when you first introduced yourself.\n</code></pre> :::</p> <p>:::js <pre><code>import { trimMessages } from \"@langchain/core/messages\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { StateGraph, START, MessagesZodState, MemorySaver } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20241022\" });\n\nconst callModel = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const messages = trimMessages(state.messages, {\n    strategy: \"last\",\n    maxTokens: 128,\n    startOn: \"human\",\n    endOn: [\"human\", \"tool\"],\n  });\n  const response = await model.invoke(messages);\n  return { messages: [response] };\n};\n\nconst checkpointer = new MemorySaver();\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"call_model\", callModel)\n  .addEdge(START, \"call_model\");\nconst graph = builder.compile({ checkpointer });\n\nconst config = { configurable: { thread_id: \"1\" } };\nawait graph.invoke({ messages: [{ role: \"user\", content: \"hi, my name is bob\" }] }, config);\nawait graph.invoke({ messages: [{ role: \"user\", content: \"write a short poem about cats\" }] }, config);\nawait graph.invoke({ messages: [{ role: \"user\", content: \"now do the same but for dogs\" }] }, config);\nconst finalResponse = await graph.invoke({ messages: [{ role: \"user\", content: \"what's my name?\" }] }, config);\n\nconsole.log(finalResponse.messages.at(-1)?.content);\n</code></pre></p> <p><pre><code>Your name is Bob, as you mentioned when you first introduced yourself.\n</code></pre> :::</p>"}, {"location": "how-tos/memory/add-memory/#delete-messages", "title": "Delete messages", "text": "<p>You can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history.</p> <p>:::python To delete messages from the graph state, you can use the <code>RemoveMessage</code>. For <code>RemoveMessage</code> to work, you need to use a state key with @[<code>add_messages</code>][add_messages] reducer, like <code>MessagesState</code>.</p> <p>To remove specific messages:</p> <pre><code># highlight-next-line\nfrom langchain_core.messages import RemoveMessage\n\ndef delete_messages(state):\n    messages = state[\"messages\"]\n    if len(messages) &gt; 2:\n        # remove the earliest two messages\n        # highlight-next-line\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n</code></pre> <p>To remove all messages:</p> <pre><code># highlight-next-line\nfrom langgraph.graph.message import REMOVE_ALL_MESSAGES\n\ndef delete_messages(state):\n    # highlight-next-line\n    return {\"messages\": [RemoveMessage(id=REMOVE_ALL_MESSAGES)]}\n</code></pre> <p>:::</p> <p>:::js To delete messages from the graph state, you can use the <code>RemoveMessage</code>. For <code>RemoveMessage</code> to work, you need to use a state key with @[<code>messagesStateReducer</code>][messagesStateReducer] reducer, like <code>MessagesZodState</code>.</p> <p>To remove specific messages:</p> <pre><code>import { RemoveMessage } from \"@langchain/core/messages\";\n\nconst deleteMessages = (state) =&gt; {\n  const messages = state.messages;\n  if (messages.length &gt; 2) {\n    // remove the earliest two messages\n    return {\n      messages: messages\n        .slice(0, 2)\n        .map((m) =&gt; new RemoveMessage({ id: m.id })),\n    };\n  }\n};\n</code></pre> <p>:::</p> <p>Warning</p> <p>When deleting messages, make sure that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example:</p> <ul> <li>some providers expect message history to start with a <code>user</code> message</li> <li>most providers require <code>assistant</code> messages with tool calls to be followed by corresponding <code>tool</code> result messages.</li> </ul> Full example: delete messages <p>:::python <pre><code># highlight-next-line\nfrom langchain_core.messages import RemoveMessage\n\ndef delete_messages(state):\n    messages = state[\"messages\"]\n    if len(messages) &gt; 2:\n        # remove the earliest two messages\n        # highlight-next-line\n        return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]}\n\ndef call_model(state: MessagesState):\n    response = model.invoke(state[\"messages\"])\n    return {\"messages\": response}\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_sequence([call_model, delete_messages])\nbuilder.add_edge(START, \"call_model\")\n\ncheckpointer = InMemorySaver()\napp = builder.compile(checkpointer=checkpointer)\n\nfor event in app.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]},\n    config,\n    stream_mode=\"values\"\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n\nfor event in app.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]},\n    config,\n    stream_mode=\"values\"\n):\n    print([(message.type, message.content) for message in event[\"messages\"]])\n</code></pre></p> <p><pre><code>[('human', \"hi! I'm bob\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\")]\n[('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n[('human', \"what's my name?\"), ('ai', 'Your name is Bob.')]\n</code></pre> :::</p> <p>:::js <pre><code>import { RemoveMessage } from \"@langchain/core/messages\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { StateGraph, START, MessagesZodState, MemorySaver } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst model = new ChatAnthropic({ model: \"claude-3-5-sonnet-20241022\" });\n\nconst deleteMessages = (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const messages = state.messages;\n  if (messages.length &gt; 2) {\n    // remove the earliest two messages\n    return { messages: messages.slice(0, 2).map(m =&gt; new RemoveMessage({ id: m.id })) };\n  }\n  return {};\n};\n\nconst callModel = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const response = await model.invoke(state.messages);\n  return { messages: [response] };\n};\n\nconst builder = new StateGraph(MessagesZodState)\n  .addNode(\"call_model\", callModel)\n  .addNode(\"delete_messages\", deleteMessages)\n  .addEdge(START, \"call_model\")\n  .addEdge(\"call_model\", \"delete_messages\");\n\nconst checkpointer = new MemorySaver();\nconst app = builder.compile({ checkpointer });\n\nconst config = { configurable: { thread_id: \"1\" } };\n\nfor await (const event of await app.stream(\n  { messages: [{ role: \"user\", content: \"hi! I'm bob\" }] },\n  { ...config, streamMode: \"values\" }\n)) {\n  console.log(event.messages.map(message =&gt; [message.getType(), message.content]));\n}\n\nfor await (const event of await app.stream(\n  { messages: [{ role: \"user\", content: \"what's my name?\" }] },\n  { ...config, streamMode: \"values\" }\n)) {\n  console.log(event.messages.map(message =&gt; [message.getType(), message.content]));\n}\n</code></pre></p> <p><pre><code>[['human', \"hi! I'm bob\"]]\n[['human', \"hi! I'm bob\"], ['ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?']]\n[['human', \"hi! I'm bob\"], ['ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'], ['human', \"what's my name?\"]]\n[['human', \"hi! I'm bob\"], ['ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'], ['human', \"what's my name?\"], ['ai', 'Your name is Bob.']]\n[['human', \"what's my name?\"], ['ai', 'Your name is Bob.']]\n</code></pre> :::</p>"}, {"location": "how-tos/memory/add-memory/#summarize-messages", "title": "Summarize messages", "text": "<p>The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model.</p> <p></p> In an agentIn a workflow <p>:::python To summarize message history in an agent, use @[<code>pre_model_hook</code>][create_react_agent] with a prebuilt <code>SummarizationNode</code> abstraction:</p> <pre><code>from langchain_anthropic import ChatAnthropic\nfrom langmem.short_term import SummarizationNode, RunningSummary\nfrom langchain_core.messages.utils import count_tokens_approximately\nfrom langgraph.prebuilt import create_react_agent\nfrom langgraph.prebuilt.chat_agent_executor import AgentState\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom typing import Any\n\nmodel = ChatAnthropic(model=\"claude-3-7-sonnet-latest\")\n\nsummarization_node = SummarizationNode( # (1)!\n    token_counter=count_tokens_approximately,\n    model=model,\n    max_tokens=384,\n    max_summary_tokens=128,\n    output_messages_key=\"llm_input_messages\",\n)\n\nclass State(AgentState):\n    # NOTE: we're adding this key to keep track of previous summary information\n    # to make sure we're not summarizing on every LLM call\n    # highlight-next-line\n    context: dict[str, RunningSummary]  # (2)!\n\n\ncheckpointer = InMemorySaver() # (3)!\n\nagent = create_react_agent(\n    model=model,\n    tools=tools,\n    # highlight-next-line\n    pre_model_hook=summarization_node, # (4)!\n    # highlight-next-line\n    state_schema=State, # (5)!\n    checkpointer=checkpointer,\n)\n</code></pre> <ol> <li>The <code>InMemorySaver</code> is a checkpointer that stores the agent's state in memory. In a production setting, you would typically use a database or other persistent storage. Please review the checkpointer documentation for more options. If you're deploying with LangGraph Platform, the platform will provide a production-ready checkpointer for you.</li> <li>The <code>context</code> key is added to the agent's state. The key contains book-keeping information for the summarization node. It is used to keep track of the last summary information and ensure that the agent doesn't summarize on every LLM call, which can be inefficient.</li> <li>The <code>checkpointer</code> is passed to the agent. This enables the agent to persist its state across invocations.</li> <li>The <code>pre_model_hook</code> is set to the <code>SummarizationNode</code>. This node will summarize the message history before sending it to the LLM. The summarization node will automatically handle the summarization process and update the agent's state with the new summary. You can replace this with a custom implementation if you prefer. Please see the @[create_react_agent][create_react_agent] API reference for more details.</li> <li>The <code>state_schema</code> is set to the <code>State</code> class, which is the custom state that contains an extra <code>context</code> key. :::</li> </ol> <p>:::python Prompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend the <code>MessagesState</code> to include a <code>summary</code> key:</p> <pre><code>from langgraph.graph import MessagesState\nclass State(MessagesState):\n    summary: str\n</code></pre> <p>Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This <code>summarize_conversation</code> node can be called after some number of messages have accumulated in the <code>messages</code> state key.</p> <p><pre><code>def summarize_conversation(state: State):\n\n    # First, we get any existing summary\n    summary = state.get(\"summary\", \"\")\n\n    # Create our summarization prompt\n    if summary:\n\n        # A summary already exists\n        summary_message = (\n            f\"This is a summary of the conversation to date: {summary}\\n\\n\"\n            \"Extend the summary by taking into account the new messages above:\"\n        )\n\n    else:\n        summary_message = \"Create a summary of the conversation above:\"\n\n    # Add prompt to our history\n    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n    response = model.invoke(messages)\n\n    # Delete all but the 2 most recent messages\n    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n    return {\"summary\": response.content, \"messages\": delete_messages}\n</code></pre> :::</p> <p>:::js Prompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend the <code>MessagesZodState</code> to include a <code>summary</code> key:</p> <pre><code>import { MessagesZodState } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = MessagesZodState.merge(z.object({\n  summary: z.string().optional(),\n}));\n</code></pre> <p>Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This <code>summarizeConversation</code> node can be called after some number of messages have accumulated in the <code>messages</code> state key.</p> <p><pre><code>import { RemoveMessage, HumanMessage } from \"@langchain/core/messages\";\n\nconst summarizeConversation = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // First, we get any existing summary\n  const summary = state.summary || \"\";\n\n  // Create our summarization prompt\n  let summaryMessage: string;\n  if (summary) {\n    // A summary already exists\n    summaryMessage =\n      `This is a summary of the conversation to date: ${summary}\\n\\n` +\n      \"Extend the summary by taking into account the new messages above:\";\n  } else {\n    summaryMessage = \"Create a summary of the conversation above:\";\n  }\n\n  // Add prompt to our history\n  const messages = [\n    ...state.messages,\n    new HumanMessage({ content: summaryMessage })\n  ];\n  const response = await model.invoke(messages);\n\n  // Delete all but the 2 most recent messages\n  const deleteMessages = state.messages\n    .slice(0, -2)\n    .map(m =&gt; new RemoveMessage({ id: m.id }));\n\n  return {\n    summary: response.content,\n    messages: deleteMessages\n  };\n};\n</code></pre> :::</p> Full example: summarize messages <p>:::python <pre><code>from typing import Any, TypedDict\n\nfrom langchain.chat_models import init_chat_model\nfrom langchain_core.messages import AnyMessage\nfrom langchain_core.messages.utils import count_tokens_approximately\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.checkpoint.memory import InMemorySaver\n# highlight-next-line\nfrom langmem.short_term import SummarizationNode, RunningSummary\n\nmodel = init_chat_model(\"anthropic:claude-3-7-sonnet-latest\")\nsummarization_model = model.bind(max_tokens=128)\n\nclass State(MessagesState):\n    # highlight-next-line\n    context: dict[str, RunningSummary]  # (1)!\n\nclass LLMInputState(TypedDict):  # (2)!\n    summarized_messages: list[AnyMessage]\n    context: dict[str, RunningSummary]\n\n# highlight-next-line\nsummarization_node = SummarizationNode(\n    token_counter=count_tokens_approximately,\n    model=summarization_model,\n    max_tokens=256,\n    max_tokens_before_summary=256,\n    max_summary_tokens=128,\n)\n\n# highlight-next-line\ndef call_model(state: LLMInputState):  # (3)!\n    response = model.invoke(state[\"summarized_messages\"])\n    return {\"messages\": [response]}\n\ncheckpointer = InMemorySaver()\nbuilder = StateGraph(State)\nbuilder.add_node(call_model)\n# highlight-next-line\nbuilder.add_node(\"summarize\", summarization_node)\nbuilder.add_edge(START, \"summarize\")\nbuilder.add_edge(\"summarize\", \"call_model\")\ngraph = builder.compile(checkpointer=checkpointer)\n\n# Invoke the graph\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\ngraph.invoke({\"messages\": \"hi, my name is bob\"}, config)\ngraph.invoke({\"messages\": \"write a short poem about cats\"}, config)\ngraph.invoke({\"messages\": \"now do the same but for dogs\"}, config)\nfinal_response = graph.invoke({\"messages\": \"what's my name?\"}, config)\n\nfinal_response[\"messages\"][-1].pretty_print()\nprint(\"\\nSummary:\", final_response[\"context\"][\"running_summary\"].summary)\n</code></pre></p> <ol> <li>We will keep track of our running summary in the <code>context</code> field (expected by the <code>SummarizationNode</code>).</li> <li>Define private state that will be used only for filtering the inputs to <code>call_model</code> node.</li> <li>We're passing a private input state here to isolate the messages returned by the summarization node</li> </ol> <p><pre><code>================================== Ai Message ==================================\n\nFrom our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking.\n\nSummary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled \"The Mystery of Cats\" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote \"The Joy of Dogs,\" which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special.\n</code></pre> :::</p> <p>:::js <pre><code>import { ChatAnthropic } from \"@langchain/anthropic\";\nimport {\n  SystemMessage,\n  HumanMessage,\n  RemoveMessage,\n  type BaseMessage\n} from \"@langchain/core/messages\";\nimport {\n  MessagesZodState,\n  StateGraph,\n  START,\n  END,\n  MemorySaver,\n} from \"@langchain/langgraph\";\nimport { z } from \"zod\";\nimport { v4 as uuidv4 } from \"uuid\";\n\nconst memory = new MemorySaver();\n\n// We will add a `summary` attribute (in addition to `messages` key,\n// which MessagesZodState already has)\nconst GraphState = z.object({\n  messages: MessagesZodState.shape.messages,\n  summary: z.string().default(\"\"),\n});\n\n// We will use this model for both the conversation and the summarization\nconst model = new ChatAnthropic({ model: \"claude-3-haiku-20240307\" });\n\n// Define the logic to call the model\nconst callModel = async (state: z.infer&lt;typeof GraphState&gt;) =&gt; {\n  // If a summary exists, we add this in as a system message\n  const { summary } = state;\n  let { messages } = state;\n  if (summary) {\n    const systemMessage = new SystemMessage({\n      id: uuidv4(),\n      content: `Summary of conversation earlier: ${summary}`,\n    });\n    messages = [systemMessage, ...messages];\n  }\n  const response = await model.invoke(messages);\n  // We return an object, because this will get added to the existing state\n  return { messages: [response] };\n};\n\n// We now define the logic for determining whether to end or summarize the conversation\nconst shouldContinue = (state: z.infer&lt;typeof GraphState&gt;) =&gt; {\n  const messages = state.messages;\n  // If there are more than six messages, then we summarize the conversation\n  if (messages.length &gt; 6) {\n    return \"summarize_conversation\";\n  }\n  // Otherwise we can just end\n  return END;\n};\n\nconst summarizeConversation = async (state: z.infer&lt;typeof GraphState&gt;) =&gt; {\n  // First, we summarize the conversation\n  const { summary, messages } = state;\n  let summaryMessage: string;\n  if (summary) {\n    // If a summary already exists, we use a different system prompt\n    // to summarize it than if one didn't\n    summaryMessage =\n      `This is summary of the conversation to date: ${summary}\\n\\n` +\n      \"Extend the summary by taking into account the new messages above:\";\n  } else {\n    summaryMessage = \"Create a summary of the conversation above:\";\n  }\n\n  const allMessages = [\n    ...messages,\n    new HumanMessage({ id: uuidv4(), content: summaryMessage }),\n  ];\n\n  const response = await model.invoke(allMessages);\n\n  // We now need to delete messages that we no longer want to show up\n  // I will delete all but the last two messages, but you can change this\n  const deleteMessages = messages\n    .slice(0, -2)\n    .map((m) =&gt; new RemoveMessage({ id: m.id! }));\n\n  if (typeof response.content !== \"string\") {\n    throw new Error(\"Expected a string response from the model\");\n  }\n\n  return { summary: response.content, messages: deleteMessages };\n};\n\n// Define a new graph\nconst workflow = new StateGraph(GraphState)\n  // Define the conversation node and the summarize node\n  .addNode(\"conversation\", callModel)\n  .addNode(\"summarize_conversation\", summarizeConversation)\n  // Set the entrypoint as conversation\n  .addEdge(START, \"conversation\")\n  // We now add a conditional edge\n  .addConditionalEdges(\n    // First, we define the start node. We use `conversation`.\n    // This means these are the edges taken after the `conversation` node is called.\n    \"conversation\",\n    // Next, we pass in the function that will determine which node is called next.\n    shouldContinue,\n  )\n  // We now add a normal edge from `summarize_conversation` to END.\n  // This means that after `summarize_conversation` is called, we end.\n  .addEdge(\"summarize_conversation\", END);\n\n// Finally, we compile it!\nconst app = workflow.compile({ checkpointer: memory });\n</code></pre> :::</p>"}, {"location": "how-tos/memory/add-memory/#manage-checkpoints", "title": "Manage checkpoints", "text": "<p>You can view and delete the information stored by the checkpointer.</p>"}, {"location": "how-tos/memory/add-memory/#view-thread-state-checkpoint", "title": "View thread state (checkpoint)", "text": "<p>:::python</p> Graph/Functional APICheckpointer API <pre><code>config = {\n    \"configurable\": {\n        # highlight-next-line\n        \"thread_id\": \"1\",\n        # optionally provide an ID for a specific checkpoint,\n        # otherwise the latest checkpoint is shown\n        # highlight-next-line\n        # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"\n\n    }\n}\n# highlight-next-line\ngraph.get_state(config)\n</code></pre> <pre><code>StateSnapshot(\n    values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(),\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n    metadata={\n        'source': 'loop',\n        'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n        'step': 4,\n        'parents': {},\n        'thread_id': '1'\n    },\n    created_at='2025-05-05T16:01:24.680462+00:00',\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n    tasks=(),\n    interrupts=()\n)\n</code></pre> <pre><code>config = {\n    \"configurable\": {\n        # highlight-next-line\n        \"thread_id\": \"1\",\n        # optionally provide an ID for a specific checkpoint,\n        # otherwise the latest checkpoint is shown\n        # highlight-next-line\n        # \"checkpoint_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"\n\n    }\n}\n# highlight-next-line\ncheckpointer.get_tuple(config)\n</code></pre> <pre><code>CheckpointTuple(\n    config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n    checkpoint={\n        'v': 3,\n        'ts': '2025-05-05T16:01:24.680462+00:00',\n        'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',\n        'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'}, 'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\n        'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n    },\n    metadata={\n        'source': 'loop',\n        'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}},\n        'step': 4,\n        'parents': {},\n        'thread_id': '1'\n    },\n    parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n    pending_writes=[]\n)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const config = {\n  configurable: {\n    thread_id: \"1\",\n    // optionally provide an ID for a specific checkpoint,\n    // otherwise the latest checkpoint is shown\n    // checkpoint_id: \"1f029ca3-1f5b-6704-8004-820c16b69a5a\"\n  },\n};\nawait graph.getState(config);\n</code></pre> <pre><code>{\n  values: { messages: [HumanMessage(...), AIMessage(...), HumanMessage(...), AIMessage(...)] },\n  next: [],\n  config: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1f5b-6704-8004-820c16b69a5a' } },\n  metadata: {\n    source: 'loop',\n    writes: { call_model: { messages: AIMessage(...) } },\n    step: 4,\n    parents: {},\n    thread_id: '1'\n  },\n  createdAt: '2025-05-05T16:01:24.680462+00:00',\n  parentConfig: { configurable: { thread_id: '1', checkpoint_ns: '', checkpoint_id: '1f029ca3-1790-6b0a-8003-baf965b6a38f' } },\n  tasks: [],\n  interrupts: []\n}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/memory/add-memory/#view-the-history-of-the-thread-checkpoints", "title": "View the history of the thread (checkpoints)", "text": "<p>:::python</p> Graph/Functional APICheckpointer API <pre><code>config = {\n    \"configurable\": {\n        # highlight-next-line\n        \"thread_id\": \"1\"\n    }\n}\n# highlight-next-line\nlist(graph.get_state_history(config))\n</code></pre> <pre><code>[\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n        next=(),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:24.680462+00:00',\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        tasks=(),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")]},\n        next=('call_model',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.863421+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n        next=('__start__',),\n        config={...},\n        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.863173+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"what's my name?\"}]}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]},\n        next=(),\n        config={...},\n        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:23.862295+00:00',\n        parent_config={...}\n        tasks=(),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': [HumanMessage(content=\"hi! I'm bob\")]},\n        next=('call_model',),\n        config={...},\n        metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:22.278960+00:00',\n        parent_config={...}\n        tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call_model', path=('__pregel_pull', 'call_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),),\n        interrupts=()\n    ),\n    StateSnapshot(\n        values={'messages': []},\n        next=('__start__',),\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\n        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\n        created_at='2025-05-05T16:01:22.277497+00:00',\n        parent_config=None,\n        tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}),),\n        interrupts=()\n    )\n]\n</code></pre> <pre><code>config = {\n    \"configurable\": {\n        # highlight-next-line\n        \"thread_id\": \"1\"\n    }\n}\n# highlight-next-line\nlist(checkpointer.list(config))\n</code></pre> <pre><code>[\n    CheckpointTuple(\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}},\n        checkpoint={\n            'v': 3,\n            'ts': '2025-05-05T16:01:24.680462+00:00',\n            'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a',\n            'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},\n            'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\n            'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]},\n        },\n        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread_id': '1'},\n        parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        pending_writes=[]\n    ),\n    CheckpointTuple(\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}},\n        checkpoint={\n            'v': 3,\n            'ts': '2025-05-05T16:01:23.863421+00:00',\n            'id': '1f029ca3-1790-6b0a-8003-baf965b6a38f',\n            'channel_versions': {'__start__': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call_model': '00000000000000000000000000000006.0.14611156755133758'},\n            'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000004.0.5736472536395331'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000005.0.1410174088651449'}},\n            'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")], 'branch:to:call_model': None}\n        },\n        metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread_id': '1'},\n        parent_config={...},\n        pending_writes=[('8ab4155e-6b15-b885-9ce5-bed69a2c305c', 'messages', AIMessage(content='Your name is Bob.'))]\n    ),\n    CheckpointTuple(\n        config={...},\n        checkpoint={\n            'v': 3,\n            'ts': '2025-05-05T16:01:23.863173+00:00',\n            'id': '1f029ca3-1790-616e-8002-9e021694a0cd',\n            'channel_versions': {'__start__': '00000000000000000000000000000004.0.5736472536395331', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},\n            'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},\n            'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}, 'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}\n        },\n        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread_id': '1'},\n        parent_config={...},\n        pending_writes=[('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'messages', [{'role': 'user', 'content': \"what's my name?\"}]), ('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'branch:to:call_model', None)]\n    ),\n    CheckpointTuple(\n        config={...},\n        checkpoint={\n            'v': 3,\n            'ts': '2025-05-05T16:01:23.862295+00:00',\n            'id': '1f029ca3-178d-6f54-8001-d7b180db0c89',\n            'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call_model': '00000000000000000000000000000003.0.22059023329132854'},\n            'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}, 'call_model': {'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'}},\n            'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}\n        },\n        metadata={'source': 'loop', 'writes': {'call_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread_id': '1'},\n        parent_config={...},\n        pending_writes=[]\n    ),\n    CheckpointTuple(\n        config={...},\n        checkpoint={\n            'v': 3,\n            'ts': '2025-05-05T16:01:22.278960+00:00',\n            'id': '1f029ca3-0874-6612-8000-339f2abc83b1',\n            'channel_versions': {'__start__': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000002.0.30296526818059655', 'branch:to:call_model': '00000000000000000000000000000002.0.9300422176788571'},\n            'versions_seen': {'__input__': {}, '__start__': {'__start__': '00000000000000000000000000000001.0.7040775356287469'}},\n            'channel_values': {'messages': [HumanMessage(content=\"hi! I'm bob\")], 'branch:to:call_model': None}\n        },\n        metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread_id': '1'},\n        parent_config={...},\n        pending_writes=[('8cbd75e0-3720-b056-04f7-71ac805140a0', 'messages', AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'))]\n    ),\n    CheckpointTuple(\n        config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}},\n        checkpoint={\n            'v': 3,\n            'ts': '2025-05-05T16:01:22.277497+00:00',\n            'id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565',\n            'channel_versions': {'__start__': '00000000000000000000000000000001.0.7040775356287469'},\n            'versions_seen': {'__input__': {}},\n            'channel_values': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}\n        },\n        metadata={'source': 'input', 'writes': {'__start__': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread_id': '1'},\n        parent_config=None,\n        pending_writes=[('d458367b-8265-812c-18e2-33001d199ce6', 'messages', [{'role': 'user', 'content': \"hi! I'm bob\"}]), ('d458367b-8265-812c-18e2-33001d199ce6', 'branch:to:call_model', None)]\n    )\n]\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const config = {\n  configurable: {\n    thread_id: \"1\",\n  },\n};\n\nconst history = [];\nfor await (const state of graph.getStateHistory(config)) {\n  history.push(state);\n}\n</code></pre> <p>:::</p>"}, {"location": "how-tos/memory/add-memory/#delete-all-checkpoints-for-a-thread", "title": "Delete all checkpoints for a thread", "text": "<p>:::python</p> <pre><code>thread_id = \"1\"\ncheckpointer.delete_thread(thread_id)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const threadId = \"1\";\nawait checkpointer.deleteThread(threadId);\n</code></pre> <p>:::</p> <p>:::python</p>"}, {"location": "how-tos/memory/add-memory/#prebuilt-memory-tools", "title": "Prebuilt memory tools", "text": "<p>LangMem is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the LangMem documentation for usage examples.</p> <p>:::</p>"}, {"location": "reference/", "title": "Reference", "text": "", "boost": 0.5}, {"location": "reference/#reference", "title": "Reference", "text": "<p>Welcome to the LangGraph reference docs! These pages detail the core interfaces you will use when building with LangGraph. Each section covers a different part of the ecosystem.</p> <p>Tip</p> <p>If you are just getting started, see LangGraph basics for an introduction to the main concepts and usage patterns.</p>", "boost": 0.5}, {"location": "reference/#langgraph", "title": "LangGraph", "text": "<p>The core APIs for the LangGraph open source library.</p> <ul> <li>Graphs: Main graph abstraction and usage.</li> <li>Functional API: Functional programming interface for graphs.</li> <li>Pregel: Pregel-inspired computation model.</li> <li>Checkpointing: Saving and restoring graph state.</li> <li>Storage: Storage backends and options.</li> <li>Caching: Caching mechanisms for performance.</li> <li>Types: Type definitions for graph components.</li> <li>Config: Configuration options.</li> <li>Errors: Error types and handling.</li> <li>Constants: Global constants.</li> <li>Channels: Message passing and channels.</li> </ul>", "boost": 0.5}, {"location": "reference/#prebuilt-components", "title": "Prebuilt components", "text": "<p>Higher-level abstractions for common workflows, agents, and other patterns.</p> <ul> <li>Agents: Built-in agent patterns.</li> <li>Supervisor: Orchestration and delegation.</li> <li>Swarm: Multi-agent collaboration.</li> <li>MCP Adapters: Integrations with external systems.</li> </ul>", "boost": 0.5}, {"location": "reference/#langgraph-platform", "title": "LangGraph Platform", "text": "<p>Tools for deploying and connecting to the LangGraph Platform.</p> <ul> <li>SDK (Python): Python SDK for interacting with instances of the LangGraph Server.</li> <li>SDK (JS/TS): JavaScript/TypeScript SDK for interacting with instances of the LangGraph Server.</li> <li>RemoteGraph: <code>Pregel</code> abstraction for connecting to LangGraph Server instances.</li> </ul> <p>See the LangGraph Platform reference for more reference documentation.</p>", "boost": 0.5}, {"location": "reference/agents/", "title": "Agents", "text": "<p>::: langgraph.prebuilt.chat_agent_executor     options:       members:         - AgentState         - create_react_agent</p> <p>::: langgraph.prebuilt.tool_node.ToolNode     options:       show_if_no_docstring: true       show_root_heading: true       show_root_full_path: false       inherited_members: false       members:         - inject_tool_args</p> <p>::: langgraph.prebuilt.tool_node     options:       members:         - InjectedState         - InjectedStore         - tools_condition</p> <p>::: langgraph.prebuilt.tool_validator.ValidationNode     options:       show_if_no_docstring: true       show_root_heading: true       show_root_full_path: false       inherited_members: false       members: false</p> <p>::: langgraph.prebuilt.interrupt     options:       members:         - HumanInterruptConfig         - ActionRequest         - HumanInterrupt         - HumanResponse</p>"}, {"location": "reference/cache/", "title": "Caching", "text": ""}, {"location": "reference/cache/#caching", "title": "Caching", "text": "<p>::: langgraph.cache.base ::: langgraph.cache.memory ::: langgraph.cache.sqlite</p>"}, {"location": "reference/channels/", "title": "Channels", "text": "<p>::: langgraph.channels.base     options:       members:         - BaseChannel</p> <p>::: langgraph.channels     options:       members:         - Topic         - LastValue         - EphemeralValue         - BinaryOperatorAggregate         - AnyValue</p>"}, {"location": "reference/checkpoints/", "title": "Checkpointers", "text": "<p>::: langgraph.checkpoint.base     options:       members:         - CheckpointMetadata         - Checkpoint         - BaseCheckpointSaver         - create_checkpoint</p> <p>::: langgraph.checkpoint.serde.base     options:       members:         - SerializerProtocol         - CipherProtocol</p> <p>::: langgraph.checkpoint.serde.jsonplus     options:       members:         - JsonPlusSerializer</p> <p>::: langgraph.checkpoint.serde.encrypted     options:       members:         - EncryptedSerializer</p> <p>::: langgraph.checkpoint.memory</p> <p>::: langgraph.checkpoint.sqlite</p> <p>::: langgraph.checkpoint.sqlite.aio</p> <p>::: langgraph.checkpoint.postgres     options:       members:         - PostgresSaver</p> <p>::: langgraph.checkpoint.postgres.aio     options:       members:         - AsyncPostgresSaver</p>"}, {"location": "reference/config/", "title": "Config", "text": "<p>::: langgraph.config     options:       members:         - get_store         - get_stream_writer</p>"}, {"location": "reference/constants/", "title": "Constants", "text": "<p>::: langgraph.constants     options:       members:         - TAG_HIDDEN         - TAG_NOSTREAM         - START         - END</p>"}, {"location": "reference/errors/", "title": "Errors", "text": "<p>::: langgraph.errors</p>"}, {"location": "reference/func/", "title": "Functional API", "text": "<p>::: langgraph.func     options:       members:         - task         - entrypoint</p>"}, {"location": "reference/graphs/", "title": "Graph Definitions", "text": "<p>::: langgraph.graph.state.StateGraph     options:       show_if_no_docstring: true       show_root_heading: true       show_root_full_path: false       members:         - add_node         - add_edge         - add_conditional_edges         - add_sequence         - compile</p> <p>::: langgraph.graph.state.CompiledStateGraph     options:       show_if_no_docstring: true       show_root_heading: true       show_root_full_path: false       members:         - stream         - astream         - invoke         - ainvoke         - get_state         - aget_state         - get_state_history         - aget_state_history         - update_state         - aupdate_state         - bulk_update_state         - abulk_update_state         - get_graph         - aget_graph         - get_subgraphs         - aget_subgraphs         - with_config</p> <p>::: langgraph.graph.message     options:       members:         - add_messages</p>"}, {"location": "reference/mcp/", "title": "LangChain Model Context Protocol (MCP) Adapters", "text": "<p>::: langchain_mcp_adapters.client     options:       members:         - MultiServerMCPClient</p> <p>::: langchain_mcp_adapters.tools     options:       members:         - load_mcp_tools</p> <p>::: langchain_mcp_adapters.prompts     options:       members:         - load_mcp_prompt</p> <p>::: langchain_mcp_adapters.resources     options:       members:         - load_mcp_resources</p>"}, {"location": "reference/pregel/", "title": "Pregel", "text": "<p>::: langgraph.pregel.NodeBuilder     options:       show_if_no_docstring: true       show_root_heading: true       show_root_full_path: false       members:         - subscribe_only         - subscribe_to         - read_from         - do         - write_to         - meta         - retry         - cache         - build</p> <p>::: langgraph.pregel.Pregel     options:       show_if_no_docstring: true       show_root_heading: true       show_root_full_path: false       members:         - stream         - astream         - invoke         - ainvoke         - get_state         - aget_state         - get_state_history         - aget_state_history         - update_state         - aupdate_state         - bulk_update_state         - abulk_update_state         - get_graph         - aget_graph         - get_subgraphs         - aget_subgraphs         - with_config</p>"}, {"location": "reference/remote_graph/", "title": "RemoteGraph", "text": "<p>::: langgraph.pregel.remote     options:       members:         - RemoteGraph</p>"}, {"location": "reference/runtime/", "title": "Runtime", "text": "<p>::: langgraph.runtime.Runtime     options:       show_root_heading: true       show_root_full_path: false       members:         - context         - store         - stream_writer         - previous</p> <p>::: langgraph.runtime     options:       members:         - get_runtime</p>"}, {"location": "reference/store/", "title": "Storage", "text": "<p>::: langgraph.store.base</p> <p>::: langgraph.store.postgres</p>"}, {"location": "reference/supervisor/", "title": "LangGraph Supervisor", "text": "<p>::: langgraph_supervisor.supervisor     options:       members:         - create_supervisor</p> <p>::: langgraph_supervisor.handoff     options:       members:         - create_handoff_tool         - create_forward_message_tool</p>"}, {"location": "reference/swarm/", "title": "LangGraph Swarm", "text": "<p>::: langgraph_swarm.swarm     options:       members:         - SwarmState         - create_swarm         - add_active_agent_router</p> <p>::: langgraph_swarm.handoff     options:       members:         - create_handoff_tool</p>"}, {"location": "reference/types/", "title": "Types", "text": "<p>::: langgraph.types     options:       members:         - All         - StreamMode         - StreamWriter         - RetryPolicy         - CachePolicy         - Interrupt         - PregelTask         - StateSnapshot         - Send         - Command         - interrupt</p>"}, {"location": "troubleshooting/errors/", "title": "Error reference", "text": "<p>This page contains guides around resolving common errors you may find while building with LangGraph. Errors referenced below will have an <code>lc_error_code</code> property corresponding to one of the below codes when they are thrown in code.</p> <ul> <li>GRAPH_RECURSION_LIMIT</li> <li>INVALID_CONCURRENT_GRAPH_UPDATE</li> <li>INVALID_GRAPH_NODE_RETURN_VALUE</li> <li>MULTIPLE_SUBGRAPHS</li> <li>INVALID_CHAT_HISTORY</li> </ul>", "boost": 0.5}, {"location": "tutorials/workflows/", "title": "Workflows and Agents", "text": "<p>This guide reviews common patterns for agentic systems. In describing these systems, it can be useful to make a distinction between \"workflows\" and \"agents\". One way to think about this difference is nicely explained in Anthropic's <code>Building Effective Agents</code> blog post:</p> <p>Workflows are systems where LLMs and tools are orchestrated through predefined code paths. Agents, on the other hand, are systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks.</p> <p>Here is a simple way to visualize these differences:</p> <p></p> <p>When building agents and workflows, LangGraph offers a number of benefits including persistence, streaming, and support for debugging as well as deployment.</p>", "boost": 2}, {"location": "tutorials/workflows/#set-up", "title": "Set up", "text": "<p>:::python You can use any chat model that supports structured outputs and tool calling. Below, we show the process of installing the packages, setting API keys, and testing structured outputs / tool calling for Anthropic.</p> Install dependencies <pre><code>pip install langchain_core langchain-anthropic langgraph\n</code></pre> <p>Initialize an LLM</p> <pre><code>import os\nimport getpass\n\nfrom langchain_anthropic import ChatAnthropic\n\ndef _set_env(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"{var}: \")\n\n\n_set_env(\"ANTHROPIC_API_KEY\")\n\nllm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n</code></pre> <p>:::</p> <p>:::js You can use any chat model that supports structured outputs and tool calling. Below, we show the process of installing the packages, setting API keys, and testing structured outputs / tool calling for Anthropic.</p> Install dependencies <pre><code>npm install @langchain/core @langchain/anthropic @langchain/langgraph\n</code></pre> <p>Initialize an LLM</p> <pre><code>import { ChatAnthropic } from \"@langchain/anthropic\";\n\nprocess.env.ANTHROPIC_API_KEY = \"YOUR_API_KEY\";\n\nconst llm = new ChatAnthropic({ model: \"claude-3-5-sonnet-latest\" });\n</code></pre> <p>:::</p>", "boost": 2}, {"location": "tutorials/workflows/#building-blocks-the-augmented-llm", "title": "Building Blocks: The Augmented LLM", "text": "<p>LLM have augmentations that support building workflows and agents. These include structured outputs and tool calling, as shown in this image from the Anthropic blog on <code>Building Effective Agents</code>:</p> <p></p> <p>:::python</p> <pre><code># Schema for structured output\nfrom pydantic import BaseModel, Field\n\nclass SearchQuery(BaseModel):\n    search_query: str = Field(None, description=\"Query that is optimized web search.\")\n    justification: str = Field(\n        None, description=\"Why this query is relevant to the user's request.\"\n    )\n\n\n# Augment the LLM with schema for structured output\nstructured_llm = llm.with_structured_output(SearchQuery)\n\n# Invoke the augmented LLM\noutput = structured_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\")\n\n# Define a tool\ndef multiply(a: int, b: int) -&gt; int:\n    return a * b\n\n# Augment the LLM with tools\nllm_with_tools = llm.bind_tools([multiply])\n\n# Invoke the LLM with input that triggers the tool call\nmsg = llm_with_tools.invoke(\"What is 2 times 3?\")\n\n# Get the tool call\nmsg.tool_calls\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { z } from \"zod\";\nimport { tool } from \"@langchain/core/tools\";\n\n// Schema for structured output\nconst SearchQuery = z.object({\n  search_query: z.string().describe(\"Query that is optimized web search.\"),\n  justification: z\n    .string()\n    .describe(\"Why this query is relevant to the user's request.\"),\n});\n\n// Augment the LLM with schema for structured output\nconst structuredLlm = llm.withStructuredOutput(SearchQuery);\n\n// Invoke the augmented LLM\nconst output = await structuredLlm.invoke(\n  \"How does Calcium CT score relate to high cholesterol?\"\n);\n\n// Define a tool\nconst multiply = tool(\n  async ({ a, b }: { a: number; b: number }) =&gt; {\n    return a * b;\n  },\n  {\n    name: \"multiply\",\n    description: \"Multiply two numbers\",\n    schema: z.object({\n      a: z.number(),\n      b: z.number(),\n    }),\n  }\n);\n\n// Augment the LLM with tools\nconst llmWithTools = llm.bindTools([multiply]);\n\n// Invoke the LLM with input that triggers the tool call\nconst msg = await llmWithTools.invoke(\"What is 2 times 3?\");\n\n// Get the tool call\nconsole.log(msg.tool_calls);\n</code></pre> <p>:::</p>", "boost": 2}, {"location": "tutorials/workflows/#prompt-chaining", "title": "Prompt chaining", "text": "<p>In prompt chaining, each LLM call processes the output of the previous one.</p> <p>As noted in the Anthropic blog on <code>Building Effective Agents</code>:</p> <p>Prompt chaining decomposes a task into a sequence of steps, where each LLM call processes the output of the previous one. You can add programmatic checks (see \"gate\" in the diagram below) on any intermediate steps to ensure that the process is still on track.</p> <p>When to use this workflow: This workflow is ideal for situations where the task can be easily and cleanly decomposed into fixed subtasks. The main goal is to trade off latency for higher accuracy, by making each LLM call an easier task.</p> <p></p> Graph APIFunctional API <p>:::python <pre><code>from typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph, START, END\nfrom IPython.display import Image, display\n\n\n# Graph state\nclass State(TypedDict):\n    topic: str\n    joke: str\n    improved_joke: str\n    final_joke: str\n\n\n# Nodes\ndef generate_joke(state: State):\n    \"\"\"First LLM call to generate initial joke\"\"\"\n\n    msg = llm.invoke(f\"Write a short joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\ndef check_punchline(state: State):\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\n\n    # Simple check - does the joke contain \"?\" or \"!\"\n    if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]:\n        return \"Pass\"\n    return \"Fail\"\n\n\ndef improve_joke(state: State):\n    \"\"\"Second LLM call to improve the joke\"\"\"\n\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state['joke']}\")\n    return {\"improved_joke\": msg.content}\n\n\ndef polish_joke(state: State):\n    \"\"\"Third LLM call for final polish\"\"\"\n\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {state['improved_joke']}\")\n    return {\"final_joke\": msg.content}\n\n\n# Build workflow\nworkflow = StateGraph(State)\n\n# Add nodes\nworkflow.add_node(\"generate_joke\", generate_joke)\nworkflow.add_node(\"improve_joke\", improve_joke)\nworkflow.add_node(\"polish_joke\", polish_joke)\n\n# Add edges to connect nodes\nworkflow.add_edge(START, \"generate_joke\")\nworkflow.add_conditional_edges(\n    \"generate_joke\", check_punchline, {\"Fail\": \"improve_joke\", \"Pass\": END}\n)\nworkflow.add_edge(\"improve_joke\", \"polish_joke\")\nworkflow.add_edge(\"polish_joke\", END)\n\n# Compile\nchain = workflow.compile()\n\n# Show workflow\ndisplay(Image(chain.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = chain.invoke({\"topic\": \"cats\"})\nprint(\"Initial joke:\")\nprint(state[\"joke\"])\nprint(\"\\n--- --- ---\\n\")\nif \"improved_joke\" in state:\n    print(\"Improved joke:\")\n    print(state[\"improved_joke\"])\n    print(\"\\n--- --- ---\\n\")\n\n    print(\"Final joke:\")\n    print(state[\"final_joke\"])\nelse:\n    print(\"Joke failed quality gate - no punchline detected!\")\n</code></pre></p> <p>LangSmith Trace</p> <p>https://smith.langchain.com/public/a0281fca-3a71-46de-beee-791468607b75/r</p> <p>Resources:</p> <p>LangChain Academy</p> <p>See our lesson on Prompt Chaining here. :::</p> <p>:::js <pre><code>import { StateGraph, START, END } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\n// Graph state\nconst State = z.object({\n  topic: z.string(),\n  joke: z.string().optional(),\n  improved_joke: z.string().optional(),\n  final_joke: z.string().optional(),\n});\n\n// Nodes\nconst generateJoke = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // First LLM call to generate initial joke\n  const msg = await llm.invoke(`Write a short joke about ${state.topic}`);\n  return { joke: msg.content };\n};\n\nconst checkPunchline = (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Gate function to check if the joke has a punchline\n  // Simple check - does the joke contain \"?\" or \"!\"\n  if (state.joke &amp;&amp; (state.joke.includes(\"?\") || state.joke.includes(\"!\"))) {\n    return \"Pass\";\n  }\n  return \"Fail\";\n};\n\nconst improveJoke = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Second LLM call to improve the joke\n  const msg = await llm.invoke(`Make this joke funnier by adding wordplay: ${state.joke}`);\n  return { improved_joke: msg.content };\n};\n\nconst polishJoke = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Third LLM call for final polish\n  const msg = await llm.invoke(`Add a surprising twist to this joke: ${state.improved_joke}`);\n  return { final_joke: msg.content };\n};\n\n// Build workflow\nconst workflow = new StateGraph(State)\n  .addNode(\"generate_joke\", generateJoke)\n  .addNode(\"improve_joke\", improveJoke)\n  .addNode(\"polish_joke\", polishJoke)\n  .addEdge(START, \"generate_joke\")\n  .addConditionalEdges(\n    \"generate_joke\",\n    checkPunchline,\n    { \"Fail\": \"improve_joke\", \"Pass\": END }\n  )\n  .addEdge(\"improve_joke\", \"polish_joke\")\n  .addEdge(\"polish_joke\", END);\n\n// Compile\nconst chain = workflow.compile();\n\n// Show workflow\nimport * as fs from \"node:fs/promises\";\nconst drawableGraph = await chain.getGraphAsync();\nconst image = await drawableGraph.drawMermaidPng();\nconst imageBuffer = new Uint8Array(await image.arrayBuffer());\nawait fs.writeFile(\"workflow.png\", imageBuffer);\n\n// Invoke\nconst state = await chain.invoke({ topic: \"cats\" });\nconsole.log(\"Initial joke:\");\nconsole.log(state.joke);\nconsole.log(\"\\n--- --- ---\\n\");\nif (state.improved_joke) {\n  console.log(\"Improved joke:\");\n  console.log(state.improved_joke);\n  console.log(\"\\n--- --- ---\\n\");\n\n  console.log(\"Final joke:\");\n  console.log(state.final_joke);\n} else {\n  console.log(\"Joke failed quality gate - no punchline detected!\");\n}\n</code></pre> :::</p> <p>:::python <pre><code>from langgraph.func import entrypoint, task\n\n\n# Tasks\n@task\ndef generate_joke(topic: str):\n    \"\"\"First LLM call to generate initial joke\"\"\"\n    msg = llm.invoke(f\"Write a short joke about {topic}\")\n    return msg.content\n\n\ndef check_punchline(joke: str):\n    \"\"\"Gate function to check if the joke has a punchline\"\"\"\n    # Simple check - does the joke contain \"?\" or \"!\"\n    if \"?\" in joke or \"!\" in joke:\n        return \"Fail\"\n\n    return \"Pass\"\n\n\n@task\ndef improve_joke(joke: str):\n    \"\"\"Second LLM call to improve the joke\"\"\"\n    msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\")\n    return msg.content\n\n\n@task\ndef polish_joke(joke: str):\n    \"\"\"Third LLM call for final polish\"\"\"\n    msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\")\n    return msg.content\n\n\n@entrypoint()\ndef prompt_chaining_workflow(topic: str):\n    original_joke = generate_joke(topic).result()\n    if check_punchline(original_joke) == \"Pass\":\n        return original_joke\n\n    improved_joke = improve_joke(original_joke).result()\n    return polish_joke(improved_joke).result()\n\n# Invoke\nfor step in prompt_chaining_workflow.stream(\"cats\", stream_mode=\"updates\"):\n    print(step)\n    print(\"\\n\")\n</code></pre></p> <p>LangSmith Trace</p> <p>https://smith.langchain.com/public/332fa4fc-b6ca-416e-baa3-161625e69163/r :::</p> <p>:::js <pre><code>import { entrypoint, task } from \"@langchain/langgraph\";\n\n// Tasks\nconst generateJoke = task(\"generate_joke\", async (topic: string) =&gt; {\n  // First LLM call to generate initial joke\n  const msg = await llm.invoke(`Write a short joke about ${topic}`);\n  return msg.content;\n});\n\nconst checkPunchline = (joke: string) =&gt; {\n  // Gate function to check if the joke has a punchline\n  // Simple check - does the joke contain \"?\" or \"!\"\n  if (joke.includes(\"?\") || joke.includes(\"!\")) {\n    return \"Pass\";\n  }\n  return \"Fail\";\n};\n\nconst improveJoke = task(\"improve_joke\", async (joke: string) =&gt; {\n  // Second LLM call to improve the joke\n  const msg = await llm.invoke(`Make this joke funnier by adding wordplay: ${joke}`);\n  return msg.content;\n});\n\nconst polishJoke = task(\"polish_joke\", async (joke: string) =&gt; {\n  // Third LLM call for final polish\n  const msg = await llm.invoke(`Add a surprising twist to this joke: ${joke}`);\n  return msg.content;\n});\n\nconst promptChainingWorkflow = entrypoint(\"promptChainingWorkflow\", async (topic: string) =&gt; {\n  const originalJoke = await generateJoke(topic);\n  if (checkPunchline(originalJoke) === \"Pass\") {\n    return originalJoke;\n  }\n\n  const improvedJoke = await improveJoke(originalJoke);\n  return await polishJoke(improvedJoke);\n});\n\n// Invoke\nconst stream = await promptChainingWorkflow.stream(\"cats\", { streamMode: \"updates\" });\nfor await (const step of stream) {\n  console.log(step);\n  console.log(\"\\n\");\n}\n</code></pre> :::</p>", "boost": 2}, {"location": "tutorials/workflows/#parallelization", "title": "Parallelization", "text": "<p>With parallelization, LLMs work simultaneously on a task:</p> <p>LLMs can sometimes work simultaneously on a task and have their outputs aggregated programmatically. This workflow, parallelization, manifests in two key variations: Sectioning: Breaking a task into independent subtasks run in parallel. Voting: Running the same task multiple times to get diverse outputs.</p> <p>When to use this workflow: Parallelization is effective when the divided subtasks can be parallelized for speed, or when multiple perspectives or attempts are needed for higher confidence results. For complex tasks with multiple considerations, LLMs generally perform better when each consideration is handled by a separate LLM call, allowing focused attention on each specific aspect.</p> <p></p> Graph APIFunctional API <p>:::python <pre><code># Graph state\nclass State(TypedDict):\n    topic: str\n    joke: str\n    story: str\n    poem: str\n    combined_output: str\n\n\n# Nodes\ndef call_llm_1(state: State):\n    \"\"\"First LLM call to generate initial joke\"\"\"\n\n    msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\ndef call_llm_2(state: State):\n    \"\"\"Second LLM call to generate story\"\"\"\n\n    msg = llm.invoke(f\"Write a story about {state['topic']}\")\n    return {\"story\": msg.content}\n\n\ndef call_llm_3(state: State):\n    \"\"\"Third LLM call to generate poem\"\"\"\n\n    msg = llm.invoke(f\"Write a poem about {state['topic']}\")\n    return {\"poem\": msg.content}\n\n\ndef aggregator(state: State):\n    \"\"\"Combine the joke and story into a single output\"\"\"\n\n    combined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\"\n    combined += f\"STORY:\\n{state['story']}\\n\\n\"\n    combined += f\"JOKE:\\n{state['joke']}\\n\\n\"\n    combined += f\"POEM:\\n{state['poem']}\"\n    return {\"combined_output\": combined}\n\n\n# Build workflow\nparallel_builder = StateGraph(State)\n\n# Add nodes\nparallel_builder.add_node(\"call_llm_1\", call_llm_1)\nparallel_builder.add_node(\"call_llm_2\", call_llm_2)\nparallel_builder.add_node(\"call_llm_3\", call_llm_3)\nparallel_builder.add_node(\"aggregator\", aggregator)\n\n# Add edges to connect nodes\nparallel_builder.add_edge(START, \"call_llm_1\")\nparallel_builder.add_edge(START, \"call_llm_2\")\nparallel_builder.add_edge(START, \"call_llm_3\")\nparallel_builder.add_edge(\"call_llm_1\", \"aggregator\")\nparallel_builder.add_edge(\"call_llm_2\", \"aggregator\")\nparallel_builder.add_edge(\"call_llm_3\", \"aggregator\")\nparallel_builder.add_edge(\"aggregator\", END)\nparallel_workflow = parallel_builder.compile()\n\n# Show workflow\ndisplay(Image(parallel_workflow.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = parallel_workflow.invoke({\"topic\": \"cats\"})\nprint(state[\"combined_output\"])\n</code></pre></p> <p>LangSmith Trace</p> <p>https://smith.langchain.com/public/3be2e53c-ca94-40dd-934f-82ff87fac277/r</p> <p>Resources:</p> <p>Documentation</p> <p>See our documentation on parallelization here.</p> <p>LangChain Academy</p> <p>See our lesson on parallelization here. :::</p> <p>:::js <pre><code>// Graph state\nconst State = z.object({\n  topic: z.string(),\n  joke: z.string().optional(),\n  story: z.string().optional(),\n  poem: z.string().optional(),\n  combined_output: z.string().optional(),\n});\n\n// Nodes\nconst callLlm1 = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // First LLM call to generate initial joke\n  const msg = await llm.invoke(`Write a joke about ${state.topic}`);\n  return { joke: msg.content };\n};\n\nconst callLlm2 = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Second LLM call to generate story\n  const msg = await llm.invoke(`Write a story about ${state.topic}`);\n  return { story: msg.content };\n};\n\nconst callLlm3 = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Third LLM call to generate poem\n  const msg = await llm.invoke(`Write a poem about ${state.topic}`);\n  return { poem: msg.content };\n};\n\nconst aggregator = (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Combine the joke and story into a single output\n  let combined = `Here's a story, joke, and poem about ${state.topic}!\\n\\n`;\n  combined += `STORY:\\n${state.story}\\n\\n`;\n  combined += `JOKE:\\n${state.joke}\\n\\n`;\n  combined += `POEM:\\n${state.poem}`;\n  return { combined_output: combined };\n};\n\n// Build workflow\nconst parallelBuilder = new StateGraph(State)\n  .addNode(\"call_llm_1\", callLlm1)\n  .addNode(\"call_llm_2\", callLlm2)\n  .addNode(\"call_llm_3\", callLlm3)\n  .addNode(\"aggregator\", aggregator)\n  .addEdge(START, \"call_llm_1\")\n  .addEdge(START, \"call_llm_2\")\n  .addEdge(START, \"call_llm_3\")\n  .addEdge(\"call_llm_1\", \"aggregator\")\n  .addEdge(\"call_llm_2\", \"aggregator\")\n  .addEdge(\"call_llm_3\", \"aggregator\")\n  .addEdge(\"aggregator\", END);\n\nconst parallelWorkflow = parallelBuilder.compile();\n\n// Invoke\nconst state = await parallelWorkflow.invoke({ topic: \"cats\" });\nconsole.log(state.combined_output);\n</code></pre> :::</p> <p>:::python <pre><code>@task\ndef call_llm_1(topic: str):\n    \"\"\"First LLM call to generate initial joke\"\"\"\n    msg = llm.invoke(f\"Write a joke about {topic}\")\n    return msg.content\n\n\n@task\ndef call_llm_2(topic: str):\n    \"\"\"Second LLM call to generate story\"\"\"\n    msg = llm.invoke(f\"Write a story about {topic}\")\n    return msg.content\n\n\n@task\ndef call_llm_3(topic):\n    \"\"\"Third LLM call to generate poem\"\"\"\n    msg = llm.invoke(f\"Write a poem about {topic}\")\n    return msg.content\n\n\n@task\ndef aggregator(topic, joke, story, poem):\n    \"\"\"Combine the joke and story into a single output\"\"\"\n\n    combined = f\"Here's a story, joke, and poem about {topic}!\\n\\n\"\n    combined += f\"STORY:\\n{story}\\n\\n\"\n    combined += f\"JOKE:\\n{joke}\\n\\n\"\n    combined += f\"POEM:\\n{poem}\"\n    return combined\n\n\n# Build workflow\n@entrypoint()\ndef parallel_workflow(topic: str):\n    joke_fut = call_llm_1(topic)\n    story_fut = call_llm_2(topic)\n    poem_fut = call_llm_3(topic)\n    return aggregator(\n        topic, joke_fut.result(), story_fut.result(), poem_fut.result()\n    ).result()\n\n# Invoke\nfor step in parallel_workflow.stream(\"cats\", stream_mode=\"updates\"):\n    print(step)\n    print(\"\\n\")\n</code></pre></p> <p>LangSmith Trace</p> <p>https://smith.langchain.com/public/623d033f-e814-41e9-80b1-75e6abb67801/r :::</p> <p>:::js <pre><code>const callLlm1 = task(\"call_llm_1\", async (topic: string) =&gt; {\n  // First LLM call to generate initial joke\n  const msg = await llm.invoke(`Write a joke about ${topic}`);\n  return msg.content;\n});\n\nconst callLlm2 = task(\"call_llm_2\", async (topic: string) =&gt; {\n  // Second LLM call to generate story\n  const msg = await llm.invoke(`Write a story about ${topic}`);\n  return msg.content;\n});\n\nconst callLlm3 = task(\"call_llm_3\", async (topic: string) =&gt; {\n  // Third LLM call to generate poem\n  const msg = await llm.invoke(`Write a poem about ${topic}`);\n  return msg.content;\n});\n\nconst aggregator = task(\"aggregator\", (topic: string, joke: string, story: string, poem: string) =&gt; {\n  // Combine the joke and story into a single output\n  let combined = `Here's a story, joke, and poem about ${topic}!\\n\\n`;\n  combined += `STORY:\\n${story}\\n\\n`;\n  combined += `JOKE:\\n${joke}\\n\\n`;\n  combined += `POEM:\\n${poem}`;\n  return combined;\n});\n\n// Build workflow\nconst parallelWorkflow = entrypoint(\"parallelWorkflow\", async (topic: string) =&gt; {\n  const jokeFut = callLlm1(topic);\n  const storyFut = callLlm2(topic);\n  const poemFut = callLlm3(topic);\n\n  return await aggregator(\n    topic,\n    await jokeFut,\n    await storyFut,\n    await poemFut\n  );\n});\n\n// Invoke\nconst stream = await parallelWorkflow.stream(\"cats\", { streamMode: \"updates\" });\nfor await (const step of stream) {\n  console.log(step);\n  console.log(\"\\n\");\n}\n</code></pre> :::</p>", "boost": 2}, {"location": "tutorials/workflows/#routing", "title": "Routing", "text": "<p>Routing classifies an input and directs it to a followup task. As noted in the Anthropic blog on <code>Building Effective Agents</code>:</p> <p>Routing classifies an input and directs it to a specialized followup task. This workflow allows for separation of concerns, and building more specialized prompts. Without this workflow, optimizing for one kind of input can hurt performance on other inputs.</p> <p>When to use this workflow: Routing works well for complex tasks where there are distinct categories that are better handled separately, and where classification can be handled accurately, either by an LLM or a more traditional classification model/algorithm.</p> <p></p> Graph APIFunctional API <p>:::python <pre><code>from typing_extensions import Literal\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\n\n# Schema for structured output to use as routing logic\nclass Route(BaseModel):\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n        None, description=\"The next step in the routing process\"\n    )\n\n\n# Augment the LLM with schema for structured output\nrouter = llm.with_structured_output(Route)\n\n\n# State\nclass State(TypedDict):\n    input: str\n    decision: str\n    output: str\n\n\n# Nodes\ndef llm_call_1(state: State):\n    \"\"\"Write a story\"\"\"\n\n    result = llm.invoke(state[\"input\"])\n    return {\"output\": result.content}\n\n\ndef llm_call_2(state: State):\n    \"\"\"Write a joke\"\"\"\n\n    result = llm.invoke(state[\"input\"])\n    return {\"output\": result.content}\n\n\ndef llm_call_3(state: State):\n    \"\"\"Write a poem\"\"\"\n\n    result = llm.invoke(state[\"input\"])\n    return {\"output\": result.content}\n\n\ndef llm_call_router(state: State):\n    \"\"\"Route the input to the appropriate node\"\"\"\n\n    # Run the augmented LLM with structured output to serve as routing logic\n    decision = router.invoke(\n        [\n            SystemMessage(\n                content=\"Route the input to story, joke, or poem based on the user's request.\"\n            ),\n            HumanMessage(content=state[\"input\"]),\n        ]\n    )\n\n    return {\"decision\": decision.step}\n\n\n# Conditional edge function to route to the appropriate node\ndef route_decision(state: State):\n    # Return the node name you want to visit next\n    if state[\"decision\"] == \"story\":\n        return \"llm_call_1\"\n    elif state[\"decision\"] == \"joke\":\n        return \"llm_call_2\"\n    elif state[\"decision\"] == \"poem\":\n        return \"llm_call_3\"\n\n\n# Build workflow\nrouter_builder = StateGraph(State)\n\n# Add nodes\nrouter_builder.add_node(\"llm_call_1\", llm_call_1)\nrouter_builder.add_node(\"llm_call_2\", llm_call_2)\nrouter_builder.add_node(\"llm_call_3\", llm_call_3)\nrouter_builder.add_node(\"llm_call_router\", llm_call_router)\n\n# Add edges to connect nodes\nrouter_builder.add_edge(START, \"llm_call_router\")\nrouter_builder.add_conditional_edges(\n    \"llm_call_router\",\n    route_decision,\n    {  # Name returned by route_decision : Name of next node to visit\n        \"llm_call_1\": \"llm_call_1\",\n        \"llm_call_2\": \"llm_call_2\",\n        \"llm_call_3\": \"llm_call_3\",\n    },\n)\nrouter_builder.add_edge(\"llm_call_1\", END)\nrouter_builder.add_edge(\"llm_call_2\", END)\nrouter_builder.add_edge(\"llm_call_3\", END)\n\n# Compile workflow\nrouter_workflow = router_builder.compile()\n\n# Show the workflow\ndisplay(Image(router_workflow.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = router_workflow.invoke({\"input\": \"Write me a joke about cats\"})\nprint(state[\"output\"])\n</code></pre></p> <p>LangSmith Trace</p> <p>https://smith.langchain.com/public/c4580b74-fe91-47e4-96fe-7fac598d509c/r</p> <p>Resources:</p> <p>LangChain Academy</p> <p>See our lesson on routing here.</p> <p>Examples</p> <p>Here is RAG workflow that routes questions. See our video here. :::</p> <p>:::js <pre><code>import { SystemMessage, HumanMessage } from \"@langchain/core/messages\";\n\n// Schema for structured output to use as routing logic\nconst Route = z.object({\n  step: z.enum([\"poem\", \"story\", \"joke\"]).describe(\"The next step in the routing process\"),\n});\n\n// Augment the LLM with schema for structured output\nconst router = llm.withStructuredOutput(Route);\n\n// State\nconst State = z.object({\n  input: z.string(),\n  decision: z.string().optional(),\n  output: z.string().optional(),\n});\n\n// Nodes\nconst llmCall1 = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Write a story\n  const result = await llm.invoke(state.input);\n  return { output: result.content };\n};\n\nconst llmCall2 = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Write a joke\n  const result = await llm.invoke(state.input);\n  return { output: result.content };\n};\n\nconst llmCall3 = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Write a poem\n  const result = await llm.invoke(state.input);\n  return { output: result.content };\n};\n\nconst llmCallRouter = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Route the input to the appropriate node\n  const decision = await router.invoke([\n    new SystemMessage(\"Route the input to story, joke, or poem based on the user's request.\"),\n    new HumanMessage(state.input),\n  ]);\n\n  return { decision: decision.step };\n};\n\n// Conditional edge function to route to the appropriate node\nconst routeDecision = (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Return the node name you want to visit next\n  if (state.decision === \"story\") {\n    return \"llm_call_1\";\n  } else if (state.decision === \"joke\") {\n    return \"llm_call_2\";\n  } else if (state.decision === \"poem\") {\n    return \"llm_call_3\";\n  }\n};\n\n// Build workflow\nconst routerBuilder = new StateGraph(State)\n  .addNode(\"llm_call_1\", llmCall1)\n  .addNode(\"llm_call_2\", llmCall2)\n  .addNode(\"llm_call_3\", llmCall3)\n  .addNode(\"llm_call_router\", llmCallRouter)\n  .addEdge(START, \"llm_call_router\")\n  .addConditionalEdges(\n    \"llm_call_router\",\n    routeDecision,\n    {\n      \"llm_call_1\": \"llm_call_1\",\n      \"llm_call_2\": \"llm_call_2\",\n      \"llm_call_3\": \"llm_call_3\",\n    }\n  )\n  .addEdge(\"llm_call_1\", END)\n  .addEdge(\"llm_call_2\", END)\n  .addEdge(\"llm_call_3\", END);\n\nconst routerWorkflow = routerBuilder.compile();\n\n// Invoke\nconst state = await routerWorkflow.invoke({ input: \"Write me a joke about cats\" });\nconsole.log(state.output);\n</code></pre> :::</p> <p>:::python <pre><code>from typing_extensions import Literal\nfrom pydantic import BaseModel\nfrom langchain_core.messages import HumanMessage, SystemMessage\n\n\n# Schema for structured output to use as routing logic\nclass Route(BaseModel):\n    step: Literal[\"poem\", \"story\", \"joke\"] = Field(\n        None, description=\"The next step in the routing process\"\n    )\n\n\n# Augment the LLM with schema for structured output\nrouter = llm.with_structured_output(Route)\n\n\n@task\ndef llm_call_1(input_: str):\n    \"\"\"Write a story\"\"\"\n    result = llm.invoke(input_)\n    return result.content\n\n\n@task\ndef llm_call_2(input_: str):\n    \"\"\"Write a joke\"\"\"\n    result = llm.invoke(input_)\n    return result.content\n\n\n@task\ndef llm_call_3(input_: str):\n    \"\"\"Write a poem\"\"\"\n    result = llm.invoke(input_)\n    return result.content\n\n\ndef llm_call_router(input_: str):\n    \"\"\"Route the input to the appropriate node\"\"\"\n    # Run the augmented LLM with structured output to serve as routing logic\n    decision = router.invoke(\n        [\n            SystemMessage(\n                content=\"Route the input to story, joke, or poem based on the user's request.\"\n            ),\n            HumanMessage(content=input_),\n        ]\n    )\n    return decision.step\n\n\n# Create workflow\n@entrypoint()\ndef router_workflow(input_: str):\n    next_step = llm_call_router(input_)\n    if next_step == \"story\":\n        llm_call = llm_call_1\n    elif next_step == \"joke\":\n        llm_call = llm_call_2\n    elif next_step == \"poem\":\n        llm_call = llm_call_3\n\n    return llm_call(input_).result()\n\n# Invoke\nfor step in router_workflow.stream(\"Write me a joke about cats\", stream_mode=\"updates\"):\n    print(step)\n    print(\"\\n\")\n</code></pre></p> <p>LangSmith Trace</p> <p>https://smith.langchain.com/public/5e2eb979-82dd-402c-b1a0-a8cceaf2a28a/r :::</p> <p>:::js <pre><code>import { SystemMessage, HumanMessage } from \"@langchain/core/messages\";\n\n// Schema for structured output to use as routing logic\nconst Route = z.object({\n  step: z.enum([\"poem\", \"story\", \"joke\"]).describe(\n    \"The next step in the routing process\"\n  ),\n});\n\n// Augment the LLM with schema for structured output\nconst router = llm.withStructuredOutput(Route);\n\nconst llmCall1 = task(\"llm_call_1\", async (input: string) =&gt; {\n  // Write a story\n  const result = await llm.invoke(input);\n  return result.content;\n});\n\nconst llmCall2 = task(\"llm_call_2\", async (input: string) =&gt; {\n  // Write a joke\n  const result = await llm.invoke(input);\n  return result.content;\n});\n\nconst llmCall3 = task(\"llm_call_3\", async (input: string) =&gt; {\n  // Write a poem\n  const result = await llm.invoke(input);\n  return result.content;\n});\n\nconst llmCallRouter = async (input: string) =&gt; {\n  // Route the input to the appropriate node\n  const decision = await router.invoke([\n    new SystemMessage(\"Route the input to story, joke, or poem based on the user's request.\"),\n    new HumanMessage(input),\n  ]);\n  return decision.step;\n};\n\n// Create workflow\nconst routerWorkflow = entrypoint(\"routerWorkflow\", async (input: string) =&gt; {\n  const nextStep = await llmCallRouter(input);\n\n  let llmCall: typeof llmCall1;\n  if (nextStep === \"story\") {\n    llmCall = llmCall1;\n  } else if (nextStep === \"joke\") {\n    llmCall = llmCall2;\n  } else if (nextStep === \"poem\") {\n    llmCall = llmCall3;\n  }\n\n  return await llmCall(input);\n});\n\n// Invoke\nconst stream = await routerWorkflow.stream(\"Write me a joke about cats\", { streamMode: \"updates\" });\nfor await (const step of stream) {\n  console.log(step);\n  console.log(\"\\n\");\n}\n</code></pre> :::</p>", "boost": 2}, {"location": "tutorials/workflows/#orchestrator-worker", "title": "Orchestrator-Worker", "text": "<p>With orchestrator-worker, an orchestrator breaks down a task and delegates each sub-task to workers. As noted in the Anthropic blog on <code>Building Effective Agents</code>:</p> <p>In the orchestrator-workers workflow, a central LLM dynamically breaks down tasks, delegates them to worker LLMs, and synthesizes their results.</p> <p>When to use this workflow: This workflow is well-suited for complex tasks where you can't predict the subtasks needed (in coding, for example, the number of files that need to be changed and the nature of the change in each file likely depend on the task). Whereas it's topographically similar, the key difference from parallelization is its flexibility\u2014subtasks aren't pre-defined, but determined by the orchestrator based on the specific input.</p> <p></p> Graph APIFunctional API <p>:::python <pre><code>from typing import Annotated, List\nimport operator\n\n\n# Schema for structured output to use in planning\nclass Section(BaseModel):\n    name: str = Field(\n        description=\"Name for this section of the report.\",\n    )\n    description: str = Field(\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n    )\n\n\nclass Sections(BaseModel):\n    sections: List[Section] = Field(\n        description=\"Sections of the report.\",\n    )\n\n\n# Augment the LLM with schema for structured output\nplanner = llm.with_structured_output(Sections)\n</code></pre></p> <p>Creating Workers in LangGraph</p> <p>Because orchestrator-worker workflows are common, LangGraph has the <code>Send</code> API to support this. It lets you dynamically create worker nodes and send each one a specific input. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. As you can see below, we iterate over a list of sections and <code>Send</code> each to a worker node. See further documentation here and here.</p> <pre><code>from langgraph.types import Send\n\n\n# Graph state\nclass State(TypedDict):\n    topic: str  # Report topic\n    sections: list[Section]  # List of report sections\n    completed_sections: Annotated[\n        list, operator.add\n    ]  # All workers write to this key in parallel\n    final_report: str  # Final report\n\n\n# Worker state\nclass WorkerState(TypedDict):\n    section: Section\n    completed_sections: Annotated[list, operator.add]\n\n\n# Nodes\ndef orchestrator(state: State):\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n\n    # Generate queries\n    report_sections = planner.invoke(\n        [\n            SystemMessage(content=\"Generate a plan for the report.\"),\n            HumanMessage(content=f\"Here is the report topic: {state['topic']}\"),\n        ]\n    )\n\n    return {\"sections\": report_sections.sections}\n\n\ndef llm_call(state: WorkerState):\n    \"\"\"Worker writes a section of the report\"\"\"\n\n    # Generate section\n    section = llm.invoke(\n        [\n            SystemMessage(\n                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\n            ),\n            HumanMessage(\n                content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"\n            ),\n        ]\n    )\n\n    # Write the updated section to completed sections\n    return {\"completed_sections\": [section.content]}\n\n\ndef synthesizer(state: State):\n    \"\"\"Synthesize full report from sections\"\"\"\n\n    # List of completed sections\n    completed_sections = state[\"completed_sections\"]\n\n    # Format completed section to str to use as context for final sections\n    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n\n    return {\"final_report\": completed_report_sections}\n\n\n# Conditional edge function to create llm_call workers that each write a section of the report\ndef assign_workers(state: State):\n    \"\"\"Assign a worker to each section in the plan\"\"\"\n\n    # Kick off section writing in parallel via Send() API\n    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\n\n\n# Build workflow\norchestrator_worker_builder = StateGraph(State)\n\n# Add the nodes\norchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\norchestrator_worker_builder.add_node(\"llm_call\", llm_call)\norchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n\n# Add edges to connect nodes\norchestrator_worker_builder.add_edge(START, \"orchestrator\")\norchestrator_worker_builder.add_conditional_edges(\n    \"orchestrator\", assign_workers, [\"llm_call\"]\n)\norchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\norchestrator_worker_builder.add_edge(\"synthesizer\", END)\n\n# Compile the workflow\norchestrator_worker = orchestrator_worker_builder.compile()\n\n# Show the workflow\ndisplay(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\n\nfrom IPython.display import Markdown\nMarkdown(state[\"final_report\"])\n</code></pre> <p>LangSmith Trace</p> <p>https://smith.langchain.com/public/78cbcfc3-38bf-471d-b62a-b299b144237d/r</p> <p>Resources:</p> <p>LangChain Academy</p> <p>See our lesson on orchestrator-worker here.</p> <p>Examples</p> <p>Here is a project that uses orchestrator-worker for report planning and writing. See our video here. :::</p> <p>:::js <pre><code>import \"@langchain/langgraph/zod\";\n\n// Schema for structured output to use in planning\nconst Section = z.object({\n  name: z.string().describe(\"Name for this section of the report.\"),\n  description: z.string().describe(\"Brief overview of the main topics and concepts to be covered in this section.\"),\n});\n\nconst Sections = z.object({\n  sections: z.array(Section).describe(\"Sections of the report.\"),\n});\n\n// Augment the LLM with schema for structured output\nconst planner = llm.withStructuredOutput(Sections);\n</code></pre></p> <p>Creating Workers in LangGraph</p> <p>Because orchestrator-worker workflows are common, LangGraph has the <code>Send</code> API to support this. It lets you dynamically create worker nodes and send each one a specific input. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. As you can see below, we iterate over a list of sections and <code>Send</code> each to a worker node. See further documentation here and here.</p> <p><pre><code>import { withLangGraph } from \"@langchain/langgraph/zod\";\nimport { Send } from \"@langchain/langgraph\";\n\n// Graph state\nconst State = z.object({\n  topic: z.string(), // Report topic\n  sections: z.array(Section).optional(), // List of report sections\n  // All workers write to this key\n  completed_sections: withLangGraph(z.array(z.string()), {\n    reducer: {\n      fn: (x, y) =&gt; x.concat(y),\n    },\n    default: () =&gt; [],\n  }),\n  final_report: z.string().optional(), // Final report\n});\n\n// Worker state\nconst WorkerState = z.object({\n  section: Section,\n  completed_sections: withLangGraph(z.array(z.string()), {\n    reducer: {\n      fn: (x, y) =&gt; x.concat(y),\n    },\n    default: () =&gt; [],\n  }),\n});\n\n// Nodes\nconst orchestrator = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Orchestrator that generates a plan for the report\n  const reportSections = await planner.invoke([\n    new SystemMessage(\"Generate a plan for the report.\"),\n    new HumanMessage(`Here is the report topic: ${state.topic}`),\n  ]);\n\n  return { sections: reportSections.sections };\n};\n\nconst llmCall = async (state: z.infer&lt;typeof WorkerState&gt;) =&gt; {\n  // Worker writes a section of the report\n  const section = await llm.invoke([\n    new SystemMessage(\n      \"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\n    ),\n    new HumanMessage(\n      `Here is the section name: ${state.section.name} and description: ${state.section.description}`\n    ),\n  ]);\n\n  // Write the updated section to completed sections\n  return { completed_sections: [section.content] };\n};\n\nconst synthesizer = (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Synthesize full report from sections\n  const completedSections = state.completed_sections;\n  const completedReportSections = completedSections.join(\"\\n\\n---\\n\\n\");\n  return { final_report: completedReportSections };\n};\n\n// Conditional edge function to create llm_call workers\nconst assignWorkers = (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Assign a worker to each section in the plan\n  return state.sections!.map((s) =&gt; new Send(\"llm_call\", { section: s }));\n};\n\n// Build workflow\nconst orchestratorWorkerBuilder = new StateGraph(State)\n  .addNode(\"orchestrator\", orchestrator)\n  .addNode(\"llm_call\", llmCall)\n  .addNode(\"synthesizer\", synthesizer)\n  .addEdge(START, \"orchestrator\")\n  .addConditionalEdges(\"orchestrator\", assignWorkers, [\"llm_call\"])\n  .addEdge(\"llm_call\", \"synthesizer\")\n  .addEdge(\"synthesizer\", END);\n\n// Compile the workflow\nconst orchestratorWorker = orchestratorWorkerBuilder.compile();\n\n// Invoke\nconst state = await orchestratorWorker.invoke({ topic: \"Create a report on LLM scaling laws\" });\nconsole.log(state.final_report);\n</code></pre> :::</p> <p>:::python <pre><code>from typing import List\n\n\n# Schema for structured output to use in planning\nclass Section(BaseModel):\n    name: str = Field(\n        description=\"Name for this section of the report.\",\n    )\n    description: str = Field(\n        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n    )\n\n\nclass Sections(BaseModel):\n    sections: List[Section] = Field(\n        description=\"Sections of the report.\",\n    )\n\n\n# Augment the LLM with schema for structured output\nplanner = llm.with_structured_output(Sections)\n\n\n@task\ndef orchestrator(topic: str):\n    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n    # Generate queries\n    report_sections = planner.invoke(\n        [\n            SystemMessage(content=\"Generate a plan for the report.\"),\n            HumanMessage(content=f\"Here is the report topic: {topic}\"),\n        ]\n    )\n\n    return report_sections.sections\n\n\n@task\ndef llm_call(section: Section):\n    \"\"\"Worker writes a section of the report\"\"\"\n\n    # Generate section\n    result = llm.invoke(\n        [\n            SystemMessage(content=\"Write a report section.\"),\n            HumanMessage(\n                content=f\"Here is the section name: {section.name} and description: {section.description}\"\n            ),\n        ]\n    )\n\n    # Write the updated section to completed sections\n    return result.content\n\n\n@task\ndef synthesizer(completed_sections: list[str]):\n    \"\"\"Synthesize full report from sections\"\"\"\n    final_report = \"\\n\\n---\\n\\n\".join(completed_sections)\n    return final_report\n\n\n@entrypoint()\ndef orchestrator_worker(topic: str):\n    sections = orchestrator(topic).result()\n    section_futures = [llm_call(section) for section in sections]\n    final_report = synthesizer(\n        [section_fut.result() for section_fut in section_futures]\n    ).result()\n    return final_report\n\n# Invoke\nreport = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\nfrom IPython.display import Markdown\nMarkdown(report)\n</code></pre></p> <p>LangSmith Trace</p> <p>https://smith.langchain.com/public/75a636d0-6179-4a12-9836-e0aa571e87c5/r :::</p> <p>:::js <pre><code>// Schema for structured output to use in planning\nconst Section = z.object({\n  name: z.string().describe(\"Name for this section of the report.\"),\n  description: z.string().describe(\"Brief overview of the main topics and concepts to be covered in this section.\"),\n});\n\nconst Sections = z.object({\n  sections: z.array(Section).describe(\"Sections of the report.\"),\n});\n\n// Augment the LLM with schema for structured output\nconst planner = llm.withStructuredOutput(Sections);\n\nconst orchestrator = task(\"orchestrator\", async (topic: string) =&gt; {\n  // Orchestrator that generates a plan for the report\n  const reportSections = await planner.invoke([\n    new SystemMessage(\"Generate a plan for the report.\"),\n    new HumanMessage(`Here is the report topic: ${topic}`),\n  ]);\n  return reportSections.sections;\n});\n\nconst llmCall = task(\"llm_call\", async (section: z.infer&lt;typeof Section&gt;) =&gt; {\n  // Worker writes a section of the report\n  const result = await llm.invoke([\n    new SystemMessage(\"Write a report section.\"),\n    new HumanMessage(\n      `Here is the section name: ${section.name} and description: ${section.description}`\n    ),\n  ]);\n  return result.content;\n});\n\nconst synthesizer = task(\"synthesizer\", (completedSections: string[]) =&gt; {\n  // Synthesize full report from sections\n  const finalReport = completedSections.join(\"\\n\\n---\\n\\n\");\n  return finalReport;\n});\n\nconst orchestratorWorker = entrypoint(\"orchestratorWorker\", async (topic: string) =&gt; {\n  const sections = await orchestrator(topic);\n  const sectionFutures = sections.map((section) =&gt; llmCall(section));\n  const finalReport = await synthesizer(\n    await Promise.all(sectionFutures)\n  );\n  return finalReport;\n});\n\n// Invoke\nconst report = await orchestratorWorker.invoke(\"Create a report on LLM scaling laws\");\nconsole.log(report);\n</code></pre> :::</p>", "boost": 2}, {"location": "tutorials/workflows/#evaluator-optimizer", "title": "Evaluator-optimizer", "text": "<p>In the evaluator-optimizer workflow, one LLM call generates a response while another provides evaluation and feedback in a loop:</p> <p>When to use this workflow: This workflow is particularly effective when we have clear evaluation criteria, and when iterative refinement provides measurable value. The two signs of good fit are, first, that LLM responses can be demonstrably improved when a human articulates their feedback; and second, that the LLM can provide such feedback. This is analogous to the iterative writing process a human writer might go through when producing a polished document.</p> <p></p> Graph APIFunctional API <p>:::python <pre><code># Graph state\nclass State(TypedDict):\n    joke: str\n    topic: str\n    feedback: str\n    funny_or_not: str\n\n\n# Schema for structured output to use in evaluation\nclass Feedback(BaseModel):\n    grade: Literal[\"funny\", \"not funny\"] = Field(\n        description=\"Decide if the joke is funny or not.\",\n    )\n    feedback: str = Field(\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\n    )\n\n\n# Augment the LLM with schema for structured output\nevaluator = llm.with_structured_output(Feedback)\n\n\n# Nodes\ndef llm_call_generator(state: State):\n    \"\"\"LLM generates a joke\"\"\"\n\n    if state.get(\"feedback\"):\n        msg = llm.invoke(\n            f\"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}\"\n        )\n    else:\n        msg = llm.invoke(f\"Write a joke about {state['topic']}\")\n    return {\"joke\": msg.content}\n\n\ndef llm_call_evaluator(state: State):\n    \"\"\"LLM evaluates the joke\"\"\"\n\n    grade = evaluator.invoke(f\"Grade the joke {state['joke']}\")\n    return {\"funny_or_not\": grade.grade, \"feedback\": grade.feedback}\n\n\n# Conditional edge function to route back to joke generator or end based upon feedback from the evaluator\ndef route_joke(state: State):\n    \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\"\n\n    if state[\"funny_or_not\"] == \"funny\":\n        return \"Accepted\"\n    elif state[\"funny_or_not\"] == \"not funny\":\n        return \"Rejected + Feedback\"\n\n\n# Build workflow\noptimizer_builder = StateGraph(State)\n\n# Add the nodes\noptimizer_builder.add_node(\"llm_call_generator\", llm_call_generator)\noptimizer_builder.add_node(\"llm_call_evaluator\", llm_call_evaluator)\n\n# Add edges to connect nodes\noptimizer_builder.add_edge(START, \"llm_call_generator\")\noptimizer_builder.add_edge(\"llm_call_generator\", \"llm_call_evaluator\")\noptimizer_builder.add_conditional_edges(\n    \"llm_call_evaluator\",\n    route_joke,\n    {  # Name returned by route_joke : Name of next node to visit\n        \"Accepted\": END,\n        \"Rejected + Feedback\": \"llm_call_generator\",\n    },\n)\n\n# Compile the workflow\noptimizer_workflow = optimizer_builder.compile()\n\n# Show the workflow\ndisplay(Image(optimizer_workflow.get_graph().draw_mermaid_png()))\n\n# Invoke\nstate = optimizer_workflow.invoke({\"topic\": \"Cats\"})\nprint(state[\"joke\"])\n</code></pre></p> <p>LangSmith Trace</p> <p>https://smith.langchain.com/public/86ab3e60-2000-4bff-b988-9b89a3269789/r</p> <p>Resources:</p> <p>Examples</p> <p>Here is an assistant that uses evaluator-optimizer to improve a report. See our video here.</p> <p>Here is a RAG workflow that grades answers for hallucinations or errors. See our video here. :::</p> <p>:::js <pre><code>// Graph state\nconst State = z.object({\n  joke: z.string().optional(),\n  topic: z.string(),\n  feedback: z.string().optional(),\n  funny_or_not: z.string().optional(),\n});\n\n// Schema for structured output to use in evaluation\nconst Feedback = z.object({\n  grade: z.enum([\"funny\", \"not funny\"]).describe(\"Decide if the joke is funny or not.\"),\n  feedback: z.string().describe(\"If the joke is not funny, provide feedback on how to improve it.\"),\n});\n\n// Augment the LLM with schema for structured output\nconst evaluator = llm.withStructuredOutput(Feedback);\n\n// Nodes\nconst llmCallGenerator = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // LLM generates a joke\n  let msg;\n  if (state.feedback) {\n    msg = await llm.invoke(\n      `Write a joke about ${state.topic} but take into account the feedback: ${state.feedback}`\n    );\n  } else {\n    msg = await llm.invoke(`Write a joke about ${state.topic}`);\n  }\n  return { joke: msg.content };\n};\n\nconst llmCallEvaluator = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // LLM evaluates the joke\n  const grade = await evaluator.invoke(`Grade the joke ${state.joke}`);\n  return { funny_or_not: grade.grade, feedback: grade.feedback };\n};\n\n// Conditional edge function to route back to joke generator or end\nconst routeJoke = (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Route back to joke generator or end based upon feedback from the evaluator\n  if (state.funny_or_not === \"funny\") {\n    return \"Accepted\";\n  } else if (state.funny_or_not === \"not funny\") {\n    return \"Rejected + Feedback\";\n  }\n};\n\n// Build workflow\nconst optimizerBuilder = new StateGraph(State)\n  .addNode(\"llm_call_generator\", llmCallGenerator)\n  .addNode(\"llm_call_evaluator\", llmCallEvaluator)\n  .addEdge(START, \"llm_call_generator\")\n  .addEdge(\"llm_call_generator\", \"llm_call_evaluator\")\n  .addConditionalEdges(\n    \"llm_call_evaluator\",\n    routeJoke,\n    {\n      \"Accepted\": END,\n      \"Rejected + Feedback\": \"llm_call_generator\",\n    }\n  );\n\n// Compile the workflow\nconst optimizerWorkflow = optimizerBuilder.compile();\n\n// Invoke\nconst state = await optimizerWorkflow.invoke({ topic: \"Cats\" });\nconsole.log(state.joke);\n</code></pre> :::</p> <p>:::python <pre><code># Schema for structured output to use in evaluation\nclass Feedback(BaseModel):\n    grade: Literal[\"funny\", \"not funny\"] = Field(\n        description=\"Decide if the joke is funny or not.\",\n    )\n    feedback: str = Field(\n        description=\"If the joke is not funny, provide feedback on how to improve it.\",\n    )\n\n\n# Augment the LLM with schema for structured output\nevaluator = llm.with_structured_output(Feedback)\n\n\n# Nodes\n@task\ndef llm_call_generator(topic: str, feedback: Feedback):\n    \"\"\"LLM generates a joke\"\"\"\n    if feedback:\n        msg = llm.invoke(\n            f\"Write a joke about {topic} but take into account the feedback: {feedback}\"\n        )\n    else:\n        msg = llm.invoke(f\"Write a joke about {topic}\")\n    return msg.content\n\n\n@task\ndef llm_call_evaluator(joke: str):\n    \"\"\"LLM evaluates the joke\"\"\"\n    feedback = evaluator.invoke(f\"Grade the joke {joke}\")\n    return feedback\n\n\n@entrypoint()\ndef optimizer_workflow(topic: str):\n    feedback = None\n    while True:\n        joke = llm_call_generator(topic, feedback).result()\n        feedback = llm_call_evaluator(joke).result()\n        if feedback.grade == \"funny\":\n            break\n\n    return joke\n\n# Invoke\nfor step in optimizer_workflow.stream(\"Cats\", stream_mode=\"updates\"):\n    print(step)\n    print(\"\\n\")\n</code></pre></p> <p>LangSmith Trace</p> <p>https://smith.langchain.com/public/f66830be-4339-4a6b-8a93-389ce5ae27b4/r :::</p> <p>:::js <pre><code>// Schema for structured output to use in evaluation\nconst Feedback = z.object({\n  grade: z.enum([\"funny\", \"not funny\"]).describe(\"Decide if the joke is funny or not.\"),\n  feedback: z.string().describe(\"If the joke is not funny, provide feedback on how to improve it.\"),\n});\n\n// Augment the LLM with schema for structured output\nconst evaluator = llm.withStructuredOutput(Feedback);\n\n// Nodes\nconst llmCallGenerator = task(\"llm_call_generator\", async (topic: string, feedback?: string) =&gt; {\n  // LLM generates a joke\n  if (feedback) {\n    const msg = await llm.invoke(\n      `Write a joke about ${topic} but take into account the feedback: ${feedback}`\n    );\n    return msg.content;\n  } else {\n    const msg = await llm.invoke(`Write a joke about ${topic}`);\n    return msg.content;\n  }\n});\n\nconst llmCallEvaluator = task(\"llm_call_evaluator\", async (joke: string) =&gt; {\n  // LLM evaluates the joke\n  const feedback = await evaluator.invoke(`Grade the joke ${joke}`);\n  return feedback;\n});\n\nconst optimizerWorkflow = entrypoint(\"optimizerWorkflow\", async (topic: string) =&gt; {\n  let feedback;\n  while (true) {\n    const joke = await llmCallGenerator(topic, feedback?.feedback);\n    feedback = await llmCallEvaluator(joke);\n    if (feedback.grade === \"funny\") {\n      return joke;\n    }\n  }\n});\n\n// Invoke\nconst stream = await optimizerWorkflow.stream(\"Cats\", { streamMode: \"updates\" });\nfor await (const step of stream) {\n  console.log(step);\n  console.log(\"\\n\");\n}\n</code></pre> :::</p>", "boost": 2}, {"location": "tutorials/workflows/#agent", "title": "Agent", "text": "<p>Agents are typically implemented as an LLM performing actions (via tool-calling) based on environmental feedback in a loop. As noted in the Anthropic blog on <code>Building Effective Agents</code>:</p> <p>Agents can handle sophisticated tasks, but their implementation is often straightforward. They are typically just LLMs using tools based on environmental feedback in a loop. It is therefore crucial to design toolsets and their documentation clearly and thoughtfully.</p> <p>When to use agents: Agents can be used for open-ended problems where it's difficult or impossible to predict the required number of steps, and where you can't hardcode a fixed path. The LLM will potentially operate for many turns, and you must have some level of trust in its decision-making. Agents' autonomy makes them ideal for scaling tasks in trusted environments.</p> <p></p> <p>:::python</p> <pre><code>from langchain_core.tools import tool\n\n\n# Define tools\n@tool\ndef multiply(a: int, b: int) -&gt; int:\n    \"\"\"Multiply a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a * b\n\n\n@tool\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Adds a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a + b\n\n\n@tool\ndef divide(a: int, b: int) -&gt; float:\n    \"\"\"Divide a and b.\n\n    Args:\n        a: first int\n        b: second int\n    \"\"\"\n    return a / b\n\n\n# Augment the LLM with tools\ntools = [add, multiply, divide]\ntools_by_name = {tool.name: tool for tool in tools}\nllm_with_tools = llm.bind_tools(tools)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { tool } from \"@langchain/core/tools\";\n\n// Define tools\nconst multiply = tool(\n  async ({ a, b }: { a: number; b: number }) =&gt; {\n    return a * b;\n  },\n  {\n    name: \"multiply\",\n    description: \"Multiply a and b.\",\n    schema: z.object({\n      a: z.number().describe(\"first int\"),\n      b: z.number().describe(\"second int\"),\n    }),\n  }\n);\n\nconst add = tool(\n  async ({ a, b }: { a: number; b: number }) =&gt; {\n    return a + b;\n  },\n  {\n    name: \"add\",\n    description: \"Adds a and b.\",\n    schema: z.object({\n      a: z.number().describe(\"first int\"),\n      b: z.number().describe(\"second int\"),\n    }),\n  }\n);\n\nconst divide = tool(\n  async ({ a, b }: { a: number; b: number }) =&gt; {\n    return a / b;\n  },\n  {\n    name: \"divide\",\n    description: \"Divide a and b.\",\n    schema: z.object({\n      a: z.number().describe(\"first int\"),\n      b: z.number().describe(\"second int\"),\n    }),\n  }\n);\n\n// Augment the LLM with tools\nconst tools = [add, multiply, divide];\nconst toolsByName = Object.fromEntries(tools.map((tool) =&gt; [tool.name, tool]));\nconst llmWithTools = llm.bindTools(tools);\n</code></pre> <p>:::</p> Graph APIFunctional API <p>:::python <pre><code>from langgraph.graph import MessagesState\nfrom langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\n\n\n# Nodes\ndef llm_call(state: MessagesState):\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\n\n    return {\n        \"messages\": [\n            llm_with_tools.invoke(\n                [\n                    SystemMessage(\n                        content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n                    )\n                ]\n                + state[\"messages\"]\n            )\n        ]\n    }\n\n\ndef tool_node(state: dict):\n    \"\"\"Performs the tool call\"\"\"\n\n    result = []\n    for tool_call in state[\"messages\"][-1].tool_calls:\n        tool = tools_by_name[tool_call[\"name\"]]\n        observation = tool.invoke(tool_call[\"args\"])\n        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n    return {\"messages\": result}\n\n\n# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\ndef should_continue(state: MessagesState) -&gt; Literal[\"Action\", END]:\n    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    # If the LLM makes a tool call, then perform an action\n    if last_message.tool_calls:\n        return \"Action\"\n    # Otherwise, we stop (reply to the user)\n    return END\n\n\n# Build workflow\nagent_builder = StateGraph(MessagesState)\n\n# Add nodes\nagent_builder.add_node(\"llm_call\", llm_call)\nagent_builder.add_node(\"environment\", tool_node)\n\n# Add edges to connect nodes\nagent_builder.add_edge(START, \"llm_call\")\nagent_builder.add_conditional_edges(\n    \"llm_call\",\n    should_continue,\n    {\n        # Name returned by should_continue : Name of next node to visit\n        \"Action\": \"environment\",\n        END: END,\n    },\n)\nagent_builder.add_edge(\"environment\", \"llm_call\")\n\n# Compile the agent\nagent = agent_builder.compile()\n\n# Show the agent\ndisplay(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n\n# Invoke\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\nmessages = agent.invoke({\"messages\": messages})\nfor m in messages[\"messages\"]:\n    m.pretty_print()\n</code></pre></p> <p>LangSmith Trace</p> <p>https://smith.langchain.com/public/051f0391-6761-4f8c-a53b-22231b016690/r</p> <p>Resources:</p> <p>LangChain Academy</p> <p>See our lesson on agents here.</p> <p>Examples</p> <p>Here is a project that uses a tool calling agent to create / store long-term memories. :::</p> <p>:::js <pre><code>import { MessagesZodState, ToolNode } from \"@langchain/langgraph/prebuilt\";\nimport { SystemMessage, HumanMessage, ToolMessage, isAIMessage } from \"@langchain/core/messages\";\n\n// Nodes\nconst llmCall = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  // LLM decides whether to call a tool or not\n  const response = await llmWithTools.invoke([\n    new SystemMessage(\n      \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n    ),\n    ...state.messages,\n  ]);\n  return { messages: [response] };\n};\n\nconst toolNode = new ToolNode(tools);\n\n// Conditional edge function to route to the tool node or end\nconst shouldContinue = (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  // Decide if we should continue the loop or stop\n  const messages = state.messages;\n  const lastMessage = messages[messages.length - 1];\n  // If the LLM makes a tool call, then perform an action\n  if (isAIMessage(lastMessage) &amp;&amp; lastMessage.tool_calls?.length) {\n    return \"Action\";\n  }\n  // Otherwise, we stop (reply to the user)\n  return END;\n};\n\n// Build workflow\nconst agentBuilder = new StateGraph(MessagesZodState)\n  .addNode(\"llm_call\", llmCall)\n  .addNode(\"environment\", toolNode)\n  .addEdge(START, \"llm_call\")\n  .addConditionalEdges(\n    \"llm_call\",\n    shouldContinue,\n    {\n      \"Action\": \"environment\",\n      [END]: END,\n    }\n  )\n  .addEdge(\"environment\", \"llm_call\");\n\n// Compile the agent\nconst agent = agentBuilder.compile();\n\n// Invoke\nconst messages = [new HumanMessage(\"Add 3 and 4.\")];\nconst result = await agent.invoke({ messages });\nfor (const m of result.messages) {\n  console.log(`${m.getType()}: ${m.content}`);\n}\n</code></pre> :::</p> <p>:::python <pre><code>from langgraph.graph import add_messages\nfrom langchain_core.messages import (\n    SystemMessage,\n    HumanMessage,\n    BaseMessage,\n    ToolCall,\n)\n\n\n@task\ndef call_llm(messages: list[BaseMessage]):\n    \"\"\"LLM decides whether to call a tool or not\"\"\"\n    return llm_with_tools.invoke(\n        [\n            SystemMessage(\n                content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n            )\n        ]\n        + messages\n    )\n\n\n@task\ndef call_tool(tool_call: ToolCall):\n    \"\"\"Performs the tool call\"\"\"\n    tool = tools_by_name[tool_call[\"name\"]]\n    return tool.invoke(tool_call)\n\n\n@entrypoint()\ndef agent(messages: list[BaseMessage]):\n    llm_response = call_llm(messages).result()\n\n    while True:\n        if not llm_response.tool_calls:\n            break\n\n        # Execute tools\n        tool_result_futures = [\n            call_tool(tool_call) for tool_call in llm_response.tool_calls\n        ]\n        tool_results = [fut.result() for fut in tool_result_futures]\n        messages = add_messages(messages, [llm_response, *tool_results])\n        llm_response = call_llm(messages).result()\n\n    messages = add_messages(messages, llm_response)\n    return messages\n\n# Invoke\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\nfor chunk in agent.stream(messages, stream_mode=\"updates\"):\n    print(chunk)\n    print(\"\\n\")\n</code></pre></p> <p>LangSmith Trace</p> <p>https://smith.langchain.com/public/42ae8bf9-3935-4504-a081-8ddbcbfc8b2e/r :::</p> <p>:::js <pre><code>import { addMessages } from \"@langchain/langgraph\";\nimport {\n  SystemMessage,\n  HumanMessage,\n  BaseMessage,\n  ToolCall,\n} from \"@langchain/core/messages\";\n\nconst callLlm = task(\"call_llm\", async (messages: BaseMessage[]) =&gt; {\n  // LLM decides whether to call a tool or not\n  return await llmWithTools.invoke([\n    new SystemMessage(\n      \"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\"\n    ),\n    ...messages,\n  ]);\n});\n\nconst callTool = task(\"call_tool\", async (toolCall: ToolCall) =&gt; {\n  // Performs the tool call\n  const tool = toolsByName[toolCall.name];\n  return await tool.invoke(toolCall);\n});\n\nconst agent = entrypoint(\"agent\", async (messages: BaseMessage[]) =&gt; {\n  let currentMessages = messages;\n  let llmResponse = await callLlm(currentMessages);\n\n  while (true) {\n    if (!llmResponse.tool_calls?.length) {\n      break;\n    }\n\n    // Execute tools\n    const toolResults = await Promise.all(\n      llmResponse.tool_calls.map((toolCall) =&gt; callTool(toolCall))\n    );\n\n    // Append to message list\n    currentMessages = addMessages(currentMessages, [\n      llmResponse,\n      ...toolResults,\n    ]);\n\n    // Call model again\n    llmResponse = await callLlm(currentMessages);\n  }\n\n  return llmResponse;\n});\n\n// Invoke\nconst messages = [new HumanMessage(\"Add 3 and 4.\")];\nconst stream = await agent.stream(messages, { streamMode: \"updates\" });\nfor await (const chunk of stream) {\n  console.log(chunk);\n  console.log(\"\\n\");\n}\n</code></pre> :::</p>", "boost": 2}, {"location": "tutorials/workflows/#pre-built", "title": "Pre-built", "text": "<p>:::python LangGraph also provides a pre-built method for creating an agent as defined above (using the @[<code>create_react_agent</code>][create_react_agent] function):</p> <p>https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/</p> <pre><code>from langgraph.prebuilt import create_react_agent\n\n# Pass in:\n# (1) the augmented LLM with tools\n# (2) the tools list (which is used to create the tool node)\npre_built_agent = create_react_agent(llm, tools=tools)\n\n# Show the agent\ndisplay(Image(pre_built_agent.get_graph().draw_mermaid_png()))\n\n# Invoke\nmessages = [HumanMessage(content=\"Add 3 and 4.\")]\nmessages = pre_built_agent.invoke({\"messages\": messages})\nfor m in messages[\"messages\"]:\n    m.pretty_print()\n</code></pre> <p>LangSmith Trace</p> <p>https://smith.langchain.com/public/abab6a44-29f6-4b97-8164-af77413e494d/r :::</p> <p>:::js LangGraph also provides a pre-built method for creating an agent as defined above (using the @[<code>createReactAgent</code>][create_react_agent] function):</p> <pre><code>import { createReactAgent } from \"@langchain/langgraph/prebuilt\";\n\n// Pass in:\n// (1) the augmented LLM with tools\n// (2) the tools list (which is used to create the tool node)\nconst preBuiltAgent = createReactAgent({ llm, tools });\n\n// Invoke\nconst messages = [new HumanMessage(\"Add 3 and 4.\")];\nconst result = await preBuiltAgent.invoke({ messages });\nfor (const m of result.messages) {\n  console.log(`${m.getType()}: ${m.content}`);\n}\n</code></pre> <p>:::</p>", "boost": 2}, {"location": "tutorials/workflows/#what-langgraph-provides", "title": "What LangGraph provides", "text": "<p>By constructing each of the above in LangGraph, we get a few things:</p>", "boost": 2}, {"location": "tutorials/workflows/#persistence-human-in-the-loop", "title": "Persistence: Human-in-the-Loop", "text": "<p>LangGraph persistence layer supports interruption and approval of actions (e.g., Human In The Loop). See Module 3 of LangChain Academy.</p>", "boost": 2}, {"location": "tutorials/workflows/#persistence-memory", "title": "Persistence: Memory", "text": "<p>LangGraph persistence layer supports conversational (short-term) memory and long-term memory. See Modules 2 and 5 of LangChain Academy:</p>", "boost": 2}, {"location": "tutorials/workflows/#streaming", "title": "Streaming", "text": "<p>LangGraph provides several ways to stream workflow / agent outputs or intermediate state. See Module 3 of LangChain Academy.</p>", "boost": 2}, {"location": "tutorials/workflows/#deployment", "title": "Deployment", "text": "<p>LangGraph provides an easy on-ramp for deployment, observability, and evaluation. See module 6 of LangChain Academy.</p>", "boost": 2}, {"location": "tutorials/get-started/1-build-basic-chatbot/", "title": "Build a basic chatbot", "text": "<p>In this tutorial, you will build a basic chatbot. This chatbot is the basis for the following series of tutorials where you will progressively add more sophisticated capabilities, and be introduced to key LangGraph concepts along the way. Let's dive in! \ud83c\udf1f</p>"}, {"location": "tutorials/get-started/1-build-basic-chatbot/#prerequisites", "title": "Prerequisites", "text": "<p>Before you start this tutorial, ensure you have access to a LLM that supports tool-calling features, such as OpenAI, Anthropic, or Google Gemini.</p>"}, {"location": "tutorials/get-started/1-build-basic-chatbot/#1-install-packages", "title": "1. Install packages", "text": "<p>Install the required packages:</p> <p>:::python</p> <pre><code>pip install -U langgraph langsmith\n</code></pre> <p>:::</p> <p>:::js</p> npmyarnpnpmbun <pre><code>npm install @langchain/langgraph @langchain/core zod\n</code></pre> <pre><code>yarn add @langchain/langgraph @langchain/core zod\n</code></pre> <pre><code>pnpm add @langchain/langgraph @langchain/core zod\n</code></pre> <pre><code>bun add @langchain/langgraph @langchain/core zod\n</code></pre> <p>:::</p> <p>Tip</p> <p>Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph. For more information on how to get started, see LangSmith docs.</p>"}, {"location": "tutorials/get-started/1-build-basic-chatbot/#2-create-a-stategraph", "title": "2. Create a <code>StateGraph</code>", "text": "<p>Now you can create a basic chatbot using LangGraph. This chatbot will respond directly to user messages.</p> <p>Start by creating a <code>StateGraph</code>. A <code>StateGraph</code> object defines the structure of our chatbot as a \"state machine\". We'll add <code>nodes</code> to represent the llm and functions our chatbot can call and <code>edges</code> to specify how the bot should transition between these functions.</p> <p>:::python</p> <pre><code>from typing import Annotated\n\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\n\nclass State(TypedDict):\n    # Messages have the type \"list\". The `add_messages` function\n    # in the annotation defines how this state key should be updated\n    # (in this case, it appends messages to the list, rather than overwriting them)\n    messages: Annotated[list, add_messages]\n\n\ngraph_builder = StateGraph(State)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MessagesZodState, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({ messages: MessagesZodState.shape.messages });\n\nconst graph = new StateGraph(State).compile();\n</code></pre> <p>:::</p> <p>Our graph can now handle two key tasks:</p> <ol> <li>Each <code>node</code> can receive the current <code>State</code> as input and output an update to the state.</li> <li>Updates to <code>messages</code> will be appended to the existing list rather than overwriting it, thanks to the prebuilt reducer function.</li> </ol> <p>Concept</p> <p>When defining a graph, the first step is to define its <code>State</code>. The <code>State</code> includes the graph's schema and reducer functions that handle state updates. In our example, <code>State</code> is a schema with one key: <code>messages</code>. The reducer function is used to append new messages to the list instead of overwriting it. Keys without a reducer annotation will overwrite previous values.</p> <p>To learn more about state, reducers, and related concepts, see LangGraph reference docs.</p>"}, {"location": "tutorials/get-started/1-build-basic-chatbot/#3-add-a-node", "title": "3. Add a node", "text": "<p>Next, add a \"<code>chatbot</code>\" node. Nodes represent units of work and are typically regular functions.</p> <p>Let's first select a chat model:</p> <p>:::python</p> OpenAIAnthropicAzureGoogle GeminiAWS Bedrock <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"openai:gpt-4.1\")\n</code></pre></p> <p>\ud83d\udc49 Read the OpenAI integration docs</p> <p><pre><code>pip install -U \"langchain[anthropic]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n</code></pre></p> <p>\ud83d\udc49 Read the Anthropic integration docs</p> <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\nllm = init_chat_model(\n    \"azure_openai:gpt-4.1\",\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n)\n</code></pre></p> <p>\ud83d\udc49 Read the Azure integration docs</p> <p><pre><code>pip install -U \"langchain[google-genai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n</code></pre></p> <p>\ud83d\udc49 Read the Google GenAI integration docs</p> <p><pre><code>pip install -U \"langchain[aws]\"\n</code></pre> <pre><code>from langchain.chat_models import init_chat_model\n\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\nllm = init_chat_model(\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    model_provider=\"bedrock_converse\",\n)\n</code></pre></p> <p>\ud83d\udc49 Read the AWS Bedrock integration docs</p> <p>:::</p> <p>:::js</p> <pre><code>import { ChatOpenAI } from \"@langchain/openai\";\n// or import { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n});\n</code></pre> <p>:::</p> <p>We can now incorporate the chat model into a simple node:</p> <p>:::python</p> <pre><code>def chatbot(state: State):\n    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n\n\n# The first argument is the unique node name\n# The second argument is the function or object that will be called whenever\n# the node is used.\ngraph_builder.add_node(\"chatbot\", chatbot)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MessagesZodState, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({ messages: MessagesZodState.shape.messages });\n\nconst graph = new StateGraph(State)\n  .addNode(\"chatbot\", async (state: z.infer&lt;typeof State&gt;) =&gt; {\n    return { messages: [await llm.invoke(state.messages)] };\n  })\n  .compile();\n</code></pre> <p>:::</p> <p>Notice how the <code>chatbot</code> node function takes the current <code>State</code> as input and returns a dictionary containing an updated <code>messages</code> list under the key \"messages\". This is the basic pattern for all LangGraph node functions.</p> <p>:::python The <code>add_messages</code> function in our <code>State</code> will append the LLM's response messages to whatever messages are already in the state. :::</p> <p>:::js The <code>addMessages</code> function used within <code>MessagesZodState</code> will append the LLM's response messages to whatever messages are already in the state. :::</p>"}, {"location": "tutorials/get-started/1-build-basic-chatbot/#4-add-an-entry-point", "title": "4. Add an <code>entry</code> point", "text": "<p>Add an <code>entry</code> point to tell the graph where to start its work each time it is run:</p> <p>:::python</p> <pre><code>graph_builder.add_edge(START, \"chatbot\")\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MessagesZodState, START } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({ messages: MessagesZodState.shape.messages });\n\nconst graph = new StateGraph(State)\n  .addNode(\"chatbot\", async (state: z.infer&lt;typeof State&gt;) =&gt; {\n    return { messages: [await llm.invoke(state.messages)] };\n  })\n  .addEdge(START, \"chatbot\")\n  .compile();\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/1-build-basic-chatbot/#5-add-an-exit-point", "title": "5. Add an <code>exit</code> point", "text": "<p>Add an <code>exit</code> point to indicate where the graph should finish execution. This is helpful for more complex flows, but even in a simple graph like this, adding an end node improves clarity.</p> <p>:::python</p> <pre><code>graph_builder.add_edge(\"chatbot\", END)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MessagesZodState, START, END } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({ messages: MessagesZodState.shape.messages });\n\nconst graph = new StateGraph(State)\n  .addNode(\"chatbot\", async (state: z.infer&lt;typeof State&gt;) =&gt; {\n    return { messages: [await llm.invoke(state.messages)] };\n  })\n  .addEdge(START, \"chatbot\")\n  .addEdge(\"chatbot\", END)\n  .compile();\n</code></pre> <p>:::</p> <p>This tells the graph to terminate after running the chatbot node.</p>"}, {"location": "tutorials/get-started/1-build-basic-chatbot/#6-compile-the-graph", "title": "6. Compile the graph", "text": "<p>Before running the graph, we'll need to compile it. We can do so by calling <code>compile()</code> on the graph builder. This creates a <code>CompiledGraph</code> we can invoke on our state.</p> <p>:::python</p> <pre><code>graph = graph_builder.compile()\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MessagesZodState, START, END } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({ messages: MessagesZodState.shape.messages });\n\nconst graph = new StateGraph(State)\n  .addNode(\"chatbot\", async (state: z.infer&lt;typeof State&gt;) =&gt; {\n    return { messages: [await llm.invoke(state.messages)] };\n  })\n  .addEdge(START, \"chatbot\")\n  .addEdge(\"chatbot\", END)\n  .compile();\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/1-build-basic-chatbot/#7-visualize-the-graph-optional", "title": "7. Visualize the graph (optional)", "text": "<p>:::python You can visualize the graph using the <code>get_graph</code> method and one of the \"draw\" methods, like <code>draw_ascii</code> or <code>draw_png</code>. The <code>draw</code> methods each require additional dependencies.</p> <pre><code>from IPython.display import Image, display\n\ntry:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass\n</code></pre> <p>:::</p> <p>:::js You can visualize the graph using the <code>getGraph</code> method and render the graph with the <code>drawMermaidPng</code> method.</p> <pre><code>import * as fs from \"node:fs/promises\";\n\nconst drawableGraph = await graph.getGraphAsync();\nconst image = await drawableGraph.drawMermaidPng();\nconst imageBuffer = new Uint8Array(await image.arrayBuffer());\n\nawait fs.writeFile(\"basic-chatbot.png\", imageBuffer);\n</code></pre> <p>:::</p> <p></p>"}, {"location": "tutorials/get-started/1-build-basic-chatbot/#8-run-the-chatbot", "title": "8. Run the chatbot", "text": "<p>Now run the chatbot!</p> <p>Tip</p> <p>You can exit the chat loop at any time by typing <code>quit</code>, <code>exit</code>, or <code>q</code>.</p> <p>:::python</p> <pre><code>def stream_graph_updates(user_input: str):\n    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n        for value in event.values():\n            print(\"Assistant:\", value[\"messages\"][-1].content)\n\n\nwhile True:\n    try:\n        user_input = input(\"User: \")\n        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n            print(\"Goodbye!\")\n            break\n        stream_graph_updates(user_input)\n    except:\n        # fallback if input() is not available\n        user_input = \"What do you know about LangGraph?\"\n        print(\"User: \" + user_input)\n        stream_graph_updates(user_input)\n        break\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { HumanMessage } from \"@langchain/core/messages\";\n\nasync function streamGraphUpdates(userInput: string) {\n  const stream = await graph.stream({\n    messages: [new HumanMessage(userInput)],\n  });\n\nimport * as readline from \"node:readline/promises\";\nimport { StateGraph, MessagesZodState, START, END } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { z } from \"zod\";\n\nconst llm = new ChatOpenAI({ model: \"gpt-4o-mini\" });\n\nconst State = z.object({ messages: MessagesZodState.shape.messages });\n\nconst graph = new StateGraph(State)\n  .addNode(\"chatbot\", async (state: z.infer&lt;typeof State&gt;) =&gt; {\n    return { messages: [await llm.invoke(state.messages)] };\n  })\n  .addEdge(START, \"chatbot\")\n  .addEdge(\"chatbot\", END)\n  .compile();\n\nasync function generateText(content: string) {\n  const stream = await graph.stream(\n    { messages: [{ type: \"human\", content }] },\n    { streamMode: \"values\" }\n  );\n\n  for await (const event of stream) {\n    for (const value of Object.values(event)) {\n      console.log(\n        \"Assistant:\",\n        value.messages[value.messages.length - 1].content\n      );\n    const lastMessage = event.messages.at(-1);\n    if (lastMessage?.getType() === \"ai\") {\n      console.log(`Assistant: ${lastMessage.text}`);\n    }\n  }\n}\n\nconst prompt = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout,\n});\n\nwhile (true) {\n  const human = await prompt.question(\"User: \");\n  if ([\"quit\", \"exit\", \"q\"].includes(human.trim())) break;\n  await generateText(human || \"What do you know about LangGraph?\");\n}\n\nprompt.close();\n</code></pre> <p>:::</p> <pre><code>Assistant: LangGraph is a library designed to help build stateful multi-agent applications using language models. It provides tools for creating workflows and state machines to coordinate multiple AI agents or language model interactions. LangGraph is built on top of LangChain, leveraging its components while adding graph-based coordination capabilities. It's particularly useful for developing more complex, stateful AI applications that go beyond simple query-response interactions.\n</code></pre> <p>:::python</p> <pre><code>Goodbye!\n</code></pre> <p>:::</p> <p>Congratulations! You've built your first chatbot using LangGraph. This bot can engage in basic conversation by taking user input and generating responses using an LLM. You can inspect a LangSmith Trace for the call above.</p> <p>:::python</p> <p>Below is the full code for this tutorial:</p> <pre><code>from typing import Annotated\n\nfrom langchain.chat_models import init_chat_model\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\n\ngraph_builder = StateGraph(State)\n\n\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n\n\ndef chatbot(state: State):\n    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n\n\n# The first argument is the unique node name\n# The second argument is the function or object that will be called whenever\n# the node is used.\ngraph_builder.add_node(\"chatbot\", chatbot)\ngraph_builder.add_edge(START, \"chatbot\")\ngraph_builder.add_edge(\"chatbot\", END)\ngraph = graph_builder.compile()\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, START, END, MessagesZodState } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\nimport { ChatOpenAI } from \"@langchain/openai\";\n\nconst llm = new ChatOpenAI({\n  model: \"gpt-4o\",\n  temperature: 0,\n});\n\nconst State = z.object({ messages: MessagesZodState.shape.messages });\n\nconst graph = new StateGraph(State);\n  // The first argument is the unique node name\n  // The second argument is the function or object that will be called whenever\n  // the node is used.\n  .addNode(\"chatbot\", async (state) =&gt; {\n    return { messages: [await llm.invoke(state.messages)] };\n  });\n  .addEdge(START, \"chatbot\");\n  .addEdge(\"chatbot\", END)\n  .compile();\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/1-build-basic-chatbot/#next-steps", "title": "Next steps", "text": "<p>You may have noticed that the bot's knowledge is limited to what's in its training data. In the next part, we'll add a web search tool to expand the bot's knowledge and make it more capable.</p>"}, {"location": "tutorials/get-started/2-add-tools/", "title": "Add tools", "text": "<p>To handle queries that your chatbot can't answer \"from memory\", integrate a web search tool. The chatbot can use this tool to find relevant information and provide better responses.</p> <p>Note</p> <p>This tutorial builds on Build a basic chatbot.</p>"}, {"location": "tutorials/get-started/2-add-tools/#prerequisites", "title": "Prerequisites", "text": "<p>Before you start this tutorial, ensure you have the following:</p> <p>:::python</p> <ul> <li>An API key for the Tavily Search Engine.</li> </ul> <p>:::</p> <p>:::js</p> <ul> <li>An API key for the Tavily Search Engine.</li> </ul> <p>:::</p>"}, {"location": "tutorials/get-started/2-add-tools/#1-install-the-search-engine", "title": "1. Install the search engine", "text": "<p>:::python Install the requirements to use the Tavily Search Engine:</p> <pre><code>pip install -U langchain-tavily\n</code></pre> <p>:::</p> <p>:::js Install the requirements to use the Tavily Search Engine:</p> npmyarnpnpmbun <pre><code>npm install @langchain/tavily\n</code></pre> <pre><code>yarn add @langchain/tavily\n</code></pre> <pre><code>pnpm add @langchain/tavily\n</code></pre> <pre><code>bun add @langchain/tavily\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/2-add-tools/#2-configure-your-environment", "title": "2. Configure your environment", "text": "<p>Configure your environment with your search engine API key:</p> <p>:::python <pre><code>import os\n\nos.environ[\"TAVILY_API_KEY\"] = \"tvly-...\"\n</code></pre> :::</p> <p>:::js</p> <pre><code>process.env.TAVILY_API_KEY = \"tvly-...\";\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/2-add-tools/#3-define-the-tool", "title": "3. Define the tool", "text": "<p>Define the web search tool:</p> <p>:::python</p> <pre><code>from langchain_tavily import TavilySearch\n\ntool = TavilySearch(max_results=2)\ntools = [tool]\ntool.invoke(\"What's a 'node' in LangGraph?\")\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { TavilySearch } from \"@langchain/tavily\";\n\nconst tool = new TavilySearch({ maxResults: 2 });\nconst tools = [tool];\n\nawait tool.invoke({ query: \"What's a 'node' in LangGraph?\" });\n</code></pre> <p>:::</p> <p>The results are page summaries our chat bot can use to answer questions:</p> <p>:::python</p> <pre><code>{'query': \"What's a 'node' in LangGraph?\",\n'follow_up_questions': None,\n'answer': None,\n'images': [],\n'results': [{'title': \"Introduction to LangGraph: A Beginner's Guide - Medium\",\n'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',\n'content': 'Stateful Graph: LangGraph revolves around the concept of a stateful graph, where each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses. LangGraph supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph. We define nodes for classifying the input, handling greetings, and handling search queries. def classify_input_node(state): LangGraph is a versatile tool for building complex, stateful applications with LLMs. By understanding its core concepts and working through simple examples, beginners can start to leverage its power for their projects. Remember to pay attention to state management, conditional edges, and ensuring there are no dead-end nodes in your graph.',\n'score': 0.7065353,\n'raw_content': None},\n{'title': 'LangGraph Tutorial: What Is LangGraph and How to Use It?',\n'url': 'https://www.datacamp.com/tutorial/langgraph-tutorial',\n'content': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.',\n'score': 0.5008063,\n'raw_content': None}],\n'response_time': 1.38}\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>{\n  \"query\": \"What's a 'node' in LangGraph?\",\n  \"follow_up_questions\": null,\n  \"answer\": null,\n  \"images\": [],\n  \"results\": [\n    {\n      \"url\": \"https://blog.langchain.dev/langgraph/\",\n      \"title\": \"LangGraph - LangChain Blog\",\n      \"content\": \"TL;DR: LangGraph is module built on top of LangChain to better enable creation of cyclical graphs, often needed for agent runtimes. This state is updated by nodes in the graph, which return operations to attributes of this state (in the form of a key-value store). After adding nodes, you can then add edges to create the graph. An example of this may be in the basic agent runtime, where we always want the model to be called after we call a tool. The state of this graph by default contains concepts that should be familiar to you if you've used LangChain agents: `input`, `chat_history`, `intermediate_steps` (and `agent_outcome` to represent the most recent agent outcome)\",\n      \"score\": 0.7407191,\n      \"raw_content\": null\n    },\n    {\n      \"url\": \"https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141\",\n      \"title\": \"Introduction to LangGraph: A Beginner's Guide - Medium\",\n      \"content\": \"*   **Stateful Graph:** LangGraph revolves around the concept of a stateful graph, where each node in the graph represents a step in your computation, and the graph maintains a state that is passed around and updated as the computation progresses. LangGraph supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph. Image 10: Introduction to AI Agent with LangChain and LangGraph: A Beginner\u2019s Guide Image 18: How to build LLM Agent with LangGraph\u200a\u2014\u200aStateGraph and Reducer Image 20: Simplest Graphs using LangGraph Framework Image 24: Building a ReAct Agent with Langgraph: A Step-by-Step Guide Image 28: Building an Agentic RAG with LangGraph: A Step-by-Step Guide\",\n      \"score\": 0.65279555,\n      \"raw_content\": null\n    }\n  ],\n  \"response_time\": 1.34\n}\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/2-add-tools/#4-define-the-graph", "title": "4. Define the graph", "text": "<p>:::python For the <code>StateGraph</code> you created in the first tutorial, add <code>bind_tools</code> on the LLM. This lets the LLM know the correct JSON format to use if it wants to use the search engine. :::</p> <p>:::js For the <code>StateGraph</code> you created in the first tutorial, add <code>bindTools</code> on the LLM. This lets the LLM know the correct JSON format to use if it wants to use the search engine. :::</p> OpenAIAnthropicAzureGoogle GeminiAWS Bedrock <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"openai:gpt-4.1\")\n</code></pre></p> <p>\ud83d\udc49 Read the OpenAI integration docs</p> <p><pre><code>pip install -U \"langchain[anthropic]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n</code></pre></p> <p>\ud83d\udc49 Read the Anthropic integration docs</p> <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\nllm = init_chat_model(\n    \"azure_openai:gpt-4.1\",\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n)\n</code></pre></p> <p>\ud83d\udc49 Read the Azure integration docs</p> <p><pre><code>pip install -U \"langchain[google-genai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n</code></pre></p> <p>\ud83d\udc49 Read the Google GenAI integration docs</p> <p><pre><code>pip install -U \"langchain[aws]\"\n</code></pre> <pre><code>from langchain.chat_models import init_chat_model\n\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\nllm = init_chat_model(\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    model_provider=\"bedrock_converse\",\n)\n</code></pre></p> <p>\ud83d\udc49 Read the AWS Bedrock integration docs</p> <p>:::</p> <p>:::js</p> <pre><code>import { ChatAnthropic } from \"@langchain/anthropic\";\n\nconst llm = new ChatAnthropic({ model: \"claude-3-5-sonnet-latest\" });\n</code></pre> <p>:::</p> <p>We can now incorporate it into a <code>StateGraph</code>:</p> <p>:::python</p> <pre><code>from typing import Annotated\n\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\ngraph_builder = StateGraph(State)\n\n# Modification: tell the LLM which tools it can call\n# highlight-next-line\nllm_with_tools = llm.bind_tools(tools)\n\ndef chatbot(state: State):\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\ngraph_builder.add_node(\"chatbot\", chatbot)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MessagesZodState } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({ messages: MessagesZodState.shape.messages });\n\nconst chatbot = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  // Modification: tell the LLM which tools it can call\n  const llmWithTools = llm.bindTools(tools);\n\n  return { messages: [await llmWithTools.invoke(state.messages)] };\n};\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/2-add-tools/#5-create-a-function-to-run-the-tools", "title": "5. Create a function to run the tools", "text": "<p>:::python</p> <p>Now, create a function to run the tools if they are called. Do this by adding the tools to a new node called <code>BasicToolNode</code> that checks the most recent message in the state and calls tools if the message contains <code>tool_calls</code>. It relies on the LLM's <code>tool_calling</code> support, which is available in Anthropic, OpenAI, Google Gemini, and a number of other LLM providers.</p> <pre><code>import json\n\nfrom langchain_core.messages import ToolMessage\n\n\nclass BasicToolNode:\n    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n\n    def __init__(self, tools: list) -&gt; None:\n        self.tools_by_name = {tool.name: tool for tool in tools}\n\n    def __call__(self, inputs: dict):\n        if messages := inputs.get(\"messages\", []):\n            message = messages[-1]\n        else:\n            raise ValueError(\"No message found in input\")\n        outputs = []\n        for tool_call in message.tool_calls:\n            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n                tool_call[\"args\"]\n            )\n            outputs.append(\n                ToolMessage(\n                    content=json.dumps(tool_result),\n                    name=tool_call[\"name\"],\n                    tool_call_id=tool_call[\"id\"],\n                )\n            )\n        return {\"messages\": outputs}\n\n\ntool_node = BasicToolNode(tools=[tool])\ngraph_builder.add_node(\"tools\", tool_node)\n</code></pre> <p>Note</p> <p>If you do not want to build this yourself in the future, you can use LangGraph's prebuilt ToolNode.</p> <p>:::</p> <p>:::js</p> <p>Now, create a function to run the tools if they are called. Do this by adding the tools to a new node called <code>\"tools\"</code> that checks the most recent message in the state and calls tools if the message contains <code>tool_calls</code>. It relies on the LLM's tool calling support, which is available in Anthropic, OpenAI, Google Gemini, and a number of other LLM providers.</p> <pre><code>import type { StructuredToolInterface } from \"@langchain/core/tools\";\nimport { isAIMessage, ToolMessage } from \"@langchain/core/messages\";\n\nfunction createToolNode(tools: StructuredToolInterface[]) {\n  const toolByName: Record&lt;string, StructuredToolInterface&gt; = {};\n  for (const tool of tools) {\n    toolByName[tool.name] = tool;\n  }\n\n  return async (inputs: z.infer&lt;typeof State&gt;) =&gt; {\n    const { messages } = inputs;\n    if (!messages || messages.length === 0) {\n      throw new Error(\"No message found in input\");\n    }\n\n    const message = messages.at(-1);\n    if (!message || !isAIMessage(message) || !message.tool_calls) {\n      throw new Error(\"Last message is not an AI message with tool calls\");\n    }\n\n    const outputs: ToolMessage[] = [];\n    for (const toolCall of message.tool_calls) {\n      if (!toolCall.id) throw new Error(\"Tool call ID is required\");\n\n      const tool = toolByName[toolCall.name];\n      if (!tool) throw new Error(`Tool ${toolCall.name} not found`);\n\n      const result = await tool.invoke(toolCall.args);\n\n      outputs.push(\n        new ToolMessage({\n          content: JSON.stringify(result),\n          name: toolCall.name,\n          tool_call_id: toolCall.id,\n        })\n      );\n    }\n\n    return { messages: outputs };\n  };\n}\n</code></pre> <p>Note</p> <p>If you do not want to build this yourself in the future, you can use LangGraph's prebuilt ToolNode.</p> <p>:::</p>"}, {"location": "tutorials/get-started/2-add-tools/#6-define-the-conditional_edges", "title": "6. Define the <code>conditional_edges</code>", "text": "<p>With the tool node added, now you can define the <code>conditional_edges</code>.</p> <p>Edges route the control flow from one node to the next. Conditional edges start from a single node and usually contain \"if\" statements to route to different nodes depending on the current graph state. These functions receive the current graph <code>state</code> and return a string or list of strings indicating which node(s) to call next.</p> <p>:::python Next, define a router function called <code>route_tools</code> that checks for <code>tool_calls</code> in the chatbot's output. Provide this function to the graph by calling <code>add_conditional_edges</code>, which tells the graph that whenever the <code>chatbot</code> node completes to check this function to see where to go next. :::</p> <p>:::js Next, define a router function called <code>routeTools</code> that checks for <code>tool_calls</code> in the chatbot's output. Provide this function to the graph by calling <code>addConditionalEdges</code>, which tells the graph that whenever the <code>chatbot</code> node completes to check this function to see where to go next. :::</p> <p>The condition will route to <code>tools</code> if tool calls are present and <code>END</code> if not. Because the condition can return <code>END</code>, you do not need to explicitly set a <code>finish_point</code> this time.</p> <p>:::python</p> <pre><code>def route_tools(\n    state: State,\n):\n    \"\"\"\n    Use in the conditional_edge to route to the ToolNode if the last message\n    has tool calls. Otherwise, route to the end.\n    \"\"\"\n    if isinstance(state, list):\n        ai_message = state[-1]\n    elif messages := state.get(\"messages\", []):\n        ai_message = messages[-1]\n    else:\n        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) &gt; 0:\n        return \"tools\"\n    return END\n\n\n# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n# it is fine directly responding. This conditional routing defines the main agent loop.\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    route_tools,\n    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n    # It defaults to the identity function, but if you\n    # want to use a node named something else apart from \"tools\",\n    # You can update the value of the dictionary to something else\n    # e.g., \"tools\": \"my_tools\"\n    {\"tools\": \"tools\", END: END},\n)\n# Any time a tool is called, we return to the chatbot to decide the next step\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(START, \"chatbot\")\ngraph = graph_builder.compile()\n</code></pre> <p>Note</p> <p>You can replace this with the prebuilt tools_condition to be more concise.</p> <p>:::</p> <p>:::js</p> <pre><code>import { END, START } from \"@langchain/langgraph\";\n\nconst routeTools = (state: z.infer&lt;typeof State&gt;) =&gt; {\n  /**\n   * Use as conditional edge to route to the ToolNode if the last message\n   * has tool calls.\n   */\n  const lastMessage = state.messages.at(-1);\n  if (\n    lastMessage &amp;&amp;\n    isAIMessage(lastMessage) &amp;&amp;\n    lastMessage.tool_calls?.length\n  ) {\n    return \"tools\";\n  }\n\n  /** Otherwise, route to the end. */\n  return END;\n};\n\nconst graph = new StateGraph(State)\n  .addNode(\"chatbot\", chatbot)\n\n  // The `routeTools` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n  // it is fine directly responding. This conditional routing defines the main agent loop.\n  .addNode(\"tools\", createToolNode(tools))\n\n  // Start the graph with the chatbot\n  .addEdge(START, \"chatbot\")\n\n  // The `routeTools` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n  // it is fine directly responding.\n  .addConditionalEdges(\"chatbot\", routeTools, [\"tools\", END])\n\n  // Any time a tool is called, we need to return to the chatbot\n  .addEdge(\"tools\", \"chatbot\")\n  .compile();\n</code></pre> <p>Note</p> <p>You can replace this with the prebuilt toolsCondition to be more concise.</p> <p>:::</p>"}, {"location": "tutorials/get-started/2-add-tools/#7-visualize-the-graph-optional", "title": "7. Visualize the graph (optional)", "text": "<p>:::python You can visualize the graph using the <code>get_graph</code> method and one of the \"draw\" methods, like <code>draw_ascii</code> or <code>draw_png</code>. The <code>draw</code> methods each require additional dependencies.</p> <pre><code>from IPython.display import Image, display\n\ntry:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass\n</code></pre> <p>:::</p> <p>:::js You can visualize the graph using the <code>getGraph</code> method and render the graph with the <code>drawMermaidPng</code> method.</p> <pre><code>import * as fs from \"node:fs/promises\";\n\nconst drawableGraph = await graph.getGraphAsync();\nconst image = await drawableGraph.drawMermaidPng();\nconst imageBuffer = new Uint8Array(await image.arrayBuffer());\n\nawait fs.writeFile(\"chatbot-with-tools.png\", imageBuffer);\n</code></pre> <p>:::</p> <p></p>"}, {"location": "tutorials/get-started/2-add-tools/#8-ask-the-bot-questions", "title": "8. Ask the bot questions", "text": "<p>Now you can ask the chatbot questions outside its training data:</p> <p>:::python</p> <pre><code>def stream_graph_updates(user_input: str):\n    for event in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": user_input}]}):\n        for value in event.values():\n            print(\"Assistant:\", value[\"messages\"][-1].content)\n\nwhile True:\n    try:\n        user_input = input(\"User: \")\n        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n            print(\"Goodbye!\")\n            break\n\n        stream_graph_updates(user_input)\n    except:\n        # fallback if input() is not available\n        user_input = \"What do you know about LangGraph?\"\n        print(\"User: \" + user_input)\n        stream_graph_updates(user_input)\n        break\n</code></pre> <pre><code>Assistant: [{'text': \"To provide you with accurate and up-to-date information about LangGraph, I'll need to search for the latest details. Let me do that for you.\", 'type': 'text'}, {'id': 'toolu_01Q588CszHaSvvP2MxRq9zRD', 'input': {'query': 'LangGraph AI tool information'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nAssistant: [{\"url\": \"https://www.langchain.com/langgraph\", \"content\": \"LangGraph sets the foundation for how we can build and scale AI workloads \\u2014 from conversational agents, complex task automation, to custom LLM-backed experiences that 'just work'. The next chapter in building complex production-ready features with LLMs is agentic, and with LangGraph and LangSmith, LangChain delivers an out-of-the-box solution ...\"}, {\"url\": \"https://github.com/langchain-ai/langgraph\", \"content\": \"Overview. LangGraph is a library for building stateful, multi-actor applications with LLMs, used to create agent and multi-agent workflows. Compared to other LLM frameworks, it offers these core benefits: cycles, controllability, and persistence. LangGraph allows you to define flows that involve cycles, essential for most agentic architectures ...\"}]\nAssistant: Based on the search results, I can provide you with information about LangGraph:\n\n1. Purpose:\n   LangGraph is a library designed for building stateful, multi-actor applications with Large Language Models (LLMs). It's particularly useful for creating agent and multi-agent workflows.\n\n2. Developer:\n   LangGraph is developed by LangChain, a company known for its tools and frameworks in the AI and LLM space.\n\n3. Key Features:\n   - Cycles: LangGraph allows the definition of flows that involve cycles, which is essential for most agentic architectures.\n   - Controllability: It offers enhanced control over the application flow.\n   - Persistence: The library provides ways to maintain state and persistence in LLM-based applications.\n\n4. Use Cases:\n   LangGraph can be used for various applications, including:\n   - Conversational agents\n   - Complex task automation\n   - Custom LLM-backed experiences\n\n5. Integration:\n   LangGraph works in conjunction with LangSmith, another tool by LangChain, to provide an out-of-the-box solution for building complex, production-ready features with LLMs.\n\n6. Significance:\n...\n   LangGraph is noted to offer unique benefits compared to other LLM frameworks, particularly in its ability to handle cycles, provide controllability, and maintain persistence.\n\nLangGraph appears to be a significant tool in the evolving landscape of LLM-based application development, offering developers new ways to create more complex, stateful, and interactive AI systems.\nGoodbye!\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import readline from \"node:readline/promises\";\n\nconst prompt = readline.createInterface({\n  input: process.stdin,\n  output: process.stdout,\n});\n\nasync function generateText(content: string) {\n  const stream = await graph.stream(\n    { messages: [{ type: \"human\", content }] },\n    { streamMode: \"values\" }\n  );\n\n  for await (const event of stream) {\n    const lastMessage = event.messages.at(-1);\n\n    if (lastMessage?.getType() === \"ai\" || lastMessage?.getType() === \"tool\") {\n      console.log(`Assistant: ${lastMessage?.text}`);\n    }\n  }\n}\n\nwhile (true) {\n  const human = await prompt.question(\"User: \");\n  if ([\"quit\", \"exit\", \"q\"].includes(human.trim())) break;\n  await generateText(human || \"What do you know about LangGraph?\");\n}\n\nprompt.close();\n</code></pre> <pre><code>User: What do you know about LangGraph?\nAssistant: I'll search for the latest information about LangGraph for you.\nAssistant: [{\"title\":\"Introduction to LangGraph: A Beginner's Guide - Medium\",\"url\":\"https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141\",\"content\":\"...\"}]\nAssistant: Based on the search results, I can provide you with information about LangGraph:\n\nLangGraph is a library within the LangChain ecosystem designed for building stateful, multi-actor applications with Large Language Models (LLMs). Here are the key aspects:\n\n**Core Purpose:**\n- LangGraph is specifically designed for creating agent and multi-agent workflows\n- It provides a framework for defining, coordinating, and executing multiple LLM agents in a structured manner\n\n**Key Features:**\n1. **Stateful Graph Architecture**: LangGraph revolves around a stateful graph where each node represents a step in computation, and the graph maintains state that is passed around and updated as the computation progresses\n\n2. **Conditional Edges**: It supports conditional edges, allowing you to dynamically determine the next node to execute based on the current state of the graph\n\n3. **Cycles**: Unlike other LLM frameworks, LangGraph allows you to define flows that involve cycles, which is essential for most agentic architectures\n\n4. **Controllability**: It offers enhanced control over the application flow\n\n5. **Persistence**: The library provides ways to maintain state and persistence in LLM-based applications\n\n**Use Cases:**\n- Conversational agents\n- Complex task automation\n- Custom LLM-backed experiences\n- Multi-agent systems that perform complex tasks\n\n**Benefits:**\nLangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination, making it easier to build complex, production-ready features with LLMs.\n\nThis makes LangGraph a significant tool in the evolving landscape of LLM-based application development.\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/2-add-tools/#9-use-prebuilts", "title": "9. Use prebuilts", "text": "<p>For ease of use, adjust your code to replace the following with LangGraph prebuilt components. These have built in functionality like parallel API execution.</p> <p>:::python</p> <ul> <li><code>BasicToolNode</code> is replaced with the prebuilt ToolNode</li> <li><code>route_tools</code> is replaced with the prebuilt tools_condition</li> </ul> OpenAIAnthropicAzureGoogle GeminiAWS Bedrock <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"openai:gpt-4.1\")\n</code></pre></p> <p>\ud83d\udc49 Read the OpenAI integration docs</p> <p><pre><code>pip install -U \"langchain[anthropic]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n</code></pre></p> <p>\ud83d\udc49 Read the Anthropic integration docs</p> <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\nllm = init_chat_model(\n    \"azure_openai:gpt-4.1\",\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n)\n</code></pre></p> <p>\ud83d\udc49 Read the Azure integration docs</p> <p><pre><code>pip install -U \"langchain[google-genai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n</code></pre></p> <p>\ud83d\udc49 Read the Google GenAI integration docs</p> <p><pre><code>pip install -U \"langchain[aws]\"\n</code></pre> <pre><code>from langchain.chat_models import init_chat_model\n\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\nllm = init_chat_model(\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    model_provider=\"bedrock_converse\",\n)\n</code></pre></p> <p>\ud83d\udc49 Read the AWS Bedrock integration docs</p> <pre><code>from typing import Annotated\n\nfrom langchain_tavily import TavilySearch\nfrom langchain_core.messages import BaseMessage\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\ngraph_builder = StateGraph(State)\n\ntool = TavilySearch(max_results=2)\ntools = [tool]\nllm_with_tools = llm.bind_tools(tools)\n\ndef chatbot(state: State):\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\ngraph_builder.add_node(\"chatbot\", chatbot)\n\ntool_node = ToolNode(tools=[tool])\ngraph_builder.add_node(\"tools\", tool_node)\n\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    tools_condition,\n)\n# Any time a tool is called, we return to the chatbot to decide the next step\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(START, \"chatbot\")\ngraph = graph_builder.compile()\n</code></pre> <p>:::</p> <p>:::js</p> <ul> <li><code>createToolNode</code> is replaced with the prebuilt ToolNode</li> <li><code>routeTools</code> is replaced with the prebuilt toolsCondition</li> </ul> <pre><code>import { TavilySearch } from \"@langchain/tavily\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { StateGraph, START, MessagesZodState, END } from \"@langchain/langgraph\";\nimport { ToolNode, toolsCondition } from \"@langchain/langgraph/prebuilt\";\nimport { z } from \"zod\";\n\nconst State = z.object({ messages: MessagesZodState.shape.messages });\n\nconst tools = [new TavilySearch({ maxResults: 2 })];\n\nconst llm = new ChatOpenAI({ model: \"gpt-4o-mini\" }).bindTools(tools);\n\nconst graph = new StateGraph(State)\n  .addNode(\"chatbot\", async (state) =&gt; ({\n    messages: [await llm.invoke(state.messages)],\n  }))\n  .addNode(\"tools\", new ToolNode(tools))\n  .addConditionalEdges(\"chatbot\", toolsCondition, [\"tools\", END])\n  .addEdge(\"tools\", \"chatbot\")\n  .addEdge(START, \"chatbot\")\n  .compile();\n</code></pre> <p>:::</p> <p>Congratulations! You've created a conversational agent in LangGraph that can use a search engine to retrieve updated information when needed. Now it can handle a wider range of user queries.</p> <p>:::python</p> <p>To inspect all the steps your agent just took, check out this LangSmith trace.</p> <p>:::</p>"}, {"location": "tutorials/get-started/2-add-tools/#next-steps", "title": "Next steps", "text": "<p>The chatbot cannot remember past interactions on its own, which limits its ability to have coherent, multi-turn conversations. In the next part, you will add memory to address this.</p>"}, {"location": "tutorials/get-started/3-add-memory/", "title": "Add memory", "text": "<p>The chatbot can now use tools to answer user questions, but it does not remember the context of previous interactions. This limits its ability to have coherent, multi-turn conversations.</p> <p>LangGraph solves this problem through persistent checkpointing. If you provide a <code>checkpointer</code> when compiling the graph and a <code>thread_id</code> when calling your graph, LangGraph automatically saves the state after each step. When you invoke the graph again using the same <code>thread_id</code>, the graph loads its saved state, allowing the chatbot to pick up where it left off.</p> <p>We will see later that checkpointing is much more powerful than simple chat memory - it lets you save and resume complex state at any time for error recovery, human-in-the-loop workflows, time travel interactions, and more. But first, let's add checkpointing to enable multi-turn conversations.</p> <p>Note</p> <p>This tutorial builds on Add tools.</p>"}, {"location": "tutorials/get-started/3-add-memory/#1-create-a-memorysaver-checkpointer", "title": "1. Create a <code>MemorySaver</code> checkpointer", "text": "<p>Create a <code>MemorySaver</code> checkpointer:</p> <p>:::python</p> <pre><code>from langgraph.checkpoint.memory import InMemorySaver\n\nmemory = InMemorySaver()\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { MemorySaver } from \"@langchain/langgraph\";\n\nconst memory = new MemorySaver();\n</code></pre> <p>:::</p> <p>This is in-memory checkpointer, which is convenient for the tutorial. However, in a production application, you would likely change this to use <code>SqliteSaver</code> or <code>PostgresSaver</code> and connect a database.</p>"}, {"location": "tutorials/get-started/3-add-memory/#2-compile-the-graph", "title": "2. Compile the graph", "text": "<p>Compile the graph with the provided checkpointer, which will checkpoint the <code>State</code> as the graph works through each node:</p> <p>:::python</p> <pre><code>graph = graph_builder.compile(checkpointer=memory)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const graph = new StateGraph(State)\n  .addNode(\"chatbot\", chatbot)\n  .addNode(\"tools\", new ToolNode(tools))\n  .addConditionalEdges(\"chatbot\", toolsCondition, [\"tools\", END])\n  .addEdge(\"tools\", \"chatbot\")\n  .addEdge(START, \"chatbot\")\n  .compile({ checkpointer: memory });\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/3-add-memory/#3-interact-with-your-chatbot", "title": "3. Interact with your chatbot", "text": "<p>Now you can interact with your bot!</p> <ol> <li> <p>Pick a thread to use as the key for this conversation.</p> <p>:::python</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"1\"}}\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const config = { configurable: { thread_id: \"1\" } };\n</code></pre> <p>:::</p> </li> <li> <p>Call your chatbot:</p> <p>:::python</p> <pre><code>user_input = \"Hi there! My name is Will.\"\n\n# The config is the **second positional argument** to stream() or invoke()!\nevents = graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    event[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================ Human Message =================================\n\nHi there! My name is Will.\n================================== Ai Message ==================================\n\nHello Will! It's nice to meet you. How can I assist you today? Is there anything specific you'd like to know or discuss?\n</code></pre> <p>Note</p> <p>The config was provided as the second positional argument when calling our graph. It importantly is not nested within the graph inputs (<code>{'messages': []}</code>).</p> <p>:::</p> <p>:::js</p> <pre><code>const userInput = \"Hi there! My name is Will.\";\n\nconst events = await graph.stream(\n  { messages: [{ type: \"human\", content: userInput }] },\n  { configurable: { thread_id: \"1\" }, streamMode: \"values\" }\n);\n\nfor await (const event of events) {\n  const lastMessage = event.messages.at(-1);\n  console.log(`${lastMessage?.getType()}: ${lastMessage?.text}`);\n}\n</code></pre> <pre><code>human: Hi there! My name is Will.\nai: Hello Will! It's nice to meet you. How can I assist you today? Is there anything specific you'd like to know or discuss?\n</code></pre> <p>Note</p> <p>Note</p> <p>The config was provided as the second parameter when calling our graph. It importantly is not nested within the graph inputs (<code>{\"messages\": []}</code>).</p> <p>:::</p> </li> </ol>"}, {"location": "tutorials/get-started/3-add-memory/#4-ask-a-follow-up-question", "title": "4. Ask a follow up question", "text": "<p>Ask a follow up question:</p> <p>:::python</p> <pre><code>user_input = \"Remember my name?\"\n\n# The config is the **second positional argument** to stream() or invoke()!\nevents = graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    event[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================ Human Message =================================\n\nRemember my name?\n================================== Ai Message ==================================\n\nOf course, I remember your name, Will. I always try to pay attention to important details that users share with me. Is there anything else you'd like to talk about or any questions you have? I'm here to help with a wide range of topics or tasks.\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const userInput2 = \"Remember my name?\";\n\nconst events2 = await graph.stream(\n  { messages: [{ type: \"human\", content: userInput2 }] },\n  { configurable: { thread_id: \"1\" }, streamMode: \"values\" }\n);\n\nfor await (const event of events2) {\n  const lastMessage = event.messages.at(-1);\n  console.log(`${lastMessage?.getType()}: ${lastMessage?.text}`);\n}\n</code></pre> <pre><code>human: Remember my name?\nai: Yes, your name is Will. How can I help you today?\n</code></pre> <p>:::</p> <p>Notice that we aren't using an external list for memory: it's all handled by the checkpointer! You can inspect the full execution in this LangSmith trace to see what's going on.</p> <p>Don't believe me? Try this using a different config.</p> <p>:::python</p> <pre><code># The only difference is we change the `thread_id` here to \"2\" instead of \"1\"\nevents = graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n    # highlight-next-line\n    {\"configurable\": {\"thread_id\": \"2\"}},\n    stream_mode=\"values\",\n)\nfor event in events:\n    event[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================ Human Message =================================\n\nRemember my name?\n================================== Ai Message ==================================\n\nI apologize, but I don't have any previous context or memory of your name. As an AI assistant, I don't retain information from past conversations. Each interaction starts fresh. Could you please tell me your name so I can address you properly in this conversation?\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const events3 = await graph.stream(\n  { messages: [{ type: \"human\", content: userInput2 }] },\n  // The only difference is we change the `thread_id` here to \"2\" instead of \"1\"\n  { configurable: { thread_id: \"2\" }, streamMode: \"values\" }\n);\n\nfor await (const event of events3) {\n  const lastMessage = event.messages.at(-1);\n  console.log(`${lastMessage?.getType()}: ${lastMessage?.text}`);\n}\n</code></pre> <pre><code>human: Remember my name?\nai: I don't have the ability to remember personal information about users between interactions. However, I'm here to help you with any questions or topics you want to discuss!\n</code></pre> <p>:::</p> <p>Notice that the only change we've made is to modify the <code>thread_id</code> in the config. See this call's LangSmith trace for comparison.</p>"}, {"location": "tutorials/get-started/3-add-memory/#5-inspect-the-state", "title": "5. Inspect the state", "text": "<p>:::python</p> <p>By now, we have made a few checkpoints across two different threads. But what goes into a checkpoint? To inspect a graph's <code>state</code> for a given config at any time, call <code>get_state(config)</code>.</p> <pre><code>snapshot = graph.get_state(config)\nsnapshot\n</code></pre> <pre><code>StateSnapshot(values={'messages': [HumanMessage(content='Hi there! My name is Will.', additional_kwargs={}, response_metadata={}, id='8c1ca919-c553-4ebf-95d4-b59a2d61e078'), AIMessage(content=\"Hello Will! It's nice to meet you. How can I assist you today? Is there anything specific you'd like to know or discuss?\", additional_kwargs={}, response_metadata={'id': 'msg_01WTQebPhNwmMrmmWojJ9KXJ', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 405, 'output_tokens': 32}}, id='run-58587b77-8c82-41e6-8a90-d62c444a261d-0', usage_metadata={'input_tokens': 405, 'output_tokens': 32, 'total_tokens': 437}), HumanMessage(content='Remember my name?', additional_kwargs={}, response_metadata={}, id='daba7df6-ad75-4d6b-8057-745881cea1ca'), AIMessage(content=\"Of course, I remember your name, Will. I always try to pay attention to important details that users share with me. Is there anything else you'd like to talk about or any questions you have? I'm here to help with a wide range of topics or tasks.\", additional_kwargs={}, response_metadata={'id': 'msg_01E41KitY74HpENRgXx94vag', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 444, 'output_tokens': 58}}, id='run-ffeaae5c-4d2d-4ddb-bd59-5d5cbf2a5af8-0', usage_metadata={'input_tokens': 444, 'output_tokens': 58, 'total_tokens': 502})]}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7d06e-93e0-6acc-8004-f2ac846575d2'}}, metadata={'source': 'loop', 'writes': {'chatbot': {'messages': [AIMessage(content=\"Of course, I remember your name, Will. I always try to pay attention to important details that users share with me. Is there anything else you'd like to talk about or any questions you have? I'm here to help with a wide range of topics or tasks.\", additional_kwargs={}, response_metadata={'id': 'msg_01E41KitY74HpENRgXx94vag', 'model': 'claude-3-5-sonnet-20240620', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'input_tokens': 444, 'output_tokens': 58}}, id='run-ffeaae5c-4d2d-4ddb-bd59-5d5cbf2a5af8-0', usage_metadata={'input_tokens': 444, 'output_tokens': 58, 'total_tokens': 502})]}}, 'step': 4, 'parents': {}}, created_at='2024-09-27T19:30:10.820758+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1ef7d06e-859f-6206-8003-e1bd3c264b8f'}}, tasks=())\n</code></pre> <pre><code>snapshot.next  # (since the graph ended this turn, `next` is empty. If you fetch a state from within a graph invocation, next tells which node will execute next)\n</code></pre> <p>:::</p> <p>:::js</p> <p>By now, we have made a few checkpoints across two different threads. But what goes into a checkpoint? To inspect a graph's <code>state</code> for a given config at any time, call <code>getState(config)</code>.</p> <pre><code>await graph.getState({ configurable: { thread_id: \"1\" } });\n</code></pre> <pre><code>{\n  values: {\n    messages: [\n      HumanMessage {\n        \"id\": \"32fabcef-b3b8-481f-8bcb-fd83399a5f8d\",\n        \"content\": \"Hi there! My name is Will.\",\n        \"additional_kwargs\": {},\n        \"response_metadata\": {}\n      },\n      AIMessage {\n        \"id\": \"chatcmpl-BrPbTsCJbVqBvXWySlYoTJvM75Kv8\",\n        \"content\": \"Hello Will! How can I assist you today?\",\n        \"additional_kwargs\": {},\n        \"response_metadata\": {},\n        \"tool_calls\": [],\n        \"invalid_tool_calls\": []\n      },\n      HumanMessage {\n        \"id\": \"561c3aad-f8fc-4fac-94a6-54269a220856\",\n        \"content\": \"Remember my name?\",\n        \"additional_kwargs\": {},\n        \"response_metadata\": {}\n      },\n      AIMessage {\n        \"id\": \"chatcmpl-BrPbU4BhhsUikGbW37hYuF5vvnnE2\",\n        \"content\": \"Yes, I remember your name, Will! How can I help you today?\",\n        \"additional_kwargs\": {},\n        \"response_metadata\": {},\n        \"tool_calls\": [],\n        \"invalid_tool_calls\": []\n      }\n    ]\n  },\n  next: [],\n  tasks: [],\n  metadata: {\n    source: 'loop',\n    step: 4,\n    parents: {},\n    thread_id: '1'\n  },\n  config: {\n    configurable: {\n      thread_id: '1',\n      checkpoint_id: '1f05cccc-9bb6-6270-8004-1d2108bcec77',\n      checkpoint_ns: ''\n    }\n  },\n  createdAt: '2025-07-09T13:58:27.607Z',\n  parentConfig: {\n    configurable: {\n      thread_id: '1',\n      checkpoint_ns: '',\n      checkpoint_id: '1f05cccc-78fa-68d0-8003-ffb01a76b599'\n    }\n  }\n}\n</code></pre> <pre><code>import * as assert from \"node:assert\";\n\n// Since the graph ended this turn, `next` is empty.\n// If you fetch a state from within a graph invocation, next tells which node will execute next)\nassert.deepEqual(snapshot.next, []);\n</code></pre> <p>:::</p> <p>The snapshot above contains the current state values, corresponding config, and the <code>next</code> node to process. In our case, the graph has reached an <code>END</code> state, so <code>next</code> is empty.</p> <p>Congratulations! Your chatbot can now maintain conversation state across sessions thanks to LangGraph's checkpointing system. This opens up exciting possibilities for more natural, contextual interactions. LangGraph's checkpointing even handles arbitrarily complex graph states, which is much more expressive and powerful than simple chat memory.</p> <p>Check out the code snippet below to review the graph from this tutorial:</p> <p>:::python</p> OpenAIAnthropicAzureGoogle GeminiAWS Bedrock <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"openai:gpt-4.1\")\n</code></pre></p> <p>\ud83d\udc49 Read the OpenAI integration docs</p> <p><pre><code>pip install -U \"langchain[anthropic]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n</code></pre></p> <p>\ud83d\udc49 Read the Anthropic integration docs</p> <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\nllm = init_chat_model(\n    \"azure_openai:gpt-4.1\",\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n)\n</code></pre></p> <p>\ud83d\udc49 Read the Azure integration docs</p> <p><pre><code>pip install -U \"langchain[google-genai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n</code></pre></p> <p>\ud83d\udc49 Read the Google GenAI integration docs</p> <p><pre><code>pip install -U \"langchain[aws]\"\n</code></pre> <pre><code>from langchain.chat_models import init_chat_model\n\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\nllm = init_chat_model(\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    model_provider=\"bedrock_converse\",\n)\n</code></pre></p> <p>\ud83d\udc49 Read the AWS Bedrock integration docs</p> <pre><code>from typing import Annotated\n\nfrom langchain.chat_models import init_chat_model\nfrom langchain_tavily import TavilySearch\nfrom langchain_core.messages import BaseMessage\nfrom typing_extensions import TypedDict\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph import StateGraph\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\ngraph_builder = StateGraph(State)\n\ntool = TavilySearch(max_results=2)\ntools = [tool]\nllm_with_tools = llm.bind_tools(tools)\n\ndef chatbot(state: State):\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\ngraph_builder.add_node(\"chatbot\", chatbot)\n\ntool_node = ToolNode(tools=[tool])\ngraph_builder.add_node(\"tools\", tool_node)\n\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    tools_condition,\n)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.set_entry_point(\"chatbot\")\nmemory = InMemorySaver()\ngraph = graph_builder.compile(checkpointer=memory)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { END, MessagesZodState, START } from \"@langchain/langgraph\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { TavilySearch } from \"@langchain/tavily\";\n\nimport { MemorySaver } from \"@langchain/langgraph\";\nimport { StateGraph } from \"@langchain/langgraph\";\nimport { ToolNode, toolsCondition } from \"@langchain/langgraph/prebuilt\";\nimport { z } from \"zod\";\n\nconst State = z.object({\n  messages: MessagesZodState.shape.messages,\n});\n\nconst tools = [new TavilySearch({ maxResults: 2 })];\nconst llm = new ChatOpenAI({ model: \"gpt-4o-mini\" }).bindTools(tools);\nconst memory = new MemorySaver();\n\nasync function generateText(content: string) {\n\nconst graph = new StateGraph(State)\n  .addNode(\"chatbot\", async (state) =&gt; ({\n    messages: [await llm.invoke(state.messages)],\n  }))\n  .addNode(\"tools\", new ToolNode(tools))\n  .addConditionalEdges(\"chatbot\", toolsCondition, [\"tools\", END])\n  .addEdge(\"tools\", \"chatbot\")\n  .addEdge(START, \"chatbot\")\n  .compile({ checkpointer: memory });\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/3-add-memory/#next-steps", "title": "Next steps", "text": "<p>In the next tutorial, you will add human-in-the-loop to the chatbot to handle situations where it may need guidance or verification before proceeding.</p>"}, {"location": "tutorials/get-started/4-human-in-the-loop/", "title": "Add human-in-the-loop controls", "text": "<p>Agents can be unreliable and may need human input to successfully accomplish tasks. Similarly, for some actions, you may want to require human approval before running to ensure that everything is running as intended.</p> <p>LangGraph's persistence layer supports human-in-the-loop workflows, allowing execution to pause and resume based on user feedback. The primary interface to this functionality is the <code>interrupt</code> function. Calling <code>interrupt</code> inside a node will pause execution. Execution can be resumed, together with new input from a human, by passing in a Command.</p> <p>:::python <code>interrupt</code> is ergonomically similar to Python's built-in <code>input()</code>, with some caveats. :::</p> <p>:::js <code>interrupt</code> is ergonomically similar to Node.js's built-in <code>readline.question()</code> function, with some caveats. <code>interrupt</code> is ergonomically similar to Node.js's built-in <code>readline.question()</code> function, with some caveats. :::</p> <p>Note</p> <p>This tutorial builds on Add memory.</p>"}, {"location": "tutorials/get-started/4-human-in-the-loop/#1-add-the-human_assistance-tool", "title": "1. Add the <code>human_assistance</code> tool", "text": "<p>Starting with the existing code from the Add memory to the chatbot tutorial, add the <code>human_assistance</code> tool to the chatbot. This tool uses <code>interrupt</code> to receive information from a human.</p> <p>Let's first select a chat model:</p> <p>:::python</p> OpenAIAnthropicAzureGoogle GeminiAWS Bedrock <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"openai:gpt-4.1\")\n</code></pre></p> <p>\ud83d\udc49 Read the OpenAI integration docs</p> <p><pre><code>pip install -U \"langchain[anthropic]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n</code></pre></p> <p>\ud83d\udc49 Read the Anthropic integration docs</p> <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\nllm = init_chat_model(\n    \"azure_openai:gpt-4.1\",\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n)\n</code></pre></p> <p>\ud83d\udc49 Read the Azure integration docs</p> <p><pre><code>pip install -U \"langchain[google-genai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n</code></pre></p> <p>\ud83d\udc49 Read the Google GenAI integration docs</p> <p><pre><code>pip install -U \"langchain[aws]\"\n</code></pre> <pre><code>from langchain.chat_models import init_chat_model\n\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\nllm = init_chat_model(\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    model_provider=\"bedrock_converse\",\n)\n</code></pre></p> <p>\ud83d\udc49 Read the AWS Bedrock integration docs</p> <p>:::</p> <p>:::js</p> <pre><code>// Add your API key here\nprocess.env.ANTHROPIC_API_KEY = \"YOUR_API_KEY\";\n</code></pre> <p>:::</p> <p>We can now incorporate it into our <code>StateGraph</code> with an additional tool:</p> <p>:::python</p> <pre><code>from typing import Annotated\n\nfrom langchain_tavily import TavilySearch\nfrom langchain_core.tools import tool\nfrom typing_extensions import TypedDict\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\nfrom langgraph.types import Command, interrupt\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\ngraph_builder = StateGraph(State)\n\n@tool\ndef human_assistance(query: str) -&gt; str:\n    \"\"\"Request assistance from a human.\"\"\"\n    human_response = interrupt({\"query\": query})\n    return human_response[\"data\"]\n\ntool = TavilySearch(max_results=2)\ntools = [tool, human_assistance]\nllm_with_tools = llm.bind_tools(tools)\n\ndef chatbot(state: State):\n    message = llm_with_tools.invoke(state[\"messages\"])\n    # Because we will be interrupting during tool execution,\n    # we disable parallel tool calling to avoid repeating any\n    # tool invocations when we resume.\n    assert len(message.tool_calls) &lt;= 1\n    return {\"messages\": [message]}\n\ngraph_builder.add_node(\"chatbot\", chatbot)\n\ntool_node = ToolNode(tools=tools)\ngraph_builder.add_node(\"tools\", tool_node)\n\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    tools_condition,\n)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(START, \"chatbot\")\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { interrupt, MessagesZodState } from \"@langchain/langgraph\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { TavilySearch } from \"@langchain/tavily\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst humanAssistance = tool(\n  async ({ query }) =&gt; {\n    const humanResponse = interrupt({ query });\n    return humanResponse.data;\n  },\n  {\n    name: \"humanAssistance\",\n    description: \"Request assistance from a human.\",\n    schema: z.object({\n      query: z.string().describe(\"Human readable question for the human\"),\n    }),\n  }\n);\n\nconst searchTool = new TavilySearch({ maxResults: 2 });\nconst searchTool = new TavilySearch({ maxResults: 2 });\nconst tools = [searchTool, humanAssistance];\n\nconst llmWithTools = new ChatAnthropic({\n  model: \"claude-3-5-sonnet-latest\",\n}).bindTools(tools);\nconst llmWithTools = new ChatAnthropic({\n  model: \"claude-3-5-sonnet-latest\",\n}).bindTools(tools);\n\nasync function chatbot(state: z.infer&lt;typeof MessagesZodState&gt;) {\nasync function chatbot(state: z.infer&lt;typeof MessagesZodState&gt;) {\n  const message = await llmWithTools.invoke(state.messages);\n\n\n  // Because we will be interrupting during tool execution,\n  // we disable parallel tool calling to avoid repeating any\n  // tool invocations when we resume.\n  if (message.tool_calls &amp;&amp; message.tool_calls.length &gt; 1) {\n    throw new Error(\"Multiple tool calls not supported with interrupts\");\n  }\n\n  return { messages: message };\n}\n</code></pre> <p>:::</p> <p>Tip</p> <p>For more information and examples of human-in-the-loop workflows, see Human-in-the-loop.</p>"}, {"location": "tutorials/get-started/4-human-in-the-loop/#2-compile-the-graph", "title": "2. Compile the graph", "text": "<p>We compile the graph with a checkpointer, as before:</p> <p>:::python</p> <pre><code>memory = InMemorySaver()\n\ngraph = graph_builder.compile(checkpointer=memory)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { StateGraph, MemorySaver, START, END } from \"@langchain/langgraph\";\n\nconst memory = new MemorySaver();\n\nconst graph = new StateGraph(MessagesZodState)\n  .addNode(\"chatbot\", chatbot)\n  .addNode(\"tools\", new ToolNode(tools))\n  .addConditionalEdges(\"chatbot\", toolsCondition, [\"tools\", END])\n  .addEdge(\"tools\", \"chatbot\")\n  .addEdge(START, \"chatbot\")\n  .compile({ checkpointer: memory });\nconst graph = new StateGraph(MessagesZodState)\n  .addNode(\"chatbot\", chatbot)\n  .addNode(\"tools\", new ToolNode(tools))\n  .addConditionalEdges(\"chatbot\", toolsCondition, [\"tools\", END])\n  .addEdge(\"tools\", \"chatbot\")\n  .addEdge(START, \"chatbot\")\n  .compile({ checkpointer: memory });\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/4-human-in-the-loop/#3-visualize-the-graph-optional", "title": "3. Visualize the graph (optional)", "text": "<p>Visualizing the graph, you get the same layout as before \u2013 just with the added tool!</p> <p>:::python</p> <pre><code>from IPython.display import Image, display\n\ntry:\n    display(Image(graph.get_graph().draw_mermaid_png()))\nexcept Exception:\n    # This requires some extra dependencies and is optional\n    pass\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import * as fs from \"node:fs/promises\";\nimport * as fs from \"node:fs/promises\";\n\nconst drawableGraph = await graph.getGraphAsync();\nconst drawableGraph = await graph.getGraphAsync();\nconst image = await drawableGraph.drawMermaidPng();\nconst imageBuffer = new Uint8Array(await image.arrayBuffer());\nconst imageBuffer = new Uint8Array(await image.arrayBuffer());\n\nawait fs.writeFile(\"chatbot-with-tools.png\", imageBuffer);\nawait fs.writeFile(\"chatbot-with-tools.png\", imageBuffer);\n</code></pre> <p>:::</p> <p></p>"}, {"location": "tutorials/get-started/4-human-in-the-loop/#4-prompt-the-chatbot", "title": "4. Prompt the chatbot", "text": "<p>Now, prompt the chatbot with a question that will engage the new <code>human_assistance</code> tool:</p> <p>:::python</p> <pre><code>user_input = \"I need some expert guidance for building an AI agent. Could you request assistance for me?\"\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nevents = graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================ Human Message =================================\n\nI need some expert guidance for building an AI agent. Could you request assistance for me?\n================================== Ai Message ==================================\n\n[{'text': \"Certainly! I'd be happy to request expert assistance for you regarding building an AI agent. To do this, I'll use the human_assistance function to relay your request. Let me do that for you now.\", 'type': 'text'}, {'id': 'toolu_01ABUqneqnuHNuo1vhfDFQCW', 'input': {'query': 'A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic?'}, 'name': 'human_assistance', 'type': 'tool_use'}]\nTool Calls:\n  human_assistance (toolu_01ABUqneqnuHNuo1vhfDFQCW)\n Call ID: toolu_01ABUqneqnuHNuo1vhfDFQCW\n  Args:\n    query: A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic?\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { isAIMessage } from \"@langchain/core/messages\";\n\nconst userInput =\n  \"I need some expert guidance for building an AI agent. Could you request assistance for me?\";\n\nconst events = await graph.stream(\n  { messages: [{ role: \"user\", content: userInput }] },\n  { configurable: { thread_id: \"1\" }, streamMode: \"values\" }\n  { configurable: { thread_id: \"1\" }, streamMode: \"values\" }\n);\n\nfor await (const event of events) {\n  if (\"messages\" in event) {\n    const lastMessage = event.messages.at(-1);\n    console.log(`[${lastMessage?.getType()}]: ${lastMessage?.text}`);\n\n    if (\n      lastMessage &amp;&amp;\n      isAIMessage(lastMessage) &amp;&amp;\n      lastMessage.tool_calls?.length\n    ) {\n    const lastMessage = event.messages.at(-1);\n    console.log(`[${lastMessage?.getType()}]: ${lastMessage?.text}`);\n\n    if (\n      lastMessage &amp;&amp;\n      isAIMessage(lastMessage) &amp;&amp;\n      lastMessage.tool_calls?.length\n    ) {\n      console.log(\"Tool calls:\", lastMessage.tool_calls);\n    }\n  }\n}\n</code></pre> <pre><code>[human]: I need some expert guidance for building an AI agent. Could you request assistance for me?\n[ai]: I'll help you request human assistance for guidance on building an AI agent.\n[ai]: I'll help you request human assistance for guidance on building an AI agent.\nTool calls: [\n  {\n    name: 'humanAssistance',\n    args: {\n      query: 'I would like expert guidance on building an AI agent. Could you please provide assistance with this topic?'\n      query: 'I would like expert guidance on building an AI agent. Could you please provide assistance with this topic?'\n    },\n    id: 'toolu_01Bpxc8rFVMhSaRosS6b85Ts',\n    type: 'tool_call'\n    id: 'toolu_01Bpxc8rFVMhSaRosS6b85Ts',\n    type: 'tool_call'\n  }\n]\n</code></pre> <p>:::</p> <p>The chatbot generated a tool call, but then execution has been interrupted. If you inspect the graph state, you see that it stopped at the tools node:</p> <p>:::python</p> <pre><code>snapshot = graph.get_state(config)\nsnapshot.next\n</code></pre> <pre><code>('tools',)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const snapshot = await graph.getState({ configurable: { thread_id: \"1\" } });\nsnapshot.next;\nconst snapshot = await graph.getState({ configurable: { thread_id: \"1\" } });\nsnapshot.next;\n</code></pre> <pre><code>[\"tools\"]\n</code></pre> <p>:::</p> <p>Info</p> <p>:::python</p> <p>Take a closer look at the <code>human_assistance</code> tool:</p> <pre><code>@tool\ndef human_assistance(query: str) -&gt; str:\n    \"\"\"Request assistance from a human.\"\"\"\n    human_response = interrupt({\"query\": query})\n    return human_response[\"data\"]\n</code></pre> <p>Similar to Python's built-in <code>input()</code> function, calling <code>interrupt</code> inside the tool will pause execution. Progress is persisted based on the checkpointer; so if it is persisting with Postgres, it can resume at any time as long as the database is alive. In this example, it is persisting with the in-memory checkpointer and can resume any time if the Python kernel is running. :::</p> <p>:::js</p> <p>Take a closer look at the <code>humanAssistance</code> tool:</p> <pre><code>const humanAssistance = tool(\n  async ({ query }) =&gt; {\n    const humanResponse = interrupt({ query });\n    return humanResponse.data;\n  },\n  {\n    name: \"humanAssistance\",\n    description: \"Request assistance from a human.\",\n    schema: z.object({\n      query: z.string().describe(\"Human readable question for the human\"),\n    }),\n  },\n);\n\nTake a closer look at the `humanAssistance` tool:\n\n```typescript hl_lines=\"3\"\nconst humanAssistance = tool(\n  async ({ query }) =&gt; {\n    const humanResponse = interrupt({ query });\n    return humanResponse.data;\n  },\n  {\n    name: \"humanAssistance\",\n    description: \"Request assistance from a human.\",\n    schema: z.object({\n      query: z.string().describe(\"Human readable question for the human\"),\n    }),\n  },\n);\n</code></pre> <p>Calling <code>interrupt</code> inside the tool will pause execution. Progress is persisted based on the checkpointer; so if it is persisting with Postgres, it can resume at any time as long as the database is alive. In this example, it is persisting with the in-memory checkpointer and can resume any time if the JavaScript runtime is running. :::</p>"}, {"location": "tutorials/get-started/4-human-in-the-loop/#5-resume-execution", "title": "5. Resume execution", "text": "<p>To resume execution, pass a <code>Command</code> object containing data expected by the tool. The format of this data can be customized based on needs.</p> <p>:::python</p> <p>For this example, use a dict with a key <code>\"data\"</code>:</p> <pre><code>human_response = (\n    \"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\"\n    \" It's much more reliable and extensible than simple autonomous agents.\"\n)\n\nhuman_command = Command(resume={\"data\": human_response})\n\nevents = graph.stream(human_command, config, stream_mode=\"values\")\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================== Ai Message ==================================\n\n[{'text': \"Certainly! I'd be happy to request expert assistance for you regarding building an AI agent. To do this, I'll use the human_assistance function to relay your request. Let me do that for you now.\", 'type': 'text'}, {'id': 'toolu_01ABUqneqnuHNuo1vhfDFQCW', 'input': {'query': 'A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic?'}, 'name': 'human_assistance', 'type': 'tool_use'}]\nTool Calls:\n  human_assistance (toolu_01ABUqneqnuHNuo1vhfDFQCW)\n Call ID: toolu_01ABUqneqnuHNuo1vhfDFQCW\n  Args:\n    query: A user is requesting expert guidance for building an AI agent. Could you please provide some expert advice or resources on this topic?\n================================= Tool Message =================================\nName: human_assistance\n\nWe, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents.\n================================== Ai Message ==================================\n\nThank you for your patience. I've received some expert advice regarding your request for guidance on building an AI agent. Here's what the experts have suggested:\n\nThe experts recommend that you look into LangGraph for building your AI agent. They mention that LangGraph is a more reliable and extensible option compared to simple autonomous agents.\n\nLangGraph is likely a framework or library designed specifically for creating AI agents with advanced capabilities. Here are a few points to consider based on this recommendation:\n\n1. Reliability: The experts emphasize that LangGraph is more reliable than simpler autonomous agent approaches. This could mean it has better stability, error handling, or consistent performance.\n\n2. Extensibility: LangGraph is described as more extensible, which suggests that it probably offers a flexible architecture that allows you to easily add new features or modify existing ones as your agent's requirements evolve.\n\n3. Advanced capabilities: Given that it's recommended over \"simple autonomous agents,\" LangGraph likely provides more sophisticated tools and techniques for building complex AI agents.\n...\n2. Look for tutorials or guides specifically focused on building AI agents with LangGraph.\n3. Check if there are any community forums or discussion groups where you can ask questions and get support from other developers using LangGraph.\n\nIf you'd like more specific information about LangGraph or have any questions about this recommendation, please feel free to ask, and I can request further assistance from the experts.\nOutput is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n</code></pre> <p>:::</p> <p>:::js For this example, use an object with a key <code>\"data\"</code>:</p> <pre><code>import { Command } from \"@langchain/langgraph\";\n\nconst humanResponse =\n  \"We, the experts are here to help! We'd recommend you check out LangGraph to build your agent.\" +\n  \" It's much more reliable and extensible than simple autonomous agents.\";\n(\" It's much more reliable and extensible than simple autonomous agents.\");\n\nconst humanCommand = new Command({ resume: { data: humanResponse } });\n\nconst resumeEvents = await graph.stream(humanCommand, {\n  configurable: { thread_id: \"1\" },\n  streamMode: \"values\",\n});\nconst resumeEvents = await graph.stream(humanCommand, {\n  configurable: { thread_id: \"1\" },\n  streamMode: \"values\",\n});\n\nfor await (const event of resumeEvents) {\n  if (\"messages\" in event) {\n    const lastMessage = event.messages.at(-1);\n    console.log(`[${lastMessage?.getType()}]: ${lastMessage?.text}`);\n    const lastMessage = event.messages.at(-1);\n    console.log(`[${lastMessage?.getType()}]: ${lastMessage?.text}`);\n  }\n}\n</code></pre> <pre><code>[tool]: We, the experts are here to help! We'd recommend you check out LangGraph to build your agent. It's much more reliable and extensible than simple autonomous agents.\n[ai]: Thank you for your patience. I've received some expert advice regarding your request for guidance on building an AI agent. Here's what the experts have suggested:\n\nThe experts recommend that you look into LangGraph for building your AI agent. They mention that LangGraph is a more reliable and extensible option compared to simple autonomous agents.\n\nLangGraph is likely a framework or library designed specifically for creating AI agents with advanced capabilities. Here are a few points to consider based on this recommendation:\n\n1. Reliability: The experts emphasize that LangGraph is more reliable than simpler autonomous agent approaches. This could mean it has better stability, error handling, or consistent performance.\n\n2. Extensibility: LangGraph is described as more extensible, which suggests that it probably offers a flexible architecture that allows you to easily add new features or modify existing ones as your agent's requirements evolve.\n\n3. Advanced capabilities: Given that it's recommended over \"simple autonomous agents,\" LangGraph likely provides more sophisticated tools and techniques for building complex AI agents.\n\n...\n</code></pre> <p>:::</p> <p>The input has been received and processed as a tool message. Review this call's LangSmith trace to see the exact work that was done in the above call. Notice that the state is loaded in the first step so that our chatbot can continue where it left off.</p> <p>Congratulations! You've used an <code>interrupt</code> to add human-in-the-loop execution to your chatbot, allowing for human oversight and intervention when needed. This opens up the potential UIs you can create with your AI systems. Since you have already added a checkpointer, as long as the underlying persistence layer is running, the graph can be paused indefinitely and resumed at any time as if nothing had happened.</p> <p>Check out the code snippet below to review the graph from this tutorial:</p> <p>:::python</p> OpenAIAnthropicAzureGoogle GeminiAWS Bedrock <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"openai:gpt-4.1\")\n</code></pre></p> <p>\ud83d\udc49 Read the OpenAI integration docs</p> <p><pre><code>pip install -U \"langchain[anthropic]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n</code></pre></p> <p>\ud83d\udc49 Read the Anthropic integration docs</p> <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\nllm = init_chat_model(\n    \"azure_openai:gpt-4.1\",\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n)\n</code></pre></p> <p>\ud83d\udc49 Read the Azure integration docs</p> <p><pre><code>pip install -U \"langchain[google-genai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n</code></pre></p> <p>\ud83d\udc49 Read the Google GenAI integration docs</p> <p><pre><code>pip install -U \"langchain[aws]\"\n</code></pre> <pre><code>from langchain.chat_models import init_chat_model\n\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\nllm = init_chat_model(\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    model_provider=\"bedrock_converse\",\n)\n</code></pre></p> <p>\ud83d\udc49 Read the AWS Bedrock integration docs</p> <pre><code>from typing import Annotated\n\nfrom langchain_tavily import TavilySearch\nfrom langchain_core.tools import tool\nfrom typing_extensions import TypedDict\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langgraph.types import Command, interrupt\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\ngraph_builder = StateGraph(State)\n\n@tool\ndef human_assistance(query: str) -&gt; str:\n    \"\"\"Request assistance from a human.\"\"\"\n    human_response = interrupt({\"query\": query})\n    return human_response[\"data\"]\n\ntool = TavilySearch(max_results=2)\ntools = [tool, human_assistance]\nllm_with_tools = llm.bind_tools(tools)\n\ndef chatbot(state: State):\n    message = llm_with_tools.invoke(state[\"messages\"])\n    assert(len(message.tool_calls) &lt;= 1)\n    return {\"messages\": [message]}\n\ngraph_builder.add_node(\"chatbot\", chatbot)\n\ntool_node = ToolNode(tools=tools)\ngraph_builder.add_node(\"tools\", tool_node)\n\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    tools_condition,\n)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(START, \"chatbot\")\n\nmemory = InMemorySaver()\ngraph = graph_builder.compile(checkpointer=memory)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import {\n  interrupt,\n  MessagesZodState,\n  StateGraph,\n  MemorySaver,\n  START,\n  END,\n} from \"@langchain/langgraph\";\nimport { ToolNode, toolsCondition } from \"@langchain/langgraph/prebuilt\";\nimport { isAIMessage } from \"@langchain/core/messages\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { TavilySearch } from \"@langchain/tavily\";\nimport {\n  interrupt,\n  MessagesZodState,\n  StateGraph,\n  MemorySaver,\n  START,\n  END,\n} from \"@langchain/langgraph\";\nimport { ToolNode, toolsCondition } from \"@langchain/langgraph/prebuilt\";\nimport { isAIMessage } from \"@langchain/core/messages\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { TavilySearch } from \"@langchain/tavily\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst humanAssistance = tool(\n  async ({ query }) =&gt; {\n    const humanResponse = interrupt({ query });\n    return humanResponse.data;\n  },\n  {\n    name: \"humanAssistance\",\n    description: \"Request assistance from a human.\",\n    schema: z.object({\n      query: z.string().describe(\"Human readable question for the human\"),\n    }),\n  }\n);\nconst humanAssistance = tool(\n  async ({ query }) =&gt; {\n    const humanResponse = interrupt({ query });\n    return humanResponse.data;\n  },\n  {\n    name: \"humanAssistance\",\n    description: \"Request assistance from a human.\",\n    schema: z.object({\n      query: z.string().describe(\"Human readable question for the human\"),\n    }),\n  }\n);\n\nconst searchTool = new TavilySearch({ maxResults: 2 });\nconst searchTool = new TavilySearch({ maxResults: 2 });\nconst tools = [searchTool, humanAssistance];\n\nconst llmWithTools = new ChatAnthropic({\n  model: \"claude-3-5-sonnet-latest\",\n}).bindTools(tools);\nconst llmWithTools = new ChatAnthropic({\n  model: \"claude-3-5-sonnet-latest\",\n}).bindTools(tools);\n\nconst chatbot = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\nconst chatbot = async (state: z.infer&lt;typeof MessagesZodState&gt;) =&gt; {\n  const message = await llmWithTools.invoke(state.messages);\n\n  // Because we will be interrupting during tool execution,\n  // we disable parallel tool calling to avoid repeating any\n  // tool invocations when we resume.\n\n  // Because we will be interrupting during tool execution,\n  // we disable parallel tool calling to avoid repeating any\n  // tool invocations when we resume.\n  if (message.tool_calls &amp;&amp; message.tool_calls.length &gt; 1) {\n    throw new Error(\"Multiple tool calls not supported with interrupts\");\n  }\n\n  return { messages: message };\n\n  return { messages: message };\n};\n\nconst memory = new MemorySaver();\n\nconst graph = new StateGraph(MessagesZodState)\n  .addNode(\"chatbot\", chatbot)\n  .addNode(\"tools\", new ToolNode(tools))\n  .addConditionalEdges(\"chatbot\", toolsCondition, [\"tools\", END])\n  .addEdge(\"tools\", \"chatbot\")\n  .addEdge(START, \"chatbot\")\n  .compile({ checkpointer: memory });\n\nconst graph = new StateGraph(MessagesZodState)\n  .addNode(\"chatbot\", chatbot)\n  .addNode(\"tools\", new ToolNode(tools))\n  .addConditionalEdges(\"chatbot\", toolsCondition, [\"tools\", END])\n  .addEdge(\"tools\", \"chatbot\")\n  .addEdge(START, \"chatbot\")\n  .compile({ checkpointer: memory });\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/4-human-in-the-loop/#next-steps", "title": "Next steps", "text": "<p>So far, the tutorial examples have relied on a simple state with one entry: a list of messages. You can go far with this simple state, but if you want to define complex behavior without relying on the message list, you can add additional fields to the state.</p>"}, {"location": "tutorials/get-started/5-customize-state/", "title": "Customize state", "text": "<p>In this tutorial, you will add additional fields to the state to define complex behavior without relying on the message list. The chatbot will use its search tool to find specific information and forward them to a human for review.</p> <p>Note</p> <p>This tutorial builds on Add human-in-the-loop controls.</p>"}, {"location": "tutorials/get-started/5-customize-state/#1-add-keys-to-the-state", "title": "1. Add keys to the state", "text": "<p>Update the chatbot to research the birthday of an entity by adding <code>name</code> and <code>birthday</code> keys to the state:</p> <p>:::python</p> <pre><code>from typing import Annotated\n\nfrom typing_extensions import TypedDict\n\nfrom langgraph.graph.message import add_messages\n\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n    # highlight-next-line\n    name: str\n    # highlight-next-line\n    birthday: str\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { MessagesZodState } from \"@langchain/langgraph\";\nimport { z } from \"zod\";\n\nconst State = z.object({\n  messages: MessagesZodState.shape.messages,\n  // highlight-next-line\n  name: z.string(),\n  // highlight-next-line\n  birthday: z.string(),\n});\n</code></pre> <p>:::</p> <p>Adding this information to the state makes it easily accessible by other graph nodes (like a downstream node that stores or processes the information), as well as the graph's persistence layer.</p>"}, {"location": "tutorials/get-started/5-customize-state/#2-update-the-state-inside-the-tool", "title": "2. Update the state inside the tool", "text": "<p>:::python</p> <p>Now, populate the state keys inside of the <code>human_assistance</code> tool. This allows a human to review the information before it is stored in the state. Use <code>Command</code> to issue a state update from inside the tool.</p> <pre><code>from langchain_core.messages import ToolMessage\nfrom langchain_core.tools import InjectedToolCallId, tool\n\nfrom langgraph.types import Command, interrupt\n\n@tool\n# Note that because we are generating a ToolMessage for a state update, we\n# generally require the ID of the corresponding tool call. We can use\n# LangChain's InjectedToolCallId to signal that this argument should not\n# be revealed to the model in the tool's schema.\ndef human_assistance(\n    name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId]\n) -&gt; str:\n    \"\"\"Request assistance from a human.\"\"\"\n    human_response = interrupt(\n        {\n            \"question\": \"Is this correct?\",\n            \"name\": name,\n            \"birthday\": birthday,\n        },\n    )\n    # If the information is correct, update the state as-is.\n    if human_response.get(\"correct\", \"\").lower().startswith(\"y\"):\n        verified_name = name\n        verified_birthday = birthday\n        response = \"Correct\"\n    # Otherwise, receive information from the human reviewer.\n    else:\n        verified_name = human_response.get(\"name\", name)\n        verified_birthday = human_response.get(\"birthday\", birthday)\n        response = f\"Made a correction: {human_response}\"\n\n    # This time we explicitly update the state with a ToolMessage inside\n    # the tool.\n    state_update = {\n        \"name\": verified_name,\n        \"birthday\": verified_birthday,\n        \"messages\": [ToolMessage(response, tool_call_id=tool_call_id)],\n    }\n    # We return a Command object in the tool to update our state.\n    return Command(update=state_update)\n</code></pre> <p>:::</p> <p>:::js</p> <p>Now, populate the state keys inside of the <code>humanAssistance</code> tool. This allows a human to review the information before it is stored in the state. Use <code>Command</code> to issue a state update from inside the tool.</p> <pre><code>import { tool } from \"@langchain/core/tools\";\nimport { ToolMessage } from \"@langchain/core/messages\";\nimport { Command, interrupt } from \"@langchain/langgraph\";\n\nconst humanAssistance = tool(\n  async (input, config) =&gt; {\n    // Note that because we are generating a ToolMessage for a state update,\n    // we generally require the ID of the corresponding tool call.\n    // This is available in the tool's config.\n    const toolCallId = config?.toolCall?.id as string | undefined;\n    if (!toolCallId) throw new Error(\"Tool call ID is required\");\n\n    const humanResponse = await interrupt({\n      question: \"Is this correct?\",\n      name: input.name,\n      birthday: input.birthday,\n    });\n\n    // We explicitly update the state with a ToolMessage inside the tool.\n    const stateUpdate = (() =&gt; {\n      // If the information is correct, update the state as-is.\n      if (humanResponse.correct?.toLowerCase().startsWith(\"y\")) {\n        return {\n          name: input.name,\n          birthday: input.birthday,\n          messages: [\n            new ToolMessage({ content: \"Correct\", tool_call_id: toolCallId }),\n          ],\n        };\n      }\n\n      // Otherwise, receive information from the human reviewer.\n      return {\n        name: humanResponse.name || input.name,\n        birthday: humanResponse.birthday || input.birthday,\n        messages: [\n          new ToolMessage({\n            content: `Made a correction: ${JSON.stringify(humanResponse)}`,\n            tool_call_id: toolCallId,\n          }),\n        ],\n      };\n    })();\n\n    // We return a Command object in the tool to update our state.\n    return new Command({ update: stateUpdate });\n  },\n  {\n    name: \"humanAssistance\",\n    description: \"Request assistance from a human.\",\n    schema: z.object({\n      name: z.string().describe(\"The name of the entity\"),\n      birthday: z.string().describe(\"The birthday/release date of the entity\"),\n    }),\n  }\n);\n</code></pre> <p>:::</p> <p>The rest of the graph stays the same.</p>"}, {"location": "tutorials/get-started/5-customize-state/#3-prompt-the-chatbot", "title": "3. Prompt the chatbot", "text": "<p>:::python Prompt the chatbot to look up the \"birthday\" of the LangGraph library and direct the chatbot to reach out to the <code>human_assistance</code> tool once it has the required information. By setting <code>name</code> and <code>birthday</code> in the arguments for the tool, you force the chatbot to generate proposals for these fields.</p> <pre><code>user_input = (\n    \"Can you look up when LangGraph was released? \"\n    \"When you have the answer, use the human_assistance tool for review.\"\n)\nconfig = {\"configurable\": {\"thread_id\": \"1\"}}\n\nevents = graph.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": user_input}]},\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</code></pre> <p>:::</p> <p>:::js Prompt the chatbot to look up the \"birthday\" of the LangGraph library and direct the chatbot to reach out to the <code>humanAssistance</code> tool once it has the required information. By setting <code>name</code> and <code>birthday</code> in the arguments for the tool, you force the chatbot to generate proposals for these fields.</p> <pre><code>import { isAIMessage } from \"@langchain/core/messages\";\n\nconst userInput =\n  \"Can you look up when LangGraph was released? \" +\n  \"When you have the answer, use the humanAssistance tool for review.\";\n\nconst events = await graph.stream(\n  { messages: [{ role: \"user\", content: userInput }] },\n  { configurable: { thread_id: \"1\" }, streamMode: \"values\" }\n);\n\nfor await (const event of events) {\n  if (\"messages\" in event) {\n    const lastMessage = event.messages.at(-1);\n\n    console.log(\n      \"=\".repeat(32),\n      `${lastMessage?.getType()} Message`,\n      \"=\".repeat(32)\n    );\n    console.log(lastMessage?.text);\n\n    if (\n      lastMessage &amp;&amp;\n      isAIMessage(lastMessage) &amp;&amp;\n      lastMessage.tool_calls?.length\n    ) {\n      console.log(\"Tool Calls:\");\n      for (const call of lastMessage.tool_calls) {\n        console.log(`  ${call.name} (${call.id})`);\n        console.log(`  Args: ${JSON.stringify(call.args)}`);\n      }\n    }\n  }\n}\n</code></pre> <p>:::</p> <pre><code>================================ Human Message =================================\n\nCan you look up when LangGraph was released? When you have the answer, use the human_assistance tool for review.\n================================== Ai Message ==================================\n\n[{'text': \"Certainly! I'll start by searching for information about LangGraph's release date using the Tavily search function. Then, I'll use the human_assistance tool for review.\", 'type': 'text'}, {'id': 'toolu_01JoXQPgTVJXiuma8xMVwqAi', 'input': {'query': 'LangGraph release date'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\n  tavily_search_results_json (toolu_01JoXQPgTVJXiuma8xMVwqAi)\n Call ID: toolu_01JoXQPgTVJXiuma8xMVwqAi\n  Args:\n    query: LangGraph release date\n================================= Tool Message =================================\nName: tavily_search_results_json\n\n[{\"url\": \"https://blog.langchain.dev/langgraph-cloud/\", \"content\": \"We also have a new stable release of LangGraph. By LangChain 6 min read Jun 27, 2024 (Oct '24) Edit: Since the launch of LangGraph Platform, we now have multiple deployment options alongside LangGraph Studio - which now fall under LangGraph Platform. LangGraph Platform is synonymous with our Cloud SaaS deployment option.\"}, {\"url\": \"https://changelog.langchain.com/announcements/langgraph-cloud-deploy-at-scale-monitor-carefully-iterate-boldly\", \"content\": \"LangChain - Changelog | \u2601 \ud83d\ude80 LangGraph Platform: Deploy at scale, monitor LangChain LangSmith LangGraph LangChain LangSmith LangGraph LangChain LangSmith LangGraph LangChain Changelog Sign up for our newsletter to stay up to date DATE: The LangChain Team LangGraph LangGraph Platform \u2601 \ud83d\ude80 LangGraph Platform: Deploy at scale, monitor carefully, iterate boldly DATE: June 27, 2024 AUTHOR: The LangChain Team LangGraph Platform is now in closed beta, offering scalable, fault-tolerant deployment for LangGraph agents. LangGraph Platform also includes a new playground-like studio for debugging agent failure modes and quick iteration: Join the waitlist today for LangGraph Platform. And to learn more, read our blog post announcement or check out our docs. Subscribe By clicking subscribe, you accept our privacy policy and terms and conditions.\"}]\n================================== Ai Message ==================================\n\n[{'text': \"Based on the search results, it appears that LangGraph was already in existence before June 27, 2024, when LangGraph Platform was announced. However, the search results don't provide a specific release date for the original LangGraph. \\n\\nGiven this information, I'll use the human_assistance tool to review and potentially provide more accurate information about LangGraph's initial release date.\", 'type': 'text'}, {'id': 'toolu_01JDQAV7nPqMkHHhNs3j3XoN', 'input': {'name': 'Assistant', 'birthday': '2023-01-01'}, 'name': 'human_assistance', 'type': 'tool_use'}]\nTool Calls:\n  human_assistance (toolu_01JDQAV7nPqMkHHhNs3j3XoN)\n Call ID: toolu_01JDQAV7nPqMkHHhNs3j3XoN\n  Args:\n    name: Assistant\n    birthday: 2023-01-01\n</code></pre> <p>:::python We've hit the <code>interrupt</code> in the <code>human_assistance</code> tool again. :::</p> <p>:::js We've hit the <code>interrupt</code> in the <code>humanAssistance</code> tool again. :::</p>"}, {"location": "tutorials/get-started/5-customize-state/#4-add-human-assistance", "title": "4. Add human assistance", "text": "<p>The chatbot failed to identify the correct date, so supply it with information:</p> <p>:::python</p> <pre><code>human_command = Command(\n    resume={\n        \"name\": \"LangGraph\",\n        \"birthday\": \"Jan 17, 2024\",\n    },\n)\n\nevents = graph.stream(human_command, config, stream_mode=\"values\")\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { Command } from \"@langchain/langgraph\";\n\nconst humanCommand = new Command({\n  resume: {\n    name: \"LangGraph\",\n    birthday: \"Jan 17, 2024\",\n  },\n});\n\nconst resumeEvents = await graph.stream(humanCommand, {\n  configurable: { thread_id: \"1\" },\n  streamMode: \"values\",\n});\n\nfor await (const event of resumeEvents) {\n  if (\"messages\" in event) {\n    const lastMessage = event.messages.at(-1);\n\n    console.log(\n      \"=\".repeat(32),\n      `${lastMessage?.getType()} Message`,\n      \"=\".repeat(32)\n    );\n    console.log(lastMessage?.text);\n\n    if (\n      lastMessage &amp;&amp;\n      isAIMessage(lastMessage) &amp;&amp;\n      lastMessage.tool_calls?.length\n    ) {\n      console.log(\"Tool Calls:\");\n      for (const call of lastMessage.tool_calls) {\n        console.log(`  ${call.name} (${call.id})`);\n        console.log(`  Args: ${JSON.stringify(call.args)}`);\n      }\n    }\n  }\n}\n</code></pre> <p>:::</p> <pre><code>================================== Ai Message ==================================\n\n[{'text': \"Based on the search results, it appears that LangGraph was already in existence before June 27, 2024, when LangGraph Platform was announced. However, the search results don't provide a specific release date for the original LangGraph. \\n\\nGiven this information, I'll use the human_assistance tool to review and potentially provide more accurate information about LangGraph's initial release date.\", 'type': 'text'}, {'id': 'toolu_01JDQAV7nPqMkHHhNs3j3XoN', 'input': {'name': 'Assistant', 'birthday': '2023-01-01'}, 'name': 'human_assistance', 'type': 'tool_use'}]\nTool Calls:\n  human_assistance (toolu_01JDQAV7nPqMkHHhNs3j3XoN)\n Call ID: toolu_01JDQAV7nPqMkHHhNs3j3XoN\n  Args:\n    name: Assistant\n    birthday: 2023-01-01\n================================= Tool Message =================================\nName: human_assistance\n\nMade a correction: {'name': 'LangGraph', 'birthday': 'Jan 17, 2024'}\n================================== Ai Message ==================================\n\nThank you for the human assistance. I can now provide you with the correct information about LangGraph's release date.\n\nLangGraph was initially released on January 17, 2024. This information comes from the human assistance correction, which is more accurate than the search results I initially found.\n\nTo summarize:\n1. LangGraph's original release date: January 17, 2024\n2. LangGraph Platform announcement: June 27, 2024\n\nIt's worth noting that LangGraph had been in development and use for some time before the LangGraph Platform announcement, but the official initial release of LangGraph itself was on January 17, 2024.\n</code></pre> <p>Note that these fields are now reflected in the state:</p> <p>:::python</p> <pre><code>snapshot = graph.get_state(config)\n\n{k: v for k, v in snapshot.values.items() if k in (\"name\", \"birthday\")}\n</code></pre> <pre><code>{'name': 'LangGraph', 'birthday': 'Jan 17, 2024'}\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>const snapshot = await graph.getState(config);\n\nconst relevantState = Object.fromEntries(\n  Object.entries(snapshot.values).filter(([k]) =&gt;\n    [\"name\", \"birthday\"].includes(k)\n  )\n);\n</code></pre> <pre><code>{ name: 'LangGraph', birthday: 'Jan 17, 2024' }\n</code></pre> <p>:::</p> <p>This makes them easily accessible to downstream nodes (e.g., a node that further processes or stores the information).</p>"}, {"location": "tutorials/get-started/5-customize-state/#5-manually-update-the-state", "title": "5. Manually update the state", "text": "<p>:::python LangGraph gives a high degree of control over the application state. For instance, at any point (including when interrupted), you can manually override a key using <code>graph.update_state</code>:</p> <pre><code>graph.update_state(config, {\"name\": \"LangGraph (library)\"})\n</code></pre> <pre><code>{'configurable': {'thread_id': '1',\n  'checkpoint_ns': '',\n  'checkpoint_id': '1efd4ec5-cf69-6352-8006-9278f1730162'}}\n</code></pre> <p>:::</p> <p>:::js LangGraph gives a high degree of control over the application state. For instance, at any point (including when interrupted), you can manually override a key using <code>graph.updateState</code>:</p> <pre><code>await graph.updateState(\n  { configurable: { thread_id: \"1\" } },\n  { name: \"LangGraph (library)\" }\n);\n</code></pre> <pre><code>{\n  configurable: {\n    thread_id: '1',\n    checkpoint_ns: '',\n    checkpoint_id: '1efd4ec5-cf69-6352-8006-9278f1730162'\n  }\n}\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/5-customize-state/#6-view-the-new-value", "title": "6. View the new value", "text": "<p>:::python If you call <code>graph.get_state</code>, you can see the new value is reflected:</p> <pre><code>snapshot = graph.get_state(config)\n\n{k: v for k, v in snapshot.values.items() if k in (\"name\", \"birthday\")}\n</code></pre> <pre><code>{'name': 'LangGraph (library)', 'birthday': 'Jan 17, 2024'}\n</code></pre> <p>:::</p> <p>:::js If you call <code>graph.getState</code>, you can see the new value is reflected:</p> <pre><code>const updatedSnapshot = await graph.getState(config);\n\nconst updatedRelevantState = Object.fromEntries(\n  Object.entries(updatedSnapshot.values).filter(([k]) =&gt;\n    [\"name\", \"birthday\"].includes(k)\n  )\n);\n</code></pre> <pre><code>{ name: 'LangGraph (library)', birthday: 'Jan 17, 2024' }\n</code></pre> <p>:::</p> <p>Manual state updates will generate a trace in LangSmith. If desired, they can also be used to control human-in-the-loop workflows. Use of the <code>interrupt</code> function is generally recommended instead, as it allows data to be transmitted in a human-in-the-loop interaction independently of state updates.</p> <p>Congratulations! You've added custom keys to the state to facilitate a more complex workflow, and learned how to generate state updates from inside tools.</p> <p>Check out the code snippet below to review the graph from this tutorial:</p> <p>:::python</p> OpenAIAnthropicAzureGoogle GeminiAWS Bedrock <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"openai:gpt-4.1\")\n</code></pre></p> <p>\ud83d\udc49 Read the OpenAI integration docs</p> <p><pre><code>pip install -U \"langchain[anthropic]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n</code></pre></p> <p>\ud83d\udc49 Read the Anthropic integration docs</p> <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\nllm = init_chat_model(\n    \"azure_openai:gpt-4.1\",\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n)\n</code></pre></p> <p>\ud83d\udc49 Read the Azure integration docs</p> <p><pre><code>pip install -U \"langchain[google-genai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n</code></pre></p> <p>\ud83d\udc49 Read the Google GenAI integration docs</p> <p><pre><code>pip install -U \"langchain[aws]\"\n</code></pre> <pre><code>from langchain.chat_models import init_chat_model\n\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\nllm = init_chat_model(\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    model_provider=\"bedrock_converse\",\n)\n</code></pre></p> <p>\ud83d\udc49 Read the AWS Bedrock integration docs</p> <pre><code>from typing import Annotated\n\nfrom langchain_tavily import TavilySearch\nfrom langchain_core.messages import ToolMessage\nfrom langchain_core.tools import InjectedToolCallId, tool\nfrom typing_extensions import TypedDict\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\nfrom langgraph.types import Command, interrupt\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n    name: str\n    birthday: str\n\n@tool\ndef human_assistance(\n    name: str, birthday: str, tool_call_id: Annotated[str, InjectedToolCallId]\n) -&gt; str:\n    \"\"\"Request assistance from a human.\"\"\"\n    human_response = interrupt(\n        {\n            \"question\": \"Is this correct?\",\n            \"name\": name,\n            \"birthday\": birthday,\n        },\n    )\n    if human_response.get(\"correct\", \"\").lower().startswith(\"y\"):\n        verified_name = name\n        verified_birthday = birthday\n        response = \"Correct\"\n    else:\n        verified_name = human_response.get(\"name\", name)\n        verified_birthday = human_response.get(\"birthday\", birthday)\n        response = f\"Made a correction: {human_response}\"\n\n    state_update = {\n        \"name\": verified_name,\n        \"birthday\": verified_birthday,\n        \"messages\": [ToolMessage(response, tool_call_id=tool_call_id)],\n    }\n    return Command(update=state_update)\n\n\ntool = TavilySearch(max_results=2)\ntools = [tool, human_assistance]\nllm_with_tools = llm.bind_tools(tools)\n\ndef chatbot(state: State):\n    message = llm_with_tools.invoke(state[\"messages\"])\n    assert(len(message.tool_calls) &lt;= 1)\n    return {\"messages\": [message]}\n\ngraph_builder = StateGraph(State)\ngraph_builder.add_node(\"chatbot\", chatbot)\n\ntool_node = ToolNode(tools=tools)\ngraph_builder.add_node(\"tools\", tool_node)\n\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    tools_condition,\n)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(START, \"chatbot\")\n\nmemory = InMemorySaver()\ngraph = graph_builder.compile(checkpointer=memory)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import {\n  Command,\n  interrupt,\n  MessagesZodState,\n  MemorySaver,\n  StateGraph,\n  END,\n  START,\n} from \"@langchain/langgraph\";\nimport { ToolNode, toolsCondition } from \"@langchain/langgraph/prebuilt\";\nimport { ChatAnthropic } from \"@langchain/anthropic\";\nimport { TavilySearch } from \"@langchain/tavily\";\nimport { ToolMessage } from \"@langchain/core/messages\";\nimport { tool } from \"@langchain/core/tools\";\nimport { z } from \"zod\";\n\nconst State = z.object({\n  messages: MessagesZodState.shape.messages,\n  name: z.string(),\n  birthday: z.string(),\n});\n\nconst humanAssistance = tool(\n  async (input, config) =&gt; {\n    // Note that because we are generating a ToolMessage for a state update, we\n    // generally require the ID of the corresponding tool call. This is available\n    // in the tool's config.\n    const toolCallId = config?.toolCall?.id as string | undefined;\n    if (!toolCallId) throw new Error(\"Tool call ID is required\");\n\n    const humanResponse = await interrupt({\n      question: \"Is this correct?\",\n      name: input.name,\n      birthday: input.birthday,\n    });\n\n    // We explicitly update the state with a ToolMessage inside the tool.\n    const stateUpdate = (() =&gt; {\n      // If the information is correct, update the state as-is.\n      if (humanResponse.correct?.toLowerCase().startsWith(\"y\")) {\n        return {\n          name: input.name,\n          birthday: input.birthday,\n          messages: [\n            new ToolMessage({ content: \"Correct\", tool_call_id: toolCallId }),\n          ],\n        };\n      }\n\n      // Otherwise, receive information from the human reviewer.\n      return {\n        name: humanResponse.name || input.name,\n        birthday: humanResponse.birthday || input.birthday,\n        messages: [\n          new ToolMessage({\n            content: `Made a correction: ${JSON.stringify(humanResponse)}`,\n            tool_call_id: toolCallId,\n          }),\n        ],\n      };\n    })();\n\n    // We return a Command object in the tool to update our state.\n    return new Command({ update: stateUpdate });\n  },\n  {\n    name: \"humanAssistance\",\n    description: \"Request assistance from a human.\",\n    schema: z.object({\n      name: z.string().describe(\"The name of the entity\"),\n      birthday: z.string().describe(\"The birthday/release date of the entity\"),\n    }),\n  }\n);\n\nconst searchTool = new TavilySearch({ maxResults: 2 });\n\nconst tools = [searchTool, humanAssistance];\nconst llmWithTools = new ChatAnthropic({\n  model: \"claude-3-5-sonnet-latest\",\n}).bindTools(tools);\n\nconst memory = new MemorySaver();\n\nconst chatbot = async (state: z.infer&lt;typeof State&gt;) =&gt; {\n  const message = await llmWithTools.invoke(state.messages);\n  return { messages: message };\n};\n\nconst graph = new StateGraph(State)\n  .addNode(\"chatbot\", chatbot)\n  .addNode(\"tools\", new ToolNode(tools))\n  .addConditionalEdges(\"chatbot\", toolsCondition, [\"tools\", END])\n  .addEdge(\"tools\", \"chatbot\")\n  .addEdge(START, \"chatbot\")\n  .compile({ checkpointer: memory });\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/5-customize-state/#next-steps", "title": "Next steps", "text": "<p>There's one more concept to review before finishing the LangGraph basics tutorials: connecting <code>checkpointing</code> and <code>state updates</code> to time travel.</p>"}, {"location": "tutorials/get-started/6-time-travel/", "title": "Time travel", "text": "<p>In a typical chatbot workflow, the user interacts with the bot one or more times to accomplish a task. Memory and a human-in-the-loop enable checkpoints in the graph state and control future responses.</p> <p>What if you want a user to be able to start from a previous response and explore a different outcome? Or what if you want users to be able to rewind your chatbot's work to fix mistakes or try a different strategy, something that is common in applications like autonomous software engineers?</p> <p>You can create these types of experiences using LangGraph's built-in time travel functionality.</p> <p>Note</p> <p>This tutorial builds on Customize state.</p>"}, {"location": "tutorials/get-started/6-time-travel/#1-rewind-your-graph", "title": "1. Rewind your graph", "text": "<p>:::python Rewind your graph by fetching a checkpoint using the graph's <code>get_state_history</code> method. You can then resume execution at this previous point in time. :::</p> <p>:::js Rewind your graph by fetching a checkpoint using the graph's <code>getStateHistory</code> method. You can then resume execution at this previous point in time. :::</p> <p>:::python</p> OpenAIAnthropicAzureGoogle GeminiAWS Bedrock <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"openai:gpt-4.1\")\n</code></pre></p> <p>\ud83d\udc49 Read the OpenAI integration docs</p> <p><pre><code>pip install -U \"langchain[anthropic]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"ANTHROPIC_API_KEY\"] = \"sk-...\"\n\nllm = init_chat_model(\"anthropic:claude-3-5-sonnet-latest\")\n</code></pre></p> <p>\ud83d\udc49 Read the Anthropic integration docs</p> <p><pre><code>pip install -U \"langchain[openai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"AZURE_OPENAI_API_KEY\"] = \"...\"\nos.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"...\"\nos.environ[\"OPENAI_API_VERSION\"] = \"2025-03-01-preview\"\n\nllm = init_chat_model(\n    \"azure_openai:gpt-4.1\",\n    azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n)\n</code></pre></p> <p>\ud83d\udc49 Read the Azure integration docs</p> <p><pre><code>pip install -U \"langchain[google-genai]\"\n</code></pre> <pre><code>import os\nfrom langchain.chat_models import init_chat_model\n\nos.environ[\"GOOGLE_API_KEY\"] = \"...\"\n\nllm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n</code></pre></p> <p>\ud83d\udc49 Read the Google GenAI integration docs</p> <p><pre><code>pip install -U \"langchain[aws]\"\n</code></pre> <pre><code>from langchain.chat_models import init_chat_model\n\n# Follow the steps here to configure your credentials:\n# https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html\n\nllm = init_chat_model(\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    model_provider=\"bedrock_converse\",\n)\n</code></pre></p> <p>\ud83d\udc49 Read the AWS Bedrock integration docs</p> <pre><code>from typing import Annotated\n\nfrom langchain_tavily import TavilySearch\nfrom langchain_core.messages import BaseMessage\nfrom typing_extensions import TypedDict\n\nfrom langgraph.checkpoint.memory import InMemorySaver\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langgraph.prebuilt import ToolNode, tools_condition\n\nclass State(TypedDict):\n    messages: Annotated[list, add_messages]\n\ngraph_builder = StateGraph(State)\n\ntool = TavilySearch(max_results=2)\ntools = [tool]\nllm_with_tools = llm.bind_tools(tools)\n\ndef chatbot(state: State):\n    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n\ngraph_builder.add_node(\"chatbot\", chatbot)\n\ntool_node = ToolNode(tools=[tool])\ngraph_builder.add_node(\"tools\", tool_node)\n\ngraph_builder.add_conditional_edges(\n    \"chatbot\",\n    tools_condition,\n)\ngraph_builder.add_edge(\"tools\", \"chatbot\")\ngraph_builder.add_edge(START, \"chatbot\")\n\nmemory = InMemorySaver()\ngraph = graph_builder.compile(checkpointer=memory)\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import {\n  StateGraph,\n  START,\n  END,\n  MessagesZodState,\n  MemorySaver,\n} from \"@langchain/langgraph\";\nimport { ToolNode, toolsCondition } from \"@langchain/langgraph/prebuilt\";\nimport { TavilySearch } from \"@langchain/tavily\";\nimport { ChatOpenAI } from \"@langchain/openai\";\nimport { z } from \"zod\";\n\nconst State = z.object({ messages: MessagesZodState.shape.messages });\n\nconst tools = [new TavilySearch({ maxResults: 2 })];\nconst llmWithTools = new ChatOpenAI({ model: \"gpt-4o-mini\" }).bindTools(tools);\nconst memory = new MemorySaver();\n\nconst graph = new StateGraph(State)\n  .addNode(\"chatbot\", async (state) =&gt; ({\n    messages: [await llmWithTools.invoke(state.messages)],\n  }))\n  .addNode(\"tools\", new ToolNode(tools))\n  .addConditionalEdges(\"chatbot\", toolsCondition, [\"tools\", END])\n  .addEdge(\"tools\", \"chatbot\")\n  .addEdge(START, \"chatbot\")\n  .compile({ checkpointer: memory });\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/6-time-travel/#2-add-steps", "title": "2. Add steps", "text": "<p>Add steps to your graph. Every step will be checkpointed in its state history:</p> <p>:::python</p> <pre><code>config = {\"configurable\": {\"thread_id\": \"1\"}}\nevents = graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": (\n                    \"I'm learning LangGraph. \"\n                    \"Could you do some research on it for me?\"\n                ),\n            },\n        ],\n    },\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================ Human Message =================================\n\nI'm learning LangGraph. Could you do some research on it for me?\n================================== Ai Message ==================================\n\n[{'text': \"Certainly! I'd be happy to research LangGraph for you. To get the most up-to-date and accurate information, I'll use the Tavily search engine to look this up. Let me do that for you now.\", 'type': 'text'}, {'id': 'toolu_01BscbfJJB9EWJFqGrN6E54e', 'input': {'query': 'LangGraph latest information and features'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\n  tavily_search_results_json (toolu_01BscbfJJB9EWJFqGrN6E54e)\n Call ID: toolu_01BscbfJJB9EWJFqGrN6E54e\n  Args:\n    query: LangGraph latest information and features\n================================= Tool Message =================================\nName: tavily_search_results_json\n\n[{\"url\": \"https://blockchain.news/news/langchain-new-features-upcoming-events-update\", \"content\": \"LangChain, a leading platform in the AI development space, has released its latest updates, showcasing new use cases and enhancements across its ecosystem. According to the LangChain Blog, the updates cover advancements in LangGraph Platform, LangSmith's self-improving evaluators, and revamped documentation for LangGraph.\"}, {\"url\": \"https://blog.langchain.dev/langgraph-platform-announce/\", \"content\": \"With these learnings under our belt, we decided to couple some of our latest offerings under LangGraph Platform. LangGraph Platform today includes LangGraph Server, LangGraph Studio, plus the CLI and SDK. ... we added features in LangGraph Server to deliver on a few key value areas. Below, we'll focus on these aspects of LangGraph Platform.\"}]\n================================== Ai Message ==================================\n\nThank you for your patience. I've found some recent information about LangGraph for you. Let me summarize the key points:\n\n1. LangGraph is part of the LangChain ecosystem, which is a leading platform in AI development.\n\n2. Recent updates and features of LangGraph include:\n\n   a. LangGraph Platform: This seems to be a cloud-based version of LangGraph, though specific details weren't provided in the search results.\n...\n3. Keep an eye on LangGraph Platform developments, as cloud-based solutions often provide an easier starting point for learners.\n4. Consider how LangGraph fits into the broader LangChain ecosystem, especially its interaction with tools like LangSmith.\n\nIs there any specific aspect of LangGraph you'd like to know more about? I'd be happy to do a more focused search on particular features or use cases.\nOutput is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n</code></pre> <pre><code>events = graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": (\n                    \"Ya that's helpful. Maybe I'll \"\n                    \"build an autonomous agent with it!\"\n                ),\n            },\n        ],\n    },\n    config,\n    stream_mode=\"values\",\n)\nfor event in events:\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================ Human Message =================================\n\nYa that's helpful. Maybe I'll build an autonomous agent with it!\n================================== Ai Message ==================================\n\n[{'text': \"That's an exciting idea! Building an autonomous agent with LangGraph is indeed a great application of this technology. LangGraph is particularly well-suited for creating complex, multi-step AI workflows, which is perfect for autonomous agents. Let me gather some more specific information about using LangGraph for building autonomous agents.\", 'type': 'text'}, {'id': 'toolu_01QWNHhUaeeWcGXvA4eHT7Zo', 'input': {'query': 'Building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\n  tavily_search_results_json (toolu_01QWNHhUaeeWcGXvA4eHT7Zo)\n Call ID: toolu_01QWNHhUaeeWcGXvA4eHT7Zo\n  Args:\n    query: Building autonomous agents with LangGraph examples and tutorials\n================================= Tool Message =================================\nName: tavily_search_results_json\n\n[{\"url\": \"https://towardsdatascience.com/building-autonomous-multi-tool-agents-with-gemini-2-0-and-langgraph-ad3d7bd5e79d\", \"content\": \"Building Autonomous Multi-Tool Agents with Gemini 2.0 and LangGraph | by Youness Mansar | Jan, 2025 | Towards Data Science Building Autonomous Multi-Tool Agents with Gemini 2.0 and LangGraph A practical tutorial with full code examples for building and running multi-tool agents Towards Data Science LLMs are remarkable \u2014 they can memorize vast amounts of information, answer general knowledge questions, write code, generate stories, and even fix your grammar. In this tutorial, we are going to build a simple LLM agent that is equipped with four tools that it can use to answer a user's question. This Agent will have the following specifications: Follow Published in Towards Data Science --------------------------------- Your home for data science and AI. Follow Follow Follow\"}, {\"url\": \"https://github.com/anmolaman20/Tools_and_Agents\", \"content\": \"GitHub - anmolaman20/Tools_and_Agents: This repository provides resources for building AI agents using Langchain and Langgraph. This repository provides resources for building AI agents using Langchain and Langgraph. This repository provides resources for building AI agents using Langchain and Langgraph. This repository serves as a comprehensive guide for building AI-powered agents using Langchain and Langgraph. It provides hands-on examples, practical tutorials, and resources for developers and AI enthusiasts to master building intelligent systems and workflows. AI Agent Development: Gain insights into creating intelligent systems that think, reason, and adapt in real time. This repository is ideal for AI practitioners, developers exploring language models, or anyone interested in building intelligent systems. This repository provides resources for building AI agents using Langchain and Langgraph.\"}]\n================================== Ai Message ==================================\n\nGreat idea! Building an autonomous agent with LangGraph is definitely an exciting project. Based on the latest information I've found, here are some insights and tips for building autonomous agents with LangGraph:\n\n1. Multi-Tool Agents: LangGraph is particularly well-suited for creating autonomous agents that can use multiple tools. This allows your agent to have a diverse set of capabilities and choose the right tool for each task.\n\n2. Integration with Large Language Models (LLMs): You can combine LangGraph with powerful LLMs like Gemini 2.0 to create more intelligent and capable agents. The LLM can serve as the \"brain\" of your agent, making decisions and generating responses.\n\n3. Workflow Management: LangGraph excels at managing complex, multi-step AI workflows. This is crucial for autonomous agents that need to break down tasks into smaller steps and execute them in the right order.\n...\n6. Pay attention to how you structure the agent's decision-making process and workflow.\n7. Don't forget to implement proper error handling and safety measures, especially if your agent will be interacting with external systems or making important decisions.\n\nBuilding an autonomous agent is an iterative process, so be prepared to refine and improve your agent over time. Good luck with your project! If you need any more specific information as you progress, feel free to ask.\nOutput is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import { randomUUID } from \"node:crypto\";\nconst threadId = randomUUID();\n\nlet iter = 0;\n\nfor (const userInput of [\n  \"I'm learning LangGraph. Could you do some research on it for me?\",\n  \"Ya that's helpful. Maybe I'll build an autonomous agent with it!\",\n]) {\n  iter += 1;\n\n  console.log(`\\n--- Conversation Turn ${iter} ---\\n`);\n  const events = await graph.stream(\n    { messages: [{ role: \"user\", content: userInput }] },\n    { configurable: { thread_id: threadId }, streamMode: \"values\" }\n  );\n\n  for await (const event of events) {\n    if (\"messages\" in event) {\n      const lastMessage = event.messages.at(-1);\n\n      console.log(\n        \"=\".repeat(32),\n        `${lastMessage?.getType()} Message`,\n        \"=\".repeat(32)\n      );\n      console.log(lastMessage?.text);\n    }\n  }\n}\n</code></pre> <pre><code>--- Conversation Turn 1 ---\n\n================================ human Message ================================\nI'm learning LangGraph.js. Could you do some research on it for me?\n================================ ai Message ================================\nI'll search for information about LangGraph.js for you.\n================================ tool Message ================================\n{\n  \"query\": \"LangGraph.js framework TypeScript langchain what is it tutorial guide\",\n  \"follow_up_questions\": null,\n  \"answer\": null,\n  \"images\": [],\n  \"results\": [\n    {\n      \"url\": \"https://techcommunity.microsoft.com/blog/educatordeveloperblog/an-absolute-beginners-guide-to-langgraph-js/4212496\",\n      \"title\": \"An Absolute Beginner's Guide to LangGraph.js\",\n      \"content\": \"(...)\",\n      \"score\": 0.79369855,\n      \"raw_content\": null\n    },\n    {\n      \"url\": \"https://langchain-ai.github.io/langgraphjs/\",\n      \"title\": \"LangGraph.js\",\n      \"content\": \"(...)\",\n      \"score\": 0.78154784,\n      \"raw_content\": null\n    }\n  ],\n  \"response_time\": 2.37\n}\n================================ ai Message ================================\nLet me provide you with an overview of LangGraph.js based on the search results:\n\nLangGraph.js is a JavaScript/TypeScript library that's part of the LangChain ecosystem, specifically designed for creating and managing complex LLM (Large Language Model) based workflows. Here are the key points about LangGraph.js:\n\n1. Purpose:\n- It's a low-level orchestration framework for building controllable agents\n- Particularly useful for creating agentic workflows where LLMs decide the course of action based on current state\n- Helps model workflows as graphs with nodes and edges\n\n(...)\n\n--- Conversation Turn 2 ---\n\n================================ human Message ================================\nYa that's helpful. Maybe I'll build an autonomous agent with it!\n================================ ai Message ================================\nLet me search for specific information about building autonomous agents with LangGraph.js.\n================================ tool Message ================================\n{\n  \"query\": \"how to build autonomous agents with LangGraph.js examples tutorial react agent\",\n  \"follow_up_questions\": null,\n  \"answer\": null,\n  \"images\": [],\n  \"results\": [\n    {\n      \"url\": \"https://ai.google.dev/gemini-api/docs/langgraph-example\",\n      \"title\": \"ReAct agent from scratch with Gemini 2.5 and LangGraph\",\n      \"content\": \"(...)\",\n      \"score\": 0.7602419,\n      \"raw_content\": null\n    },\n    {\n      \"url\": \"https://www.youtube.com/watch?v=ZfjaIshGkmk\",\n      \"title\": \"Build Autonomous AI Agents with ReAct and LangGraph Tools\",\n      \"content\": \"(...)\",\n      \"score\": 0.7471924,\n      \"raw_content\": null\n    }\n  ],\n  \"response_time\": 1.98\n}\n================================ ai Message ================================\nBased on the search results, I can provide you with a practical overview of how to build an autonomous agent with LangGraph.js. Here's what you need to know:\n\n1. Basic Structure for Building an Agent:\n- LangGraph.js provides a ReAct (Reason + Act) pattern implementation\n- The basic components include:\n  - State management for conversation history\n  - Nodes for different actions\n  - Edges for decision-making flow\n  - Tools for specific functionalities\n\n(...)\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/6-time-travel/#3-replay-the-full-state-history", "title": "3. Replay the full state history", "text": "<p>Now that you have added steps to the chatbot, you can <code>replay</code> the full state history to see everything that occurred.</p> <p>:::python</p> <pre><code>to_replay = None\nfor state in graph.get_state_history(config):\n    print(\"Num Messages: \", len(state.values[\"messages\"]), \"Next: \", state.next)\n    print(\"-\" * 80)\n    if len(state.values[\"messages\"]) == 6:\n        # We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.\n        to_replay = state\n</code></pre> <pre><code>Num Messages:  8 Next:  ()\n--------------------------------------------------------------------------------\nNum Messages:  7 Next:  ('chatbot',)\n--------------------------------------------------------------------------------\nNum Messages:  6 Next:  ('tools',)\n--------------------------------------------------------------------------------\nNum Messages:  5 Next:  ('chatbot',)\n--------------------------------------------------------------------------------\nNum Messages:  4 Next:  ('__start__',)\n--------------------------------------------------------------------------------\nNum Messages:  4 Next:  ()\n--------------------------------------------------------------------------------\nNum Messages:  3 Next:  ('chatbot',)\n--------------------------------------------------------------------------------\nNum Messages:  2 Next:  ('tools',)\n--------------------------------------------------------------------------------\nNum Messages:  1 Next:  ('chatbot',)\n--------------------------------------------------------------------------------\nNum Messages:  0 Next:  ('__start__',)\n--------------------------------------------------------------------------------\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>import type { StateSnapshot } from \"@langchain/langgraph\";\n\nlet toReplay: StateSnapshot | undefined;\nfor await (const state of graph.getStateHistory({\n  configurable: { thread_id: threadId },\n})) {\n  console.log(\n    `Num Messages: ${state.values.messages.length}, Next: ${JSON.stringify(\n      state.next\n    )}`\n  );\n  console.log(\"-\".repeat(80));\n  if (state.values.messages.length === 6) {\n    // We are somewhat arbitrarily selecting a specific state based on the number of chat messages in the state.\n    toReplay = state;\n  }\n}\n</code></pre> <pre><code>Num Messages: 8 Next:  []\n--------------------------------------------------------------------------------\nNum Messages: 7 Next:  [\"chatbot\"]\n--------------------------------------------------------------------------------\nNum Messages: 6 Next:  [\"tools\"]\n--------------------------------------------------------------------------------\nNum Messages: 7, Next: [\"chatbot\"]\n--------------------------------------------------------------------------------\nNum Messages: 6, Next: [\"tools\"]\n--------------------------------------------------------------------------------\nNum Messages: 5, Next: [\"chatbot\"]\n--------------------------------------------------------------------------------\nNum Messages: 4, Next: [\"__start__\"]\n--------------------------------------------------------------------------------\nNum Messages: 4, Next: []\n--------------------------------------------------------------------------------\nNum Messages: 3, Next: [\"chatbot\"]\n--------------------------------------------------------------------------------\nNum Messages: 2, Next: [\"tools\"]\n--------------------------------------------------------------------------------\nNum Messages: 1, Next: [\"chatbot\"]\n--------------------------------------------------------------------------------\nNum Messages: 0, Next: [\"__start__\"]\n--------------------------------------------------------------------------------\n</code></pre> <p>:::</p> <p>Checkpoints are saved for every step of the graph. This spans invocations so you can rewind across a full thread's history.</p>"}, {"location": "tutorials/get-started/6-time-travel/#resume-from-a-checkpoint", "title": "Resume from a checkpoint", "text": "<p>:::python</p> <p>Resume from the <code>to_replay</code> state, which is after the <code>chatbot</code> node in the second graph invocation. Resuming from this point will call the action node next. :::</p> <p>:::js Resume from the <code>toReplay</code> state, which is after a specific node in one of the graph invocations. Resuming from this point will call the next scheduled node. :::</p> <p>:::python</p> <pre><code>print(to_replay.next)\nprint(to_replay.config)\n</code></pre> <pre><code>('tools',)\n{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1efd43e3-0c1f-6c4e-8006-891877d65740'}}\n</code></pre> <p>:::</p> <p>:::js</p> <p>Resume from the <code>toReplay</code> state, which is after the <code>chatbot</code> node in one of the graph invocations. Resuming from this point will call the next scheduled node.</p> <pre><code>console.log(toReplay.next);\nconsole.log(toReplay.config);\n</code></pre> <pre><code>[\"tools\"]\n{\n  configurable: {\n    thread_id: \"007708b8-ea9b-4ff7-a7ad-3843364dbf75\",\n    checkpoint_ns: \"\",\n    checkpoint_id: \"1efd43e3-0c1f-6c4e-8006-891877d65740\"\n  }\n}\n</code></pre> <p>:::</p>"}, {"location": "tutorials/get-started/6-time-travel/#4-load-a-state-from-a-moment-in-time", "title": "4. Load a state from a moment-in-time", "text": "<p>:::python</p> <p>The checkpoint's <code>to_replay.config</code> contains a <code>checkpoint_id</code> timestamp. Providing this <code>checkpoint_id</code> value tells LangGraph's checkpointer to load the state from that moment in time.</p> <pre><code># The `checkpoint_id` in the `to_replay.config` corresponds to a state we've persisted to our checkpointer.\nfor event in graph.stream(None, to_replay.config, stream_mode=\"values\"):\n    if \"messages\" in event:\n        event[\"messages\"][-1].pretty_print()\n</code></pre> <pre><code>================================== Ai Message ==================================\n\n[{'text': \"That's an exciting idea! Building an autonomous agent with LangGraph is indeed a great application of this technology. LangGraph is particularly well-suited for creating complex, multi-step AI workflows, which is perfect for autonomous agents. Let me gather some more specific information about using LangGraph for building autonomous agents.\", 'type': 'text'}, {'id': 'toolu_01QWNHhUaeeWcGXvA4eHT7Zo', 'input': {'query': 'Building autonomous agents with LangGraph examples and tutorials'}, 'name': 'tavily_search_results_json', 'type': 'tool_use'}]\nTool Calls:\n  tavily_search_results_json (toolu_01QWNHhUaeeWcGXvA4eHT7Zo)\n Call ID: toolu_01QWNHhUaeeWcGXvA4eHT7Zo\n  Args:\n    query: Building autonomous agents with LangGraph examples and tutorials\n================================= Tool Message =================================\nName: tavily_search_results_json\n\n[{\"url\": \"https://towardsdatascience.com/building-autonomous-multi-tool-agents-with-gemini-2-0-and-langgraph-ad3d7bd5e79d\", \"content\": \"Building Autonomous Multi-Tool Agents with Gemini 2.0 and LangGraph | by Youness Mansar | Jan, 2025 | Towards Data Science Building Autonomous Multi-Tool Agents with Gemini 2.0 and LangGraph A practical tutorial with full code examples for building and running multi-tool agents Towards Data Science LLMs are remarkable \u2014 they can memorize vast amounts of information, answer general knowledge questions, write code, generate stories, and even fix your grammar. In this tutorial, we are going to build a simple LLM agent that is equipped with four tools that it can use to answer a user's question. This Agent will have the following specifications: Follow Published in Towards Data Science --------------------------------- Your home for data science and AI. Follow Follow Follow\"}, {\"url\": \"https://github.com/anmolaman20/Tools_and_Agents\", \"content\": \"GitHub - anmolaman20/Tools_and_Agents: This repository provides resources for building AI agents using Langchain and Langgraph. This repository provides resources for building AI agents using Langchain and Langgraph. This repository provides resources for building AI agents using Langchain and Langgraph. This repository serves as a comprehensive guide for building AI-powered agents using Langchain and Langgraph. It provides hands-on examples, practical tutorials, and resources for developers and AI enthusiasts to master building intelligent systems and workflows. AI Agent Development: Gain insights into creating intelligent systems that think, reason, and adapt in real time. This repository is ideal for AI practitioners, developers exploring language models, or anyone interested in building intelligent systems. This repository provides resources for building AI agents using Langchain and Langgraph.\"}]\n================================== Ai Message ==================================\n\nGreat idea! Building an autonomous agent with LangGraph is definitely an exciting project. Based on the latest information I've found, here are some insights and tips for building autonomous agents with LangGraph:\n\n1. Multi-Tool Agents: LangGraph is particularly well-suited for creating autonomous agents that can use multiple tools. This allows your agent to have a diverse set of capabilities and choose the right tool for each task.\n\n2. Integration with Large Language Models (LLMs): You can combine LangGraph with powerful LLMs like Gemini 2.0 to create more intelligent and capable agents. The LLM can serve as the \"brain\" of your agent, making decisions and generating responses.\n\n3. Workflow Management: LangGraph excels at managing complex, multi-step AI workflows. This is crucial for autonomous agents that need to break down tasks into smaller steps and execute them in the right order.\n...\n\nRemember, building an autonomous agent is an iterative process. Start simple and gradually increase complexity as you become more comfortable with LangGraph and its capabilities.\n\nWould you like more information on any specific aspect of building your autonomous agent with LangGraph?\nOutput is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...\n</code></pre> <p>The graph resumed execution from the <code>tools</code> node. You can tell this is the case since the first value printed above is the response from our search engine tool. :::</p> <p>:::js</p> <p>The checkpoint's <code>toReplay.config</code> contains a <code>checkpoint_id</code> timestamp. Providing this <code>checkpoint_id</code> value tells LangGraph's checkpointer to load the state from that moment in time.</p> <pre><code>// The `checkpoint_id` in the `toReplay.config` corresponds to a state we've persisted to our checkpointer.\nfor await (const event of await graph.stream(null, {\n  ...toReplay?.config,\n  streamMode: \"values\",\n})) {\n  if (\"messages\" in event) {\n    const lastMessage = event.messages.at(-1);\n\n    console.log(\n      \"=\".repeat(32),\n      `${lastMessage?.getType()} Message`,\n      \"=\".repeat(32)\n    );\n    console.log(lastMessage?.text);\n  }\n}\n</code></pre> <pre><code>================================ ai Message ================================\nLet me search for specific information about building autonomous agents with LangGraph.js.\n================================ tool Message ================================\n{\n  \"query\": \"how to build autonomous agents with LangGraph.js examples tutorial\",\n  \"follow_up_questions\": null,\n  \"answer\": null,\n  \"images\": [],\n  \"results\": [\n    {\n      \"url\": \"https://www.mongodb.com/developer/languages/typescript/build-javascript-ai-agent-langgraphjs-mongodb/\",\n      \"title\": \"Build a JavaScript AI Agent With LangGraph.js and MongoDB\",\n      \"content\": \"(...)\",\n      \"score\": 0.7672197,\n      \"raw_content\": null\n    },\n    {\n      \"url\": \"https://medium.com/@lorevanoudenhove/how-to-build-ai-agents-with-langgraph-a-step-by-step-guide-5d84d9c7e832\",\n      \"title\": \"How to Build AI Agents with LangGraph: A Step-by-Step Guide\",\n      \"content\": \"(...)\",\n      \"score\": 0.7407191,\n      \"raw_content\": null\n    }\n  ],\n  \"response_time\": 0.82\n}\n================================ ai Message ================================\nBased on the search results, I can share some practical information about building autonomous agents with LangGraph.js. Here are some concrete examples and approaches:\n\n1. Example HR Assistant Agent:\n- Can handle HR-related queries using employee information\n- Features include:\n  - Starting and continuing conversations\n  - Looking up information using vector search\n  - Persisting conversation state using checkpoints\n  - Managing threaded conversations\n\n2. Energy Savings Calculator Agent:\n- Functions as a lead generation tool for solar panel sales\n- Capabilities include:\n  - Calculating potential energy savings\n  - Handling multi-step conversations\n  - Processing user inputs for personalized estimates\n  - Managing conversation state\n\n(...)\n</code></pre> <p>The graph resumed execution from the <code>tools</code> node. You can tell this is the case since the first value printed above is the response from our search engine tool. :::</p> <p>Congratulations! You've now used time-travel checkpoint traversal in LangGraph. Being able to rewind and explore alternative paths opens up a world of possibilities for debugging, experimentation, and interactive applications.</p>"}, {"location": "tutorials/get-started/6-time-travel/#learn-more", "title": "Learn more", "text": "<p>Take your LangGraph journey further by exploring deployment and advanced features:</p> <ul> <li>LangGraph Server quickstart: Launch a LangGraph server locally and interact with it using the REST API and LangGraph Studio Web UI.</li> <li>LangGraph Platform quickstart: Deploy your LangGraph app using LangGraph Platform.</li> <li>LangGraph Platform concepts: Understand the foundational concepts of the LangGraph Platform.</li> </ul>"}, {"location": "tutorials/langgraph-platform/local-server/", "title": "Run a local server", "text": "<p>This guide shows you how to run a LangGraph application locally.</p>"}, {"location": "tutorials/langgraph-platform/local-server/#prerequisites", "title": "Prerequisites", "text": "<p>Before you begin, ensure you have the following:</p> <ul> <li>An API key for LangSmith - free to sign up</li> </ul>"}, {"location": "tutorials/langgraph-platform/local-server/#1-install-the-langgraph-cli", "title": "1. Install the LangGraph CLI", "text": "<p>:::python</p> <pre><code># Python &gt;= 3.11 is required.\n\npip install --upgrade \"langgraph-cli[inmem]\"\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>npx @langchain/langgraph-cli\n</code></pre> <p>:::</p>"}, {"location": "tutorials/langgraph-platform/local-server/#2-create-a-langgraph-app", "title": "2. Create a LangGraph app \ud83c\udf31", "text": "<p>:::python Create a new app from the <code>new-langgraph-project-python</code> template. This template demonstrates a single-node application you can extend with your own logic.</p> <pre><code>langgraph new path/to/your/app --template new-langgraph-project-python\n</code></pre> <p>Additional templates</p> <p>If you use <code>langgraph new</code> without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.</p> <p>:::</p> <p>:::js Create a new app from the <code>new-langgraph-project-js</code> template. This template demonstrates a single-node application you can extend with your own logic.</p> <pre><code>npm create langgraph\n</code></pre> <p>:::</p>"}, {"location": "tutorials/langgraph-platform/local-server/#3-install-dependencies", "title": "3. Install dependencies", "text": "<p>In the root of your new LangGraph app, install the dependencies in <code>edit</code> mode so your local changes are used by the server:</p> <p>:::python</p> <pre><code>cd path/to/your/app\npip install -e .\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>cd path/to/your/app\nnpm install\n</code></pre> <p>:::</p>"}, {"location": "tutorials/langgraph-platform/local-server/#4-create-a-env-file", "title": "4. Create a <code>.env</code> file", "text": "<p>You will find a <code>.env.example</code> in the root of your new LangGraph app. Create a <code>.env</code> file in the root of your new LangGraph app and copy the contents of the <code>.env.example</code> file into it, filling in the necessary API keys:</p> <pre><code>LANGSMITH_API_KEY=lsv2...\n</code></pre>"}, {"location": "tutorials/langgraph-platform/local-server/#5-launch-langgraph-server", "title": "5. Launch LangGraph Server \ud83d\ude80", "text": "<p>Start the LangGraph API server locally:</p> <p>:::python</p> <pre><code>langgraph dev\n</code></pre> <p>:::</p> <p>:::js</p> <pre><code>npx @langchain/langgraph-cli dev\n</code></pre> <p>:::</p> <p>Sample output:</p> <pre><code>&gt;    Ready!\n&gt;\n&gt;    - API: [http://localhost:2024](http://localhost:2024/)\n&gt;\n&gt;    - Docs: http://localhost:2024/docs\n&gt;\n&gt;    - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n</code></pre> <p>The <code>langgraph dev</code> command starts LangGraph Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy LangGraph Server with access to a persistent storage backend. For more information, see Deployment options.</p>"}, {"location": "tutorials/langgraph-platform/local-server/#6-test-your-application-in-langgraph-studio", "title": "6. Test your application in LangGraph Studio", "text": "<p>LangGraph Studio is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in LangGraph Studio by visiting the URL provided in the output of the <code>langgraph dev</code> command:</p> <pre><code>&gt;    - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n</code></pre> <p>For a LangGraph Server running on a custom host/port, update the baseURL parameter.</p> Safari compatibility <p>Use the <code>--tunnel</code> flag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers:</p> <pre><code>langgraph dev --tunnel\n</code></pre>"}, {"location": "tutorials/langgraph-platform/local-server/#7-test-the-api", "title": "7. Test the API", "text": "<p>:::python</p> Python SDK (async)Python SDK (sync)Rest API <ol> <li> <p>Install the LangGraph Python SDK:</p> <pre><code>pip install langgraph-sdk\n</code></pre> </li> <li> <p>Send a message to the assistant (threadless run):</p> <pre><code>from langgraph_sdk import get_client\nimport asyncio\n\nclient = get_client(url=\"http://localhost:2024\")\n\nasync def main():\n    async for chunk in client.runs.stream(\n        None,  # Threadless run\n        \"agent\", # Name of assistant. Defined in langgraph.json.\n        input={\n        \"messages\": [{\n            \"role\": \"human\",\n            \"content\": \"What is LangGraph?\",\n            }],\n        },\n    ):\n        print(f\"Receiving new event of type: {chunk.event}...\")\n        print(chunk.data)\n        print(\"\\n\\n\")\n\nasyncio.run(main())\n</code></pre> </li> </ol> <ol> <li> <p>Install the LangGraph Python SDK:</p> <pre><code>pip install langgraph-sdk\n</code></pre> </li> <li> <p>Send a message to the assistant (threadless run):</p> <pre><code>from langgraph_sdk import get_sync_client\n\nclient = get_sync_client(url=\"http://localhost:2024\")\n\nfor chunk in client.runs.stream(\n    None,  # Threadless run\n    \"agent\", # Name of assistant. Defined in langgraph.json.\n    input={\n        \"messages\": [{\n            \"role\": \"human\",\n            \"content\": \"What is LangGraph?\",\n        }],\n    },\n    stream_mode=\"messages-tuple\",\n):\n    print(f\"Receiving new event of type: {chunk.event}...\")\n    print(chunk.data)\n    print(\"\\n\\n\")\n</code></pre> </li> </ol> <pre><code>curl -s --request POST \\\n    --url \"http://localhost:2024/runs/stream\" \\\n    --header 'Content-Type: application/json' \\\n    --data \"{\n        \\\"assistant_id\\\": \\\"agent\\\",\n        \\\"input\\\": {\n            \\\"messages\\\": [\n                {\n                    \\\"role\\\": \\\"human\\\",\n                    \\\"content\\\": \\\"What is LangGraph?\\\"\n                }\n            ]\n        },\n        \\\"stream_mode\\\": \\\"messages-tuple\\\"\n    }\"\n</code></pre> <p>:::</p> <p>:::js</p> Javascript SDKRest API <ol> <li> <p>Install the LangGraph JS SDK:</p> <pre><code>npm install @langchain/langgraph-sdk\n</code></pre> </li> <li> <p>Send a message to the assistant (threadless run):</p> <pre><code>const { Client } = await import(\"@langchain/langgraph-sdk\");\n\n// only set the apiUrl if you changed the default port when calling langgraph dev\nconst client = new Client({ apiUrl: \"http://localhost:2024\"});\n\nconst streamResponse = client.runs.stream(\n    null, // Threadless run\n    \"agent\", // Assistant ID\n    {\n        input: {\n            \"messages\": [\n                { \"role\": \"user\", \"content\": \"What is LangGraph?\"}\n            ]\n        },\n        streamMode: \"messages-tuple\",\n    }\n);\n\nfor await (const chunk of streamResponse) {\n    console.log(`Receiving new event of type: ${chunk.event}...`);\n    console.log(JSON.stringify(chunk.data));\n    console.log(\"\\n\\n\");\n}\n</code></pre> </li> </ol> <pre><code>curl -s --request POST \\\n    --url \"http://localhost:2024/runs/stream\" \\\n    --header 'Content-Type: application/json' \\\n    --data \"{\n        \\\"assistant_id\\\": \\\"agent\\\",\n        \\\"input\\\": {\n            \\\"messages\\\": [\n                {\n                    \\\"role\\\": \\\"human\\\",\n                    \\\"content\\\": \\\"What is LangGraph?\\\"\n                }\n            ]\n        },\n        \\\"stream_mode\\\": \\\"messages-tuple\\\"\n    }\"\n</code></pre> <p>:::</p>"}, {"location": "tutorials/langgraph-platform/local-server/#next-steps", "title": "Next steps", "text": "<p>Now that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features:</p> <ul> <li>Deployment quickstart: Deploy your LangGraph app using LangGraph Platform.</li> <li>LangGraph Platform overview: Learn about foundational LangGraph Platform concepts.</li> <li>LangGraph Server API Reference: Explore the LangGraph Server API documentation.</li> </ul> <p>:::python</p> <ul> <li>Python SDK Reference: Explore the Python SDK API Reference.   :::</li> </ul> <p>:::js</p> <ul> <li>JS/TS SDK Reference: Explore the JS/TS SDK API Reference.   :::</li> </ul>"}, {"location": "tutorials/multi_agent/agent_supervisor/", "title": "Multi-agent supervisor", "text": "<p>Supervisor is a multi-agent architecture where specialized agents are coordinated by a central supervisor agent. The supervisor agent controls all communication flow and task delegation, making decisions about which agent to invoke based on the current context and task requirements.</p> <p>In this tutorial, you will build a supervisor system with two agents \u2014 a research and a math expert. By the end of the tutorial you will:</p> <ol> <li>Build specialized research and math agents</li> <li>Build a supervisor for orchestrating them with the prebuilt <code>langgraph-supervisor</code></li> <li>Build a supervisor from scratch</li> <li>Implement advanced task delegation</li> </ol> <p></p>"}, {"location": "tutorials/multi_agent/agent_supervisor/#setup", "title": "Setup", "text": "<p>First, let's install required packages and set our API keys</p> <pre><code>%%capture --no-stderr\n%pip install -U langgraph langgraph-supervisor langchain-tavily \"langchain[openai]\"\n</code></pre> <pre><code>import getpass\nimport os\n\n\ndef _set_if_undefined(var: str):\n    if not os.environ.get(var):\n        os.environ[var] = getpass.getpass(f\"Please provide your {var}\")\n\n\n_set_if_undefined(\"OPENAI_API_KEY\")\n_set_if_undefined(\"TAVILY_API_KEY\")\n</code></pre> <p>Tip</p> <p>Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.</p>"}, {"location": "tutorials/multi_agent/agent_supervisor/#1-create-worker-agents", "title": "1. Create worker agents", "text": "<p>First, let's create our specialized worker agents \u2014 research agent and math agent:</p> <ul> <li>Research agent will have access to a web search tool using Tavily API</li> <li>Math agent will have access to simple math tools (<code>add</code>, <code>multiply</code>, <code>divide</code>)</li> </ul>"}, {"location": "tutorials/multi_agent/agent_supervisor/#research-agent", "title": "Research agent", "text": "<p>For web search, we will use <code>TavilySearch</code> tool from <code>langchain-tavily</code>:</p> <pre><code>from langchain_tavily import TavilySearch\n\nweb_search = TavilySearch(max_results=3)\nweb_search_results = web_search.invoke(\"who is the mayor of NYC?\")\n\nprint(web_search_results[\"results\"][0][\"content\"])\n</code></pre> <p>Output: <pre><code>Find events, attractions, deals, and more at nyctourism.com Skip Main Navigation Menu The Official Website of the City of New York Text Size Powered by Translate SearchSearch Primary Navigation The official website of NYC Home NYC Resources NYC311 Office of the Mayor Events Connect Jobs Search Office of the Mayor | Mayor's Bio | City of New York Secondary Navigation MayorBiographyNewsOfficials Eric L. Adams 110th Mayor of New York City Mayor Eric Adams has served the people of New York City as an NYPD officer, State Senator, Brooklyn Borough President, and now as the 110th Mayor of the City of New York. Mayor Eric Adams has served the people of New York City as an NYPD officer, State Senator, Brooklyn Borough President, and now as the 110th Mayor of the City of New York. He gave voice to a diverse coalition of working families in all five boroughs and is leading the fight to bring back New York City's economy, reduce inequality, improve public safety, and build a stronger, healthier city that delivers for all New Yorkers. As the representative of one of the nation's largest counties, Eric fought tirelessly to grow the local economy, invest in schools, reduce inequality, improve public safety, and advocate for smart policies and better government that delivers for all New Yorkers.\n</code></pre></p> <p>To create individual worker agents, we will use LangGraph's prebuilt agent.</p> <pre><code>from langgraph.prebuilt import create_react_agent\n\nresearch_agent = create_react_agent(\n    model=\"openai:gpt-4.1\",\n    tools=[web_search],\n    prompt=(\n        \"You are a research agent.\\n\\n\"\n        \"INSTRUCTIONS:\\n\"\n        \"- Assist ONLY with research-related tasks, DO NOT do any math\\n\"\n        \"- After you're done with your tasks, respond to the supervisor directly\\n\"\n        \"- Respond ONLY with the results of your work, do NOT include ANY other text.\"\n    ),\n    name=\"research_agent\",\n)\n</code></pre> <p>Let's run the agent to verify that it behaves as expected. </p> <p>We'll use <code>pretty_print_messages</code> helper to render the streamed agent outputs nicely</p> <pre><code>from langchain_core.messages import convert_to_messages\n\n\ndef pretty_print_message(message, indent=False):\n    pretty_message = message.pretty_repr(html=True)\n    if not indent:\n        print(pretty_message)\n        return\n\n    indented = \"\\n\".join(\"\\t\" + c for c in pretty_message.split(\"\\n\"))\n    print(indented)\n\n\ndef pretty_print_messages(update, last_message=False):\n    is_subgraph = False\n    if isinstance(update, tuple):\n        ns, update = update\n        # skip parent graph updates in the printouts\n        if len(ns) == 0:\n            return\n\n        graph_id = ns[-1].split(\":\")[0]\n        print(f\"Update from subgraph {graph_id}:\")\n        print(\"\\n\")\n        is_subgraph = True\n\n    for node_name, node_update in update.items():\n        update_label = f\"Update from node {node_name}:\"\n        if is_subgraph:\n            update_label = \"\\t\" + update_label\n\n        print(update_label)\n        print(\"\\n\")\n\n        messages = convert_to_messages(node_update[\"messages\"])\n        if last_message:\n            messages = messages[-1:]\n\n        for m in messages:\n            pretty_print_message(m, indent=is_subgraph)\n        print(\"\\n\")\n</code></pre> <pre><code>for chunk in research_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"who is the mayor of NYC?\"}]}\n):\n    pretty_print_messages(chunk)\n</code></pre> <p>Output: <pre><code>Update from node agent:\n\n\n================================== Ai Message ==================================\nName: research_agent\nTool Calls:\n  tavily_search (call_U748rQhQXT36sjhbkYLSXQtJ)\n Call ID: call_U748rQhQXT36sjhbkYLSXQtJ\n  Args:\n    query: current mayor of New York City\n    search_depth: basic\n\n\nUpdate from node tools:\n\n\n================================= Tool Message ==================================\nName: tavily_search\n\n{\"query\": \"current mayor of New York City\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"title\": \"List of mayors of New York City - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/List_of_mayors_of_New_York_City\", \"content\": \"The mayor of New York City is the chief executive of the Government of New York City, as stipulated by New York City's charter.The current officeholder, the 110th in the sequence of regular mayors, is Eric Adams, a member of the Democratic Party.. During the Dutch colonial period from 1624 to 1664, New Amsterdam was governed by the Director of Netherland.\", \"score\": 0.9039154, \"raw_content\": null}, {\"title\": \"Office of the Mayor | Mayor's Bio | City of New York - NYC.gov\", \"url\": \"https://www.nyc.gov/office-of-the-mayor/bio.page\", \"content\": \"Mayor Eric Adams has served the people of New York City as an NYPD officer, State Senator, Brooklyn Borough President, and now as the 110th Mayor of the City of New York. He gave voice to a diverse coalition of working families in all five boroughs and is leading the fight to bring back New York City's economy, reduce inequality, improve\", \"score\": 0.8405867, \"raw_content\": null}, {\"title\": \"Eric Adams - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/Eric_Adams\", \"content\": \"Eric Leroy Adams (born September 1, 1960) is an American politician and former police officer who has served as the 110th mayor of New York City since 2022. Adams was an officer in the New York City Transit Police and then the New York City Police Department (```\n</code></pre></p>"}, {"location": "tutorials/multi_agent/agent_supervisor/#math-agent", "title": "Math agent", "text": "<p>For math agent tools we will use vanilla Python functions:</p> <pre><code>def add(a: float, b: float):\n    \"\"\"Add two numbers.\"\"\"\n    return a + b\n\n\ndef multiply(a: float, b: float):\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n\n\ndef divide(a: float, b: float):\n    \"\"\"Divide two numbers.\"\"\"\n    return a / b\n\n\nmath_agent = create_react_agent(\n    model=\"openai:gpt-4.1\",\n    tools=[add, multiply, divide],\n    prompt=(\n        \"You are a math agent.\\n\\n\"\n        \"INSTRUCTIONS:\\n\"\n        \"- Assist ONLY with math-related tasks\\n\"\n        \"- After you're done with your tasks, respond to the supervisor directly\\n\"\n        \"- Respond ONLY with the results of your work, do NOT include ANY other text.\"\n    ),\n    name=\"math_agent\",\n)\n</code></pre> <p>Let's run the math agent:</p> <pre><code>for chunk in math_agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 7\"}]}\n):\n    pretty_print_messages(chunk)\n</code></pre> <p>Output: <pre><code>Update from node agent:\n\n\n================================== Ai Message ==================================\nName: math_agent\nTool Calls:\n  add (call_p6OVLDHB4LyCNCxPOZzWR15v)\n Call ID: call_p6OVLDHB4LyCNCxPOZzWR15v\n  Args:\n    a: 3\n    b: 5\n\n\nUpdate from node tools:\n\n\n================================= Tool Message ==================================\nName: add\n\n8.0\n\n\nUpdate from node agent:\n\n\n================================== Ai Message ==================================\nName: math_agent\nTool Calls:\n  multiply (call_EoaWHMLFZAX4AkajQCtZvbli)\n Call ID: call_EoaWHMLFZAX4AkajQCtZvbli\n  Args:\n    a: 8\n    b: 7\n\n\nUpdate from node tools:\n\n\n================================= Tool Message ==================================\nName: multiply\n\n56.0\n\n\nUpdate from node agent:\n\n\n================================== Ai Message ==================================\nName: math_agent\n\n56\n</code></pre></p>"}, {"location": "tutorials/multi_agent/agent_supervisor/#2-create-supervisor-with-langgraph-supervisor", "title": "2. Create supervisor with <code>langgraph-supervisor</code>", "text": "<p>To implement out multi-agent system, we will use @[<code>create_supervisor</code>][create_supervisor] from the prebuilt <code>langgraph-supervisor</code> library:</p> <pre><code>from langgraph_supervisor import create_supervisor\nfrom langchain.chat_models import init_chat_model\n\nsupervisor = create_supervisor(\n    model=init_chat_model(\"openai:gpt-4.1\"),\n    agents=[research_agent, math_agent],\n    prompt=(\n        \"You are a supervisor managing two agents:\\n\"\n        \"- a research agent. Assign research-related tasks to this agent\\n\"\n        \"- a math agent. Assign math-related tasks to this agent\\n\"\n        \"Assign work to one agent at a time, do not call agents in parallel.\\n\"\n        \"Do not do any work yourself.\"\n    ),\n    add_handoff_back_messages=True,\n    output_mode=\"full_history\",\n).compile()\n</code></pre> <pre><code>from IPython.display import display, Image\n\ndisplay(Image(supervisor.get_graph().draw_mermaid_png()))\n</code></pre> <p></p> <p>Note: When you run this code, it will generate and display a visual representation of the supervisor graph showing the flow between the supervisor and worker agents.</p> <p>Let's now run it with a query that requires both agents:</p> <ul> <li>research agent will look up the necessary GDP information</li> <li>math agent will perform division to find the percentage of NY state GDP, as requested</li> </ul> <pre><code>for chunk in supervisor.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"find US and New York state GDP in 2024. what % of US GDP was New York state?\",\n            }\n        ]\n    },\n):\n    pretty_print_messages(chunk, last_message=True)\n\nfinal_message_history = chunk[\"supervisor\"][\"messages\"]\n</code></pre> <p>Output: <pre><code>Update from node supervisor:\n\n\n================================= Tool Message ==================================\nName: transfer_to_research_agent\n\nSuccessfully transferred to research_agent\n\n\nUpdate from node research_agent:\n\n\n================================= Tool Message ==================================\nName: transfer_back_to_supervisor\n\nSuccessfully transferred back to supervisor\n\n\nUpdate from node supervisor:\n\n\n================================= Tool Message ==================================\nName: transfer_to_math_agent\n\nSuccessfully transferred to math_agent\n\n\nUpdate from node math_agent:\n\n\n================================= Tool Message ==================================\nName: transfer_back_to_supervisor\n\nSuccessfully transferred back to supervisor\n\n\nUpdate from node supervisor:\n\n\n================================== Ai Message ==================================\nName: supervisor\n\nIn 2024, the US GDP was $29.18 trillion and New York State's GDP was $2.297 trillion. New York State accounted for approximately 7.87% of the total US GDP in 2024.\n</code></pre></p>"}, {"location": "tutorials/multi_agent/agent_supervisor/#3-create-supervisor-from-scratch", "title": "3. Create supervisor from scratch", "text": "<p>Let's now implement this same multi-agent system from scratch. We will need to:</p> <ol> <li>Set up how the supervisor communicates with individual agents</li> <li>Create the supervisor agent</li> <li>Combine supervisor and worker agents into a single multi-agent graph.</li> </ol>"}, {"location": "tutorials/multi_agent/agent_supervisor/#set-up-agent-communication", "title": "Set up agent communication", "text": "<p>We will need to define a way for the supervisor agent to communicate with the worker agents. A common way to implement this in multi-agent architectures is using handoffs, where one agent hands off control to another. Handoffs allow you to specify:</p> <ul> <li>destination: target agent to transfer to</li> <li>payload: information to pass to that agent</li> </ul> <p>We will implement handoffs via handoff tools and give these tools to the supervisor agent: when the supervisor calls these tools, it will hand off control to a worker agent, passing the full message history to that agent.</p> <pre><code>from typing import Annotated\nfrom langchain_core.tools import tool, InjectedToolCallId\nfrom langgraph.prebuilt import InjectedState\nfrom langgraph.graph import StateGraph, START, MessagesState\nfrom langgraph.types import Command\n\n\ndef create_handoff_tool(*, agent_name: str, description: str | None = None):\n    name = f\"transfer_to_{agent_name}\"\n    description = description or f\"Ask {agent_name} for help.\"\n\n    @tool(name, description=description)\n    def handoff_tool(\n        state: Annotated[MessagesState, InjectedState],\n        tool_call_id: Annotated[str, InjectedToolCallId],\n    ) -&gt; Command:\n        tool_message = {\n            \"role\": \"tool\",\n            \"content\": f\"Successfully transferred to {agent_name}\",\n            \"name\": name,\n            \"tool_call_id\": tool_call_id,\n        }\n        # highlight-next-line\n        return Command(\n            # highlight-next-line\n            goto=agent_name,  # (1)!\n            # highlight-next-line\n            update={**state, \"messages\": state[\"messages\"] + [tool_message]},  # (2)!\n            # highlight-next-line\n            graph=Command.PARENT,  # (3)!\n        )\n\n    return handoff_tool\n\n\n# Handoffs\nassign_to_research_agent = create_handoff_tool(\n    agent_name=\"research_agent\",\n    description=\"Assign task to a researcher agent.\",\n)\n\nassign_to_math_agent = create_handoff_tool(\n    agent_name=\"math_agent\",\n    description=\"Assign task to a math agent.\",\n)\n</code></pre> <ol> <li>Name of the agent or node to hand off to.</li> <li>Take the agent's messages and add them to the parent's state as part of the handoff. The next agent will see the parent state.</li> <li>Indicate to LangGraph that we need to navigate to agent node in a parent multi-agent graph.</li> </ol>"}, {"location": "tutorials/multi_agent/agent_supervisor/#create-supervisor-agent", "title": "Create supervisor agent", "text": "<p>Then, let's create the supervisor agent with the handoff tools we just defined. We will use the prebuilt @[<code>create_react_agent</code>][create_react_agent]:</p> <pre><code>supervisor_agent = create_react_agent(\n    model=\"openai:gpt-4.1\",\n    tools=[assign_to_research_agent, assign_to_math_agent],\n    prompt=(\n        \"You are a supervisor managing two agents:\\n\"\n        \"- a research agent. Assign research-related tasks to this agent\\n\"\n        \"- a math agent. Assign math-related tasks to this agent\\n\"\n        \"Assign work to one agent at a time, do not call agents in parallel.\\n\"\n        \"Do not do any work yourself.\"\n    ),\n    name=\"supervisor\",\n)\n</code></pre>"}, {"location": "tutorials/multi_agent/agent_supervisor/#create-multi-agent-graph", "title": "Create multi-agent graph", "text": "<p>Putting this all together, let's create a graph for our overall multi-agent system. We will add the supervisor and the individual agents as subgraph nodes.</p> <pre><code>from langgraph.graph import END\n\n# Define the multi-agent supervisor graph\nsupervisor = (\n    StateGraph(MessagesState)\n    # NOTE: `destinations` is only needed for visualization and doesn't affect runtime behavior\n    .add_node(supervisor_agent, destinations=(\"research_agent\", \"math_agent\", END))\n    .add_node(research_agent)\n    .add_node(math_agent)\n    .add_edge(START, \"supervisor\")\n    # always return back to the supervisor\n    .add_edge(\"research_agent\", \"supervisor\")\n    .add_edge(\"math_agent\", \"supervisor\")\n    .compile()\n)\n</code></pre> <p>Notice that we've added explicit edges from worker agents back to the supervisor \u2014 this means that they are guaranteed to return control back to the supervisor. If you want the agents to respond directly to the user (i.e., turn the system into a router, you can remove these edges).</p> <pre><code>from IPython.display import display, Image\n\ndisplay(Image(supervisor.get_graph().draw_mermaid_png()))\n</code></pre> <p></p> <p>Note: When you run this code, it will generate and display a visual representation of the multi-agent supervisor graph showing the flow between the supervisor and worker agents.</p> <p>With the multi-agent graph created, let's now run it!</p> <pre><code>for chunk in supervisor.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"find US and New York state GDP in 2024. what % of US GDP was New York state?\",\n            }\n        ]\n    },\n):\n    pretty_print_messages(chunk, last_message=True)\n\nfinal_message_history = chunk[\"supervisor\"][\"messages\"]\n</code></pre> <p>Output: <pre><code>Update from node supervisor:\n\n\n================================= Tool Message ==================================\nName: transfer_to_research_agent\n\nSuccessfully transferred to research_agent\n\n\nUpdate from node research_agent:\n\n\n================================== Ai Message ==================================\nName: research_agent\n\n- US GDP in 2024 is projected to be about $28.18 trillion USD (Statista; CBO projection).\n- New York State's nominal GDP for 2024 is estimated at approximately $2.16 trillion USD (various economic reports).\n- New York State's share of US GDP in 2024 is roughly 7.7%.\n\nSources:\n- https://www.statista.com/statistics/216985/forecast-of-us-gross-domestic-product/\n- https://nyassembly.gov/Reports/WAM/2025economic_revenue/2025_report.pdf?v=1740533306\n\n\nUpdate from node supervisor:\n\n\n================================= Tool Message ==================================\nName: transfer_to_math_agent\n\nSuccessfully transferred to math_agent\n\n\nUpdate from node math_agent:\n\n\n================================== Ai Message ==================================\nName: math_agent\n\nUS GDP in 2024: $28.18 trillion\nNew York State GDP in 2024: $2.16 trillion\nPercentage of US GDP from New York State: 7.67%\n\n\nUpdate from node supervisor:\n\n\n================================== Ai Message ==================================\nName: supervisor\n\nHere are your results:\n\n- 2024 US GDP (projected): $28.18 trillion USD\n- 2024 New York State GDP (estimated): $2.16 trillion USD\n- New York State's share of US GDP: approximately 7.7%\n\nIf you need the calculation steps or sources, let me know!\n</code></pre></p> <p>Let's examine the full resulting message history:</p> <pre><code>for message in final_message_history:\n    message.pretty_print()\n</code></pre> <p>Output: <pre><code>================================ Human Message ==================================\n\nfind US and New York state GDP in 2024. what % of US GDP was New York state?\n================================== Ai Message ===================================\nName: supervisor\nTool Calls:\n  transfer_to_research_agent (call_KlGgvF5ahlAbjX8d2kHFjsC3)\n Call ID: call_KlGgvF5ahlAbjX8d2kHFjsC3\n  Args:\n================================= Tool Message ==================================\nName: transfer_to_research_agent\n\nSuccessfully transferred to research_agent\n================================== Ai Message ===================================\nName: research_agent\nTool Calls:\n  tavily_search (call_ZOaTVUA6DKrOjWQldLhtrsO2)\n Call ID: call_ZOaTVUA6DKrOjWQldLhtrsO2\n  Args:\n    query: US GDP 2024 estimate or actual\n    search_depth: advanced\n  tavily_search (call_QsRAasxW9K03lTlqjuhNLFbZ)\n Call ID: call_QsRAasxW9K03lTlqjuhNLFbZ\n  Args:\n    query: New York state GDP 2024 estimate or actual\n    search_depth: advanced\n================================= Tool Message ==================================\nName: tavily_search\n\n{\"query\": \"US GDP 2024 estimate or actual\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.advisorperspectives.com/dshort/updates/2025/05/29/gdp-gross-domestic-product-q1-2025-second-estimate\", \"title\": \"Q1 GDP Second Estimate: Real GDP at -0.2%, Higher Than Expected\", \"content\": \"&gt; Real gross domestic product (GDP) decreased at an annual rate of 0.2 percent in the first quarter of 2025 (January, February, and March), according to the second estimate released by the U.S. Bureau of Economic Analysis. In the fourth quarter of 2024, real GDP increased 2.4 percent. The decrease in real GDP in the first quarter primarily reflected an increase in imports, which are a subtraction in the calculation of GDP, and a decrease in government spending. These movements were partly [...] by [Harry Mamaysky](https://www.advisor```\n</code></pre></p> <p>Important</p> <p>You can see that the supervisor system appends all of the individual agent messages (i.e., their internal tool-calling loop) to the full message history. This means that on every supervisor turn, supervisor agent sees this full history. If you want more control over:</p> <ul> <li>how inputs are passed to agents: you can use LangGraph @[<code>Send()</code>][Send] primitive to directly send data to the worker agents during the handoff. See the task delegation example below</li> <li> <p>how agent outputs are added: you can control how much of the agent's internal message history is added to the overall supervisor message history by wrapping the agent in a separate node function:</p> <pre><code>def call_research_agent(state):\n    # return agent's final response,\n    # excluding inner monologue\n    response = research_agent.invoke(state)\n    # highlight-next-line\n    return {\"messages\": response[\"messages\"][-1]}\n</code></pre> </li> </ul>"}, {"location": "tutorials/multi_agent/agent_supervisor/#4-create-delegation-tasks", "title": "4. Create delegation tasks", "text": "<p>So far the individual agents relied on interpreting full message history to determine their tasks. An alternative approach is to ask the supervisor to formulate a task explicitly. We can do so by adding a <code>task_description</code> parameter to the <code>handoff_tool</code> function.</p> <pre><code>from langgraph.types import Send\n\n\ndef create_task_description_handoff_tool(\n    *, agent_name: str, description: str | None = None\n):\n    name = f\"transfer_to_{agent_name}\"\n    description = description or f\"Ask {agent_name} for help.\"\n\n    @tool(name, description=description)\n    def handoff_tool(\n        # this is populated by the supervisor LLM\n        task_description: Annotated[\n            str,\n            \"Description of what the next agent should do, including all of the relevant context.\",\n        ],\n        # these parameters are ignored by the LLM\n        state: Annotated[MessagesState, InjectedState],\n    ) -&gt; Command:\n        task_description_message = {\"role\": \"user\", \"content\": task_description}\n        agent_input = {**state, \"messages\": [task_description_message]}\n        return Command(\n            # highlight-next-line\n            goto=[Send(agent_name, agent_input)],\n            graph=Command.PARENT,\n        )\n\n    return handoff_tool\n\n\nassign_to_research_agent_with_description = create_task_description_handoff_tool(\n    agent_name=\"research_agent\",\n    description=\"Assign task to a researcher agent.\",\n)\n\nassign_to_math_agent_with_description = create_task_description_handoff_tool(\n    agent_name=\"math_agent\",\n    description=\"Assign task to a math agent.\",\n)\n\nsupervisor_agent_with_description = create_react_agent(\n    model=\"openai:gpt-4.1\",\n    tools=[\n        assign_to_research_agent_with_description,\n        assign_to_math_agent_with_description,\n    ],\n    prompt=(\n        \"You are a supervisor managing two agents:\\n\"\n        \"- a research agent. Assign research-related tasks to this assistant\\n\"\n        \"- a math agent. Assign math-related tasks to this assistant\\n\"\n        \"Assign work to one agent at a time, do not call agents in parallel.\\n\"\n        \"Do not do any work yourself.\"\n    ),\n    name=\"supervisor\",\n)\n\nsupervisor_with_description = (\n    StateGraph(MessagesState)\n    .add_node(\n        supervisor_agent_with_description, destinations=(\"research_agent\", \"math_agent\")\n    )\n    .add_node(research_agent)\n    .add_node(math_agent)\n    .add_edge(START, \"supervisor\")\n    .add_edge(\"research_agent\", \"supervisor\")\n    .add_edge(\"math_agent\", \"supervisor\")\n    .compile()\n)\n</code></pre> <p>Note</p> <p>We're using @[<code>Send()</code>][Send] primitive in the <code>handoff_tool</code>. This means that instead of receiving the full <code>supervisor</code> graph state as input, each worker agent only sees the contents of the <code>Send</code> payload. In this example, we're sending the task description as a single \"human\" message.</p> <p>Let's now running it with the same input query:</p> <pre><code>for chunk in supervisor_with_description.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"find US and New York state GDP in 2024. what % of US GDP was New York state?\",\n            }\n        ]\n    },\n    subgraphs=True,\n):\n    pretty_print_messages(chunk, last_message=True)\n</code></pre> <p>Output: <pre><code>Update from subgraph supervisor:\n\n\n    Update from node agent:\n\n\n    ================================== Ai Message ==================================\n    Name: supervisor\n    Tool Calls:\n      transfer_to_research_agent (call_tk8q8py8qK6MQz6Kj6mijKua)\n     Call ID: call_tk8q8py8qK6MQz6Kj6mijKua\n      Args:\n        task_description: Find the 2024 GDP (Gross Domestic Product) for both the United States and New York state, using the most up-to-date and reputable sources available. Provide both GDP values and cite the data sources.\n\n\nUpdate from subgraph research_agent:\n\n\n    Update from node agent:\n\n\n    ================================== Ai Message ==================================\n    Name: research_agent\n    Tool Calls:\n      tavily_search (call_KqvhSvOIhAvXNsT6BOwbPlRB)\n     Call ID: call_KqvhSvOIhAvXNsT6BOwbPlRB\n      Args:\n        query: 2024 United States GDP value from a reputable source\n        search_depth: advanced\n      tavily_search (call_kbbAWBc9KwCWKHmM5v04H88t)\n     Call ID: call_kbbAWBc9KwCWKHmM5v04H88t\n      Args:\n        query: 2024 New York state GDP value from a reputable source\n        search_depth: advanced\n\n\nUpdate from subgraph research_agent:\n\n\n    Update from node tools:\n\n\n    ================================= Tool Message ==================================\n    Name: tavily_search\n\n    {\"query\": \"2024 United States GDP value from a reputable source\", \"follow_up_questions\": null, \"answer\": null, \"images\": [], \"results\": [{\"url\": \"https://www.focus-economics.com/countries/united-states/\", \"title\": \"United States Economy Overview - Focus Economics\", \"content\": \"The United States' Macroeconomic Analysis:\\n------------------------------------------\\n\\n**Nominal GDP of USD 29,185 billion in 2024.**\\n\\n**Nominal GDP of USD 29,179 billion in 2024.**\\n\\n**GDP per capita of USD 86,635 compared to the global average of USD 10,589.**\\n\\n**GDP per capita of USD 86,652 compared to the global average of USD 10,589.**\\n\\n**Average real GDP growth of 2.5% over the last decade.**\\n\\n**Average real GDP growth of ```\n</code></pre></p>"}, {"location": "tutorials/rag/langgraph_agentic_rag/", "title": "Agentic RAG", "text": "<p>In this tutorial we will build a retrieval agent. Retrieval agents are useful when you want an LLM to make a decision about whether to retrieve context from a vectorstore or respond to the user directly.</p> <p>By the end of the tutorial we will have done the following:</p> <ol> <li>Fetch and preprocess documents that will be used for retrieval.</li> <li>Index those documents for semantic search and create a retriever tool for the agent.</li> <li>Build an agentic RAG system that can decide when to use the retriever tool.</li> </ol> <p></p>"}, {"location": "tutorials/rag/langgraph_agentic_rag/#setup", "title": "Setup", "text": "<p>Let's download the required packages and set our API keys:</p> <pre><code>%%capture --no-stderr\n%pip install -U --quiet langgraph \"langchain[openai]\" langchain-community langchain-text-splitters\n</code></pre> <pre><code>import getpass\nimport os\n\n\ndef _set_env(key: str):\n    if key not in os.environ:\n        os.environ[key] = getpass.getpass(f\"{key}:\")\n\n\n_set_env(\"OPENAI_API_KEY\")\n</code></pre> <p>Tip</p> <p>Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.</p>"}, {"location": "tutorials/rag/langgraph_agentic_rag/#1-preprocess-documents", "title": "1. Preprocess documents", "text": "<ol> <li> <p>Fetch documents to use in our RAG system. We will use three of the most recent pages from Lilian Weng's excellent blog. We'll start by fetching the content of the pages using <code>WebBaseLoader</code> utility:</p> <pre><code>from langchain_community.document_loaders import WebBaseLoader\n\nurls = [\n    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n]\n\ndocs = [WebBaseLoader(url).load() for url in urls]\n</code></pre> <pre><code>docs[0][0].page_content.strip()[:1000]\n</code></pre> </li> <li> <p>Split the fetched documents into smaller chunks for indexing into our vectorstore:</p> <pre><code>from langchain_text_splitters import RecursiveCharacterTextSplitter\n\ndocs_list = [item for sublist in docs for item in sublist]\n\ntext_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n    chunk_size=100, chunk_overlap=50\n)\ndoc_splits = text_splitter.split_documents(docs_list)\n</code></pre> <pre><code>doc_splits[0].page_content.strip()\n</code></pre> </li> </ol>"}, {"location": "tutorials/rag/langgraph_agentic_rag/#2-create-a-retriever-tool", "title": "2. Create a retriever tool", "text": "<p>Now that we have our split documents, we can index them into a vector store that we'll use for semantic search. </p> <ol> <li> <p>Use an in-memory vector store and OpenAI embeddings:</p> <pre><code>from langchain_core.vectorstores import InMemoryVectorStore\nfrom langchain_openai import OpenAIEmbeddings\n\nvectorstore = InMemoryVectorStore.from_documents(\n    documents=doc_splits, embedding=OpenAIEmbeddings()\n)\nretriever = vectorstore.as_retriever()\n</code></pre> </li> <li> <p>Create a retriever tool using LangChain's prebuilt <code>create_retriever_tool</code>:</p> <pre><code>from langchain.tools.retriever import create_retriever_tool\n\nretriever_tool = create_retriever_tool(\n    retriever,\n    \"retrieve_blog_posts\",\n    \"Search and return information about Lilian Weng blog posts.\",\n)\n</code></pre> </li> <li> <p>Test the tool:</p> <pre><code>retriever_tool.invoke({\"query\": \"types of reward hacking\"})\n</code></pre> </li> </ol>"}, {"location": "tutorials/rag/langgraph_agentic_rag/#3-generate-query", "title": "3. Generate query", "text": "<p>Now we will start building components (nodes and edges) for our agentic RAG graph.</p> <p>:::python Note that the components will operate on the <code>MessagesState</code> \u2014 graph state that contains a <code>messages</code> key with a list of chat messages. :::</p> <p>:::js Note that the components will operate on the <code>MessagesZodState</code> \u2014 graph state that contains a <code>messages</code> key with a list of chat messages. :::</p> <ol> <li> <p>Build a <code>generate_query_or_respond</code> node. It will call an LLM to generate a response based on the current graph state (list of messages). Given the input messages, it will decide to retrieve using the retriever tool, or respond directly to the user. Note that we're giving the chat model access to the <code>retriever_tool</code> we created earlier via <code>.bind_tools</code>:</p> <pre><code>from langgraph.graph import MessagesState\nfrom langchain.chat_models import init_chat_model\n\nresponse_model = init_chat_model(\"openai:gpt-4.1\", temperature=0)\n\n\ndef generate_query_or_respond(state: MessagesState):\n    \"\"\"Call the model to generate a response based on the current state. Given\n    the question, it will decide to retrieve using the retriever tool, or simply respond to the user.\n    \"\"\"\n    response = (\n        response_model\n        # highlight-next-line\n        .bind_tools([retriever_tool]).invoke(state[\"messages\"])\n    )\n    return {\"messages\": [response]}\n</code></pre> </li> <li> <p>Try it on a random input:</p> <pre><code>input = {\"messages\": [{\"role\": \"user\", \"content\": \"hello!\"}]}\ngenerate_query_or_respond(input)[\"messages\"][-1].pretty_print()\n</code></pre> <p>Output: <pre><code>================================== Ai Message ==================================\n\nHello! How can I help you today?\n</code></pre></p> </li> <li> <p>Ask a question that requires semantic search:</p> <pre><code>input = {\n    \"messages\": [\n        {\n            \"role\": \"user\",\n            \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n        }\n    ]\n}\ngenerate_query_or_respond(input)[\"messages\"][-1].pretty_print()\n</code></pre> <p>Output: <pre><code>================================== Ai Message ==================================\nTool Calls:\nretrieve_blog_posts (call_tYQxgfIlnQUDMdtAhdbXNwIM)\nCall ID: call_tYQxgfIlnQUDMdtAhdbXNwIM\nArgs:\n    query: types of reward hacking\n</code></pre></p> </li> </ol>"}, {"location": "tutorials/rag/langgraph_agentic_rag/#4-grade-documents", "title": "4. Grade documents", "text": "<ol> <li> <p>Add a conditional edge \u2014 <code>grade_documents</code> \u2014 to determine whether the retrieved documents are relevant to the question. We will use a model with a structured output schema <code>GradeDocuments</code> for document grading. The <code>grade_documents</code> function will return the name of the node to go to based on the grading decision (<code>generate_answer</code> or <code>rewrite_question</code>):</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Literal\n\nGRADE_PROMPT = (\n    \"You are a grader assessing relevance of a retrieved document to a user question. \\n \"\n    \"Here is the retrieved document: \\n\\n {context} \\n\\n\"\n    \"Here is the user question: {question} \\n\"\n    \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\"\n    \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n)\n\n\n# highlight-next-line\nclass GradeDocuments(BaseModel):\n    \"\"\"Grade documents using a binary score for relevance check.\"\"\"\n\n    binary_score: str = Field(\n        description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\"\n    )\n\n\ngrader_model = init_chat_model(\"openai:gpt-4.1\", temperature=0)\n\n\ndef grade_documents(\n    state: MessagesState,\n) -&gt; Literal[\"generate_answer\", \"rewrite_question\"]:\n    \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\"\n    question = state[\"messages\"][0].content\n    context = state[\"messages\"][-1].content\n\n    prompt = GRADE_PROMPT.format(question=question, context=context)\n    response = (\n        grader_model\n        # highlight-next-line\n        .with_structured_output(GradeDocuments).invoke(\n            [{\"role\": \"user\", \"content\": prompt}]\n        )\n    )\n    score = response.binary_score\n\n    if score == \"yes\":\n        return \"generate_answer\"\n    else:\n        return \"rewrite_question\"\n</code></pre> </li> <li> <p>Run this with irrelevant documents in the tool response:</p> <pre><code>from langchain_core.messages import convert_to_messages\n\ninput = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_blog_posts\",\n                        \"args\": {\"query\": \"types of reward hacking\"},\n                    }\n                ],\n            },\n            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n        ]\n    )\n}\ngrade_documents(input)\n</code></pre> </li> <li> <p>Confirm that the relevant documents are classified as such:</p> <pre><code>input = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_blog_posts\",\n                        \"args\": {\"query\": \"types of reward hacking\"},\n                    }\n                ],\n            },\n            {\n                \"role\": \"tool\",\n                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n                \"tool_call_id\": \"1\",\n            },\n        ]\n    )\n}\ngrade_documents(input)\n</code></pre> </li> </ol>"}, {"location": "tutorials/rag/langgraph_agentic_rag/#5-rewrite-question", "title": "5. Rewrite question", "text": "<ol> <li> <p>Build the <code>rewrite_question</code> node. The retriever tool can return potentially irrelevant documents, which indicates a need to improve the original user question. To do so, we will call the <code>rewrite_question</code> node:</p> <pre><code>REWRITE_PROMPT = (\n    \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\"\n    \"Here is the initial question:\"\n    \"\\n ------- \\n\"\n    \"{question}\"\n    \"\\n ------- \\n\"\n    \"Formulate an improved question:\"\n)\n\n\ndef rewrite_question(state: MessagesState):\n    \"\"\"Rewrite the original user question.\"\"\"\n    messages = state[\"messages\"]\n    question = messages[0].content\n    prompt = REWRITE_PROMPT.format(question=question)\n    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n    return {\"messages\": [{\"role\": \"user\", \"content\": response.content}]}\n</code></pre> </li> <li> <p>Try it out:</p> <pre><code>input = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_blog_posts\",\n                        \"args\": {\"query\": \"types of reward hacking\"},\n                    }\n                ],\n            },\n            {\"role\": \"tool\", \"content\": \"meow\", \"tool_call_id\": \"1\"},\n        ]\n    )\n}\n\nresponse = rewrite_question(input)\nprint(response[\"messages\"][-1][\"content\"])\n</code></pre> <p>Output: <pre><code>What are the different types of reward hacking described by Lilian Weng, and how does she explain them?\n</code></pre></p> </li> </ol>"}, {"location": "tutorials/rag/langgraph_agentic_rag/#6-generate-an-answer", "title": "6. Generate an answer", "text": "<ol> <li> <p>Build <code>generate_answer</code> node: if we pass the grader checks, we can generate the final answer based on the original question and the retrieved context:</p> <pre><code>GENERATE_PROMPT = (\n    \"You are an assistant for question-answering tasks. \"\n    \"Use the following pieces of retrieved context to answer the question. \"\n    \"If you don't know the answer, just say that you don't know. \"\n    \"Use three sentences maximum and keep the answer concise.\\n\"\n    \"Question: {question} \\n\"\n    \"Context: {context}\"\n)\n\n\ndef generate_answer(state: MessagesState):\n    \"\"\"Generate an answer.\"\"\"\n    question = state[\"messages\"][0].content\n    context = state[\"messages\"][-1].content\n    prompt = GENERATE_PROMPT.format(question=question, context=context)\n    response = response_model.invoke([{\"role\": \"user\", \"content\": prompt}])\n    return {\"messages\": [response]}\n</code></pre> </li> <li> <p>Try it:</p> <pre><code>input = {\n    \"messages\": convert_to_messages(\n        [\n            {\n                \"role\": \"user\",\n                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n            },\n            {\n                \"role\": \"assistant\",\n                \"content\": \"\",\n                \"tool_calls\": [\n                    {\n                        \"id\": \"1\",\n                        \"name\": \"retrieve_blog_posts\",\n                        \"args\": {\"query\": \"types of reward hacking\"},\n                    }\n                ],\n            },\n            {\n                \"role\": \"tool\",\n                \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\",\n                \"tool_call_id\": \"1\",\n            },\n        ]\n    )\n}\n\nresponse = generate_answer(input)\nresponse[\"messages\"][-1].pretty_print()\n</code></pre> <p>Output: <pre><code>================================== Ai Message ==================================\n\nLilian Weng categorizes reward hacking into two types: environment or goal misspecification, and reward tampering. She considers reward hacking as a broad concept that includes both of these categories. Reward hacking occurs when an agent exploits flaws or ambiguities in the reward function to achieve high rewards without performing the intended behaviors.\n</code></pre></p> </li> </ol>"}, {"location": "tutorials/rag/langgraph_agentic_rag/#7-assemble-the-graph", "title": "7. Assemble the graph", "text": "<ul> <li>Start with a <code>generate_query_or_respond</code> and determine if we need to call <code>retriever_tool</code></li> <li>Route to next step using <code>tools_condition</code>:<ul> <li>If <code>generate_query_or_respond</code> returned <code>tool_calls</code>, call <code>retriever_tool</code> to retrieve context </li> <li>Otherwise, respond directly to the user</li> </ul> </li> <li>Grade retrieved document content for relevance to the question (<code>grade_documents</code>) and route to next step:<ul> <li>If not relevant, rewrite the question using <code>rewrite_question</code> and then call <code>generate_query_or_respond</code> again</li> <li>If relevant, proceed to <code>generate_answer</code> and generate final response using the <code>ToolMessage</code> with the retrieved document context</li> </ul> </li> </ul> <pre><code>from langgraph.graph import StateGraph, START, END\nfrom langgraph.prebuilt import ToolNode\nfrom langgraph.prebuilt import tools_condition\n\nworkflow = StateGraph(MessagesState)\n\n# Define the nodes we will cycle between\nworkflow.add_node(generate_query_or_respond)\nworkflow.add_node(\"retrieve\", ToolNode([retriever_tool]))\nworkflow.add_node(rewrite_question)\nworkflow.add_node(generate_answer)\n\nworkflow.add_edge(START, \"generate_query_or_respond\")\n\n# Decide whether to retrieve\nworkflow.add_conditional_edges(\n    \"generate_query_or_respond\",\n    # Assess LLM decision (call `retriever_tool` tool or respond to the user)\n    tools_condition,\n    {\n        # Translate the condition outputs to nodes in our graph\n        \"tools\": \"retrieve\",\n        END: END,\n    },\n)\n\n# Edges taken after the `action` node is called.\nworkflow.add_conditional_edges(\n    \"retrieve\",\n    # Assess agent decision\n    grade_documents,\n)\nworkflow.add_edge(\"generate_answer\", END)\nworkflow.add_edge(\"rewrite_question\", \"generate_query_or_respond\")\n\n# Compile\ngraph = workflow.compile()\n</code></pre> <p>Visualize the graph:</p> <pre><code>from IPython.display import Image, display\n\ndisplay(Image(graph.get_graph().draw_mermaid_png()))\n</code></pre> <p></p>"}, {"location": "tutorials/rag/langgraph_agentic_rag/#8-run-the-agentic-rag", "title": "8. Run the agentic RAG", "text": "<pre><code>for chunk in graph.stream(\n    {\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": \"What does Lilian Weng say about types of reward hacking?\",\n            }\n        ]\n    }\n):\n    for node, update in chunk.items():\n        print(\"Update from node\", node)\n        update[\"messages\"][-1].pretty_print()\n        print(\"\\n\\n\")\n</code></pre> <p>Output: <pre><code>Update from node generate_query_or_respond\n================================== Ai Message ==================================\nTool Calls:\n  retrieve_blog_posts (call_NYu2vq4km9nNNEFqJwefWKu1)\n Call ID: call_NYu2vq4km9nNNEFqJwefWKu1\n  Args:\n    query: types of reward hacking\n\n\n\nUpdate from node retrieve\n================================= Tool Message ==================================\nName: retrieve_blog_posts\n\n(Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.)\nAt a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering.\n\nWhy does Reward Hacking Exist?#\n\nPan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size, (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy of three types of misspecified proxy rewards:\n\nLet's Define Reward Hacking#\nReward shaping in RL is challenging. Reward hacking occurs when an RL agent exploits flaws or ambiguities in the reward function to obtain high rewards without genuinely learning the intended behaviors or completing the task as designed. In recent years, several related concepts have been proposed, all referring to some form of reward hacking:\n\n\n\nUpdate from node generate_answer\n================================== Ai Message ==================================\n\nLilian Weng categorizes reward hacking into two types: environment or goal misspecification, and reward tampering. She considers reward hacking as a broad concept that includes both of these categories. Reward hacking occurs when an agent exploits flaws or ambiguities in the reward function to achieve high rewards without performing the intended behaviors.\n</code></pre></p>"}, {"location": "tutorials/sql/sql-agent/", "title": "Build a SQL agent", "text": "<p>In this tutorial, we will walk through how to build an agent that can answer questions about a SQL database.</p> <p>At a high level, the agent will:</p> <ol> <li>Fetch the available tables from the database</li> <li>Decide which tables are relevant to the question</li> <li>Fetch the schemas for the relevant tables</li> <li>Generate a query based on the question and information from the schemas</li> <li>Double-check the query for common mistakes using an LLM</li> <li>Execute the query and return the results</li> <li>Correct mistakes surfaced by the database engine until the query is successful</li> <li>Formulate a response based on the results</li> </ol> <p>Security note</p> <p>Building Q&amp;A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent's needs. This will mitigate though not eliminate the risks of building a model-driven system.</p>"}, {"location": "tutorials/sql/sql-agent/#1-setup", "title": "1. Setup", "text": "<p>Let's first install some dependencies. This tutorial uses SQL database and tool abstractions from langchain-community. We will also require a LangChain chat model.</p> <pre><code>%%capture --no-stderr\n%pip install -U langgraph langchain_community \"langchain[openai]\"\n</code></pre> <p>Tip</p> <p>Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.</p>"}, {"location": "tutorials/sql/sql-agent/#select-a-llm", "title": "Select a LLM", "text": "<p>First we initialize our LLM. Any model supporting tool-calling should work. We use OpenAI below.</p> <pre><code>from langchain.chat_models import init_chat_model\n\nllm = init_chat_model(\"openai:gpt-4.1\")\n</code></pre>"}, {"location": "tutorials/sql/sql-agent/#configure-the-database", "title": "Configure the database", "text": "<p>We will be creating a SQLite database for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the <code>chinook</code> database, which is a sample database that represents a digital media store. Find more information about the database here.</p> <p>For convenience, we have hosted the database (<code>Chinook.db</code>) on a public GCS bucket.</p> <pre><code>import requests\n\nurl = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\"\n\nresponse = requests.get(url)\n\nif response.status_code == 200:\n    # Open a local file in binary write mode\n    with open(\"Chinook.db\", \"wb\") as file:\n        # Write the content of the response (the file) to the local file\n        file.write(response.content)\n    print(\"File downloaded and saved as Chinook.db\")\nelse:\n    print(f\"Failed to download the file. Status code: {response.status_code}\")\n</code></pre> <p>We will use a handy SQL database wrapper available in the <code>langchain_community</code> package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results:</p> <pre><code>from langchain_community.utilities import SQLDatabase\n\ndb = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")\n\nprint(f\"Dialect: {db.dialect}\")\nprint(f\"Available tables: {db.get_usable_table_names()}\")\nprint(f'Sample output: {db.run(\"SELECT * FROM Artist LIMIT 5;\")}')\n</code></pre> <p>Output: <pre><code>Dialect: sqlite\nAvailable tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track']\nSample output: [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')]\n</code></pre></p>"}, {"location": "tutorials/sql/sql-agent/#tools-for-database-interactions", "title": "Tools for database interactions", "text": "<p><code>langchain-community</code> implements some built-in tools for interacting with our <code>SQLDatabase</code>, including tools for listing tables, reading table schemas, and checking and running queries:</p> <pre><code>from langchain_community.agent_toolkits import SQLDatabaseToolkit\n\ntoolkit = SQLDatabaseToolkit(db=db, llm=llm)\n\ntools = toolkit.get_tools()\n\nfor tool in tools:\n    print(f\"{tool.name}: {tool.description}\\n\")\n</code></pre> <p>Output: <pre><code>sql_db_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\n\nsql_db_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3\n\nsql_db_list_tables: Input is an empty string, output is a comma-separated list of tables in the database.\n\nsql_db_query_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!\n</code></pre></p>"}, {"location": "tutorials/sql/sql-agent/#2-using-a-prebuilt-agent", "title": "2. Using a prebuilt agent", "text": "<p>Given these tools, we can initialize a pre-built agent in a single line. To customize our agents behavior, we write a descriptive system prompt.</p> <pre><code>from langgraph.prebuilt import create_react_agent\n\nsystem_prompt = \"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most {top_k} results.\n\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\n\nYou MUST double check your query before executing it. If you get an error while\nexecuting a query, rewrite the query and try again.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the\ndatabase.\n\nTo start you should ALWAYS look at the tables in the database to see what you\ncan query. Do NOT skip this step.\n\nThen you should query the schema of the most relevant tables.\n\"\"\".format(\n    dialect=db.dialect,\n    top_k=5,\n)\n\nagent = create_react_agent(\n    llm,\n    tools,\n    prompt=system_prompt,\n)\n</code></pre> <p>Note</p> <p>This system prompt includes a number of instructions, such as always running specific tools before or after others. In the next section, we will enforce these behaviors through the graph's structure, providing us a greater degree of control and allowing us to simplify the prompt.</p> <p>Let's run this agent on a sample query and observe its behavior:</p> <pre><code>question = \"Which genre on average has the longest tracks?\"\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n</code></pre> <p>Output: <pre><code>================================ Human Message =================================\n\nWhich genre on average has the longest tracks?\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_list_tables (call_d8lCgywSroCgpVl558nmXKwA)\n Call ID: call_d8lCgywSroCgpVl558nmXKwA\n  Args:\n================================= Tool Message =================================\nName: sql_db_list_tables\n\nAlbum, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_schema (call_nNf6IIUcwMYLIkE0l6uWkZHe)\n Call ID: call_nNf6IIUcwMYLIkE0l6uWkZHe\n  Args:\n    table_names: Genre, Track\n================================= Tool Message =================================\nName: sql_db_schema\n\n\nCREATE TABLE \"Genre\" (\n    \"GenreId\" INTEGER NOT NULL, \n    \"Name\" NVARCHAR(120), \n    PRIMARY KEY (\"GenreId\")\n)\n\n/*\n3 rows from Genre table:\nGenreId Name\n1   Rock\n2   Jazz\n3   Metal\n*/\n\n\nCREATE TABLE \"Track\" (\n    \"TrackId\" INTEGER NOT NULL, \n    \"Name\" NVARCHAR(200) NOT NULL, \n    \"AlbumId\" INTEGER, \n    \"MediaTypeId\" INTEGER NOT NULL, \n    \"GenreId\" INTEGER, \n    \"Composer\" NVARCHAR(220), \n    \"Milliseconds\" INTEGER NOT NULL, \n    \"Bytes\" INTEGER, \n    \"UnitPrice\" NUMERIC(10, 2) NOT NULL, \n    PRIMARY KEY (\"TrackId\"), \n    FOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), \n    FOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), \n    FOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n)\n\n/*\n3 rows from Track table:\nTrackId Name    AlbumId MediaTypeId GenreId Composer    Milliseconds    Bytes   UnitPrice\n1   For Those About To Rock (We Salute You) 1   1   1   Angus Young, Malcolm Young, Brian Johnson   343719  11170334    0.99\n2   Balls to the Wall   2   2   1   None    342562  5510424 0.99\n3   Fast As a Shark 3   2   1   F. Baltes, S. Kaufman, U. Dirkscneider &amp; W. Hoffman 230619  3990994 0.99\n*/\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query_checker (call_urTRmtiGtTxkwHtscec7Fd2K)\n Call ID: call_urTRmtiGtTxkwHtscec7Fd2K\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds\nFROM Track\nJOIN Genre ON Track.GenreId = Genre.GenreId\nGROUP BY Genre.Name\nORDER BY AvgMilliseconds DESC\nLIMIT 1;\n================================= Tool Message =================================\nName: sql_db_query_checker\n\n\\`\\`\\`sql\nSELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds\nFROM Track\nJOIN Genre ON Track.GenreId = Genre.GenreId\nGROUP BY Genre.Name\nORDER BY AvgMilliseconds DESC\nLIMIT 1;\n\\`\\`\\`\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_RNMqyUEMv0rvy0UxSwrXY2AV)\n Call ID: call_RNMqyUEMv0rvy0UxSwrXY2AV\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds\nFROM Track\nJOIN Genre ON Track.GenreId = Genre.GenreId\nGROUP BY Genre.Name\nORDER BY AvgMilliseconds DESC\nLIMIT 1;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi &amp; Fantasy', 2911783.0384615385)]\n================================== Ai Message ==================================\n\nThe genre with the longest average track length is \"Sci Fi &amp; Fantasy,\" with an average duration of about 2,911,783 milliseconds (approximately 48.5 minutes) per track.\n</code></pre></p> <p>This worked well enough: the agent correctly listed the tables, obtained the schemas, wrote a query, checked the query, and ran it to inform its final response.</p> <p>Tip</p> <p>You can inspect all aspects of the above run, including steps taken, tools invoked, what prompts were seen by the LLM, and more in the LangSmith trace.</p>"}, {"location": "tutorials/sql/sql-agent/#3-customizing-the-agent", "title": "3. Customizing the agent", "text": "<p>The prebuilt agent lets us get started quickly, but at each step the agent has access to the full set of tools. Above, we relied on the system prompt to constrain its behavior\u2014 for example, we instructed the agent to always start with the \"list tables\" tool, and to always run a query-checker tool before executing the query.</p> <p>We can enforce a higher degree of control in LangGraph by customizing the agent. Below, we implement a simple ReAct-agent setup, with dedicated nodes for specific tool-calls. We will use the same state as the pre-built agent.</p> <p>We construct dedicated nodes for the following steps:</p> <ul> <li>Listing DB tables</li> <li>Calling the \"get schema\" tool</li> <li>Generating a query</li> <li>Checking the query</li> </ul> <p>Putting these steps in dedicated nodes lets us (1) force tool-calls when needed, and (2) customize the prompts associated with each step.</p> <pre><code>from typing import Literal\nfrom langchain_core.messages import AIMessage\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import END, START, MessagesState, StateGraph\nfrom langgraph.prebuilt import ToolNode\n\n\nget_schema_tool = next(tool for tool in tools if tool.name == \"sql_db_schema\")\nget_schema_node = ToolNode([get_schema_tool], name=\"get_schema\")\n\nrun_query_tool = next(tool for tool in tools if tool.name == \"sql_db_query\")\nrun_query_node = ToolNode([run_query_tool], name=\"run_query\")\n\n\n# Example: create a predetermined tool call\ndef list_tables(state: MessagesState):\n    tool_call = {\n        \"name\": \"sql_db_list_tables\",\n        \"args\": {},\n        \"id\": \"abc123\",\n        \"type\": \"tool_call\",\n    }\n    tool_call_message = AIMessage(content=\"\", tool_calls=[tool_call])\n\n    list_tables_tool = next(tool for tool in tools if tool.name == \"sql_db_list_tables\")\n    tool_message = list_tables_tool.invoke(tool_call)\n    response = AIMessage(f\"Available tables: {tool_message.content}\")\n\n    return {\"messages\": [tool_call_message, tool_message, response]}\n\n\n# Example: force a model to create a tool call\ndef call_get_schema(state: MessagesState):\n    # Note that LangChain enforces that all models accept `tool_choice=\"any\"`\n    # as well as `tool_choice=&lt;string name of tool&gt;`.\n    llm_with_tools = llm.bind_tools([get_schema_tool], tool_choice=\"any\")\n    response = llm_with_tools.invoke(state[\"messages\"])\n\n    return {\"messages\": [response]}\n\n\ngenerate_query_system_prompt = \"\"\"\nYou are an agent designed to interact with a SQL database.\nGiven an input question, create a syntactically correct {dialect} query to run,\nthen look at the results of the query and return the answer. Unless the user\nspecifies a specific number of examples they wish to obtain, always limit your\nquery to at most {top_k} results.\n\nYou can order the results by a relevant column to return the most interesting\nexamples in the database. Never query for all the columns from a specific table,\nonly ask for the relevant columns given the question.\n\nDO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n\"\"\".format(\n    dialect=db.dialect,\n    top_k=5,\n)\n\n\ndef generate_query(state: MessagesState):\n    system_message = {\n        \"role\": \"system\",\n        \"content\": generate_query_system_prompt,\n    }\n    # We do not force a tool call here, to allow the model to\n    # respond naturally when it obtains the solution.\n    llm_with_tools = llm.bind_tools([run_query_tool])\n    response = llm_with_tools.invoke([system_message] + state[\"messages\"])\n\n    return {\"messages\": [response]}\n\n\ncheck_query_system_prompt = \"\"\"\nYou are a SQL expert with a strong attention to detail.\nDouble check the {dialect} query for common mistakes, including:\n- Using NOT IN with NULL values\n- Using UNION when UNION ALL should have been used\n- Using BETWEEN for exclusive ranges\n- Data type mismatch in predicates\n- Properly quoting identifiers\n- Using the correct number of arguments for functions\n- Casting to the correct data type\n- Using the proper columns for joins\n\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes,\njust reproduce the original query.\n\nYou will call the appropriate tool to execute the query after running this check.\n\"\"\".format(dialect=db.dialect)\n\n\ndef check_query(state: MessagesState):\n    system_message = {\n        \"role\": \"system\",\n        \"content\": check_query_system_prompt,\n    }\n\n    # Generate an artificial user message to check\n    tool_call = state[\"messages\"][-1].tool_calls[0]\n    user_message = {\"role\": \"user\", \"content\": tool_call[\"args\"][\"query\"]}\n    llm_with_tools = llm.bind_tools([run_query_tool], tool_choice=\"any\")\n    response = llm_with_tools.invoke([system_message, user_message])\n    response.id = state[\"messages\"][-1].id\n\n    return {\"messages\": [response]}\n</code></pre> <p>Finally, we assemble these steps into a workflow using the Graph API. We define a conditional edge at the query generation step that will route to the query checker if a query is generated, or end if there are no tool calls present, such that the LLM has delivered a response to the query.</p> <pre><code>def should_continue(state: MessagesState) -&gt; Literal[END, \"check_query\"]:\n    messages = state[\"messages\"]\n    last_message = messages[-1]\n    if not last_message.tool_calls:\n        return END\n    else:\n        return \"check_query\"\n\n\nbuilder = StateGraph(MessagesState)\nbuilder.add_node(list_tables)\nbuilder.add_node(call_get_schema)\nbuilder.add_node(get_schema_node, \"get_schema\")\nbuilder.add_node(generate_query)\nbuilder.add_node(check_query)\nbuilder.add_node(run_query_node, \"run_query\")\n\nbuilder.add_edge(START, \"list_tables\")\nbuilder.add_edge(\"list_tables\", \"call_get_schema\")\nbuilder.add_edge(\"call_get_schema\", \"get_schema\")\nbuilder.add_edge(\"get_schema\", \"generate_query\")\nbuilder.add_conditional_edges(\n    \"generate_query\",\n    should_continue,\n)\nbuilder.add_edge(\"check_query\", \"run_query\")\nbuilder.add_edge(\"run_query\", \"generate_query\")\n\nagent = builder.compile()\n</code></pre> <p>We visualize the application below:</p> <pre><code>from IPython.display import Image, display\nfrom langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n\ndisplay(Image(agent.get_graph().draw_mermaid_png()))\n</code></pre> <p></p> <p>Note: When you run this code, it will generate and display a visual representation of the SQL agent graph showing the flow between the different nodes (list_tables \u2192 call_get_schema \u2192 get_schema \u2192 generate_query \u2192 check_query \u2192 run_query).</p> <p>We can now invoke the graph exactly as before:</p> <pre><code>question = \"Which genre on average has the longest tracks?\"\n\nfor step in agent.stream(\n    {\"messages\": [{\"role\": \"user\", \"content\": question}]},\n    stream_mode=\"values\",\n):\n    step[\"messages\"][-1].pretty_print()\n</code></pre> <p>Output: <pre><code>================================ Human Message =================================\n\nWhich genre on average has the longest tracks?\n================================== Ai Message ==================================\n\nAvailable tables: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_schema (call_qxKtYiHgf93AiTDin9ez5wFp)\n Call ID: call_qxKtYiHgf93AiTDin9ez5wFp\n  Args:\n    table_names: Genre,Track\n================================= Tool Message =================================\nName: sql_db_schema\n\n\nCREATE TABLE \"Genre\" (\n    \"GenreId\" INTEGER NOT NULL, \n    \"Name\" NVARCHAR(120), \n    PRIMARY KEY (\"GenreId\")\n)\n\n/*\n3 rows from Genre table:\nGenreId Name\n1   Rock\n2   Jazz\n3   Metal\n*/\n\n\nCREATE TABLE \"Track\" (\n    \"TrackId\" INTEGER NOT NULL, \n    \"Name\" NVARCHAR(200) NOT NULL, \n    \"AlbumId\" INTEGER, \n    \"MediaTypeId\" INTEGER NOT NULL, \n    \"GenreId\" INTEGER, \n    \"Composer\" NVARCHAR(220), \n    \"Milliseconds\" INTEGER NOT NULL, \n    \"Bytes\" INTEGER, \n    \"UnitPrice\" NUMERIC(10, 2) NOT NULL, \n    PRIMARY KEY (\"TrackId\"), \n    FOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), \n    FOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), \n    FOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\")\n)\n\n/*\n3 rows from Track table:\nTrackId Name    AlbumId MediaTypeId GenreId Composer    Milliseconds    Bytes   UnitPrice\n1   For Those About To Rock (We Salute You) 1   1   1   Angus Young, Malcolm Young, Brian Johnson   343719  11170334    0.99\n2   Balls to the Wall   2   2   1   None    342562  5510424 0.99\n3   Fast As a Shark 3   2   1   F. Baltes, S. Kaufman, U. Dirkscneider &amp; W. Hoffman 230619  3990994 0.99\n*/\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_RPN3GABMfb6DTaFTLlwnZxVN)\n Call ID: call_RPN3GABMfb6DTaFTLlwnZxVN\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgTrackLength\nFROM Track\nJOIN Genre ON Track.GenreId = Genre.GenreId\nGROUP BY Genre.GenreId\nORDER BY AvgTrackLength DESC\nLIMIT 1;\n================================== Ai Message ==================================\nTool Calls:\n  sql_db_query (call_PR4s8ymiF3ZQLaoZADXtdqcl)\n Call ID: call_PR4s8ymiF3ZQLaoZADXtdqcl\n  Args:\n    query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgTrackLength\nFROM Track\nJOIN Genre ON Track.GenreId = Genre.GenreId\nGROUP BY Genre.GenreId\nORDER BY AvgTrackLength DESC\nLIMIT 1;\n================================= Tool Message =================================\nName: sql_db_query\n\n[('Sci Fi &amp; Fantasy', 2911783.0384615385)]\n================================== Ai Message ==================================\n\nThe genre with the longest tracks on average is \"Sci Fi &amp; Fantasy,\" with an average track length of approximately 2,911,783 milliseconds.\n</code></pre></p> <p>Tip</p> <p>See LangSmith trace for the above run.</p>"}, {"location": "tutorials/sql/sql-agent/#next-steps", "title": "Next steps", "text": "<p>Check out this guide for evaluating LangGraph applications, including SQL agents like this one, using LangSmith. </p>"}]}